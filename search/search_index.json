{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Home", "text": "<p>Karafka is a Ruby and Rails multi-threaded efficient Kafka processing framework that:</p> <ul> <li>Has a built-in Web UI providing a convenient way to monitor and manage Karafka-based applications.</li> <li>Supports parallel processing in multiple threads (also for a single topic partition work) and processes.</li> <li>Automatically integrates with Ruby on Rails</li> <li>Has ActiveJob backend support (including ordered jobs)</li> <li>Has a seamless Dead Letter Queue functionality built-in</li> <li>Supports in-development code reloading</li> <li>Is powered by librdkafka (the Apache Kafka C/C++ client library)</li> <li>Has an out-of the box AppSignal and StatsD/DataDog monitoring with dashboard templates.</li> </ul>"}, {"location": "#basics", "title": "Basics", "text": "<ul> <li>Getting Started</li> <li>Components</li> <li>Configuration</li> <li>Producing Messages</li> <li>Consuming Messages</li> <li>Web UI</li> <li>Testing</li> <li>FAQ</li> <li>Support</li> </ul>"}, {"location": "#web-ui", "title": "Web UI", "text": "<ul> <li>About</li> <li>Getting Started</li> <li>Configuration</li> <li>Transactions</li> <li>Features<ul> <li>Consumers</li> <li>Jobs</li> <li>Health</li> <li>Routing</li> <li>Explorer</li> <li>Errors</li> <li>DLQ / Dead</li> <li>Cluster</li> <li>Status</li> </ul> </li> <li>Tagging</li> <li>Multi App Mode</li> <li>Single Process Setup</li> <li>Development vs Production</li> <li>Data Management</li> <li>Operational Cost Breakdown</li> <li>Components</li> </ul>"}, {"location": "#waterdrop", "title": "WaterDrop", "text": "<ul> <li>About</li> <li>Getting Started</li> <li>Configuration</li> <li>Usage</li> <li>Error Handling</li> <li>Monitoring and Logging</li> <li>Transactions</li> <li>Testing</li> <li>Middleware</li> <li>Labeling</li> <li>Variants</li> <li>Custom Partitioners</li> <li>Idempotence and Acknowledgements</li> </ul>"}, {"location": "#production-usage", "title": "Production Usage", "text": "<ul> <li>Development vs Production</li> <li>Signals and States</li> <li>Deployment<ul> <li>systemd (+ Capistrano)</li> <li>Docker</li> <li>AWS + MSK (Fully Managed Apache Kafka)</li> <li>Heroku</li> <li>Kubernetes</li> <li>Confluent Cloud</li> <li>Custom OAuth Token Providers</li> </ul> </li> <li>Monitoring and Logging</li> <li>Error Handling and back off policy</li> </ul>"}, {"location": "#advanced", "title": "Advanced", "text": "<ul> <li>Upgrading</li> <li>Routing</li> <li>Active Job</li> <li>Dead Letter Queue</li> <li>Declarative Topics</li> <li>Admin API</li> <li>ACLs API</li> <li>Configs API</li> <li>Auto reload of code changes in development</li> <li>CLI</li> <li>Integrating with Ruby on Rails and other frameworks</li> <li>Concurrency and multithreading</li> <li>Deserialization</li> <li>Offset management (checkpointing)</li> <li>Pausing, Seeking and Rate-Limiting</li> <li>Inline Insights</li> <li>WaterDrop reconfiguration</li> <li>Exit codes</li> <li>Embedding</li> <li>Swarm / Multi Process</li> <li>Multi-Cluster Setup</li> <li>Env Variables</li> <li>Assignments Tracking</li> <li>Active Record Connections Management</li> <li>Forking</li> <li>Resources Management</li> <li>Latency and Throughput</li> <li>Articles and other references</li> <li>Versions Lifecycle and EOL</li> <li>Problems and Troubleshooting</li> <li>Debugging</li> <li>Software Bill of Materials (SBOM)</li> </ul>"}, {"location": "#karafka-pro", "title": "Karafka Pro", "text": "<ul> <li>Build vs. Buy</li> <li>Purchase Karafka Pro</li> <li>Getting Started</li> <li>Rotating credentials</li> <li>Pro FAQ</li> <li>Pro Support</li> <li>Security</li> <li>HIPAA, PHI, PII Support</li> <li>FIPS Support</li> <li>Enterprise</li> <li>Enterprise Workshop Session</li> <li>Enterprise License Setup</li> </ul>"}, {"location": "#features-and-enhancements", "title": "Features and Enhancements", "text": "<ul> <li>Features List</li> <li>Features Compatibility</li> <li>Transactions</li> <li>Offset Metadata Storage</li> <li>Virtual Partitions</li> <li>Parallel Segments</li> <li>Delayed Topics</li> <li>Long-Running Jobs</li> <li>Non-Blocking Jobs</li> <li>Adaptive Iterator</li> <li>Periodic Jobs</li> <li>Expiring Messages</li> <li>Routing Patterns</li> <li>Rate Limiting</li> <li>Filtering API</li> <li>Scheduling API</li> <li>Iterator API</li> <li>Cleaner API</li> <li>Granular Backoffs</li> <li>Direct Assignments</li> <li>Multiplexing</li> <li>Piping</li> <li>Recurring Tasks</li> <li>Scheduled Messages</li> <li>Messages At Rest Encryption</li> <li>Enhanced Swarm / Multi Process</li> <li>Enhanced Dead Letter Queue</li> <li>Enhanced Active Job</li> <li>Enhanced Reliability</li> <li>Enhanced Inline Insights</li> <li>Enhanced Web UI<ul> <li>Getting Started</li> <li>Consumers</li> <li>Commanding</li> <li>Health</li> <li>Explorer</li> <li>Policies</li> <li>Search</li> <li>Recurring Tasks</li> <li>Scheduled Messages</li> <li>Topics Insights</li> <li>Errors</li> <li>DLQ / Dead</li> <li>Branding</li> <li>Custom Styling</li> <li>Topics Management</li> </ul> </li> </ul>"}, {"location": "#librdkafka", "title": "Librdkafka", "text": "<ul> <li>Configuration</li> <li>Statistics</li> <li>Errors</li> <li>Changelog</li> </ul>"}, {"location": "#kafka", "title": "Kafka", "text": "<ul> <li>Setting Up</li> <li>Topic Configuration</li> <li>Cluster Configuration</li> </ul>"}, {"location": "#upgrade-notes", "title": "Upgrade Notes", "text": "<p>It is recommended to do one major upgrade at a time.</p> <ul> <li>Karafka</li> <li>Web UI</li> <li>WaterDrop</li> </ul>"}, {"location": "#changelogs", "title": "Changelogs", "text": "<ul> <li>Karafka</li> <li>WaterDrop</li> <li>Karafka-Web</li> <li>Karafka-Testing</li> <li>Karafka-Core</li> <li>Karafka-Rdkafka</li> <li>Rdkafka</li> <li>Librdkafka</li> </ul>"}, {"location": "#code-docs", "title": "Code Docs", "text": "<ul> <li>Karafka</li> <li>WaterDrop</li> <li>Karafka-Web</li> <li>Karafka-Testing</li> <li>Karafka-Core</li> <li>Karafka-Rdkafka</li> <li>Rdkafka</li> </ul>"}, {"location": "#development", "title": "Development", "text": "<ul> <li>Gems Publishing</li> <li>Precompilation</li> <li>Native Extensions</li> <li>Naming Conventions</li> </ul>"}, {"location": "Active-Job/", "title": "Active Job", "text": "<p>Active Job is a standard interface for interacting with job runners in Ruby on Rails. Active Job can be configured to work with Karafka.</p>"}, {"location": "Active-Job/#active-job-setup", "title": "Active Job Setup", "text": "<p>The Active Job adapter must be set to <code>:karafka</code> or it will use the default value provided by Rails, which is <code>:async</code>. This can be done in the <code>config/application.rb</code>:</p> <pre><code>class Application &lt; Rails::Application\n  # ...\n  config.active_job.queue_adapter = :karafka\nend\n</code></pre> <p>We can use the generator to create a new job:</p> <pre><code>rails generate job Example\n</code></pre> <p>The above command will create <code>app/jobs/example_job.rb</code>:</p> <pre><code>class ExampleJob &lt; ActiveJob::Base\n  # Set the topic name to default\n  queue_as :default\n\n  def perform(*args)\n    # Perform Job\n  end\nend\n</code></pre> <p>For Karafka server to understand which of the topics contain Active Job data, you need to indicate this in your <code>karafka.rb</code> routing section using the <code>#active_job_topic</code>:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    active_job_topic :default\n  end\nend\n</code></pre> <p><code>#active_job_topic</code> similar to <code>#topic</code> accepts block for additional configuration:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    active_job_topic :default do\n      # Available only in Pro\n      long_running_job true\n    end\n  end\nend\n</code></pre> <p>Pro Enhanced ActiveJob adapter supports <code>Long-Running Jobs</code>, <code>Virtual Partitions</code>, <code>Ordered Jobs</code>, <code>Scheduled Jobs</code>, and other Pro features.</p>"}, {"location": "Active-Job/#usage", "title": "Usage", "text": "<p>Jobs can be added to the job queue from anywhere. You can add a job to the queue by:</p> <pre><code>ExampleJob.perform_later args\n</code></pre> <p>At this point, Karafka will run the job for us. If the job fails, Karafka will retry the job as normal.</p>"}, {"location": "Active-Job/#enqueuing-modes", "title": "Enqueuing Modes", "text": ""}, {"location": "Active-Job/#perform_later", "title": "<code>#perform_later</code>", "text": "<p>When you enqueue a job using <code>#perform_later</code>, Karafka, by default, will produce a message to Kafka in an async fashion. A job will be added to a background process queue and dispatched without blocking the processing flow.</p> <p>You may want to alter this behavior for critical jobs and use synchronous enqueuing. To use it, just call the <code>karafka_options</code> method within your job class definition and set the <code>dispatch_method</code> to <code>:produce_sync</code> as followed:</p> <pre><code>class Job &lt; ActiveJob::Base\n  queue_as :my_kafka_jobs\n\n  karafka_options(\n    dispatch_method: :produce_sync\n  )\n\n  def perform(value1, value2)\n    puts \"value1: #{value1}\"\n    puts \"value2: #{value2}\"\n  end\nend\n\nJob.perform_later(1, 2)\n</code></pre>"}, {"location": "Active-Job/#perform_all_later", "title": "<code>#perform_all_later</code>", "text": "<p>When you enqueue a jobs using <code>#perform_all_later</code>, Karafka, by default, will produce messages to Kafka in an async fashion. Jobs will be added to a background process queue and dispatched without blocking the processing flow.</p> <p>You may want to alter this behavior for critical jobs and use synchronous enqueuing. To use it, just call the <code>karafka_options</code> method within your job class definition and set the <code>dispatch_many_method</code> to <code>:produce_many_sync</code> as followed:</p> <pre><code>class Job &lt; ActiveJob::Base\n  queue_as :my_kafka_jobs\n\n  karafka_options(\n    dispatch_many_method: :produce_many_sync\n  )\n\n  def perform(value1, value2)\n    puts \"value1: #{value1}\"\n    puts \"value2: #{value2}\"\n  end\nend\n\njobs = 2.times.map { |i| Job.new(i, i + 1) }\n\nJob.perform_all_later(jobs)\n</code></pre>"}, {"location": "Active-Job/#karafka_options-partial-inheritance", "title": "<code>karafka_options</code> Partial Inheritance", "text": "<p>When an ActiveJob class defines <code>karafka_options</code>, these options are designed to be inherited by any subclass of the job. This inheritance mechanism ensures that all the settings are consistently applied across different jobs, simplifying configuration management and promoting reusability.</p> <p>By default, when a subclass inherits from a parent job class with predefined <code>karafka_options</code>, the subclass automatically inherits all of these options. If no explicit <code>karafka_options</code> are defined in the subclass, it will use the options set in its parent class.</p> <p>However, when <code>karafka_options</code> are set in a subclass, it does not necessarily have to redefine all the options specified in the parent class. Instead, it can choose to overwrite only specific options. Karafka will merge the options defined in the subclass with those of the parent class. This merging process ensures that any option not explicitly overridden in the subclass retains its value from the parent class.</p> <p>For example, consider a parent job class configured with multiple karafka_options:</p> <pre><code>class ParentJob &lt; ApplicationJob\n  karafka_options(\n    dispatch_method: :produce_sync,\n    dispatch_many_method: :produce_many_async\n  )\nend\n</code></pre> <p>If a subclass intends to modify only the <code>dispatch_method</code> option, it can do so without having to redefine all other options:</p> <pre><code>class ChildJob &lt; ParentJob\n  karafka_options dispatch_method: :produce_async\nend\n</code></pre> <p>In this case, <code>ChildJob</code> will have <code>dispatch_many_method</code> taken from the <code>ParentJob</code>.</p> <p>This feature of partial options overriding allows for flexible configuration adjustments in subclassed jobs, making it easier to manage variations in job behavior without duplicating the entire set of options across multiple classes.</p>"}, {"location": "Active-Job/#execution-warranties", "title": "Execution Warranties", "text": "<p>Karafka marks each job as consumed using <code>#mark_as_consumed</code> after successfully processing it. This means that the same job should not be processed twice unless the process is killed before the async marking in Kafka happens.</p>"}, {"location": "Active-Job/#behaviour-on-errors", "title": "Behaviour on Errors", "text": "<p>Active Job Karafka adapter will follow the Karafka general runtime errors handling strategy. Upon error, the partition will be paused, a backoff will happen, and Karafka will attempt to retry the job after a specific time.</p> <p>Please keep in mind that as long as the error persists, no other jobs from a given partition will be processed.</p> <p> </p>"}, {"location": "Active-Job/#usage-with-the-dead-letter-queue", "title": "Usage With the Dead Letter Queue", "text": "<p>The Karafka Active Job adapter is fully compatible with the Dead Letter Queue (DLQ) feature. Setting the <code>independent</code> flag to <code>true</code> when configuring DLQ with Active Job is advisable. This recommendation is based on the nature of ActiveJob jobs being inherently independent. The <code>independent</code> flag enhances the DLQ's handling of job failures by treating each job separately, aligning with Active Job's operational characteristics.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    active_job_topic :default do\n      dead_letter_queue(\n        topic: 'dead_jobs',\n        max_retries: 2,\n        # Set this to true for AJ as AJ jobs are independent\n        independent: true\n      )\n    end\n  end\nend\n</code></pre>"}, {"location": "Active-Job/#behaviour-on-shutdown", "title": "Behaviour on Shutdown", "text": "<p>After the shutdown is issued, Karafka will finish processing the current job. After it is processed, will mark it as consumed and will close. Other jobs that may be buffered will not be processed and picked up after the process is started again.</p>"}, {"location": "Active-Job/#behaviour-on-revocation", "title": "Behaviour on Revocation", "text": "<p>Revocation awareness is not part of the standard Active Job adapter. We recommend you either:</p> <ol> <li>Have short-running jobs.</li> <li>Build your jobs to work in an at-least-once fashion.</li> <li>Set <code>max_messages</code> to a small value, so fewer jobs are fetched.</li> <li>Use Pro Enhanced Active Job with revocation awareness and other Pro features.</li> </ol>"}, {"location": "Active-Job/#queue-prefixes", "title": "Queue Prefixes", "text": "<p>Active Job allows you to configure a queue prefix. Karafka does not support prefixes at the moment.</p>"}, {"location": "Active-Job/#current-attributes", "title": "Current Attributes", "text": "<p>The Karafka adapter supports the use of CurrentAttributes. You need to put this in your <code>karafka.rb</code> config file (or initializer):</p> <pre><code>require 'karafka/active_job/current_attributes'\nKarafka::ActiveJob::CurrentAttributes.persist('YourCurrentAttributesClass')\n# or multiple current attributes\nKarafka::ActiveJob::CurrentAttributes.persist('YourCurrentAttributesClass', 'AnotherCurrentAttributesClass')\n</code></pre> <p>Now when you set your current attributes and create a background job, it will execute with them set.</p> <pre><code>class Current &lt; ActiveSupport::CurrentAttributes\n  attribute :user_id\nend\n\nclass Job &lt; ActiveJob::Base\n  def perform\n    puts 'user_id: #{Current.user_id}'\n  end\nend\n\nKarafka::ActiveJob::CurrentAttributes.persist('Current')\nCurrent.user_id = 1\nJob.perform_later # the job will output \"user_id: 1\"\n</code></pre> <p>Karafka handles CurrentAttributes by including them as part of the job serialization process before pushing them to Kafka. These attributes are then deserialized by the ActiveJob consumer and set back in your CurrentAttributes classes before executing the job.</p> <p>This approach is based on Sidekiq's approach to persisting current attributes: Sidekiq and Request-Specific Context.</p> <p>Last modified: 2024-09-09 13:17:18</p>"}, {"location": "Active-Record-Connections-Management/", "title": "Active Record Connections Management", "text": "<p>Karafka interacts with ActiveRecord in the context of database connection management. This integration is designed to ensure efficient resource use and stability during message processing.</p>"}, {"location": "Active-Record-Connections-Management/#rails-and-activerecord-connection-management", "title": "Rails and ActiveRecord Connection Management", "text": "<p>Rails, with ActiveRecord, employs a connection pooling mechanism to manage database connections. Connection pooling aims to reuse existing connections, avoiding the overhead of establishing new connections for every database interaction. This mechanism is particularly beneficial in web applications and background job processing, where efficient database connections can significantly impact performance and scalability.</p> <p>In a typical Rails application, ActiveRecord automatically manages database connections. It checks out connections from the pool when needed and returns them after the request or job is completed. However, in the context of background processing frameworks like Karafka, especially when dealing with database replicas, additional steps, as mentioned above, might be necessary to ensure connections are properly managed.</p>"}, {"location": "Active-Record-Connections-Management/#automatic-connection-management", "title": "Automatic Connection Management", "text": "<p>When no database replication is involved, Karafka automatically manages ActiveRecord database connections. This means that after processing messages, Karafka releases the connections back to the ActiveRecord connection pool, preventing potential connection leakage and ensuring connections are available for other processes or threads that might need them.</p>"}, {"location": "Active-Record-Connections-Management/#dealing-with-database-replicas", "title": "Dealing with Database Replicas", "text": "<p>In database replication scenarios, Karafka requires manual intervention to ensure connections are appropriately handled. Specifically, for each database replica, connections need to be manually released back to the pool. This is achieved by subscribing to the <code>worker.completed</code> event, which signals the completion of a worker's messages processing task. Implementing a handler for this event allows for explicit connection management, ensuring that resources are correctly managed and reducing the risk of connection saturation.</p> <pre><code>::Karafka::App.monitor.subscribe('worker.completed') do\n  rails7plus = Rails.gem_version &gt;= Gem::Version.new('7.0.0')\n\n  # Replace this with the proper reference to the replica connections\n  if rails7plus\n    ActiveRecord::ReplicaBase.connection_handler.clear_active_connections!\n  else\n    ActiveRecord::ReplicaBase.clear_active_connections!\n  end\nend\n</code></pre>"}, {"location": "Active-Record-Connections-Management/#dealing-with-dead-database-connections", "title": "Dealing with Dead Database Connections", "text": "<p>In production environments, database connections can sometimes become  \"dead\" or unusable due to various issues like network disruptions,  database restarts, or other unexpected problems. Rails, through ActiveRecord, provides mechanisms to handle such situations, ensuring your application can recover and continue functioning smoothly.</p> <p>ActiveRecord includes a feature known as the connection \"reaper.\" The reaper periodically checks connections in the pool and removes any dead or idle for too long. This helps maintain a pool of valid connections, but immediate action might be needed when a connection is found dead during a database operation.</p> <p>ActiveRecord provides the <code>#verify!</code> method to handle dead connections dynamically. This method can be called to check if the current connection is still valid. If it is found invalid, <code>#verify!</code> will automatically attempt to re-establish it. This method is essential for ensuring that your application can recover from connection issues on the fly.</p>"}, {"location": "Active-Record-Connections-Management/#implementing-immediate-dead-connection-handling", "title": "Implementing Immediate Dead Connection Handling", "text": "<p>The process of managing dead database connections in Karafka is straightforward. You should subscribe to the <code>error.occurred</code> event. This event is triggered for many errors, including when an error indicating a dead connection is detected. To handle this, you can simply call 'ActiveRecord::Base.connection.verify! ', which checks the connection and re-establishes it if needed.</p> <pre><code>Karafka::App.monitor.subscribe('error.occurred') do |event|\n  # Check if the error is a known dead connection error for your database\n  case event[:error]\n  when PG::ConnectionBad\n    # For PostgreSQL\n    ActiveRecord::Base.connection.verify!\n  when Mysql2::Error::ConnectionError\n    # For MySQL\n    ActiveRecord::Base.connection.verify!\n  # Add more cases here for other database-specific dead connection errors\n  else\n    # Handle other types of errors or log them\n  end\nend\n</code></pre> <p>Rails Reaper and Connection Verification Intervals</p> <p>Rails reaper checks and verifies connections at fixed intervals (<code>reaping_frequency</code>). If many connections become dead, more than verifying the used one may be needed, as retries might pick another dead connection before the reaper runs. Implementing granular backoffs, which wait longer than the reaping frequency, can help ensure successful retries.</p>"}, {"location": "Active-Record-Connections-Management/#conclusion", "title": "Conclusion", "text": "<p>Karafka provides automatic database connection management for standard setups. However, when using database replicas, it's crucial to manage those connections to maintain system performance and stability manually. This process involves subscribing to Karafka's <code>worker.completed</code> event and explicitly releasing connections, ensuring they are available for subsequent use.</p> <p>Last modified: 2024-06-04 14:43:33</p>"}, {"location": "Admin-API/", "title": "Admin API", "text": "<p>Karafka provides application administrative functions via the built-in <code>Karafka::Admin</code> module, including topic and consumer group management, cluster administration, and more.</p> <p>Asynchronous Operation Propagation</p> <p>Many Kafka administrative operations (ACLs, configs, topics) are asynchronous in nature. When an API call returns successfully, this means the controller has accepted the request, not that the change has been fully propagated across the cluster. Configuration changes, ACL updates, and topic modifications may take several seconds to be applied on all brokers, depending on cluster size and network conditions. Always allow time for propagation and verify changes are applied across your cluster before proceeding with dependent operations.</p> <p>Default Cluster Limitation</p> <p>All admin operations in Karafka always run on the default cluster. To run admin operations on multiple clusters, you need separate Karafka boot files for each cluster. For more details, visit the Multi-Cluster Setup section.</p>"}, {"location": "Admin-API/#configuration", "title": "Configuration", "text": "<p><code>Karafka::Admin</code> operates using the default cluster configuration, employing a distinct consumer group name, specifically <code>karafka_admin</code>. It's essential to understand that the Web UI also leverages this same consumer group as it utilizes the Admin API internally. If you're implementing granular Kafka ACLs (Access Control List) permissions, ensure that the <code>karafka_admin</code> consumer group is granted the necessary permissions to function effectively. If you're using <code>karafka-web</code>, you will also need the same permissions applied to the <code>karafka_web</code> group as well.</p> <p><code>Karafka::Admin</code> gets a consistent prefix alongside all other consumer groups, allowing you to streamline permissions across all the consumer groups associated with that application.</p> <p>For example, if you're using Kafka ACLs with prefixed wildcard permissions, <code>Karafka::Admin</code> will be subject to the naming patterns established by the Consumer Mapper, ensuring security and consistent access control.</p> <p>If you wish to reconfigure <code>Karafka::Admin</code>, you may alter the <code>config.admin</code> scope during the Karafka framework configuration. You may, for example, use an alternative <code>group_id</code> to replace the default <code>karafka_admin</code> part and alter the <code>kafka</code> scope settings.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092'\n    }\n\n    # Fetch 20MB at most with admin, instead of the default 5MB\n    config.admin.kafka[:'fetch.message.max.bytes'] = 20 * 1_048_576\n    # Replace karafka_admin group name component with app_admin\n    config.admin.group_id = 'app_admin'\n  end\nend\n</code></pre> <p>We strongly advise against modifying the <code>config.admin.kafka</code> settings in their entirety, as they have been configured to meet the demands of admin operations. For any adjustments, we suggest adopting the granular approach outlined above.</p> <p>Please note that in the case of <code>kafka</code> settings, if a given setting is not found, the root <code>kafka</code> scope value will be used.</p>"}, {"location": "Admin-API/#multi-cluster-setup", "title": "Multi-Cluster Setup", "text": "<p>Karafka allows you to manage multiple Kafka clusters using the <code>KARAFKA_BOOT_FILE</code> environment variable. This variable can point to different Karafka boot files configured for a specific cluster. By changing the value of <code>KARAFKA_BOOT_FILE</code>, you can specify which cluster to use when performing any Karafka admin and declarative topics operations.</p> <pre><code># cluster1.rb\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n\n    config.kafka = {\n      'bootstrap.servers': '192.168.1.2:9092'\n    }\n  end\nend\n</code></pre> <pre><code># cluster2.rb\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n\n    config.kafka = {\n      'bootstrap.servers': '192.168.1.10:9092'\n    }\n  end\nend\n</code></pre> <pre><code># Access Karafka console to operate in cluster 1\nexport KARAFKA_BOOT_FILE=cluster1.rb\nbundle exec karafka console\n\n# Same applies to declarative topics management\nexport KARAFKA_BOOT_FILE=cluster2.rb\nbundle exec karafka topics migrate\n</code></pre>"}, {"location": "Admin-API/#creating-a-topic", "title": "Creating a Topic", "text": "<pre><code>topic_name = 'my_cool_topic'\npartitions_count = 2\nreplication_factor = 1 # 1 for dev, for prod you want more\n\nKarafka::Admin.create_topic(topic_name, partitions_count, replication_factor)\n</code></pre>"}, {"location": "Admin-API/#deleting-a-topic", "title": "Deleting a Topic", "text": "<pre><code>topic_name = 'my_cool_topic'\n\nKarafka::Admin.delete_topic(topic_name)\n</code></pre>"}, {"location": "Admin-API/#altering-a-topic", "title": "Altering a Topic", "text": "<p>Karafka provides two distinct approaches for managing and altering topic configurations:</p> <ul> <li>Declarative Topics (Recommended for most cases)</li> <li>Admin Configs API (For lower-level control)</li> </ul>"}, {"location": "Admin-API/#declarative-topics-approach", "title": "Declarative Topics Approach", "text": "<p>The Declarative Topics feature provides a high-level, code-based way to manage topic configurations. This approach is recommended for most users as it offers:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ... other config ...\n  end\n\n  routes.draw do\n    topic :orders do\n      config(\n        partitions: 6,\n        replication_factor: 3,\n        'cleanup.policy': 'compact',\n        'retention.ms': 604_800_000 # 7 days\n      )\n    end\n  end\nend\n</code></pre> <p>Apply changes using:</p> <pre><code>bundle exec karafka topics migrate\n</code></pre>"}, {"location": "Admin-API/#admin-configs-api-approach", "title": "Admin Configs API Approach", "text": "<p>For cases requiring more granular control, you can use the Admin Configs API to directly manage topic configurations:</p> <pre><code># Describe current topic configuration\nresource = Karafka::Admin::Configs::Resource.new(type: :topic, name: 'orders')\ntopics = Karafka::Admin::Configs.describe(resource)\n\n# Alter topic configuration\nresource = Karafka::Admin::Configs::Resource.new(type: :topic, name: 'orders')\nresource.set('retention.ms', '7200000')  # Set retention to 2 hours\nKarafka::Admin::Configs.alter(resource)\n</code></pre>"}, {"location": "Admin-API/#getting-cluster-info", "title": "Getting Cluster Info", "text": "<pre><code># Get cluster info and list all the topics\ninfo = Karafka::Admin.cluster_info\n\nputs info.topics.map { |topic| topic[:topic_name] }.join(', ')\n</code></pre>"}, {"location": "Admin-API/#reading-topic-messages", "title": "Reading Topic Messages", "text": "<p>By using the <code>read_topic</code> method, you can read data from a given topic partition without subscribing to it.</p> <p>While the returned messages are <code>Karafka::Messages::Message</code> objects, they may not hold the correct notion of the topic details unless the given topic is defined in Karafka routes. For topics that are not defined, defaults will be used.</p> <p>When using the <code>#read_topic</code> method in the Karafka Admin API to retrieve messages from a topic partition, it's essential to understand that this method skips offsets of compacted messages and transactions-related messages. This means that these specific messages won't be fetched or displayed even if they exist in the topic partition. However, while these messages are skipped during retrieval, they are still included in the total counts for those in that partition.</p> <p>This behavior implies that if you request a certain number of the most recent messages, say the last 10, and all these 10 messages were either related to transactions or were compacted, then the <code>#read_topic</code> method will return no data. Thus, you might find situations where you expect data based on the total count but get none due to this offset-skipping behavior.</p>"}, {"location": "Admin-API/#getting-last-n-messages", "title": "Getting Last N Messages", "text": "<pre><code>topic = 'my_topic'\npartition = 0\nhow_many = 10\n\nmessages = Karafka::Admin.read_topic(topic, partition, how_many)\n\nmessages.each do |message|\n  puts message.raw_payload\nend\n</code></pre>"}, {"location": "Admin-API/#getting-messages-from-a-given-offset", "title": "Getting Messages From a Given Offset", "text": "<pre><code>topic = 'my_topic'\npartition = 0\nhow_many = 10\nfirst_offset = 50\n\nmessages = Karafka::Admin.read_topic(topic, partition, how_many, first_offset)\n\nmessages.each do |message|\n  puts message.raw_payload\nend\n</code></pre>"}, {"location": "Admin-API/#getting-messages-from-a-given-time", "title": "Getting Messages From a Given Time", "text": "<pre><code>topic = 'my_topic'\npartition = 0\nhow_many = 10\n# Read at most 10 messages starting 60 seconds ago\nstart_at = Time.now - 60\n\nmessages = Karafka::Admin.read_topic(topic, partition, how_many, start_at)\n\nmessages.each do |message|\n  puts message.raw_payload\nend\n</code></pre>"}, {"location": "Admin-API/#adding-partitions-to-a-topic", "title": "Adding Partitions To a Topic", "text": "<p>If you want to add partitions to an existing topic, you can use the <code>create_partitions</code> admin method:</p> <pre><code>topic_name = 'my-busy-topic'\n# Indicates how many partitions we want to end up with\ntotal_partitions = 20\n\nKarafka::Admin.create_partitions(topic_name, total_partitions)\n</code></pre> <p>This method will create all the additional partitions to reach your desired count.</p>"}, {"location": "Admin-API/#reading-the-watermark-offsets", "title": "Reading the Watermark Offsets", "text": "<p>Watermark offsets represent the current high and low offsets for each partition in a topic.</p> <ul> <li>The high-watermark offset is the offset of the last message that has been fully replicated across all of the brokers in the Kafka cluster. </li> <li>The low watermark offset is important because it determines the starting point for a new consumer reading from a partition. When a consumer subscribes to a Kafka topic, it must know where to start reading messages from in each partition. A new consumer will start reading from the low watermark offset by default. </li> </ul> <p>Watermark offsets are used to track consumers' progress in a Kafka topic. As a consumer reads messages from a topic, it keeps track of the offset of the last message consumed for each partition. By comparing this offset to the watermark offset, the consumer can determine how far behind it is in processing messages.</p> <p>You can access this information using the <code>#read_watermark_offsets</code> admin method as follows:</p> <pre><code>topic = 'my_topic'\npartition = 1\n\nlow, high = Karafka::Admin.read_watermark_offsets(topic, partition)\n\nputs \"Low watermark offset: #{low}\"\nputs \"High watermark offset: #{high}\"\n</code></pre>"}, {"location": "Admin-API/#reading-lags-and-offsets-of-a-consumer-group", "title": "Reading Lags and Offsets of a Consumer Group", "text": "<p>This functionality provides the means to track and understand the consumption progress of consumer groups across specific topics and partitions within your Kafka setup. It is crucial for monitoring the health and performance of your Kafka consumers, as it helps identify any delays or backlogs in processing messages.</p> <p>Using the following method, you can obtain the lags and offsets for a specific consumer group across selected topics. This will provide insights into how far behind each partition is from the latest message.</p> <p>If consumer groups are not specified, the method will default to monitoring all consumer groups defined in the Karafka routing configuration with active topics, according to the <code>active_topics_only</code> parameter. This ensures comprehensive coverage of your Karafka setup, providing visibility into all potentially active consumption patterns without explicit specification.</p>"}, {"location": "Admin-API/#reading-lags-of-all-active-routing-topics", "title": "Reading Lags of All Active Routing Topics", "text": "<p>To get the lag for all active topics in your Karafka setup, invoke <code>#read_lags_with_offsets</code> without any arguments. Karafka will automatically use the consumer groups and active topics defined in <code>karafka.rb</code>:</p> <pre><code>lags_with_offsets = Karafka::Admin.read_lags_with_offsets\n\nlags_with_offsets.each do |group_name, topics|\n  puts \"Consumer group: #{group_name}\"\n\n  topics.each do |topic_name, partitions|\n    puts \"  Topic: #{topic_name}\"\n\n    partitions.each do |partition, data|\n      offset = data[:offset]\n      lag = data[:lag]\n      puts \"    Partition: #{partition}, Offset: #{offset}, Lag: #{lag}\"\n    end\n  end\n\n  puts\nend\n\n# Consumer group: cg0\n#   Topic: visits\n#     Partition: 0, Offset: 204468, Lag: 6\n#     Partition: 1, Offset: 180631, Lag: 0\n#     Partition: 2, Offset: 181414, Lag: 5\n#     Partition: 3, Offset: 181070, Lag: 8\n#     Partition: 4, Offset: 180974, Lag: 4\n# \n# Consumer group: karafka_web\n#   Topic: karafka_consumers_reports\n#     Partition: 0, Offset: 48669, Lag: 2\n</code></pre>"}, {"location": "Admin-API/#reading-lags-of-all-routing-topics", "title": "Reading Lags of All Routing Topics", "text": "<p>To fetch lags for all topics, including inactive ones, set <code>active_topics_only</code> to <code>false</code> when calling <code>Karafka::Admin.read_lags_with_offsets</code>:</p> <pre><code>lags_with_offsets = Karafka::Admin.read_lags_with_offsets(active_topics_only: false)\n\nlags_with_offsets.each do |group_name, topics|\n  puts \"Consumer group: #{group_name}\"\n\n  topics.each do |topic_name, partitions|\n    puts \"  Topic: #{topic_name}\"\n\n    partitions.each do |partition, data|\n      offset = data[:offset]\n      lag = data[:lag]\n      puts \"    Partition: #{partition}, Offset: #{offset}, Lag: #{lag}\"\n    end\n  end\n\n  puts\nend\n\n# Consumer group: cg0\n#   Topic: visits\n#     Partition: 0, Offset: 204569, Lag: 12\n#     Partition: 1, Offset: 180736, Lag: 9\n#     Partition: 2, Offset: 181511, Lag: 11\n#     Partition: 3, Offset: 181163, Lag: 3\n#     Partition: 4, Offset: 181075, Lag: 14\n# \n# Consumer group: karafka_web\n#   Topic: karafka_consumers_reports\n#     Partition: 0, Offset: 48689, Lag: 3\n#   Topic: karafka_consumers_states\n#     Partition: 0, Offset: -1, Lag: -1\n#   Topic: karafka_consumers_metrics\n#     Partition: 0, Offset: -1, Lag: -1\n#   Topic: karafka_consumers_commands\n#     Partition: 0, Offset: -1, Lag: -1\n#   Topic: karafka_errors\n#     Partition: 0, Offset: -1, Lag: -1\n</code></pre>"}, {"location": "Admin-API/#reading-lags-of-selected-consumer-groups-and-topics", "title": "Reading Lags of Selected Consumer Groups and Topics", "text": "<p>To get lags for a specific subset of data, you can explicitly define the consumer groups and topics you're interested in:</p> <pre><code># Specify the consumer group and the topics you are interested in\nconsumer_group_with_topics = {\n  'example_consumer_group' =&gt; ['topic1', 'topic2']\n}\n\n# Fetch the lags and offsets\nlags_with_offsets = Karafka::Admin.read_lags_with_offsets(consumer_group_with_topics)\n\nlags_with_offsets.each do |group_name, topics|\n  puts \"Consumer group: #{group_name}\"\n\n  topics.each do |topic_name, partitions|\n    puts \"  Topic: #{topic_name}\"\n\n    partitions.each do |partition, data|\n      offset = data[:offset]\n      lag = data[:lag]\n      puts \"    Partition: #{partition}, Offset: #{offset}, Lag: #{lag}\"\n    end\n  end\n\n  puts\nend\n\n# Consumer group: example_consumer_group\n#   Topic: topic1\n#     Partition: 0, Offset: 204676, Lag: 3\n#     Partition: 1, Offset: 180825, Lag: 3\n#     Partition: 2, Offset: 181592, Lag: 4\n#     Partition: 3, Offset: 181249, Lag: 7\n#     Partition: 4, Offset: 181154, Lag: 6\n#\n#   Topic: topic2\n#     Partition: 0, Offset: 4676, Lag: 7\n#     Partition: 1, Offset: 825, Lag: 12\n</code></pre>"}, {"location": "Admin-API/#edge-cases-and-considerations", "title": "Edge Cases and Considerations", "text": "<ul> <li> <p>Non-existent Topics: Returns an empty hash for non-existent topics to indicate the absence of data.</p> </li> <li> <p>Unconsumed Topics: Topics with no recorded consumption by the specified consumer group will show a lag and offset of -1 for each partition, highlighting inactivity.</p> </li> <li> <p>Kafka-centric Lag Reporting: The reported lags reflect Kafka's internal tracking rather than the consumer's acknowledged state, which may differ based on the consumer's internal processing.</p> </li> </ul>"}, {"location": "Admin-API/#renaming-a-consumer-group", "title": "Renaming a Consumer Group", "text": "<p>Never Rename Active Consumer Groups</p> <p>This method should not be used on actively running consumer groups, as it involves creating a temporary consumer to handle offset migration. Running this operation on active groups may cause unexpected behavior.</p> <p>The <code>rename_consumer_group</code> method in Karafka Admin API allows you to rename an existing consumer group while preserving its offsets for specific topics. This method is beneficial when reorganizing or consolidating consumer group names without losing track of the consumption state.</p> <pre><code>Karafka::Admin.rename_consumer_group(\n  'old_group_name',\n  'new_group_name',\n  ['topic1', 'topic2']\n)\n</code></pre> <p>When using <code>rename_consumer_group</code>, the method ensures that offsets from the old consumer group are transferred to the new one, maintaining continuity in message consumption. You need to specify which topics should have their offsets migrated during the rename, giving you control over the process. By default, the original consumer group is deleted after the rename, but you can retain it by setting <code>delete_previous</code> to <code>false</code>.</p> <p>Offset Merger with Existing Consumer Groups</p> <p>If the new consumer group already exists, the offsets from the old group will be merged into it. This may result in the continuation of message processing from the combined offsets, so plan accordingly.</p>"}, {"location": "Admin-API/#copying-a-consumer-group", "title": "Copying a Consumer Group", "text": "<p>Never Copy Active Consumer Groups</p> <p>This method should not be used on actively running consumer groups, as it involves creating a temporary consumer to handle offset migration. Running this operation on active groups may cause unexpected behavior.</p> <p>The <code>#copy_consumer_group</code> method in Karafka Admin API allows you to copy offsets from an existing consumer group to another while preserving its consumption state for specific topics. This functionality is useful when creating a duplicate consumer group with the same consumption progress as an existing one.</p> <pre><code>Karafka::Admin.copy_consumer_group(\n  'source_group_name',\n  'target_group_name',\n  ['topic1', 'topic2']\n)\n</code></pre> <p>When using <code>#copy_consumer_group</code>, the method ensures that offsets from the source consumer group are transferred to the target one, maintaining continuity in message consumption. You need to specify which topics should have their offsets copied during the process, giving you control over what gets migrated.</p> <p>Offset Merger with Existing Consumer Groups</p> <p>If the target consumer group already exists, the offsets from the source group will be merged into it. This may result in the continuation of message processing from the combined offsets, so plan accordingly.</p> <p>The method returns <code>true</code> if offsets were successfully copied or <code>false</code> if there was nothing to copy (for example, if the source consumer group doesn't exist or has no committed offsets for the specified topics).</p> <p>This functionality is particularly useful for:</p> <ul> <li>Creating backup consumer groups before making significant changes</li> <li>Testing new consumer configurations with the same consumption progress</li> <li>Setting up disaster recovery scenarios</li> </ul> <p>Unlike <code>#rename_consumer_group</code>, this method preserves the source consumer group, allowing both groups to exist simultaneously.</p>"}, {"location": "Admin-API/#deleting-a-consumer-group", "title": "Deleting a Consumer Group", "text": "<p>Never Delete Active Consumer Groups</p> <p>This method should only be used for consumer groups not actively used. Deleting a consumer group that is currently in use (running) can lead to data loss, inconsistencies, or unexpected behavior in your Kafka cluster.</p> <p>The <code>Karafka::Admin.delete_consumer_group</code> method is designed to remove an existing consumer group from a Kafka cluster. This method effectively disbands the specified group, removing its metadata and state from the Kafka cluster.</p> <p>To use it, all you need to do is to provide your consumer group name:</p> <pre><code>Karafka::Admin.delete_consumer_group('your_consumer_group_name')\n</code></pre>"}, {"location": "Admin-API/#changing-an-offset-of-a-consumer-group", "title": "Changing an Offset of a Consumer Group", "text": "<p>Never Alter Active Consumer Groups</p> <p>This method should only be used for consumer groups not actively used. Altering a consumer group that is currently in use (running) can lead to data loss, inconsistencies, or unexpected behavior in your Kafka cluster.</p> <p>This method allows you to modify the offset for a specific topic within a consumer group, effectively controlling where the group starts consuming messages from within the topic.</p> <p>The <code>Karafka::Admin.seek_consumer_group</code> method takes two primary arguments:</p> <p>the name of the consumer group a topic-partition hash with topics, partitions, and offsets where the consumer group should be moved</p> <p>When invoked, it changes the current offset of the specified consumer group for the given topics to the new offsets provided. This method is beneficial when you need to reprocess or skip specific messages due to various operational requirements.</p>"}, {"location": "Admin-API/#changing-an-offset-for-all-partitions", "title": "Changing an Offset for All Partitions", "text": "<p>You can specify a hash mapping topics to integer offsets, allowing the adjustment of offsets for entire partitions within those topics:</p> <pre><code>Karafka::Admin.seek_consumer_group(\n  'my_consumer_group',\n  {\n    # move offset to 0 on all partitions of this topic\n    'my_topic1' =&gt; 0,\n    # move offset to 1000 on all partitions of this topic\n    'my_other_topic' =&gt; 1000\n  }\n\n)\n</code></pre>"}, {"location": "Admin-API/#changing-an-offset-for-a-particular-partition", "title": "Changing an Offset for a Particular Partition", "text": "<p>Alternatively, you can always move offset on specific partitions by specifying them directly:</p> <pre><code>Karafka::Admin.seek_consumer_group(\n  'my_consumer_group',\n  {\n    # move offset to 0 on partition 0 and to 10 on partition 1\n    'my_topic1' =&gt; { 0 =&gt; 0, 1 =&gt; 10 },\n    # move offset to 10 000 on partition 5 and to 50 000 on partition 20\n    'my_other_topic' =&gt; { 5 =&gt; 10_000, 20 =&gt; 50_000 }\n  }\n)\n</code></pre>"}, {"location": "Admin-API/#changing-an-offset-to-a-time-based-location", "title": "Changing an Offset to a Time-Based Location", "text": "<p><code>seek_consumer_group</code> method also accepts time references as offsets, allowing for precise time-based location seeking. Karafka automatically locates the matching offsets for the specified times and moves the consumer group position to this location.</p> <pre><code>now = Time.now\n\nKarafka::Admin.seek_consumer_group(\n  'my_consumer_group',\n  {\n    # move offset back by 1 hour for partition 0, and 2 hours for partition 5\n    'my_topic1' =&gt; { 0 =&gt; now - 60 * 60, 5 =&gt; now - 60 * 60 * 2 },\n    # move offset back by five minutes for all the partitions of the given topic\n    'my_other_topic' =&gt; now - 60 * 5\n  }\n)\n</code></pre>"}, {"location": "Admin-API/#changing-an-offset-to-earliest-or-latest", "title": "Changing an Offset to Earliest or Latest", "text": "<p>Adjusting consumer group offsets to \"earliest\" or \"latest\" helps control where consumption begins within a topic. However, these terms can be misleading, particularly with compacted topics where the \"earliest\" offset may not be zero due to log cleanup and message expiration. The \"earliest\" setting allows consumers to start from the oldest available message, ensuring comprehensive data coverage.</p> <p>Conversely, the \"latest\" offset refers to the end of the log, indicating the point where new messages will be appended. Setting the offset to \"latest\" means consumption will start with new messages arriving post-adjustment, which is ideal for applications focused on real-time data.</p> <p>The <code>Karafka::Admin.seek_consumer_group</code> method facilitates easy adjustment of offsets to either \"earliest\" or \"latest\", applicable across entire topics or specific partitions:</p> <pre><code># Adjust offsets for specific partitions\nKarafka::Admin.seek_consumer_group(\n  'your_consumer_group',\n  {\n    'your_topic' =&gt; {\n      0 =&gt; :latest,    # Start reading new messages from partition 0\n      1 =&gt; :earliest  # Start from the oldest message in partition 1\n    }\n  }\n)\n\n# Adjust all partitions of a topic to the earliest or latest\nKarafka::Admin.seek_consumer_group(\n  'your_consumer_group',\n  { 'your_topic' =&gt; :earliest }  # Start all partitions from the oldest available message\n)\n\nKarafka::Admin.seek_consumer_group(\n  'your_consumer_group',\n  { 'your_topic' =&gt; :latest }  # Start consuming new messages across all partitions\n)\n</code></pre> <p>Using <code>:earliest</code> and <code>:latest</code> is vital for managing consumer behavior, ensuring flexibility in data consumption strategies according to specific needs.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Admin-Acls-API/", "title": "Admin ACLs (Access Control Lists) API", "text": "<p>Apache Kafka ACLs (Access Control Lists) provide a robust mechanism to control permissions and access rights for Kafka resources. They are crucial for ensuring data security, managing consumer and producer interactions, and maintaining overall cluster integrity. Karafka extends these capabilities with a simplified, Ruby-friendly API.</p> <p>The Karafka Admin ACLs API provides a structured and easy-to-use interface for managing Kafka ACLs. It allows developers to create, delete, and describe ACLs with Ruby symbol-based definitions, enhancing readability and ease of use compared to the direct usage of <code>librdkafka</code> types.</p> <p>This documentation provides an overview of Kafka ACLs, how to use the ACLs with Karafka, primary use cases, and code samples to get you started.</p> <p>Asynchronous Operation Propagation</p> <p>Many Kafka administrative operations (ACLs, configs, topics) are asynchronous in nature. When an API call returns successfully, this means the controller has accepted the request, not that the change has been fully propagated across the cluster. Configuration changes, ACL updates, and topic modifications may take several seconds to be applied on all brokers, depending on cluster size and network conditions. Always allow time for propagation and verify changes are applied across your cluster before proceeding with dependent operations.</p>"}, {"location": "Admin-Acls-API/#what-are-kafka-acls", "title": "What are Kafka ACLs?", "text": "<p>Kafka ACLs are rules that determine how users and applications can interact with Kafka resources, such as topics, consumer groups, and brokers. Each ACL entry specifies the allowed or denied operations for a particular principal (user or client) on a given resource. Operations can include reading from a topic, writing to a topic, or creating a consumer group.</p> <p>ACLs ensure that only authorized entities can access Kafka's functionalities, which is vital for maintaining data security and operational integrity.</p>"}, {"location": "Admin-Acls-API/#types", "title": "Types", "text": "<p>The Karafka Admin ACLs API defines several \"types\" that categorize and specify details for ACL management in a Kafka environment. These types are crucial for setting up detailed and secure access controls.</p> <p>These types play a critical role in the Karafka Admin ACLs API, providing a structured and comprehensive way to manage access controls within a Kafka environment. Understanding and utilizing these types enables you to secure and regulate operations in your Kafka clusters effectively.</p>"}, {"location": "Admin-Acls-API/#resource-types", "title": "Resource Types", "text": "<p>Resource types are entities within Kafka for which ACLs can be defined. They specify the scope of the ACL.</p> Type Description <code>:any</code> Used for lookups, not for setting permissions. <code>:topic</code> Represents a single Kafka topic. <code>:consumer_group</code> Represents a single Kafka consumer group. <code>:broker</code> Represents a given Kafka broker."}, {"location": "Admin-Acls-API/#resource-pattern-types", "title": "Resource Pattern Types", "text": "<p>Resource pattern types dictate how ACLs are applied to resources, defining the precision and scope of access rules.</p> Pattern Type Description <code>:any</code> For lookups only; not for setting permissions. <code>:match</code> Targets resources with a pattern matching for broader control. <code>:literal</code> Applies ACLs to a specifically named resource. <code>:prefixed</code> Applies ACLs to all resources with a common name prefix."}, {"location": "Admin-Acls-API/#operation-types", "title": "Operation Types", "text": "<p>Operations are the actions that can be permitted or denied on Kafka resources, defining what a user or service can do.</p> Operation Type Description <code>:any</code> For lookups, indicating no specific operation type. <code>:all</code> Allows all operations (complete access). <code>:read</code> Grants read access to a resource. <code>:write</code> Allows writing data to a resource. <code>:create</code> Permits the creation of resources. <code>:delete</code> Enables the deletion of resources. <code>:alter</code> Allows modifications to resources. <code>:describe</code> Grants the ability to view resource details. <code>:cluster_action</code> Permits actions related to the Kafka cluster. <code>:describe_configs</code> Allows viewing configurations for resources. <code>:alter_configs</code> Enables modification of resource configurations. <code>:idempotent_write</code> Grants the ability for idempotent writes."}, {"location": "Admin-Acls-API/#permission-types", "title": "Permission Types", "text": "<p>Permission types indicate the nature of the access being granted or denied, essentially determining whether an operation is allowed.</p> Permission Type Description <code>:any</code> Used for lookups, indicating no specific permission type. <code>:allow</code> Grants the specified operations, enabling actions on the resource. <code>:deny</code> Blocks the specified operations, preventing actions on the resource."}, {"location": "Admin-Acls-API/#usage", "title": "Usage", "text": "<p>When initializing an ACL in Karafka, you'll use several parameters to define these rules. Here's a breakdown of each argument you'll provide:</p> Argument Description <code>resource_type</code> Determines the type of Kafka resource you're securing, such as a topic (<code>:topic</code>) or consumer group (<code>:consumer_group</code>). You can specify this as a symbol from <code>RESOURCE_TYPES_MAP</code> for readability or use a direct numerical type from <code>rdkafka</code>. Choose the resource type that aligns with the item you wish to control access to. <code>resource_name</code> The specific name of the resource, like the name of a topic. This can sometimes be <code>nil</code>, mainly when your resource pattern type doesn't require a particular name. Use this to pinpoint the exact resource you're setting the ACL for. <code>resource_pattern_type</code> Defines how the ACL applies to the resource. You might set it to a literal match for a specific resource or a prefixed pattern to cover a group of resources. This is specified using a symbol from <code>RESOURCE_PATTERNS_TYPE_MAP</code> or a direct numerical type. <code>principal</code> The principal (usually a user or client identity) the ACL is for. This specifies who the ACL will apply to. It can sometimes be <code>nil</code> if you're defining a more general rule that isn't principal-specific. <code>host</code> Indicates the host from which the principal can access the resource. It defaults to <code>*</code> (all hosts), but can be set to a specific IP or hostname to restrict access further. <code>operation</code> What action the principal can or cannot perform, like read, write, or create. This is chosen from the <code>OPERATIONS_MAP</code> and can be a descriptive symbol or a numerical type. Select the operation that best fits the action you wish to allow or prevent. <code>permission_type</code> Specifies whether to allow or deny the operation defined. This is where you enforce the rule, granting or restricting access as needed. Choose 'allow' to enable the operation for the principal or 'deny' to block it."}, {"location": "Admin-Acls-API/#creating-an-acl", "title": "Creating an ACL", "text": "<p>To create an ACL, you need to instantiate a <code>Karafka::Admin::Acl</code> object and use the <code>#create</code> method:</p> <pre><code>acl = Karafka::Admin::Acl.new(\n  resource_type: :topic,\n  resource_name: 'my_topic',\n  resource_pattern_type: :literal,\n  principal: 'user:Bob',\n  host: '*',\n  operation: :write,\n  permission_type: :allow\n)\n\nKarafka::Admin::Acl.create(acl)\n</code></pre>"}, {"location": "Admin-Acls-API/#deleting-an-acl", "title": "Deleting an ACL", "text": "<p>To delete an ACL, you can use the <code>#delete</code> method with an existing ACL object:</p> <pre><code>acls = Karafka::Admin::Acl.all\n\nKarafka::Admin::Acl.delete(acls.first)\n</code></pre> <p>or you may explicitly define the ACL to remove:</p> <pre><code>acl = Karafka::Admin::Acl.new(\n  resource_type: :topic,\n  resource_name: 'my_topic',\n  resource_pattern_type: :literal,\n  principal: 'user:Bob',\n  host: '*',\n  operation: :write,\n  permission_type: :allow\n)\n\nKarafka::Admin::Acl.delete(acl)\n</code></pre>"}, {"location": "Admin-Acls-API/#describing-acls", "title": "Describing ACLs", "text": "<p>To retrieve details about existing ACLs, use the <code>#describe</code> method. It returns all ACLs matching the provided criteria:</p> <pre><code>acl_match = Karafka::Admin::Acl.all.first\n\nacls = Karafka::Admin::Acl.describe(acl_match)\n\nacls.each do |acl|\n  puts acl.inspect\nend\n</code></pre>"}, {"location": "Admin-Acls-API/#listing-all-acls", "title": "Listing All ACLs", "text": "<p>To list all ACLs within the Kafka cluster, you can use the <code>#all</code> method:</p> <pre><code>all_acls = Karafka::Admin::Acl.all\n\nall_acls.each do |acl|\n  puts acl.inspect\nend\n</code></pre>"}, {"location": "Admin-Acls-API/#example-use-cases-and-when-to-use-it", "title": "Example Use Cases and When to Use It", "text": "<ul> <li>Topic Access Control: Restrict read/write operations on specific topics to certain users or applications.</li> <li>Consumer Group Management: Control which principals can create or interact with consumer groups.</li> <li>Administrative Restriction: Limit who can create, alter, or delete topics within the Kafka cluster.</li> <li>Security: Ensure that only authorized entities can perform operations, maintaining data integrity and security.</li> <li>Securing Data: Whenever you need to secure your Kafka data, ensure that only authorized users and services can access or modify it.</li> <li>Multi-tenant Systems: In systems where multiple users or services interact with Kafka, you must enforce strict access controls.</li> <li>Compliance and Auditing: Your application must comply with security standards or require auditing capabilities for access and operations.</li> </ul>"}, {"location": "Admin-Acls-API/#summary", "title": "Summary", "text": "<p>Karafka's Admin ACLs API provides a powerful yet user-friendly way to manage Kafka ACLs, ensuring secure and authorized access to Kafka resources. By leveraging Ruby symbols and a structured API, it simplifies the process of ACL management, making it more accessible and less error-prone for Ruby developers.</p> <p>Whether securing a small project or an enterprise-scale system, understanding and utilizing Kafka ACLs through Karafka can significantly enhance your application's security and data governance.</p> <p>Last modified: 2025-05-05 16:42:02</p>"}, {"location": "Admin-Configs-API/", "title": "Configs API", "text": "<p>The Karafka Admin Configs API provides tools for managing configuration settings for Kafka brokers and topics. This API supports retrieving configuration details (<code>describe</code>) and incremental alterations (<code>alter</code>) to these configurations. The operations are designed to be flexible, supporting both individual and batch operations.</p> <p>Asynchronous Operation Propagation</p> <p>Many Kafka administrative operations (ACLs, configs, topics) are asynchronous in nature. When an API call returns successfully, this means the controller has accepted the request, not that the change has been fully propagated across the cluster. Configuration changes, ACL updates, and topic modifications may take several seconds to be applied on all brokers, depending on cluster size and network conditions. Always allow time for propagation and verify changes are applied across your cluster before proceeding with dependent operations.</p> <p>Declarative Topics Feature For Topics Management</p> <p>The Admin Configs API provides low-level access for managing Kafka topics and broker configurations. While powerful, it requires detailed knowledge and careful management of individual settings. For a more streamlined and error-resistant approach, consider using the high-level declarative topics feature provided by Karafka. This feature allows for easier and more declarative management of topic configurations, making it a superior choice for most use cases.</p>"}, {"location": "Admin-Configs-API/#what-are-kafka-configurations", "title": "What are Kafka Configurations?", "text": "<p>Kafka configurations are key-value pairs that define the behavior of brokers and topics. Managing these configurations properly is essential for optimizing performance, ensuring security, and maintaining reliability in your Kafka environment.</p>"}, {"location": "Admin-Configs-API/#types", "title": "Types", "text": "<p>The Karafka Admin Configs API involves several types that categorize the details of configuration management in Kafka. Understanding these types is crucial for effective configuration management.</p>"}, {"location": "Admin-Configs-API/#resource-types", "title": "Resource Types", "text": "<p>Resource types refer to Kafka entities that can be configured, such as topics or brokers. These are fundamental to identifying which parts of your Kafka cluster you are managing.</p> Type Description <code>:topic</code> Represents a single Kafka topic. <code>:broker</code> Represents a specific Kafka broker."}, {"location": "Admin-Configs-API/#operation-types", "title": "Operation Types", "text": "<p>Operations types within this API are used to specify the configuration changes being applied, such as setting or deleting values.</p> Operation Type Description <code>:set</code> Sets or updates a configuration value. <code>:delete</code> Removes a configuration entry. <code>:append</code> Adds a value to a list-based configuration. <code>:subtract</code> Removes a value from a list-based configuration."}, {"location": "Admin-Configs-API/#methods", "title": "Methods", "text": "Method Description Arguments Returns Notes <code>describe(*resources)</code> Retrieves configurations for specified Kafka resources. <code>resources</code> - A single resource or an array of resources. An array of <code>Resource</code> objects with configuration details. Fetches configuration details in a single operation, even for multiple resources. <code>alter(*resources)</code> Applies configuration changes to specified Kafka resources using incremental updates. <code>resources</code> - A single resource or an array of resources. Applies the changes and provides a confirmation of the update. This method is non-transactional and may succeed partially; validate configurations before applying. <p>These methods provide a robust interface for detailed management of configurations in a Kafka environment, offering both retrieval and update functionalities.</p>"}, {"location": "Admin-Configs-API/#usage", "title": "Usage", "text": "<p>Here's how you might typically use the API to manage Kafka configurations:</p> <pre><code># Describe topic configurations\nresource = Karafka::Admin::Configs::Resource.new(type: :topic, name: 'example')\ntopics = Karafka::Admin::Configs.describe(resource)\ntopics.each do |topic|\n  topic.configs.each do |config|\n    puts \"#{topic.name} #{config.name}: #{config.value}\"\n  end\nend\n\n# Alter topic configurations\nresource = Karafka::Admin::Configs::Resource.new(type: :topic, name: 'example')\n# Set retention to 2 hours\nresource.set('retention.ms', '7200000')\n\n# Apply the changes\nKarafka::Admin::Configs.alter(resource)\n</code></pre>"}, {"location": "Admin-Configs-API/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Configuration Audits: Retrieve configurations to review and ensure compliance with operational standards.</p> </li> <li> <p>Dynamic Configuration Tuning: Adjust topic or broker configurations in response to changes in usage patterns or operational requirements.</p> </li> <li> <p>Batch Configuration Management: Simultaneously update configurations for multiple brokers or topics, improving efficiency in large-scale environments.</p> </li> </ul> <p>Last modified: 2025-05-05 16:42:02</p>"}, {"location": "Articles-and-other-references/", "title": "Articles and other references", "text": ""}, {"location": "Articles-and-other-references/#libraries-and-components", "title": "Libraries and components", "text": "<ul> <li>Karafka</li> <li>Karafka Web UI</li> <li>WaterDrop</li> <li>Karafka-Testing</li> <li>Karafka-Core</li> <li>Karafka Rdkafka</li> <li>Rdkafka-Ruby</li> <li>Apache Kafka</li> <li>librdkafka</li> </ul>"}, {"location": "Articles-and-other-references/#articles-and-references-about-karafka", "title": "Articles and references about Karafka", "text": "<p>Some of those might be outdated and may refer to previous Karafka versions. Keep that in mind.</p> <ul> <li>Deimos \u2014 The Journey to Karafka</li> <li>Under the Hood: Enhancing Karafka\u2019s CPU and Memory Efficiency</li> <li>The librdkafka Supply Chain Breakdown: rdkafka-ruby\u2019s Darkest Hour</li> <li>From Sleep to Speed: Making Rdkafka Sync Operations 16 Times Faster</li> <li>Karafka 2.5 and Web UI 0.11: Next-Gen Consumer Control and Operational Excellence</li> <li>Karafka 2.4 Release Announcement: Advancing Kafka Processing for Ruby and Rails</li> <li>Refactoring in Practice (Using Kafka and Karafka)</li> <li>Karafka Framework 2.3 + Web UI 0.8 Release Announcement</li> <li>The Art of Forking: Unlocking Scalability in Ruby (Swarm)</li> <li>Karafka framework 2.1 announcement</li> <li>Karafka Web UI announcement</li> <li>Karafka framework 2.0 announcement</li> <li>Monitoring Karafka Jobs Progress Using Web UI</li> <li>Enhancing Data Reliability Through Transactional Offsets with Karafka</li> <li>Delaying Kafka Messages Processing with Karafka: A Deep Dive into Delayed Topics</li> <li>Kafka topics as a code \u2013 declarative Kafka topics management in Ruby</li> <li>Batch Processing with Kafka using Karafka Batch</li> <li>Integration Patterns for Distributed Architecture - Intro to Kafka</li> <li>Add a Kafka Consumer to Rails</li> <li>An alternative approach to custom partition assignment strategy for Kafka consumers with Karafka</li> <li>Kafka on Rails: Using Kafka with Ruby on Rails - Part 1 - Kafka basics and its advantages</li> <li>Kafka on Rails: Using Kafka with Ruby on Rails - Part 2 - Getting started with Rails and Kafka</li> <li>Karafka example applications</li> <li>Karafka (Ruby + Kafka) framework 1.4.0 Release Notes</li> <li>Karafka (Ruby + Kafka) framework 1.3.0 Release Notes</li> <li>Karafka \u2013 Ruby micro-framework for building Apache Kafka message-based applications</li> <li>Benchmarking Karafka \u2013 how does it handle multiple TCP connections</li> <li>Reduce your method calls by 99.9% by replacing Thread#pass with Queue#pop</li> <li>Karafka \u2013 Ruby framework for building Kafka message based applications (presentation)</li> </ul>"}, {"location": "Articles-and-other-references/#videos", "title": "Videos", "text": "<ul> <li>Kafka with Ruby on Rails by CJ Avilla</li> <li>RailsConf 2023 - Applying microservices patterns to a modular monolith by Guillermo Aguirre</li> <li>RubyConfTH 2023 - Event Streaming Patterns for Ruby Services by Brad Urani</li> <li>Kafka For Rubyists YouTube series by Karol Galanciak</li> <li>RedDotRubyConf 2017 - Spinning up micro-services using Ruby/Kafka by Ankita Gupta</li> <li>Pivorak - Karafka - Getting beyond HTTP by Maciej Mensfeld</li> </ul>"}, {"location": "Articles-and-other-references/#general-articles-and-references-about-working-with-apache-kafka", "title": "General articles and references about working with Apache Kafka", "text": "<ul> <li>Kafka topic naming conventions - 5 recommendations with examples</li> <li>Kafka Topic Naming Conventions</li> <li>Should You Put Several Event Types in the Same Kafka Topic?</li> <li>Kafka topic creation best-practice</li> </ul> <p>Last modified: 2025-06-15 22:06:02</p>"}, {"location": "Assignments-Tracking/", "title": "Assignments Tracking", "text": "<p>The Karafka Assignments Tracker is a feature designed to keep track of the active assignments within a Karafka process. It serves as a tool for understanding the current assignments of topics partitions from a Kafka cluster. This tracker is handy in environments with dynamic partition reassignments, such as those involving multiple consumer groups or subscription groups.</p>"}, {"location": "Assignments-Tracking/#usage", "title": "Usage", "text": "<p>To access your process's current assignments, you only need to execute <code>Karafka::App.assignments</code> method from any place in the process. This thread-safe method will return a Hash object where keys correspond to the assigned topics and values are arrays with IDs of the partitions assigned for a corresponding topic.</p> <p>It is worth pointing out that the topics are <code>Karafka::Routing::Topic</code> objects and not just topic names. This is done that way so the system supports providing you insights about the same topic if it is being consumed in multiple consumer groups simultaneously. You can use it from any place in your code.</p> <pre><code>def consume\n  Karafka::App.assignments.each do |topic, partitions|\n    puts \"In #{topic.name} I'm currently assigned to partitions: #{partitions.join(', ')}\"\n  end\nend\n</code></pre>"}, {"location": "Assignments-Tracking/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Building Assignments Aware Schedulers: A custom scheduler that takes into account assignments can significantly improve efficiency and performance. Such a scheduler optimizes resource utilization and reduces latency by making intelligent scheduling decisions based on real-time cluster state.</p> </li> <li> <p>Dynamic Resources Allocation: In scenarios where workloads fluctuate, the tracker can dynamically redistribute tasks based on the current partition assignments or apply specific processing strategies, ensuring optimal resource utilization.</p> </li> <li> <p>Monitoring and Alerting: Developers can monitor partition assignments to detect anomalies (e.g., imbalanced loads, partition loss) and trigger alerts for proactive issue resolution.</p> </li> <li> <p>Consumer Group Management: For applications involving multiple consumer groups, the tracker provides visibility into how partitions are distributed across different groups, aiding debugging and performance tuning.</p> </li> <li> <p>Performance Tuning: Observing partition traffic can help identify hotspots or bottlenecks in data flow, enabling fine-tuning partition sizes or numbers to optimize performance.</p> </li> <li> <p>Dynamic Consumer Scaling: For systems that dynamically scale consumers based on load, knowledge of partition assignments is necessary to redistribute partitions among the new set of consumers efficiently.</p> </li> </ul> <p>Last modified: 2023-12-12 12:18:06</p>"}, {"location": "Auto-reload-of-code-changes-in-development/", "title": "Auto reload of code changes in development", "text": "<p>Karafka supports auto-reload of code changes for Ruby on Rails, similar to Puma or Sidekiq.</p> <p>Due to consumers persistence in Karafka (long-living consumer instances), in order to make it work, you need to turn it on yourself by setting a <code>consumer_persistence</code> configuration option in the <code>karafka.rb</code> file to <code>false</code> in the development mode:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = { 'bootstrap.servers': '127.0.0.1:9092' }\n    config.client_id = 'example_app'\n    config.concurrency = 2\n    # Recreate consumers with each batch. This will allow Rails code reload to work in the\n    # development mode. Otherwise Karafka process would not be aware of code changes\n    config.consumer_persistence = !Rails.env.development?\n  end\nend\n</code></pre> <p>Your code changes will be applied after processing of current messages batch.</p> <p>Keep in mind, though, that there are a couple of limitations to it:</p> <ul> <li>Changes in the routing are not reflected. This would require reconnections and would drastically complicate reloading.</li> <li>Any background work you run outside the Karafka framework but within the process might not be caught in the reloading.</li> <li>If you use in-memory consumer data buffering that spans multiple batches, it won't work as code reload means re-initializing consumer instances. In cases like that, you will be better off not using the reload mode.</li> </ul> <p>Last modified: 2023-11-03 14:36:20</p>"}, {"location": "Build-vs-Buy/", "title": "Build vs. Buy", "text": "<p>Open Source software is great. Why should you pay money for Karafka Pro when you can extend it yourself or use it for free?</p>"}, {"location": "Build-vs-Buy/#pricing", "title": "Pricing", "text": "<p>A senior developer is typically $10,000/mo. One with Kafka knowledge even more. The price of Karafka Pro is $1495 a year. Is your business building infrastructure or user functionalities?</p> <ul> <li>How many days or weeks will it take your team to piece together similar functionalities?</li> <li>How long will they spend fixing bugs in production?</li> <li>How many weeks will your team spend understanding the complexity of Kafka operations?</li> <li>How many weeks will your team spend building a performant and reliable framework for your business?</li> </ul> <p>Paying money for good infrastructure means you have more time to focus on user-facing features.</p> <p>Baremetrics has a Build vs. Buy calculator.  Here are some extremely optimistic projections, now let the numbers speak for themselves:</p> <p> </p>"}, {"location": "Build-vs-Buy/#features", "title": "Features", "text": "<p>Karafka Pro has lots of really valuable, well-documented, well-tested functionalities. You can reproduce some of this functionality with 3rd party open source libraries but:</p> <ol> <li>Will they be supported years from now?</li> <li>Will they work with newer Karafka versions?</li> <li>Will they get a steady stream of updates to fix bugs?</li> <li>How many days or weeks will it take you to integrate everything together and test it?</li> </ol> <p>All the Karafka Pro functionalities are designed to integrate and work well together.</p> <p>You can spend days or weeks integrating various OSS gems, or you can pay me for more features and have it running in minutes.</p>"}, {"location": "Build-vs-Buy/#support", "title": "Support", "text": "<p>You can scale your Ruby application with Karafka quickly. At that point, Karafka is a significant piece of your business's infrastructure, so a dedicated support policy becomes insurance: three years from now, after many on your engineering team have moved on, who will help you if a problem suddenly develops?  It's very common for issues with OSS projects to linger for weeks or months.</p> <p>Upgrades months or years from now are as simple as <code>bundle up karafka.</code> The upgrade path between major versions is always well documented.  So why pay? Because it's my business to make your life as simple and easy as possible. On top of that, by supporting Karafka Pro, you're also supporting the broader open source ecosystem, as a portion of the income is distributed back to other OSS projects that Karafka relies on under the hood.</p> <p>Help me build and maintain a high-quality Kafka ecosystem for Ruby and Ruby on Rails.</p> <p>Buy Karafka Pro.</p> <p>Last modified: 2025-02-17 14:08:42</p>"}, {"location": "CLI/", "title": "CLI", "text": "<p>Karafka has a simple CLI built in. It provides the following commands:</p> Command Description help [COMMAND] Describe available commands or one specific command console Start the Karafka irb console similar to the Rails console (short-cut alias: \"c\") info Print configuration details and other options of your application install Installs all required things for Karafka application in current directory server Start the Karafka server (short-cut aliases: \"s\", \"consumer\") swarm Start the Karafka server in the swarm mode (multiple forked processes) topics Allows for topics management (create, delete, repartition, reset, migrate) <p>All the commands are executed the same way:</p> <pre><code>bundle exec karafka [COMMAND]\n</code></pre> <p>If you need more details about each of the CLI commands, you can execute the following command:</p> <pre><code>bundle exec karafka help [COMMAND]\n</code></pre>"}, {"location": "CLI/#karafka-server", "title": "Karafka server", "text": ""}, {"location": "CLI/#limiting-consumer-groups-used-per-process", "title": "Limiting consumer groups used per process", "text": "<p>Karafka supports having multiple consumer groups within a single application. You can run multiple Karafka instances, specifying consumer groups that should be running per each process using the <code>--include-consumer-groups</code> server flag as follows:</p> <pre><code>bundle exec karafka server --include-consumer-groups group_name1,group_name3\n</code></pre> <p>If you specify none, by default, all will run.</p> <p>You can also exclude certain consumer groups by using the <code>--exclude-consumer-groups</code> flag:</p> <pre><code>bundle exec karafka server --exclude-consumer-groups group_name2,group_name3\n</code></pre>"}, {"location": "CLI/#limiting-subscription-groups-used-per-process", "title": "Limiting subscription groups used per process", "text": "<p>Karafka supports having multiple subscription groups within a single application. You can run multiple Karafka instances, specifying subscription groups that should be running per each process using the <code>--include-subscription-groups</code> server flag as follows:</p> <pre><code>bundle exec karafka server --include-subscription-groups group_name1,group_name3\n</code></pre> <p>If you specify none, by default, all will run.</p> <p>You can also exclude certain subscription groups by using the <code>--exclude-subscription-groups</code> flag:</p> <pre><code>bundle exec karafka server --exclude-subscription-groups group_name2,group_name3\n</code></pre> <p>Handling Multiplexed Subscription Groups in CLI Commands</p> <p>When using CLI commands to include or exclude subscription groups, it is important to remember that multiplexed subscription group names carry a <code>_multiplex_NR</code> postfix matching the multiplexing level. This postfix distinguishes these groups from others and ensures proper identification and handling within the system. For accurate command execution, always verify and include the correct <code>_multiplex_NR</code> postfix for the intended multiplexed subscription groups.</p>"}, {"location": "CLI/#limiting-topics-used-per-process", "title": "Limiting topics used per process", "text": "<p>Karafka supports having multiple topics within a single application. You can run multiple Karafka instances, specifying topics that should be running per each process using the <code>--include-topics</code> server flag as follows:</p> <pre><code>bundle exec karafka server --include-topics topic_name1,topic_name3\n</code></pre> <p>If you specify none, by default, all will run.</p> <p>You can also exclude certain topics by using the <code>--exclude-topics</code> flag:</p> <pre><code>bundle exec karafka server --exclude-topics topic_name2,topic_name5\n</code></pre>"}, {"location": "CLI/#karafka-swarm", "title": "Karafka Swarm", "text": "<p>Swarm has its own section. You can read about it here.</p>"}, {"location": "CLI/#declarative-topics", "title": "Declarative Topics", "text": "<p>Declarative Topics managament via the CLI has its own section. You can read about that here.</p>"}, {"location": "CLI/#routing-patterns", "title": "Routing Patterns", "text": "<p>Routing Patterns managament via the CLI has its own section. You can read about that here.</p> <p>Last modified: 2024-02-16 17:14:22</p>"}, {"location": "Changelog-Karafka-Core/", "title": "Karafka Core Changelog", "text": ""}, {"location": "Changelog-Karafka-Core/#252-2025-06-11", "title": "2.5.2 (2025-06-11)", "text": "<ul> <li>[Enhancement] Support <code>#unsubscribe</code>.</li> <li>[Enhancement] Allow for providing a root scope path for error keys.</li> <li>[Fix] Fix a bug where on no errors the result would be an array instead of a hash.</li> <li>[Fix] Fix spec hanging when Kafka cluster on 9092 is running.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#251-2025-05-23", "title": "2.5.1 (2025-05-23)", "text": "<ul> <li>[Change] Move to trusted-publishers and remove signing since no longer needed.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#250-2025-05-21", "title": "2.5.0 (2025-05-21)", "text": "<ul> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.19.2</code> to support new features.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#2411-2025-03-20", "title": "2.4.11 (2025-03-20)", "text": "<ul> <li>[Enhancement] Rename internal node <code>#name</code> method to <code>#node_name</code> so we can use <code>#name</code> as the attribute name.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#2410-2025-03-14", "title": "2.4.10 (2025-03-14)", "text": "<ul> <li>[Fix] Relax lock on <code>karafka-rdkafka</code></li> </ul>"}, {"location": "Changelog-Karafka-Core/#249-2025-03-03", "title": "2.4.9 (2025-03-03)", "text": "<ul> <li>[Enhancement] Remove <code>RspecLocator</code> dependency on <code>activesupport</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#248-2024-12-26", "title": "2.4.8 (2024-12-26)", "text": "<ul> <li>[Maintenance] Declare <code>logger</code> as a dependency.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#247-2024-11-26", "title": "2.4.7 (2024-11-26)", "text": "<ul> <li>[Fix] Make sure that <code>karafka-core</code> works with older versions of <code>karafka-rdkafka</code> that do not support macos fork mitigation.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#246-2024-11-26", "title": "2.4.6 (2024-11-26)", "text": "<ul> <li>[Enhancement] Mitigate macos forking issues when librdkafka is not loaded to memory.</li> <li>[Change] Allow <code>karafka-rdkafka</code> <code>0.18.0</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#245-2024-11-19", "title": "2.4.5 (2024-11-19)", "text": "<ul> <li>[Breaking] Drop Ruby <code>3.0</code> support according to the EOL schedule.</li> <li>[Enhancement] Support listeners inspection via <code>#listeners</code>.</li> <li>[Fix] Restore <code>#available_events</code> notifications bus method.</li> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.17.6</code> to support new features.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#244-2024-07-20", "title": "2.4.4 (2024-07-20)", "text": "<ul> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.16.0</code> to support new features and allow for <code>0.17.0</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#243-2024-06-18", "title": "2.4.3 (2024-06-18)", "text": "<ul> <li>[Fix] Use <code>Object</code> instead of <code>BasicObject</code> for rule result comparison because of Time mismatch with BasicObject.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#242-2024-06-17", "title": "2.4.2 (2024-06-17)", "text": "<ul> <li>[Enhancement] Allow <code>karafka-rdkafka</code> <code>0.16.x</code> to be used since API compatible.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#241-2024-06-17", "title": "2.4.1 (2024-06-17)", "text": "<ul> <li>[Enhancement] Provide fast-track for events without subscriptions to save on allocations.</li> <li>[Enhancement] Save memory allocation on each contract rule validation execution.</li> <li>[Enhancement] Save one allocation per <code>float_now</code> + 2-3x performance by using the Posix clock instead of <code>Time.now.utc.to_f</code>.</li> <li>[Enhancement] Use direct <code>float_millisecond</code> precision in <code>monotonic_now</code> not to multiply by 1000 (allocations and CPU savings).</li> <li>[Enhancement] Save one array allocation on one instrumentation.</li> <li>[Enhancement] Allow clearing one event type (dorner).</li> </ul>"}, {"location": "Changelog-Karafka-Core/#240-2024-04-26", "title": "2.4.0 (2024-04-26)", "text": "<ul> <li>[Breaking] Drop Ruby <code>2.7</code> support.</li> <li>[Enhancement] Provide necessary alterations for custom oauth token callbacks to operate.</li> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.15.0</code> to support new features.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#230-2024-01-26", "title": "2.3.0 (2024-01-26)", "text": "<ul> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.14.8</code> to support new features.</li> <li>[Change] Remove <code>concurrent-ruby</code> usage.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#227-2023-11-07", "title": "2.2.7 (2023-11-07)", "text": "<ul> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.13.9</code> to support alternative consumer builder.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#226-2023-11-03", "title": "2.2.6 (2023-11-03)", "text": "<ul> <li>[Enhancement] Set backtrace for errors propagated via the errors callbacks.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#225-2023-10-31", "title": "2.2.5 (2023-10-31)", "text": "<ul> <li>[Change] Drop support for Ruby 2.6 due to incompatibilities in usage of <code>ObjectSpace::WeakMap</code></li> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.13.8</code> to support consumer <code>#position</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#224-2023-10-25", "title": "2.2.4 (2023-10-25)", "text": "<ul> <li>[Enhancement] Allow for <code>lazy</code> evaluated constructors.</li> <li>[Enhancement] Allow no-arg constructors.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#223-2023-10-17", "title": "2.2.3 (2023-10-17)", "text": "<ul> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.13.6</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#222-2023-09-11", "title": "2.2.2 (2023-09-11)", "text": "<ul> <li>[Fix] Reuse previous frozen duration as a base for incoming computation.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#221-2023-09-10", "title": "2.2.1 (2023-09-10)", "text": "<ul> <li>Optimize statistics decorator by minimizing number of new objects created.</li> <li>Expand the decoration to include new value <code>_fd</code> providing freeze duration in milliseconds. This value informs us for how many consecutive ms the given value did not change. It can be useful for detecting values that should change once in a while but are stale.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#220-2023-09-01", "title": "2.2.0 (2023-09-01)", "text": "<ul> <li>[Maintenance] Update the signing cert (old expired)</li> </ul>"}, {"location": "Changelog-Karafka-Core/#211-2023-06-28", "title": "2.1.1 (2023-06-28)", "text": "<ul> <li>[Change] Set minimum <code>karafka-rdkafka</code> on <code>0.13.1</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#210-2023-06-19", "title": "2.1.0 (2023-06-19)", "text": "<ul> <li>[Change] Set <code>karafka-rdkafka</code> requirement from <code>&gt;= 0.13.0</code> to <code>&lt;= 0.14.0</code>.</li> <li>[Change] Remove no longer needed patch.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#2013-2023-05-26", "title": "2.0.13 (2023-05-26)", "text": "<ul> <li>Set minimum <code>karafka-rdkafka</code> on <code>0.12.3</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#2012-2023-02-23", "title": "2.0.12 (2023-02-23)", "text": "<ul> <li>Introduce ability to tag certain objects by including the <code>Karafka::Core::Taggable</code> module.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#2011-2023-02-12", "title": "2.0.11 (2023-02-12)", "text": "<ul> <li>Set minimum <code>karafka-rdkafka</code> on <code>0.12.1</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#2010-2023-02-01", "title": "2.0.10 (2023-02-01)", "text": "<ul> <li>Move <code>RspecLocator</code> to core.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#209-2023-01-11", "title": "2.0.9 (2023-01-11)", "text": "<ul> <li>Use <code>karafka-rdkafka</code> instead of <code>rdkafka</code>. This change is needed to ensure that all consecutive releases are stable and compatible.</li> <li>Relax Ruby requirement to <code>2.6</code>. It does not mean we officially support it but it may work. Go to Versions Lifecycle and EOL for more details.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#208-2023-01-07", "title": "2.0.8 (2023-01-07)", "text": "<ul> <li>Add <code>Karafka::Core::Helpers::Time</code> utility for time reporting.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#207-2022-12-18", "title": "2.0.7 (2022-12-18)", "text": "<ul> <li>Allow for recompilation of config upon injecting new config nodes.</li> <li>Compile given config scope automatically after it is defined.</li> <li>Support sub-config merging via their nested definitions.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#206-2022-12-07", "title": "2.0.6 (2022-12-07)", "text": "<ul> <li>Reverse node compilation state tracking removal.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#205-2022-12-07", "title": "2.0.5 (2022-12-07)", "text": "<ul> <li>Move <code>librdkafka</code> generic (producer and consumer) patches from WaterDrop here.</li> <li>Move dependency on <code>librdkafka</code> here from both Karafka and WaterDrop to unify management.</li> <li>Move <code>CallbacksManager</code> from WaterDrop because it's shared.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#204-2022-11-20", "title": "2.0.4 (2022-11-20)", "text": "<ul> <li>Disallow publishing events that were not registered.</li> <li>Fix a potential race condition when adding listeners concurrently from multiple threads.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#203-2022-10-13", "title": "2.0.3 (2022-10-13)", "text": "<ul> <li>Maintenance release. Cert chain update. No code changes.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#202-2022-08-01", "title": "2.0.2 (2022-08-01)", "text": "<ul> <li>Add extracted statistics decorator (#932)</li> </ul>"}, {"location": "Changelog-Karafka-Core/#201-2022-07-30", "title": "2.0.1 (2022-07-30)", "text": "<ul> <li>Fix a case where setting would match a method monkey-patched on an object (#1) causing initializers not to build proper accessors on nodes. This is not the core bug, but still worth handling this case.</li> </ul>"}, {"location": "Changelog-Karafka-Core/#200-2022-07-28", "title": "2.0.0 (2022-07-28)", "text": "<ul> <li>Initial extraction of common components used in the Karafka ecosystem from WaterDrop.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/", "title": "Rdkafka Changelog", "text": ""}, {"location": "Changelog-Karafka-Rdkafka/#0200-unreleased", "title": "0.20.0 (Unreleased)", "text": "<ul> <li>[Feature] Add precompiled <code>x86_64-linux-gnu</code> setup.</li> <li>[Feature] Add precompiled <code>x86_64-linux-musl</code> setup.</li> <li>[Feature] Add precompiled <code>macos_arm64</code> setup.</li> <li>[Enhancement] Run all specs on each of the platforms with and without precompilation.</li> <li>[Enhancement] Support transactional id in the ACL API.</li> <li>[Fix] Fix a case where using empty key on the <code>musl</code> architecture would cause a segfault.</li> <li>[Fix] Fix for null pointer reference bypass on empty string being too wide causing segfault.</li> </ul> <p>Note: Precompiled extensions are a new feature in this release. While they significantly improve installation speed and reduce build dependencies, they should be thoroughly tested in your staging environment before deploying to production. If you encounter any issues with precompiled extensions, you can fall back to building from sources.</p>"}, {"location": "Changelog-Karafka-Rdkafka/#0195-2025-05-30", "title": "0.19.5 (2025-05-30)", "text": "<ul> <li>[Enhancement] Allow for producing to non-existing topics with <code>key</code> and <code>partition_key</code> present.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0194-2025-05-23", "title": "0.19.4 (2025-05-23)", "text": "<ul> <li>[Change] Move to trusted-publishers and remove signing since no longer needed.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0193-2025-05-23", "title": "0.19.3 (2025-05-23)", "text": "<ul> <li>[Enhancement] Include broker message in the error full message if provided.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0192-2025-05-20", "title": "0.19.2 (2025-05-20)", "text": "<ul> <li>[Enhancement] Replace TTL-based partition count cache with a global cache that reuses <code>librdkafka</code> statistics data when possible.</li> <li>[Enhancement] Roll out experimental jruby support.</li> <li>[Fix] Fix issue where post-closed producer C topics refs would not be cleaned.</li> <li>[Fix] Fiber causes Segmentation Fault.</li> <li>[Change] Move to trusted-publishers and remove signing since no longer needed.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0191-2025-04-07", "title": "0.19.1 (2025-04-07)", "text": "<ul> <li>[Enhancement] Support producing and consuming of headers with mulitple values (KIP-82).</li> <li>[Enhancement] Allow native Kafka customization poll time.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0190-2025-01-20", "title": "0.19.0 (2025-01-20)", "text": "<ul> <li>[Breaking] Deprecate and remove <code>#each_batch</code> due to data consistency concerns.</li> <li>[Enhancement] Bump librdkafka to 2.8.0</li> <li>[Fix] Restore <code>Rdkafka::Bindings.rd_kafka_global_init</code> as it was not the source of the original issue.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0181-2024-12-04", "title": "0.18.1 (2024-12-04)", "text": "<ul> <li>[Fix] Do not run <code>Rdkafka::Bindings.rd_kafka_global_init</code> on require to prevent some of macos versions from hanging on Puma fork.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0180-2024-11-26", "title": "0.18.0 (2024-11-26)", "text": "<ul> <li>[Breaking] Drop Ruby 3.0 support</li> <li>[Enhancement] Bump librdkafka to 2.6.1</li> <li>[Enhancement] Use default oauth callback if none is passed (bachmanity1)</li> <li>[Enhancement] Expose <code>rd_kafka_global_init</code> to mitigate macos forking issues.</li> <li>[Patch] Retire no longer needed cooperative-sticky patch.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0176-2024-09-03", "title": "0.17.6 (2024-09-03)", "text": "<ul> <li>[Fix] Fix incorrectly behaving CI on failures. </li> <li>[Fix] Fix invalid patches librdkafka references.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0175-2024-09-03", "title": "0.17.5 (2024-09-03)", "text": "<ul> <li>[Patch] Patch with \"Add forward declaration to fix compilation without ssl\" fix</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0174-2024-09-02", "title": "0.17.4 (2024-09-02)", "text": "<ul> <li>[Enhancement] Bump librdkafka to 2.5.3</li> <li>[Enhancement] Do not release GVL on <code>rd_kafka_name</code> (ferrous26)</li> <li>[Fix] Fix unused variable reference in producer (lucasmvnascimento)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0173-2024-08-09", "title": "0.17.3 (2024-08-09)", "text": "<ul> <li>[Fix] Mitigate a case where FFI would not restart the background events callback dispatcher in forks.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0172-2024-08-07", "title": "0.17.2 (2024-08-07)", "text": "<ul> <li>[Enhancement] Support returning <code>#details</code> for errors that do have topic/partition related extra info.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0171-2024-08-01", "title": "0.17.1 (2024-08-01)", "text": "<ul> <li>[Enhancement] Support ability to release patches to librdkafka.</li> <li>[Patch] Patch cooperative-sticky assignments in librdkafka.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0170-2024-07-21", "title": "0.17.0 (2024-07-21)", "text": "<ul> <li>[Enhancement] Bump librdkafka to 2.5.0</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0161-2024-07-10", "title": "0.16.1 (2024-07-10)", "text": "<ul> <li>[Feature] Add <code>#seek_by</code> to be able to seek for a message by topic, partition and offset (zinahia)</li> <li>[Change] Remove old producer timeout API warnings.</li> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0160-2024-06-17", "title": "0.16.0 (2024-06-17)", "text": "<ul> <li>[Breaking] Messages without headers returned by <code>#poll</code> contain frozen empty hash.</li> <li>[Breaking] <code>HashWithSymbolKeysTreatedLikeStrings</code> has been removed so headers are regular hashes with string keys.</li> <li>[Enhancement] Bump librdkafka to 2.4.0</li> <li>[Enhancement] Save two objects on message produced and lower CPU usage on message produced with small improvements.</li> <li>[Fix] Remove support for Ruby 2.7. Supporting it was a bug since rest of the karafka ecosystem no longer supports it.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0152-2024-07-10", "title": "0.15.2 (2024-07-10)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0151-2024-05-09", "title": "0.15.1 (2024-05-09)", "text": "<ul> <li>[Feature] Provide ability to use topic config on a producer for custom behaviors per dispatch.</li> <li>[Enhancement] Use topic config reference cache for messages production to prevent topic objects allocation with each message.</li> <li>[Enhancement] Provide <code>Rrdkafka::Admin#describe_errors</code> to get errors descriptions (mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0150-2024-04-26", "title": "0.15.0 (2024-04-26)", "text": "<ul> <li>[Feature] Oauthbearer token refresh callback (bruce-szalwinski-he)</li> <li>[Feature] Support incremental config describe + alter API (mensfeld)</li> <li>[Enhancement] name polling Thread as <code>rdkafka.native_kafka#&lt;name&gt;</code> (nijikon)</li> <li>[Enhancement] Replace time poll based wait engine with an event based to improve response times on blocking operations and wait (nijikon + mensfeld)</li> <li>[Enhancement] Allow for usage of the second regex engine of librdkafka by setting <code>RDKAFKA_DISABLE_REGEX_EXT</code> during build (mensfeld)</li> <li>[Enhancement] name polling Thread as <code>rdkafka.native_kafka#&lt;name&gt;</code> (nijikon)</li> <li>[Change] Allow for native kafka thread operations deferring and manual start for consumer, producer and admin.</li> <li>[Change] The <code>wait_timeout</code> argument in <code>AbstractHandle.wait</code> method is deprecated and will be removed in future versions without replacement. We don't rely on it's value anymore (nijikon)</li> <li>[Fix] Fix bogus case/when syntax. Levels 1, 2, and 6 previously defaulted to UNKNOWN (jjowdy)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#01411-2024-07-10", "title": "0.14.11 (2024-07-10)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#01410-2024-02-08", "title": "0.14.10 (2024-02-08)", "text": "<ul> <li>[Fix] Background logger stops working after forking causing memory leaks (mensfeld).</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0149-2024-01-29", "title": "0.14.9 (2024-01-29)", "text": "<ul> <li>[Fix] Partition cache caches invalid <code>nil</code> result for <code>PARTITIONS_COUNT_TTL</code>.</li> <li>[Enhancement] Report <code>-1</code> instead of <code>nil</code> in case <code>partition_count</code> failure.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0148-2024-01-24", "title": "0.14.8 (2024-01-24)", "text": "<ul> <li>[Enhancement] Provide support for Nix OS (alexandriainfantino)</li> <li>[Enhancement] Skip intermediate array creation on delivery report callback execution (one per message) (mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0147-2023-12-29", "title": "0.14.7 (2023-12-29)", "text": "<ul> <li>[Fix] Recognize that Karafka uses a custom partition object (fixed in 2.3.0) and ensure it is recognized.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0146-2023-12-29", "title": "0.14.6 (2023-12-29)", "text": "<ul> <li>[Feature] Support storing metadata alongside offsets via <code>rd_kafka_offsets_store</code> in <code>#store_offset</code> (mensfeld)</li> <li>[Enhancement] Increase the <code>#committed</code> default timeout from 1_200ms to 2000ms. This will compensate for network glitches and remote clusters operations and will align with metadata query timeout.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0145-2023-12-20", "title": "0.14.5 (2023-12-20)", "text": "<ul> <li>[Enhancement] Provide <code>label</code> producer handler and report reference for improved traceability.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0144-2023-12-19", "title": "0.14.4 (2023-12-19)", "text": "<ul> <li>[Enhancement] Add ability to store offsets in a transaction (mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0143-2023-12-17", "title": "0.14.3 (2023-12-17)", "text": "<ul> <li>[Enhancement] Replace <code>rd_kafka_offset_store</code> with <code>rd_kafka_offsets_store</code> (mensfeld)</li> <li>[Fix] Missing ACL <code>RD_KAFKA_RESOURCE_BROKER</code> constant reference (mensfeld)</li> <li>[Change] Rename <code>matching_acl_pattern_type</code> to <code>matching_acl_resource_pattern_type</code> to align the whole API (mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0142-2023-12-11", "title": "0.14.2 (2023-12-11)", "text": "<ul> <li>[Enhancement] Alias <code>topic_name</code> as <code>topic</code> in the delivery report (mensfeld)</li> <li>[Fix] Fix return type on <code>#rd_kafka_poll</code> (mensfeld)</li> <li>[Fix] <code>uint8_t</code> does not exist on Apple Silicon (mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0141-2023-12-02", "title": "0.14.1 (2023-12-02)", "text": "<ul> <li>[Feature] Add <code>Admin#metadata</code> (mensfeld)</li> <li>[Feature] Add <code>Admin#create_partitions</code> (mensfeld)</li> <li>[Feature] Add <code>Admin#delete_group</code> utility (piotaixr)</li> <li>[Feature] Add Create and Delete ACL Feature To Admin Functions (vgnanasekaran)</li> <li>[Enhancement] Improve error reporting on <code>unknown_topic_or_part</code> and include missing topic (mensfeld)</li> <li>[Enhancement] Improve error reporting on consumer polling errors (mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0140-2023-11-17", "title": "0.14.0 (2023-11-17)", "text": "<ul> <li>[Enhancement] Bump librdkafka to 2.3.0</li> <li>[Enhancement] Increase the <code>#lag</code> and <code>#query_watermark_offsets</code> default timeouts from 100ms to 1000ms. This will compensate for network glitches and remote clusters operations.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#01310-2024-07-10", "title": "0.13.10 (2024-07-10)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0139-2023-11-07", "title": "0.13.9 (2023-11-07)", "text": "<ul> <li>[Enhancement] Expose alternative way of managing consumer events via a separate queue.</li> <li>[Enhancement] Allow for setting <code>statistics_callback</code> as nil to reset predefined settings configured by a different gem.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0138-2023-10-31", "title": "0.13.8 (2023-10-31)", "text": "<ul> <li>[Enhancement] Get consumer position (thijsc &amp; mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0137-2023-10-31", "title": "0.13.7 (2023-10-31)", "text": "<ul> <li>[Change] Drop support for Ruby 2.6 due to incompatibilities in usage of <code>ObjectSpace::WeakMap</code></li> <li>[Fix] Fix dangling Opaque references.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0136-2023-10-17", "title": "0.13.6 (2023-10-17)", "text": "<ul> <li>[Feature] Support transactions API in the producer</li> <li>[Enhancement] Add <code>raise_response_error</code> flag to the <code>Rdkafka::AbstractHandle</code>.</li> <li>[Enhancement] Provide <code>#purge</code> to remove any outstanding requests from the producer.</li> <li>[Enhancement] Fix <code>#flush</code> does not handle the timeouts errors by making it return true if all flushed or false if failed. We do not raise an exception here to keep it backwards compatible.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0135", "title": "0.13.5", "text": "<ul> <li>Fix DeliveryReport <code>create_result#error</code> being nil despite an error being associated with it</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0134", "title": "0.13.4", "text": "<ul> <li>Always call initial poll on librdkafka to make sure oauth bearer cb is handled pre-operations.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0133", "title": "0.13.3", "text": "<ul> <li>Bump librdkafka to 2.2.0</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0132", "title": "0.13.2", "text": "<ul> <li>Ensure operations counter decrement is fully thread-safe</li> <li>Bump librdkafka to 2.1.1</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0131", "title": "0.13.1", "text": "<ul> <li>Add offsets_for_times method on consumer (timflapper)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0130-2023-07-24", "title": "0.13.0 (2023-07-24)", "text": "<ul> <li>Support cooperative sticky partition assignment in the rebalance callback (methodmissing)</li> <li>Support both string and symbol header keys (ColinDKelley)</li> <li>Handle tombstone messages properly (kgalieva)</li> <li>Add topic name to delivery report (maeve)</li> <li>Allow string partitioner config (mollyegibson)</li> <li>Fix documented type for DeliveryReport#error (jimmydo)</li> <li>Bump librdkafka to 2.0.2 (lmaia)</li> <li>Use finalizers to cleanly exit producer and admin (thijsc)</li> <li>Lock access to the native kafka client (thijsc)</li> <li>Fix potential race condition in multi-threaded producer (mensfeld)</li> <li>Fix leaking FFI resources in specs (mensfeld)</li> <li>Improve specs stability (mensfeld)</li> <li>Make metadata request timeout configurable (mensfeld)</li> <li>call_on_partitions_assigned and call_on_partitions_revoked only get a tpl passed in (thijsc)</li> <li>Support <code>#assignment_lost?</code> on a consumer to check for involuntary assignment revocation (mensfeld)</li> <li>Expose <code>#name</code> on the consumer and producer (mensfeld)</li> <li>Introduce producer partitions count metadata cache (mensfeld)</li> <li>Retry metadta fetches on certain errors with a backoff (mensfeld)</li> <li>Do not lock access to underlying native kafka client and rely on Karafka granular locking (mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0124-2024-07-10", "title": "0.12.4 (2024-07-10)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0123", "title": "0.12.3", "text": "<ul> <li>Include backtrace in non-raised binded errors.</li> <li>Include topic name in the delivery reports</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0122", "title": "0.12.2", "text": "<ul> <li>Increase the metadata default timeout from 250ms to 2 seconds. This should allow for working with remote clusters.</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0121", "title": "0.12.1", "text": "<ul> <li>Bumps librdkafka to 2.0.2 (lmaia)</li> <li>Add support for adding more partitions via Admin API</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0120-2022-06-17", "title": "0.12.0 (2022-06-17)", "text": "<ul> <li>Bumps librdkafka to 1.9.0</li> <li>Fix crash on empty partition key (mensfeld)</li> <li>Pass the delivery handle to the callback (gvisokinskas)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0110-2021-11-17", "title": "0.11.0 (2021-11-17)", "text": "<ul> <li>Upgrade librdkafka to 1.8.2</li> <li>Bump supported minimum Ruby version to 2.6</li> <li>Better homebrew path detection</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#0100-2021-09-07", "title": "0.10.0 (2021-09-07)", "text": "<ul> <li>Upgrade librdkafka to 1.5.0</li> <li>Add error callback config</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#090-2021-06-23", "title": "0.9.0 (2021-06-23)", "text": "<ul> <li>Fixes for Ruby 3.0</li> <li>Allow any callable object for callbacks (gremerritt)</li> <li>Reduce memory allocations in Rdkafka::Producer#produce (jturkel)</li> <li>Use queue as log callback to avoid unsafe calls from trap context (breunigs)</li> <li>Allow passing in topic configuration on create_topic (dezka)</li> <li>Add each_batch method to consumer (mgrosso)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#081-2020-12-07", "title": "0.8.1 (2020-12-07)", "text": "<ul> <li>Fix topic_flag behaviour and add tests for Metadata (geoff2k)</li> <li>Add topic admin interface (geoff2k)</li> <li>Raise an exception if @native_kafka is nil (geoff2k)</li> <li>Option to use zstd compression (jasonmartens)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#080-2020-06-02", "title": "0.8.0 (2020-06-02)", "text": "<ul> <li>Upgrade librdkafka to 1.4.0</li> <li>Integrate librdkafka metadata API and add partition_key (by Adithya-copart)</li> <li>Ruby 2.7 compatibility fix (by Geoff The\u0301)A</li> <li>Add error to delivery report (by Alex Stanovsky)</li> <li>Don't override CPPFLAGS and LDFLAGS if already set on Mac (by Hiroshi Hatake)</li> <li>Allow use of Rake 13.x and up (by Tomasz Pajor)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#070-2019-09-21", "title": "0.7.0 (2019-09-21)", "text": "<ul> <li>Bump librdkafka to 1.2.0 (by rob-as)</li> <li>Allow customizing the wait time for delivery report availability (by mensfeld)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#060-2019-07-23", "title": "0.6.0 (2019-07-23)", "text": "<ul> <li>Bump librdkafka to 1.1.0 (by Chris Gaffney)</li> <li>Implement seek (by breunigs)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#050-2019-04-11", "title": "0.5.0 (2019-04-11)", "text": "<ul> <li>Bump librdkafka to 1.0.0 (by breunigs)</li> <li>Add cluster and member information (by dmexe)</li> <li>Support message headers for consumer &amp; producer (by dmexe)</li> <li>Add consumer rebalance listener (by dmexe)</li> <li>Implement pause/resume partitions (by dmexe)</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#042-2019-01-12", "title": "0.4.2 (2019-01-12)", "text": "<ul> <li>Delivery callback for producer</li> <li>Document list param of commit method</li> <li>Use default Homebrew openssl location if present</li> <li>Consumer lag handles empty topics</li> <li>End iteration in consumer when it is closed</li> <li>Add support for storing message offsets</li> <li>Add missing runtime dependency to rake</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#041-2018-10-19", "title": "0.4.1 (2018-10-19)", "text": "<ul> <li>Bump librdkafka to 0.11.6</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#040-2018-09-24", "title": "0.4.0 (2018-09-24)", "text": "<ul> <li>Improvements in librdkafka archive download</li> <li>Add global statistics callback</li> <li>Use Time for timestamps, potentially breaking change if you   rely on the previous behavior where it returns an integer with   the number of milliseconds.</li> <li>Bump librdkafka to 0.11.5</li> <li>Implement TopicPartitionList in Ruby so we don't have to keep   track of native objects.</li> <li>Support committing a topic partition list</li> <li>Add consumer assignment method</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#035-2018-01-17", "title": "0.3.5 (2018-01-17)", "text": "<ul> <li>Fix crash when not waiting for delivery handles</li> <li>Run specs on Ruby 2.5</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#034-2017-12-05", "title": "0.3.4 (2017-12-05)", "text": "<ul> <li>Bump librdkafka to 0.11.3</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#033-2017-10-27", "title": "0.3.3 (2017-10-27)", "text": "<ul> <li>Fix bug that prevent display of <code>RdkafkaError</code> message</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#032-2017-10-25", "title": "0.3.2 (2017-10-25)", "text": "<ul> <li><code>add_topic</code> now supports using a partition count</li> <li>Add way to make errors clearer with an extra message</li> <li>Show topics in subscribe error message</li> <li>Show partition and topic in query watermark offsets error message</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#031-2017-10-23", "title": "0.3.1 (2017-10-23)", "text": "<ul> <li>Bump librdkafka to 0.11.1</li> <li>Officially support ranges in <code>add_topic</code> for topic partition list.</li> <li>Add consumer lag calculator</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#030-2017-10-17", "title": "0.3.0 (2017-10-17)", "text": "<ul> <li>Move both add topic methods to one <code>add_topic</code> in <code>TopicPartitionList</code></li> <li>Add committed offsets to consumer</li> <li>Add query watermark offset to consumer</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#020-2017-10-13", "title": "0.2.0 (2017-10-13)", "text": "<ul> <li>Some refactoring and add inline documentation</li> </ul>"}, {"location": "Changelog-Karafka-Rdkafka/#01x-2017-09-10", "title": "0.1.x (2017-09-10)", "text": "<ul> <li>Initial working version including producing and consuming</li> </ul>"}, {"location": "Changelog-Karafka-Testing/", "title": "Karafka Testing Changelog", "text": ""}, {"location": "Changelog-Karafka-Testing/#251-2025-05-25", "title": "2.5.1 (2025-05-25)", "text": "<ul> <li>[Change] Move to trusted-publishers and remove signing since no longer needed.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#250-2025-05-21", "title": "2.5.0 (2025-05-21)", "text": "<ul> <li>[Maintenance] Release matching Karafka <code>2.5.0</code> release.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#247-2025-04-01", "title": "2.4.7 (2025-04-01)", "text": "<ul> <li>[Breaking] Drop Ruby <code>3.0</code> support according to the EOL schedule.</li> <li>[Fix] Check not only that <code>Mocha</code> is loaded but also that its stubs are used.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#246-2024-07-31", "title": "2.4.6 (2024-07-31)", "text": "<ul> <li>[Fix] uninitialized constant <code>Karafka::Testing::Errors::ConsumerGroupNotFound</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#245-2024-07-21", "title": "2.4.5 (2024-07-21)", "text": "<ul> <li>[Enhancement] Provide <code>karafka.consumer_messages</code> to get (or alter) the messages that will go into created consumer.</li> <li>[Fix] <code>#consumer_group_metadata_pointer</code> is not stubbed in the consumer client.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#244-2024-07-02", "title": "2.4.4 (2024-07-02)", "text": "<ul> <li>[Enhancement] Memoize <code>consumer_for</code> so consumers can be set up for multiple topics and <code>let(:consumer)</code> is no longer a requirement. (dorner)</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#243-2024-05-06", "title": "2.4.3 (2024-05-06)", "text": "<ul> <li>[Fix] Fix: raw_key is not being assigned for rspec (CaioPenhalver)</li> <li>[Fix] Fix: raw_key is not being assigned for minitest</li> <li>[Fix] Fix: headers is not being assigned for minitest and rspec</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#242-2024-04-30", "title": "2.4.2 (2024-04-30)", "text": "<ul> <li>[Fix] Fix FrozenError when accessing key and headers in <code>Karafka::Messages::Metadata</code> (tldn0718)</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#241-2024-04-29", "title": "2.4.1 (2024-04-29)", "text": "<ul> <li>[Fix] Fix instance variable in minitest helper (tldn0718)</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#240-2024-04-26", "title": "2.4.0 (2024-04-26)", "text": "<ul> <li>[Breaking] Drop Ruby <code>2.7</code> support.</li> <li>[Refactor] Extract common components for Minitest and RSpec.</li> <li>[Fix] Support again <code>require: false</code> on gem loading.</li> <li>[Fix] Fix a case where multiplexed SG would fail with <code>TopicInManyConsumerGroupsError</code></li> </ul>"}, {"location": "Changelog-Karafka-Testing/#232-2024-04-03", "title": "2.3.2 (2024-04-03)", "text": "<ul> <li>[Enhancement] Support <code>Minitest::Spec</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#231-2024-03-07", "title": "2.3.1 (2024-03-07)", "text": "<ul> <li>[Enhancement] Prevent usage of testing when Karafka is not loaded.</li> <li>[Enhancement] Prevent usage of testing when Karafka is not initialized.</li> <li>[Fix] Same CG multiplexing topic builder fails with <code>TopicInManyConsumerGroupsError</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#230-2024-01-26", "title": "2.3.0 (2024-01-26)", "text": "<ul> <li>[Maintenance] Release matching Karafka <code>2.3.0</code> release.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#222-2023-11-16", "title": "2.2.2 (2023-11-16)", "text": "<ul> <li>[Feature] Provide support for Minitest (ValentinoRusconi-EH)</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#221-2023-10-26", "title": "2.2.1 (2023-10-26)", "text": "<ul> <li>[Enhancement] Support patterns in <code>#consumer_for</code> consumer builder.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#220-2023-09-01", "title": "2.2.0 (2023-09-01)", "text": "<ul> <li>[Maintenance] Ensure that <code>2.2.0</code> works with consumers for patterns.</li> <li>[Maintenance] Replace signing key with a new one (old expired).</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#216-2023-08-06", "title": "2.1.6 (2023-08-06)", "text": "<ul> <li>[Enhancement] Make <code>#used?</code> API always return true.</li> <li>[Enhancement] Expand dummy client API with #seek.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#215-2023-07-22", "title": "2.1.5 (2023-07-22)", "text": "<ul> <li>[Enhancement] User <code>prepend_before</code> instead of <code>prepend</code> for RSpec (ojab)</li> <li>[Enhancement] Add support for client <code>#commit_offsets</code> and <code>#commit_offsets!</code> stubs.</li> <li>[Fix] Make sure that <code>#mark_as_consumed!</code> and <code>#mark_as_consumed</code> return true.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#214-2023-06-20", "title": "2.1.4 (2023-06-20)", "text": "<ul> <li>[Fix] Fix invalid consumer group assignment for consumers created for non-default consumer group when same topic is being used multiple times.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#213-2023-06-19", "title": "2.1.3 (2023-06-19)", "text": "<ul> <li>[Enhancement] Align with Karafka <code>2.1.5</code> API.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#212-2023-06-13", "title": "2.1.2 (2023-06-13)", "text": "<ul> <li>[Enhancement] Depend on WaterDrop <code>&gt;=</code> <code>2.6.0</code> directly and not via Karafka to make sure correct version is used.</li> <li>[Fix] Use proper WaterDrop <code>&gt;=</code> <code>2.6.0</code> buffered client reference.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#211-2023-06-07", "title": "2.1.1 (2023-06-07)", "text": "<ul> <li>[Enhancement] Support WaterDrop stubs with Mocha.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#210-2023-05-22", "title": "2.1.0 (2023-05-22)", "text": "<ul> <li>[Maintenance] Align Karafka expectations to match <code>2.1.0</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#2011-2023-04-13", "title": "2.0.11 (2023-04-13)", "text": "<ul> <li>Align metadata builder format with Karafka <code>2.0.40</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#2010-2023-04-11", "title": "2.0.10 (2023-04-11)", "text": "<ul> <li>Align with changes in Karafka <code>2.0.39</code></li> <li>Replace direct <code>described_class</code> reference for consumer building with <code>topic.consumer</code> Karafka routing based one. This change will allow for usage of <code>karafka.consumer_for</code> from any specs.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#209-2023-02-10", "title": "2.0.9 (2023-02-10)", "text": "<ul> <li>Inject consumer strategy to the test consumer instance.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#208-2022-11-03", "title": "2.0.8 (2022-11-03)", "text": "<ul> <li>Do not lock Ruby and rely on <code>karafka-core</code> via <code>karafka</code>.</li> <li>Due to changes in the engine, lock to <code>2.0.20</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#207-2022-11-03", "title": "2.0.7 (2022-11-03)", "text": "<ul> <li>Release version with cert with valid access permissions (#114).</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#206-2022-10-26", "title": "2.0.6 (2022-10-26)", "text": "<ul> <li>Replace the <code>subject</code> reference with named <code>consumer</code> reference.</li> <li>Do not forward sent messages to <code>consumer</code> unless it's a Karafka consumer.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#205-2022-10-19", "title": "2.0.5 (2022-10-19)", "text": "<ul> <li>Fix for: Test event production without defining a subject (#102)</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#204-2022-10-14", "title": "2.0.4 (2022-10-14)", "text": "<ul> <li>Align changes with Karafka <code>2.0.13</code> changes.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#203-2022-09-30", "title": "2.0.3 (2022-09-30)", "text": "<ul> <li>Fix direct name reference <code>consumer</code> instead of <code>subject</code> (#97).</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#202-2022-09-29", "title": "2.0.2 (2022-09-29)", "text": "<ul> <li>Provide multi-consumer group testing support (#92)</li> <li>Fail fast if requested topic is present in multiple consumer groups but consumer group is not specified.</li> <li>Allow for usage of <code>Karafka.producer</code> directly and make it buffered.</li> <li>Rename <code>karafka.publish</code> to <code>karafka.produce</code> to align naming conventions [breaking change].</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#201-2022-08-05", "title": "2.0.1 (2022-08-05)", "text": "<ul> <li>Require non rc version of Karafka.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#200-2022-08-01", "title": "2.0.0 (2022-08-01)", "text": "<ul> <li>No changes. Just non-rc release.</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#200rc1-2022-07-19", "title": "2.0.0.rc1 (2022-07-19)", "text": "<ul> <li>Require Karafka <code>2.0.0.rc2</code></li> </ul>"}, {"location": "Changelog-Karafka-Testing/#200alpha4-2022-07-06", "title": "2.0.0.alpha4 (2022-07-06)", "text": "<ul> <li>Require Karafka <code>2.0.0.beta5</code> and fix non-existing coordinator reference</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#200alpha3-2022-03-14", "title": "2.0.0.alpha3 (2022-03-14)", "text": "<ul> <li>Provide support for referencing producer from consumer</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#200alpha2-2022-02-19", "title": "2.0.0.alpha2 (2022-02-19)", "text": "<ul> <li>Add <code>rubygems_mfa_required</code></li> </ul>"}, {"location": "Changelog-Karafka-Testing/#200alpha1-2022-01-30", "title": "2.0.0.alpha1 (2022-01-30)", "text": "<ul> <li>Change the API to be more comprehensive</li> <li>Update to work with Karafka 2.0</li> <li>Support for Ruby 3.1</li> <li>Drop support for ruby 2.6</li> </ul>"}, {"location": "Changelog-Karafka-Testing/#14", "title": "1.4.*", "text": "<p>If you are looking for the changelog of <code>1.4</code>, please go here.</p>"}, {"location": "Changelog-Karafka-Web-UI/", "title": "Karafka Web Changelog", "text": ""}, {"location": "Changelog-Karafka-Web-UI/#0111-2025-06-23", "title": "0.11.1 (2025-06-23)", "text": "<ul> <li>[Fix] Extremely high error turnover from hundreds of partitions can cause a deadlock in the reporter for transactional Web producer.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#0110-2025-06-15", "title": "0.11.0 (2025-06-15)", "text": "<ul> <li>[Feature] Provide ability to pause/resume partitions on running consumers via the UI (Pro).</li> <li>[Feature] Provide ability to edit offsets of running consumers (Pro).</li> <li>[Feature] Support consumers that have mismatching schema in the Status page.</li> <li>[Feature] Provide ability to navigate to a timestamp in the Explorer (Pro).</li> <li>[Feature] Provide ability to create and delete topics from the Web UI (Pro).</li> <li>[Feature] Provide ability to manage topics configuration from the Web UI (Pro).</li> <li>[Feature] Provide ability to manage topics partitioning from the Web UI (Pro).</li> <li>[Feature] Provide ability to inject custom CSS and JS to adjust the Web UI.</li> <li>[Enhancement] Support KIP-82 (header values of arrays).</li> <li>[Enhancement] Include crawl-based link validator to the CI to ensure no dead links are generated.</li> <li>[Enhancement] Allow for custom links in the navigation (Pro).</li> <li>[Enhancement] Optimize topic specific lookups (Pro).</li> <li>[Enhancement] Replace simple in-process metadata cache with user tracking version for multi-process deployments improvements.</li> <li>[Enhancement] Move web ui topics configuration into config.</li> <li>[Enhancement] Upgrade DaisyUI to 5.0 and Tailwind to 4.0.</li> <li>[Enhancement] Make consumer sampler/stats gathering compatible across debian/alpine/wolfi OSes (chen-anders)</li> <li>[Enhancement] Promote consumers lags statistics chart to OSS.</li> <li>[Enhancement] Promote consumers RSS statistics chart to OSS.</li> <li>[Enhancement] Remove state cache usage that complicated ability to manage topics.</li> <li>[Enhancement] Improve flash messages.</li> <li>[Enhancement] Improve handling of post-submit redirects.</li> <li>[Enhancement] Provide better support for fully transactional consumers.</li> <li>[Enhancement] Error out when <code>#setup</code> is called after <code>#enable!</code>.</li> <li>[Enhancement] Use more performant Kafka API calls to describe topics.</li> <li>[Enhancement] Inject <code>.action-NAME</code> and <code>.controller-NAME</code> body classes for usage with custom CSS and JS.</li> <li>[Enhancement] Improve error handling in the commanding iterator listener (Pro).</li> <li>[Enhancement] Introduce <code>trace_id</code> to the errors tracked for DLQ correlation (if in use) (Pro).</li> <li>[Enhancement] Normalize how topics with partitions data is being displayed (<code>topic-[0,1,2]</code> etc).</li> <li>[Change] Do not fully hide config-disabled features but make them disabled.</li> <li>[Change] Remove per-consumer process duplicated details from Subscriptions and Jobs tabs.</li> <li>[Change] Move to trusted-publishers and remove signing since no longer needed.</li> <li>[Refactor] Make sure all temporary topics have a <code>it-</code> prefix in their name.</li> <li>[Refactor] Introduce a <code>bin/verify_topics_naming</code> script to ensure proper test topics naming convention.</li> <li>[Fix] Closest time based lookup redirect fails.</li> <li>[Fix] Fi incorrect error type in commanding listener from <code>web.controlling.controller.error</code> to <code>web.commanding.listener.error</code> (Pro).</li> <li>[Fix] Topic named messages collides with the explorer routes.</li> <li>[Fix] Fix a case where live poll button enabling would not immediately start refreshes.</li> <li>[Fix] Fix negative message deserialization allocation stats.</li> <li>[Fix] Fix incorrect background color in some of the alert notices.</li> <li>[Fix] Support dark mode in error pages.</li> <li>[Fix] Fix incorrect names in some of the tables headers.</li> <li>[Fix] Normalize position of commanding buttons in regards to other UI elements.</li> <li>[Fix] Fix incorrect indentation of some of the info messages.</li> <li>[Fix] Fix tables headers inconsistent alignments.</li> <li>[Fix] Fix incorrect warning box header color in the dark mode.</li> <li>[Fix] Fix missing breadcrumbs on the consumers overview page.</li> <li>[Fix] Fix a case where disabled buttons would be enabled back too early.</li> <li>[Fix] The recent page breadcrumbs and offset id are not refreshed on change.</li> <li>[Fix] Direct URL access with too big partition causes librdkafka crash.</li> <li>[Fix] Fix incorrect breadcrumbs for pending consumer jobs.</li> <li>[Fix] Allow for using default search matchers in Karafka Web UI topics including Errors.</li> <li>[Fix] Ensure that when flashes or alerts are visible, pages are not auto-refreshed (would cause them to dissapear).</li> <li>[Fix] Time selector in the explorer does not disappear after clicking out.</li> <li>[Fix] Tombstone message presentation epoch doesn't make sense.</li> <li>[Fix] Fix incorrectly displayed \"No jobs\" alert info.</li> <li>[Fix] Previous / next navigation in the explorer does not work when moving to transactional checkpoints.</li> <li>[Fix] Errors explorer does not work with transactional produced data.</li> <li>[Fix] Errors explorer in OSS does not have pagination.</li> <li>[Maintenance] Require <code>karafka-core</code> <code>&gt;= 2.4.8</code> and <code>karafka</code> <code>&gt;= 2.4.16</code>.</li> <li>[Maintenance] Update <code>AirDatepicker</code> to <code>3.6.0</code>.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#0104-2024-11-26", "title": "0.10.4 (2024-11-26)", "text": "<ul> <li>[Breaking] Drop Ruby <code>3.0</code> support according to the EOL schedule.</li> <li>[Enhancement] Extract producers tracking <code>sync_threshold</code> into an internal config.</li> <li>[Enhancement] Support complex Pro license loading strategies (Pro).</li> <li>[Enhancement] Change default <code>retention.ms</code> for the metrics topic to support Redpanda Cloud defaults (#450).</li> <li>[Enhancement] Include subscription group id in the consumers error tracking metadata.</li> <li>[Enhancement] Collect metadata details of low level client errors when error tracking.</li> <li>[Enhancement] Collect metadata details of low level listener errors when error tracking.</li> <li>[Fix] Toggle menu button post-turbo refresh stops working.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#0103-2024-09-17", "title": "0.10.3 (2024-09-17)", "text": "<ul> <li>[Feature] Introduce ability to brand Web UI with environment (Pro).</li> <li>[Enhancement] Provide assignment status in the routing (Pro).</li> <li>[Enhancement] Support schedule cancellation via Web UI.</li> <li>[Enhancement] Rename \"probing\" to \"tracing\" to better reflect what this commanding option does.</li> <li>[Fix] Fix not working primary and secondary alert styles.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#0102-2024-09-03", "title": "0.10.2 (2024-09-03)", "text": "<ul> <li>[Feature] Support Future Messages management (Pro).</li> <li>[Enhancement] Do not live-reload when form active.</li> <li>[Fix] Undefined method <code>deep_merge</code> for an instance of Hash.</li> <li>[Fix] Prevent live-polling on elements wrapped in a button.</li> <li>[Fix] Fix errors extractor failure on first message-less tick / eofed</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#0101-2024-08-23", "title": "0.10.1 (2024-08-23)", "text": "<ul> <li>[Feature] Support Recurring Tasks management (Pro).</li> <li>[Enhancement] Optimize command buttons so they occupy less space.</li> <li>[Enhancement] Improve tables headers capitalization.</li> <li>[Enhancement] Prevent live-polling when user hovers over actionable links to mitigate race conditions.</li> <li>[Fix] Fix partial lack of tables hover in daily mode.</li> <li>[Fix] Fix lack of tables hover in dark mode.</li> <li>[Fix] Normalize various tables types styling.</li> <li>[Fix] Fix ranges selectors position on wide screens.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#0100-2024-08-19", "title": "0.10.0 (2024-08-19)", "text": "<ul> <li>[Breaking] Rename and reorganize visibility filter to policies engine since it is not only about visibility.</li> <li>[Feature] Replace Bootstrap with with tailwind + DaisyUI.</li> <li>[Feature] Redesign the UI and move navigation to the left to make space for future features.</li> <li>[Feature] Support per request policies for inspection and operations limitation.</li> <li>[Feature] Provide Search capabilities in the Explorer (Pro).</li> <li>[Feature] Provide dark mode.</li> <li>[Enhancement] Set <code>enable.partition.eof</code> to <code>false</code> for Web UI consumer group as it is not needed.</li> <li>[Enhancement] Allow for configuration of extra <code>kafka</code> scope options for the Web UI consumer group.</li> <li>[Enhancement] Support Karafka <code>#eofed</code> consumer action.</li> <li>[Enhancement] Provide topics watermarks inspection page (Pro).</li> <li>[Enhancement] Use Turbo to improve usability.</li> <li>[Enhancement] Round poll age reporting to precision of 2 reducing the payload size.</li> <li>[Enhancement] Round utilization reporting to precision of 2 reducing the payload size.</li> <li>[Enhancement] Validate states materialization lag in the status view.</li> <li>[Enhancement] Promote topics data pace to OSS.</li> <li>[Enhancement] Rename and normalize dashboard tabs.</li> <li>[Enhancement] Enable live data polling on the first visit so it does not have to be enabled manually.</li> <li>[Enhancement] Allow disabling ability to republish messages via policies.</li> <li>[Enhancement] Display raw numerical timestamp alongside message time.</li> <li>[Enhancement] Support <code>/topics</code> root redirect.</li> <li>[Enhancement] Prevent explorer from displaying too big payloads (bigger than 1MB by default)</li> <li>[Enhancement] Include deserialization object allocation stats.</li> <li>[Enhancement] Improve how charts with many topics work.</li> <li>[Enhancement] Count and display executed jobs independently from processed batches.</li> <li>[Enhancement] Prevent karafka-web from being configured before karafka is configured.</li> <li>[Enhancement] Use <code>ostruct</code> from RubyGems in testing.</li> <li>[Enhancement] Indicate in the status reporting whether Karafka is OSS or Pro.</li> <li>[Enhancement] Ship JS and CSS assets using Brotli and Gzip when possible.</li> <li>[Enhancement] Introduce a <code>/ux</code> page to ease with styling improvements and components management.</li> <li>[Enhancement] disallow usage of <code>&lt;script&gt;</code> blocks to prevent XSS.</li> <li>[Enhancement] Display full subscription group information in the Routing view, including injectable defaults.</li> <li>[Enhancement] Report Karafka consumer server execution mode.</li> <li>[Enhancement] Expose <code>sync_threshold</code> consumer tracking config to allow aligning of error-intense applications.</li> <li>[Refactor] Optimize subscription group data tracking flow.</li> <li>[Refactor] Namespace migrations so migrations related to each topic data are in an independent directory.</li> <li>[Refactor] Use errors for deny flow so request denials can occur from the inspection layer.</li> <li>[Maintenance] Require <code>karafka</code> <code>2.4.7</code> due to fixes and API changes.</li> <li>[Fix] Disallow quiet and stop commands for swarm workers.</li> <li>[Fix] Disallow quiet and stop commands for embedded workers.</li> <li>[Fix] Fix invalid deserialization metadata display in the per-message Explorer view.</li> <li>[Fix] Fix a case where started page refresh would update content despite limiters being in place.</li> <li>[Fix] Ruby 3.4.0 preview1 - No such file or directory.</li> <li>[Fix] Fix the live poll button state flickering when disabled.</li> <li>[Fix] Pace computation does not compensate for partial data reported.</li> <li>[Fix] DLQ parent topics get classified as DLQ in the Web.</li> <li>[Fix] Add missing space in the attempt label.</li> <li>[Fix] Fix lack of highlight of \"Consumers\" navigation when in the \"Commands\" tab.</li> <li>[Fix] Fix not working page reporting in breadcrumbs.</li> <li>[Fix] Fix invalid redirect when trying to view particular errors partition time location.</li> <li>[Fix] Fix several UI inconsistencies.</li> <li>[Fix] License identifier <code>LGPL-3.0</code> is deprecated for SPDX (#2177).</li> <li>[Fix] Do not include prettifying the payload for visibility in the resource computation cost.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#091-2024-05-03", "title": "0.9.1 (2024-05-03)", "text": "<ul> <li>[Fix] OSS <code>lag_stored</code> for not-subscribed consumers causes Web UI to crash.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#090-2024-04-26", "title": "0.9.0 (2024-04-26)", "text": "<ul> <li>[Breaking] Drop Ruby <code>2.7</code> support.</li> <li>[Feature] Provide ability to stop and quiet running consumers (Pro).</li> <li>[Feature] Provide ability to probe (get backtraces) of any running consumer (Pro).</li> <li>[Feature] Provide cluster lags in Health (Pro).</li> <li>[Feature] Provide ability to inspect cluster nodes configuration (Pro).</li> <li>[Feature] Provide ability to inspect cluster topics configuration (Pro).</li> <li>[Feature] Provide messages distribution graph statistics for topics (Pro).</li> <li>[Enhancement] Provide first offset in the OSS jobs tab.</li> <li>[Enhancement] Support failover for custom deserialization of headers and key in the explorer (Pro).</li> <li>[Enhancement] Support failover for custom deserialization of headers and key in the explorer (Pro).</li> <li>[Enhancement] Limit length of <code>key</code> presented in the list view of the explorer.</li> <li>[Enhancement] Improve responsiveness on big screens by increasing max width.</li> <li>[Enhancement] Auto-qualify topics with dlq/dead_letter case insensitive name components to DLQ view.</li> <li>[Enhancement] Make tables responsive.</li> <li>[Enhancement] Provide page titles for ease of navigation.</li> <li>[Change] Rename Cluster =&gt; Topics to Cluster =&gt; Replication to better align with what is shows.</li> <li>[Change] Make support messages more entertaining.</li> <li>[Change] Rename <code>processing.consumer_group</code> to <code>admin.group_id</code> for consistency with Karafka.</li> <li>[Refactor] Normalize what is process name and process id.</li> <li>[Refactor] Create one <code>pro/</code> namespace for all Web related sub-modules.</li> <li>[Refactor] Extract alerts into a common component.</li> <li>[Refactor] Generalize charts generation.</li> <li>[Fix] Fix invalid return when paginating with offsets.</li> <li>[Fix] Improve responsiveness of summary in the consumers view for lower resolutions.</li> <li>[Fix] Align pages titles format.</li> <li>[Fix] Fix missing link from lag counter to Health.</li> <li>[Fix] Fix a case where on mobile charts would not load correctly.</li> <li>[Fix] Fix cases where long consumer names would break UI.</li> <li>[Fix] Explorer deserializer wrongly selected for pattern matched topics.</li> <li>[Fix] Fix 404 error page invalid recommendation of <code>install</code> instead of <code>migrate</code>.</li> <li>[Fix] Fix dangling <code>console.log</code>.</li> <li>[Fix] Fix a case where consumer assignments would not be truncated on the consumers view.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#082-2024-02-16", "title": "0.8.2 (2024-02-16)", "text": "<ul> <li>[Enhancement] Defer scheduler background thread creation until needed allowing for forks.</li> <li>[Enhancement] Tag forks with fork indication + ppid reference when operating in swarm.</li> <li>[Fix] Fix issue where Health tabs would not be visible when no data reported.</li> <li>[Fix] Stopped processes subscriptions lacks indicator of no groups.</li> <li>[Fix] Terminated process state not supported in the web ui.</li> <li>[Fix] Rebalance reason can be empty on a SG when no network.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#081-2024-02-01", "title": "0.8.1 (2024-02-01)", "text": "<ul> <li>[Enhancement] Introduce \"Lags\" health view.</li> <li>[Enhancement] Remove \"Stored Lag\" and \"Committed Offset\" from Health Overview due to Lags Tab.</li> <li>[Enhancement] Report lag on consumers that did not yet marked offsets.</li> <li>[Enhancement] Use more accurate lag reporting that compensates for lack of stored lag.</li> <li>[Fix] When first message after process start is crashed without DLQ lag is not reported.</li> <li>[Fix] Wrong order of enabled injection causes fresh install to crash.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#080-2024-01-26", "title": "0.8.0 (2024-01-26)", "text": "<ul> <li>[Feature] Provide ability to sort table data for part of the views (note: not all attributes can be sorted due to technical limitations of sub-components fetching from Kafka).</li> <li>[Feature] Track and report pause timeouts via \"Changes\" view in Health.</li> <li>[Feature] Introduce pending jobs visibility alongside of running jobs both in total and per process.</li> <li>[Feature] Introduce states migrations for seamless upgrades.</li> <li>[Feature] Introduce \"Data transfers\" chart with data received and data sent to the cluster.</li> <li>[Feature] Introduce ability to download raw payloads.</li> <li>[Feature] Introduce ability to download deserialized message payload as JSON.</li> <li>[Enhancement] Support reporting of standby and active listeners for connection multiplexed subscription groups.</li> <li>[Enhancement] Support Periodic Jobs reporting.</li> <li>[Enhancement] Support multiplexed subscription groups.</li> <li>[Enhancement] Split cluster info into two tabs, one for brokers and one for topics with partitions.</li> <li>[Enhancement] Track pending jobs. Pending jobs are jobs that are not yet scheduled for execution by advanced schedulers.</li> <li>[Enhancement] Rename \"Enqueued\" to \"Pending\" to support jobs that are not yet enqueued but within a scheduler.</li> <li>[Enhancement] Make sure only running jobs are displayed in running jobs</li> <li>[Enhancement] Improve jobs related breadcrumbs</li> <li>[Enhancement] Display errors backtraces in OSS.</li> <li>[Enhancement] Display concurrency graph in OSS.</li> <li>[Enhancement] Support time ranges for graphs in OSS.</li> <li>[Enhancement] Report last poll time for each subscription group.</li> <li>[Enhancement] Show last poll time per consumer instance.</li> <li>[Enhancement] Display number of jobs in a particular process jobs view.</li> <li>[Enhancement] Promote \"Batches\" chart to OSS.</li> <li>[Enhancement] Promote \"Utilization\" chart to OSS.</li> <li>[Enhancement] Allow for explicit disabling of the Web UI tracking.</li> <li>[Fix] Web UI will keep reporting status even when not activated as long as required and in routes.</li> <li>[Fix] Fix times precisions that could be incorrectly reported by 1 second in few places.</li> <li>[Fix] Fix random order in Consumers groups Health view.</li> <li>[Change] Rename \"Busy\" to \"Running\" to align with \"Running Jobs\".</li> <li>[Change] Rename \"Active subscriptions\" to \"Subscriptions\" as process subscriptions are always active.</li> <li>[Maintenance] Introduce granular subscription group contracts.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#0710-2023-10-31", "title": "0.7.10 (2023-10-31)", "text": "<ul> <li>[Fix] Max LSO chart does not work as expected (#201)</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#079-2023-10-25", "title": "0.7.9 (2023-10-25)", "text": "<ul> <li>[Enhancement] Allow for <code>Karafka::Web.producer</code> reconfiguration from the default (<code>Karafka.producer</code>).</li> <li>[Change] Rely on <code>karafka-core</code> <code>&gt;=</code> <code>2.2.4</code> to support lazy loaded custom web producer.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#078-2023-10-24", "title": "0.7.8 (2023-10-24)", "text": "<ul> <li>[Enhancement] Support transactional producer usage with Web UI.</li> <li>[Fix] Prevent a scenario where an ongoing transactional producer would have stats emitted and an error that could not have been dispatched because of the transaction, creating a dead-lock.</li> <li>[Fix] Make sure that the <code>recent</code> displays the most recent non-compacted, non-system message.</li> <li>[Fix] Improve the <code>recent</code> message display to compensate for aborted transactions.</li> <li>[Fix] Fix <code>ReferenceError: response is not defined</code> that occurs when Web UI returns refresh non 200.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#077-2023-10-20", "title": "0.7.7 (2023-10-20)", "text": "<ul> <li>[Fix] Remove <code>thor</code> as a CLI engine due to breaking changes.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#076-2023-10-10", "title": "0.7.6 (2023-10-10)", "text": "<ul> <li>[Fix] Fix nested SASL/SAML data visible in the routing details (#173)</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#075-2023-09-29", "title": "0.7.5 (2023-09-29)", "text": "<ul> <li>[Enhancement] Update order of topics creation for the setup of Web to support zero-downtime setup of Web in running Karafka projects.</li> <li>[Enhancement] Add space delimiter to counters numbers to make them look better.</li> <li>[Improvement] Normalize per-process job tables and health tables structure (topic name on top).</li> <li>[Fix] Fix a case where charts aggregated data would not include all topics.</li> <li>[Fix] Make sure, that most recent per partition data for Health is never overwritten by an old state from a previous partition owner.</li> <li>[Fix] Cache assets for 1 year instead of 7 days.</li> <li>[Fix] Remove source maps pointing to non-existing locations.</li> <li>[Maintenance] Include license and copyrights notice for <code>timeago.js</code> that was missing in the JS min file. </li> <li>[Refactor] Rename <code>ui.show_internal_topics</code> to <code>ui.visibility.internal_topics_display</code></li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#074-2023-09-19", "title": "0.7.4 (2023-09-19)", "text": "<ul> <li>[Improvement] Skip aggregations on older schemas during upgrades. This only skips process-reports (that are going to be rolled) on the 5s window in case of an upgrade that should not be a rolling one anyhow. This simplifies the operations and minimizes the risk on breaking upgrades.</li> <li>[Fix] Fix not working <code>ps</code> for macOS.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#073-2023-09-18", "title": "0.7.3 (2023-09-18)", "text": "<ul> <li>[Improvement] Mitigate a case where a race-condition during upgrade would crash data.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#072-2023-09-18", "title": "0.7.2 (2023-09-18)", "text": "<ul> <li>[Improvement] Display hidden by accident errors for OSS metrics.</li> <li>[Improvement] Use a five second cache for non-production environments to improve dev experience.</li> <li>[Improvement] Limit number of partitions listed on the Consumers view if they exceed 10 to improve readability and indicate, that there are more in OSS similar to Pro.</li> <li>[Improvement] Squash processes reports based on the key instead of payload skipping deserialization for duplicated reports.</li> <li>[Improvement] Make sure, that the Karafka topics present data can be deserialized and report on the status page if not.</li> <li>[Fix] Extensive data-poll on processes despite no processes being available.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#071-2023-09-15", "title": "0.7.1 (2023-09-15)", "text": "<ul> <li>[Improvement] Limit number of partitions listed on the Consumers view if they exceed 10 to improve readability and indicate, that there are more in Pro.</li> <li>[Improvement] Make sure, that small messages size (less than 100 bytes) is correctly displayed.</li> <li>[Fix] Validate refresh time.</li> <li>[Fix] Fix invalid message payload size display (KB instead of B, etc).</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#070-2023-09-14", "title": "0.7.0 (2023-09-14)", "text": "<ul> <li>[Feature] Introduce graphs.</li> <li>[Feature] Introduce historical metrics storage.</li> <li>[Feature] Introduce per-topic data exploration in the Explorer.</li> <li>[Feature] Introduce per-topic and per-partition most recent message view with live reload.</li> <li>[Feature] Introduce a new per-process inspection view called \"Details\" ti display all process real-time aggregated data.</li> <li>[Feature] Introduce <code>bundle exec karafka-web migrate</code> that can be used to bootstrap the proper topics and initial data in environments where Karafka Web-UI should be used but is missing the initial setup.</li> <li>[Feature] Replace <code>decrypt</code> with a pluggable API for deciding which topics data to display.</li> <li>[Feature] Make sure, that the karafka server process that is materializing UI states is not processing any data having unsupported (newer) schemas. This state will be also visible in the status page.</li> <li>[Feature] Provide ability to reproduce a given message to the same topic partition with all the details from the per message explorer view.</li> <li>[Feature] Provide \"surrounding\" navigation link that allows to view the given message in the context of its surrounding. Useful for debugging of failures where the batch context may be relevant.</li> <li>[Feature] Allow for time based lookups per topic partition.</li> <li>[Feature] Introduce Offsets Health inspection view for frozen LSO lookups with <code>lso_threshold</code> configuration option.</li> <li>[Improvement] Support pattern subscriptions details in the routing view both by displaying the pattern as well as expanded routing details.</li> <li>[Improvement] Collect total number of threads per process for the process details view.</li> <li>[Improvement] Normalize naming of metrics to better reflect what they do (in reports and in the Web UI).</li> <li>[Improvement] Link error reported first and last offset to the explorer.</li> <li>[Improvement] Expand routing details to compensate for nested values in declarative topics.</li> <li>[Improvement] Include last rebalance age in the health view per consumer group.</li> <li>[Improvement] Provide previous / next navigation when viewing particular messages in the explorer.</li> <li>[Improvement] Provide previous / next navigation when viewing particular errors.</li> <li>[Improvement] Link all explorable offsets to the explorer.</li> <li>[Improvement] Extend status page checks to ensure, that it detects a case when Web-UI is not part of <code>karafka.rb</code> but still referenced in routes.</li> <li>[Improvement] Extend status page checks to ensure, that it detects a case where there is no initial consumers metrics in Kafka topic.</li> <li>[Improvement] Report Rails version when viewing status page (if Rails used).</li> <li>[Improvement] List Web UI topics names on the status page in the info section.</li> <li>[Improvement] Start versioning the materialized states schemas.</li> <li>[Improvement] Drastically improve the consumers view performance.</li> <li>[Improvement] Ship versioned assets to prevent invalid assets loading due to cache.</li> <li>[Improvement] Use <code>Cache-Control</code> to cache all the static assets.</li> <li>[Improvement] Link <code>counters</code> counter to jobs page.</li> <li>[Improvement] Include a sticky footer with the most important links and copyrights.</li> <li>[Improvement] Store lag in counters for performance improvement and historical metrics.</li> <li>[Improvement] Introduce in-memory cluster state cached to improve performance.</li> <li>[Improvement] Switch to offset based pagination instead of per-page pagination.</li> <li>[Improvement] Avoid double-reading of watermark offsets for explorer and errors display.</li> <li>[Improvement] When no params needed for a page, do not include empty params.</li> <li>[Improvement] Do not include page when page is 1 in the url.</li> <li>[Improvement] Align descriptions for the status page, to reflect that state check happens for consumers initial state.</li> <li>[Improvement] Report bytesize of raw payload when viewing message in the explorer.</li> <li>[Improvement] Use zlib compression for Karafka Web UI topics reports (all). Reduces space needed from 50 to 91%.</li> <li>[Improvement] Rename lag to lag stored in counters to reflect what it does.</li> <li>[Improvement] Collect both stored lag and lag.</li> <li>[Improvement] Introduce states and metrics schema validation.</li> <li>[Improvement] Prevent locking in sampler for time of OS data aggregation.</li> <li>[Improvement] Collect and report number of messages in particular jobs.</li> <li>[Improvement] Limit segment size for Web topics to ensure, that Web-UI does not drain resources.</li> <li>[Improvement] Introduce cookie based sessions management for future usage.</li> <li>[Improvement] Introduce config validation.</li> <li>[Improvement] Provide flash messages support.</li> <li>[Improvement] Use replication factor of two by default (if not overridden) for Web UI topics when there is more than one broker.</li> <li>[Improvement] Show a warning when replication factor of 1 is used for Web UI topics in production.</li> <li>[Improvement] Collect extra additional metrics useful for hanging transactions detection.</li> <li>[Improvement] Reorganize how the Health view looks.</li> <li>[Improvement] Hide all private Kafka topics by default in the explorer. Configurable with <code>show_internal_topics</code> config setting.</li> <li>[Fix] Return 402 status instead of 500 on Pro features that are not available in OSS.</li> <li>[Fix] Fix a case where errors would not be visible without Rails due to the <code>String#first</code> usage.</li> <li>[Fix] Fix a case where live-poll would be disabled but would still update data.</li> <li>[Fix] Fix a case where states materializing consumer would update state too often.</li> <li>[Fix] Fix a bug when rapid non-initialized shutdown could mess up the metrics.</li> <li>[Fix] Fix a case where upon multiple rebalances, part of the states materialization could be lost.</li> <li>[Fix] Make sure, that the flushing interval computation division happens with float.</li> <li>[Fix] Fix a case where app client id change could force web-ui to recompute the metrics.</li> <li>[Fix] Make sure, that when re-using same Karafka Web-UI topics as a different up, all states and reports are not recomputed back.</li> <li>[Fix] Fix headers size inconsistency between Health and Routing.</li> <li>[Fix] Fix invalid padding on status page.</li> <li>[Fix] Fix a case where root mounted Karafka Web-UI would not work.</li> <li>[Fix] Fix a case where upon hitting a too high page of consumers we would inform that no consumers are reporting instead of information that this page does not contain any reporting.</li> <li>[Refactor] Limit usage of UI models for data intense computation to speed up states materialization under load.</li> <li>[Refactor] Reorganize pagination engine to support offset based pagination.</li> <li>[Refactor] Use Roda <code>custom_block_results</code> plugin for controllers results handling.</li> <li>[Maintenance] Require <code>karafka</code> <code>2.2.0</code> due to fixes in the Iterator API and routing API extensions.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#063-2023-07-22", "title": "0.6.3 (2023-07-22)", "text": "<ul> <li>[Fix] Remove files from 0.7.0 accidentally added to the release.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#062-2023-07-22", "title": "0.6.2 (2023-07-22)", "text": "<ul> <li>[Fix] Fix extensive CPU usage when using HPET clock instead of TSC due to interrupt frequency.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#061-2023-06-25", "title": "0.6.1 (2023-06-25)", "text": "<ul> <li>[Improvement] Include the karafka-web version in the status page tags.</li> <li>[Improvement] Report <code>karafka-web</code> version that is running in particular processes.</li> <li>[Improvement] Display <code>karafka-web</code> version in the per-process view.</li> <li>[Improvement] Report in the web-ui a scenario, where getting cluster info takes more than 500ms as a warning to make people realize, that operating with Kafka with extensive latencies is not recommended.</li> <li>[Improvement] Continue the status assessment flow on warnings.</li> <li>[Fix] Do not recommend running a server as a way to bootstrap the initial state.</li> <li>[Fix] Ensure in the report contract, that <code>karafka-core</code>, <code>karafka-web</code>, <code>rdkafka</code> and <code>librdkafka</code> are validated.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#060-2023-06-13", "title": "0.6.0 (2023-06-13)", "text": "<ul> <li>[Feature] Introduce producers errors tracking.</li> <li>[Improvement] Display the error origin as a badge to align with consumers view topic assignments.</li> <li>[Improvement] Collect more job metrics for future usage.</li> <li>[Improvement] Normalize order of job columns on multiple views.</li> <li>[Improvement] Improve pagination by providing a \"Go to first page\" fast button.</li> <li>[Improvement] Provide more explicit info in the consumers view when no consumers running.</li> <li>[Improvement] Validate error reporting with unified error contract.</li> <li>[Improvement] Use estimated errors count for counters presentation taken from the errors topic instead of materialization via consumers states to allow for producers errors tracking.</li> <li>[Improvement] Introduce <code>schema_version</code> to error reports.</li> <li>[Improvement] Do not display the dispatched error message offset in the breadcrumb and title as it was confused with the error message content.</li> <li>[Improvement] Display <code>error_class</code> value wrapped with code tag.</li> <li>[Improvement] Display error <code>type</code> value wrapped with label tag.</li> <li>[Improvement] Include a blurred backtrace for non-Pro error inspection as a form of indication of this Pro feature.</li> <li>[Fix] Fix invalid arrows style in the pagination.</li> <li>[Fix] Fix missing empty <code>Process name</code> value in the errors index view.</li> <li>[Fix] Fix potential empty dispatch of consumer metrics.</li> <li>[Fix] Remove confusing part about real time resources from the \"Pro feature\" page.</li> <li>[Refactor] Cleanup common components for errors extraction.</li> <li>[Refactor] Remove not used and redundant partials.</li> <li>[Maintenance] Require <code>karafka</code> <code>2.1.4</code> due to fixes in metrics usage for workless flows.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#052-2023-05-22", "title": "0.5.2 (2023-05-22)", "text": "<ul> <li>[Improvement] Label ActiveJob consumers jobs with <code>active_job</code> tag.</li> <li>[Improvement] Label Virtual Partitions consumers with <code>virtual</code> tag.</li> <li>[Improvement] Label Long Running Jobs with <code>long_running_job</code> tag.</li> <li>[Improvement] Label collapsed Virtual Partition with <code>collapsed</code> tag.</li> <li>[Improvement] Display consumer tags always below the consumer class name in Jobs/Consumer Jobs views.</li> <li>[Improvement] Add label with the attempt count on work being retried.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#051-2023-04-16", "title": "0.5.1 (2023-04-16)", "text": "<ul> <li>[Fix] Use CSP header matching Sidekiq one to ensure styles and js loading (#55)</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#050-2023-04-13", "title": "0.5.0 (2023-04-13)", "text": "<ul> <li>[Improvement] Report job <code>-1001</code> offsets as <code>N/A</code> as in all the other places.</li> <li>[Fix] Fix misspelling of word <code>committed</code>.</li> <li>[Fix] Shutdown and revocation jobs statistics extraction crashes when idle initialized without messages (#53)</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#041-2023-04-12", "title": "0.4.1 (2023-04-12)", "text": "<ul> <li>[Improvement] Replace the \"x time ago\" in the code explorer with an exact date (<code>2023-04-12 10:16:48.596 +0200</code>).</li> <li>[Improvement] When hovering over a message timestamp, a label with raw numeric timestamp will be presented.</li> <li>[Improvement] Do not skip reporting on partitions subscribed that never received any messages.</li> <li>[Fix] Skip reporting data on subscriptions that were revoked and not only stopped by us.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#040-2023-04-07", "title": "0.4.0 (2023-04-07)", "text": "<ul> <li>[Improvement] Include active jobs and active partitions subscriptions count in the per-process tab navigation.</li> <li>[Improvement] Include subscription groups names in the per-process subscriptions view.</li> <li>[Fix] Add missing support for using multiple subscription groups within a single consumer group.</li> <li>[Fix] Mask SASL credentials in topic routing view (#46)</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#031-2023-03-27", "title": "0.3.1 (2023-03-27)", "text": "<ul> <li>[Fix] Add missing retention policy for states topic.</li> <li>[Fix] Fix display of compacted messages placeholders for offsets lower than low watermark.</li> <li>[Fix] Fix invalid pagination per page count.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#030-2023-03-27", "title": "0.3.0 (2023-03-27)", "text": "<ul> <li>[Feature] Support paginating over compacted topics partitions.</li> <li>[Improvement] Display watermark offsets in the errors view.</li> <li>[Improvement] Display informative message when partition is empty due to a retention policy.</li> <li>[Improvement] Display informative message when partition is empty instead of displaying nothing.</li> <li>[Improvement] Display current watermark offsets in the Explorer when viewing list of messages from a given partition.</li> <li>[Improvement] Report extra debug info in the status section.</li> <li>[Improvement] Report not only <code>Karafka</code> and <code>WaterDrop</code> versions but also <code>Karafka::Core</code>, <code>Rdkafka</code> and <code>librdkafka</code> versions.</li> <li>[Improvement] Small CSS improvements.</li> <li>[Improvement] Provide nicer info when errors topic does not contain any errors or was compacted.</li> <li>[Improvement] Improve listing of errors including compacted once.</li> <li>[Fix] Fix pagination for compacted indexes that would display despite no data being available below the low watermark offset.</li> <li>[Fix] Fix a case where reading from a compacted offset would return no data despite data being available.</li> <li>[Fix] Fix a case where explorer pagination would suggest more pages for compacted topics.</li> <li>[Fix] Fix incorrect support of compacted partitions and partitions with low watermark offset other than 0.</li> <li>[Fix] Display <code>N/A</code> instead of <code>-1</code> and <code>-1001</code> on lag stored and stored offset for consumer processes that did not mark any messages as consumed yet in the per consumer view.</li> <li>[Maintenance] Remove compatibility fallbacks for job and process tags (#1342)</li> <li>[Maintenance] Extract base sampler for tracking and web.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#025-2023-03-17", "title": "0.2.5 (2023-03-17)", "text": "<ul> <li>[Fix] Critical instrumentation async errors intercepted by Web don't have JID for job removal (#1366)</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#024-2023-03-14", "title": "0.2.4 (2023-03-14)", "text": "<ul> <li>[Improvement] Paginate topics list in cluster info on every 100 partitions.</li> <li>[Improvement] Provide current page in the pagination.</li> <li>[Improvement] Report usage of Karafka Pro on the status page view.</li> <li>[Fix] Add missing three months limit on errors storage.</li> <li>[Maintenance] Exclude Karafka Web UI topics from declarative topics.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#023-2023-03-04", "title": "0.2.3 (2023-03-04)", "text": "<ul> <li>[Improvement] Snapshot current consumer tags upon consumer errors.</li> <li>[Improvement] Optimize exception message extraction from errors.</li> <li>[Improvement] Slightly change error reporting structure (backwards compatible) to collect process tags on errors and to align with other reports.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#022-2023-02-25", "title": "0.2.2 (2023-02-25)", "text": "<ul> <li>[Fix] Fix status page reference in Pro.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#021-2023-02-24", "title": "0.2.1 (2023-02-24)", "text": "<ul> <li>[Fix] Fix format incompatibility between 0.1.x and 0.2.x data formats. This will allow for the 0.2 Web UI to work with 0.1.x reporting.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#020-2023-02-24", "title": "0.2.0 (2023-02-24)", "text": "<ul> <li>[Feature] Introduce ability to tag <code>Karafka::Process</code> to display process-centric tags in the Web UI.</li> <li>[Feature] Introduce ability to tag consumer instances to display consumption-centric tags in the Web UI.</li> <li>[Feature] Introduce a /status page that can validate the setup and tell what is missing (#1318)</li> <li>[Improvement] Allow for disabling the consumer subscription from Web for multi-tenant Web UI usage (#1331)</li> <li>[Improvement] Make sure that states and reports are always dispatched to the partition <code>0</code>. This should prevent UI from not fully working when someone accidentally creates more partitions than expected.</li> <li>[Fix] Fix a bug where bootstrapping would create two initial states.</li> <li>[Fix] Fix a case, where errors listener would try to force encoding on a frozen error message.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#013-2023-02-14", "title": "0.1.3 (2023-02-14)", "text": "<ul> <li>Skip topics creation if web topics already exist (do not raise error)</li> <li>Support ability to provide replication factor in the install command</li> <li>Provide ability to reset the state with a <code>reset</code> command. It will remove and re-create the topics.</li> <li>Provide ability to uninstall the web via the CLI <code>uninstall</code> command</li> <li>Remove the <code>Karafka::Web.bootstrap!</code> method as the install should happen via <code>bundle exec karafka-web install</code></li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#012-2023-02-10", "title": "0.1.2 (2023-02-10)", "text": "<ul> <li>Provide more comprehensive info when lag stored and stored offset are not available.</li> <li>Setup rspec scaffold.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#011-2023-01-30", "title": "0.1.1 (2023-01-30)", "text": "<ul> <li>Rename <code>Karafka::Web.bootstrap_topics!</code> to <code>Karafka::Web.bootstrap!</code> and expand it with the zero state injection.</li> <li>Require Karafka <code>2.0.28</code> due to some instrumentation fixes.</li> <li>Provide an auto-installer under the <code>bundle exec karafka-web install</code> command.</li> </ul>"}, {"location": "Changelog-Karafka-Web-UI/#010", "title": "0.1.0", "text": "<ul> <li>Initial code of the Web and Web Pro.</li> </ul>"}, {"location": "Changelog-Karafka/", "title": "Karafka Framework Changelog", "text": ""}, {"location": "Changelog-Karafka/#251-unreleased", "title": "2.5.1 (Unreleased)", "text": "<ul> <li>[Enhancement] Support immediate error raising with <code>auto.offset.reset</code> set to <code>error</code>.</li> <li>[Enhancement] Don't create not needed dirs in the non-Rails setup template.</li> <li>[Enhancement] Improve printing of TTIN to separate threads</li> <li>[Maintenance] Add basic direct DD integration spec via DD gem karafka monitoring feature.</li> </ul>"}, {"location": "Changelog-Karafka/#250-2025-06-15", "title": "2.5.0 (2025-06-15)", "text": "<ul> <li>[Breaking] Change how consistency of DLQ dispatches works in Pro (<code>partition_key</code> vs. direct partition id mapping).</li> <li>[Breaking] Remove the headers <code>source_key</code> from the Pro DLQ dispatched messages as the original key is now fully preserved.</li> <li>[Breaking] Use DLQ and Piping prefix <code>source_</code> instead of <code>original_</code> to align with naming convention of Kafka Streams and Apache Flink for future usage.</li> <li>[Breaking] Rename scheduled jobs topics names in their config (Pro).</li> <li>[Breaking] Change K8s listener response from <code>204</code> to <code>200</code> and include JSON body with reasons.</li> <li>[Breaking] Replace admin config <code>max_attempts</code> with <code>max_retries_duration</code> and <code>retry_backoff</code>.</li> <li>[Feature] Parallel Segments for concurrent processing of the same partition with more than partition count of processes (Pro).</li> <li>[Enhancement] Normalize topic + partition logs format.</li> <li>[Enhancement] Support KIP-82 (header values of arrays).</li> <li>[Enhancement] Enhance errors tracker with <code>#counts</code> that contains per-error class specific counters for granular flow handling.</li> <li>[Enhancement] Provide explicit <code>Karafka::Admin.copy_consumer_group</code> API.</li> <li>[Enhancement] Return explicit value from <code>Karafka::Admin.copy_consumer_group</code> and <code>Karafka::Admin.rename_consumer_group</code> APIs.</li> <li>[Enhancement] Introduce balanced non-consistent VP distributor improving the utilization up to 50% (Pro).</li> <li>[Enhancement] Make the error tracker for advanced DLQ strategies respond to <code>#topic</code> and <code>#partition</code> for context aware dispatches.</li> <li>[Enhancement] Allow setting the workers thread priority and set it to -1 (50ms) by default.</li> <li>[Enhancement] Enhance low-level <code>client.pause</code> event with timeout value (if provided).</li> <li>[Enhancement] Introduce <code>#marking_cursor</code> API (defaults to <code>#cursor</code>) in the filtering API (Pro).</li> <li>[Enhancement] Support multiple DLQ target topics via context aware strategies (Pro).</li> <li>[Enhancement] Raise error when post-transactional committing of offset is done outside of the transaction (Pro).</li> <li>[Enhancement] Include info level rebalance logger listener data.</li> <li>[Enhancement] Include info level subscription start info.</li> <li>[Enhancement] Make the generic error handling in the <code>LoggerListener</code> more descriptive by logging also the error class.</li> <li>[Enhancement] Allow marking older offsets to support advanced rewind capabilities.</li> <li>[Enhancement] Change optional <code>#seek</code> reset offset flag default to <code>true</code> as <code>false</code> is almost never used and seek by default should move the internal consumer offset position as well.</li> <li>[Enhancement] Include Swarm node ID in the swarm process tags.</li> <li>[Enhancement] Replace internal usage of MD5 with SHA256 for FIPS.</li> <li>[Enhancement] Improve OSS vs. Pro specs execution isolation.</li> <li>[Enhancement] Preload <code>librdkafka</code> code prior to forking in the Swarm mode to save memory.</li> <li>[Enhancement] Extract errors tracker class reference into an internal <code>errors_tracker_class</code> config option (Pro).</li> <li>[Enhancement] Support rdkafka native kafka polling customization for admin.</li> <li>[Enhancement] Customize the multiplexing scale delay (Pro) per consumer group (Pro).</li> <li>[Enhancement] Set <code>topic.metadata.refresh.interval.ms</code> for default producer in dev to 5s to align with consumer setup.</li> <li>[Enhancement] Alias <code>-2</code> and <code>-1</code> with <code>latest</code> and <code>earliest</code> for seeking.</li> <li>[Enhancement] Allow for usage of <code>latest</code> and <code>earliest</code> in the <code>Karafka::Pro::Iterator</code>.</li> <li>[Enhancement] Failures during <code>topics migrate</code> (and other subcommands) don't show what topic failed, and why it's invalid.</li> <li>[Enhancement] Apply changes to topics configuration in atomic independent requests when using Declarative Topics.</li> <li>[Enhancement] Execute the help CLI command when no command provided (similar to Rails) to improve DX.</li> <li>[Enhancement] Remove backtrace from the CLI error for incorrect commands (similar to Rails) to improve DX.</li> <li>[Enhancement] Provide <code>karafka topics help</code> sub-help due to nesting of Declarative Topics actions.</li> <li>[Enhancement] Use independent keys for different states of reporting in scheduled messages.</li> <li>[Enhancement] Enrich scheduled messages state reporter with debug data.</li> <li>[Enhancement] Introduce a new state called <code>stopped</code> to the scheduled messages.</li> <li>[Enhancement] Do not overwrite the <code>key</code> in the Pro DLQ dispatched messages for routing reasons.</li> <li>[Enhancement] Introduce <code>errors_tracker.trace_id</code> for distributed error details correlation with the Web UI.</li> <li>[Enhancement] Improve contracts validations reporting.</li> <li>[Enhancement] Optimize topic creation and repartitioning admin operations for topics with hundreds of partitions.</li> <li>[Refactor] Introduce a <code>bin/verify_kafka_warnings</code> script to clean Kafka from temporary test-suite topics.</li> <li>[Refactor] Introduce a <code>bin/verify_topics_naming</code> script to ensure proper test topics naming convention.</li> <li>[Refactor] Make sure all temporary topics have a <code>it-</code> prefix in their name.</li> <li>[Refactor] Improve CI specs parallelization.</li> <li>[Maintenance] Lower the <code>Karafka::Admin</code> <code>poll_timeout</code> to 50 ms to improve responsiveness of admin operations.</li> <li>[Maintenance] Require <code>karafka-rdkafka</code> <code>&gt;=</code> <code>0.19.5</code> due to usage of <code>#rd_kafka_global_init</code>, KIP-82, new producer caching engine and improvements to the <code>partition_key</code> assignments.</li> <li>[Maintenance] Add Deimos routing patch into integration suite not to break it in the future.</li> <li>[Maintenance] Remove Rails <code>7.0</code> specs due to upcoming EOL.</li> <li>[Fix] Fix Recurring Tasks and Scheduled Messages not working with Swarm (using closed producer).</li> <li>[Fix] Fix a case where <code>unknown_topic_or_part</code> error could leak out of the consumer on consumer shutdown.</li> <li>[Fix] Fix missing <code>virtual_partitions.partitioner.error</code> custom error logging in the <code>LoggerListener</code>.</li> <li>[Fix] Prevent applied system filters <code>#timeout</code> from potentially interacting with user filters.</li> <li>[Fix] Use more sane value in <code>Admin#seek_consumer_group</code> for long ago.</li> <li>[Fix] Prevent multiplexing of 1:1 from routing.</li> <li>[Fix] WaterDrop level aborting transaction may cause seek offset to move (Pro).</li> <li>[Fix] Fix inconsistency in the logs where <code>Karafka::Server</code> originating logs would not have server id reference.</li> <li>[Fix] Fix inconsistency in the logs where OS signal originating logs would not have server id reference.</li> <li>[Fix] Post-fork WaterDrop instance looses some of the non-kafka settings.</li> <li>[Fix] Max epoch tracking for early cleanup causes messages to be skipped until reload.</li> <li>[Fix] optparse double parse loses ARGV.</li> <li>[Fix] <code>karafka</code> cannot be required without Bundler.</li> <li>[Fix] Scheduled Messages re-seek moves to <code>latest</code> on inheritance of initial offset when <code>0</code> offset is compacted.</li> <li>[Fix] Seek to <code>:latest</code> without <code>topic_partition_position</code> (-1) will not seek at all.</li> <li>[Fix] Extremely high turn over of scheduled messages can cause them not to reach EOF/Loaded state.</li> <li>[Fix] Fix incorrectly passed <code>max_wait_time</code> to rdkafka (ms instead of seconds) causing too long wait.</li> <li>[Fix] Remove aggresive requerying of the Kafka cluster on topic creation/removal/altering.</li> <li>[Change] Move to trusted-publishers and remove signing since no longer needed.</li> </ul>"}, {"location": "Changelog-Karafka/#2418-2025-04-09", "title": "2.4.18 (2025-04-09)", "text": "<ul> <li>[Fix] Make sure <code>Bundler.with_unbundled_env</code> is not called multiple times.</li> </ul>"}, {"location": "Changelog-Karafka/#2417-2025-01-15", "title": "2.4.17 (2025-01-15)", "text": "<ul> <li>[Enhancement] Clean message key and headers when cleaning messages via the cleaner API (Pro).</li> <li>[Enhancement] Allow for setting <code>metadata: false</code> in the cleaner API for granular cleaning control (Pro)</li> <li>[Enhancement] Instrument successful transaction via <code>consumer.consuming.transaction</code> event (Pro).</li> </ul>"}, {"location": "Changelog-Karafka/#2416-2024-12-27", "title": "2.4.16 (2024-12-27)", "text": "<ul> <li>[Enhancement] Improve post-rebalance revocation messages filtering.</li> <li>[Enhancement] Introduce <code>Consumer#wrap</code> for connection pooling management and other wrapped operations.</li> <li>[Enhancement] Guard transactional operations from marking beyond assignment ownership under some extreme edge-cases.</li> <li>[Enhancement] Improve VPs work with transactional producers.</li> <li>[Enhancement] Prevent non-transactional operations leakage into transactional managed offset management consumers.</li> <li>[Fix] Prevent transactions from being marked with a non-transactional default producer when automatic offset management and other advanced features are on.</li> <li>[Fix] Fix <code>kafka_format</code> <code>KeyError</code> that occurs when a non-hash is assigned to the kafka scope of the settings.</li> <li>[Fix] Non cooperative-sticky transactional offset management can refetch reclaimed partitions.</li> </ul>"}, {"location": "Changelog-Karafka/#2415-2024-12-04", "title": "2.4.15 (2024-12-04)", "text": "<ul> <li>[Fix] Assignment tracker current state fetch during a rebalance loop can cause an error on multi CG setup.</li> <li>[Fix] Prevent double post-transaction offset dispatch to Kafka.</li> </ul>"}, {"location": "Changelog-Karafka/#2414-2024-11-25", "title": "2.4.14 (2024-11-25)", "text": "<ul> <li>[Enhancement] Improve low-level critical error reporting.</li> <li>[Enhancement] Expand Kubernetes Liveness state reporting with critical errors detection.</li> <li>[Enhancement] Save several string allocations and one array allocation on each job execution when using Datadog instrumentation.</li> <li>[Enhancement] Support <code>eofed</code> jobs in the AppSignal instrumentation.</li> <li>[Enhancement] Allow running bootfile-less Rails setup Karafka CLI commands where stuff is configured in the initializers.</li> <li>[Fix] <code>Instrumentation::Vendors::Datadog::LoggerListener</code> treats eof jobs as consume jobs.</li> </ul>"}, {"location": "Changelog-Karafka/#2413-2024-10-11", "title": "2.4.13 (2024-10-11)", "text": "<ul> <li>[Enhancement] Make declarative topics return different exit codes on migrable/non-migrable states (0 - no changes, 2 - changes) when used with <code>--detailed-exitcode</code> flag.</li> <li>[Enhancement] Introduce <code>config.strict_declarative_topics</code> that should force declaratives on all non-pattern based topics and DLQ topics</li> <li>[Enhancement] Report ignored repartitioning to lower number of partitions in declarative topics.</li> <li>[Enhancement] Promote the <code>LivenessListener#healty?</code> to a public API.</li> <li>[Fix] Fix <code>Karafka::Errors::MissingBootFileError</code> when debugging in VScode with ruby-lsp.</li> <li>[Fix] Require <code>karafka-core</code> <code>&gt;=</code> <code>2.4.4</code> to prevent dependencies conflicts. </li> <li>[Fix] Validate swarm cli and always parse options from argv (roelbondoc)</li> </ul>"}, {"location": "Changelog-Karafka/#2412-2024-09-17", "title": "2.4.12 (2024-09-17)", "text": "<ul> <li>[Feature] Provide Adaptive Iterator feature as a fast alternative to Long-Running Jobs (Pro).</li> <li>[Enhancement] Provide <code>Consumer#each</code> as a delegation to messages batch.</li> <li>[Enhancement] Verify cancellation request envelope topic similar to the schedule one.</li> <li>[Enhancement] Validate presence of <code>bootstrap.servers</code> to avoid incomplete partial reconfiguration.</li> <li>[Enhancement] Support <code>ActiveJob#enqueue_at</code> via Scheduled Messages feature (Pro).</li> <li>[Enhancement] Introduce <code>Karafka::App#debug!</code> that will switch Karafka and the default producer into extensive debug mode. Useful for CLI debugging.</li> <li>[Enhancement] Support full overwrite of the <code>BaseConsumer#producer</code>.</li> <li>[Enhancement] Transfer the time of last poll back to the coordinator for more accurate metrics tracking.</li> <li>[Enhancement] Instrument <code>Consumer#seek</code> via <code>consumer.consuming.seek</code>.</li> <li>[Fix] Fix incorrect time reference reload in scheduled messages.</li> </ul>"}, {"location": "Changelog-Karafka/#2411-2024-09-04", "title": "2.4.11 (2024-09-04)", "text": "<ul> <li>[Enhancement] Validate envelope target topic type for Scheduled Messages.</li> <li>[Enhancement] Support for enqueue_after_transaction_commit in rails active job.</li> <li>[Fix] Fix invalid reference to AppSignal version.</li> </ul>"}, {"location": "Changelog-Karafka/#2410-2024-09-03", "title": "2.4.10 (2024-09-03)", "text": "<ul> <li>[Feature] Provide Kafka based Scheduled Messages to be able to send messages in the future via a proxy topic.</li> <li>[Enhancement] Introduce a <code>#assigned</code> hook for consumers to be able to trigger actions when consumer is built and assigned but before first consume/ticking, etc.</li> <li>[Enhancement] Provide <code>Karafka::Messages::Message#tombstone?</code> to be able to quickly check if a message is a tombstone message.</li> <li>[Enhancement] Provide more flexible API for Recurring Tasks topics reconfiguration.</li> <li>[Enhancement] Remove no longer needed Rails connection releaser.</li> <li>[Enhancement] Update AppSignal client to support newer versions (tombruijn and hieuk09).</li> <li>[Fix] Fix a case where there would be a way to define multiple subscription groups for same topic with different consumer.</li> </ul>"}, {"location": "Changelog-Karafka/#249-2024-08-23", "title": "2.4.9 (2024-08-23)", "text": "<ul> <li>[Feature] Provide Kafka based Recurring (Cron) Tasks.</li> <li>[Enhancement] Wrap worker work with Rails Reloader/Executor (fusion2004)</li> <li>[Enhancement] Allow for partial topic level kafka scope settings reconfiguration via <code>inherit</code> flag.</li> <li>[Enhancement] Validate <code>eof</code> kafka scope flag when <code>eofed</code> in routing enabled.</li> <li>[Enhancement] Provide <code>mark_after_dispatch</code> setting for granular DLQ marking control.</li> <li>[Enhancement] Provide <code>Karafka::Admin.rename_consumer_group</code>.</li> </ul>"}, {"location": "Changelog-Karafka/#248-2024-08-09", "title": "2.4.8 (2024-08-09)", "text": "<ul> <li>[Feature] Introduce ability to react to <code>#eof</code> either from <code>#consume</code> or from <code>#eofed</code> when EOF without new messages.</li> <li>[Enhancement] Provide <code>Consumer#eofed?</code> to indicate reaching EOF.</li> <li>[Enhancement] Always immediately report on <code>inconsistent_group_protocol</code> error.</li> <li>[Enhancement] Reduce virtual partitioning to 1 partition when any partitioner execution in a partitioned batch crashes.</li> <li>[Enhancement] Provide <code>KARAFKA_REQUIRE_RAILS</code> to disable default Rails <code>require</code> to run Karafka without Rails despite having Rails in the Gemfile.</li> <li>[Enhancement] Increase final listener recovery from 1 to 60 seconds to prevent constant rebalancing. This is the last resort recovery and should never happen unless critical errors occur.</li> </ul>"}, {"location": "Changelog-Karafka/#247-2024-08-01", "title": "2.4.7 (2024-08-01)", "text": "<ul> <li>[Enhancement] Introduce <code>Karafka::Server.execution_mode</code> to check in what mode Karafka process operates (<code>standalone</code>, <code>swarm</code>, <code>supervisor</code>, <code>embedded</code>).</li> <li>[Enhancement] Ensure <code>max.poll.interval.ms</code> is always present and populate it with librdkafka default.</li> <li>[Enhancement] Introduce a shutdown time limit for unsubscription wait.</li> <li>[Enhancement] Tag with <code>mode:swarm</code> each of the running swarm consumers.</li> <li>[Change] Tag with <code>mode:embedded</code> instead of <code>embedded</code> the embedded consumers.</li> <li>[Fix] License identifier <code>LGPL-3.0</code> is deprecated for SPDX (#2177).</li> <li>[Fix] Fix an issue where custom clusters would not have default settings populated same as the primary cluster.</li> <li>[Fix] Fix Rspec warnings of nil mocks.</li> <li>[Maintenance] Cover <code>cooperative-sticky</code> librdkafka issues with integration spec.</li> </ul>"}, {"location": "Changelog-Karafka/#246-2024-07-22", "title": "2.4.6 (2024-07-22)", "text": "<ul> <li>[Fix] Mitigate <code>rd_kafka_cgrp_terminated</code> and other <code>librdkafka</code> shutdown issues by unsubscribing fully prior to shutdown.</li> </ul>"}, {"location": "Changelog-Karafka/#245-2024-07-18", "title": "2.4.5 (2024-07-18)", "text": "<ul> <li>[Change] Inject <code>client.id</code> when building subscription group and not during the initial setup.</li> <li>[Fix] Mitigate <code>confluentinc/librdkafka/issues/4783</code> by injecting dynamic client id when using <code>cooperative-sticky</code> strategy.</li> </ul>"}, {"location": "Changelog-Karafka/#change-note", "title": "Change Note", "text": "<p><code>client.id</code> is technically a low-importance value that should not (aside from this error) impact operations. This is why it is not considered a breaking change. This change may be reverted when the original issue is fixed in librdkafka.</p>"}, {"location": "Changelog-Karafka/#244-2024-07-04", "title": "2.4.4 (2024-07-04)", "text": "<ul> <li>[Enhancement] Allow for offset storing from the Filtering API.</li> <li>[Enhancement] Print more extensive error info on forceful shutdown.</li> <li>[Enhancement] Include <code>original_key</code> in the DLQ dispatch headers.</li> <li>[Enhancement] Support embedding mode control management from the trap context.</li> <li>[Enhancement] Make sure, that the listener thread is stopped before restarting.</li> <li>[Fix] Do not block on hanging listener shutdown when invoking forceful shutdown.</li> <li>[Fix] Static membership fencing error is not propagated explicitly enough.</li> <li>[Fix] Make sure DLQ dispatches raw headers and not deserialized headers (same as payload).</li> <li>[Fix] Fix a typo where <code>ms</code> in logger listener would not have space before it.</li> <li>[Maintenance] Require <code>karafka-core</code> <code>&gt;=</code> <code>2.4.3</code>.</li> <li>[Maintenance] Allow for usage of <code>karafka-rdkafka</code> <code>~</code> <code>0.16</code> to support librdkafka <code>2.4.0</code>.</li> <li>[Maintenance] Lower the precision reporting to 100 microseconds in the logger listener.</li> </ul>"}, {"location": "Changelog-Karafka/#243-2024-06-12", "title": "2.4.3 (2024-06-12)", "text": "<ul> <li>[Enhancement] Allow for customization of Virtual Partitions reducer for enhanced parallelization.</li> <li>[Enhancement] Add more error codes to early report on polling issues (kidlab)</li> <li>[Enhancement] Add <code>transport</code>, <code>network_exception</code> and <code>coordinator_load_in_progress</code> alongside <code>timed_out</code> to retryable errors for the proxy.</li> <li>[Enhancement] Improve <code>strict_topics_namespacing</code> validation message.</li> <li>[Change] Remove default empty thread name from <code>Async</code> since Web has been upgraded.</li> <li>[Fix] Installer doesn't respect directories in <code>KARAFKA_BOOT_FILE</code>.</li> <li>[Fix] Fix case where non absolute boot file path would not work as expected.</li> <li>[Fix] Allow for installing Karafka in a non-existing (yet) directory</li> <li>[Maintenance] Require <code>waterdrop</code> <code>&gt;=</code> <code>2.7.3</code> to support idempotent producer detection.</li> </ul>"}, {"location": "Changelog-Karafka/#242-2024-05-14", "title": "2.4.2 (2024-05-14)", "text": "<ul> <li>[Enhancement] Validate ActiveJob adapter custom producer format.</li> <li>[Fix] Internal seek does not resolve the offset correctly for time based lookup.</li> </ul>"}, {"location": "Changelog-Karafka/#241-2024-05-10", "title": "2.4.1 (2024-05-10)", "text": "<ul> <li>[Enhancement] Allow for usage of producer variants and alternative producers with ActiveJob Jobs (Pro).</li> <li>[Enhancement] Support <code>:earliest</code> and <code>:latest</code> in <code>Karafka::Admin#seek_consumer_group</code>.</li> <li>[Enhancement] Align configuration attributes mapper with exact librdkafka version used and not master.</li> <li>[Maintenance] Use <code>base64</code> from RubyGems as it will no longer be part of standard library in Ruby 3.4.</li> <li>[Fix] Support migrating via aliases and plan with aliases usage.</li> <li>[Fix] Active with default set to <code>false</code> cannot be overwritten</li> <li>[Fix] Fix inheritance of ActiveJob adapter <code>karafka_options</code> partitioner and dispatch method.</li> </ul>"}, {"location": "Changelog-Karafka/#240-2024-04-26", "title": "2.4.0 (2024-04-26)", "text": "<p>This release contains BREAKING changes. Make sure to read and apply upgrade notes.</p> <ul> <li>[Breaking] Drop Ruby <code>2.7</code> support.</li> <li>[Breaking] Drop the concept of consumer group mapping.</li> <li>[Breaking] <code>karafka topics migrate</code> will now perform declarative topics configuration alignment.</li> <li>[Breaking] Replace <code>deserializer</code> config with <code>#deserializers</code> in routing to support key and lazy header deserializers.</li> <li>[Breaking] Rename <code>Karafka::Serializers::JSON::Deserializer</code> to <code>Karafka::Deserializers::Payload</code> to reflect its role.</li> <li>[Feature] Support custom OAuth providers (with a lot of help from bruce-szalwinski-he and hotelengine.com).</li> <li>[Feature] Provide <code>karafka topics alter</code> for declarative topics alignment.</li> <li>[Feature] Introduce ability to use direct assignments (Pro).</li> <li>[Feature] Provide consumer piping API (Pro).</li> <li>[Feature] Introduce <code>karafka topics plan</code> to describe changes that will be applied when migrating.</li> <li>[Feature] Introduce ability to use custom message key deserializers.</li> <li>[Feature] Introduce ability to use custom message headers deserializers.</li> <li>[Feature] Provide <code>Karafka::Admin::Configs</code> API for cluster and topics configuration management.</li> <li>[Enhancement] Protect critical <code>rdkafka</code> thread executable code sections.</li> <li>[Enhancement] Assign names to internal threads for better debuggability when on <code>TTIN</code>.</li> <li>[Enhancement] Provide <code>log_polling</code> setting to the <code>Karafka::Instrumentation::LoggerListener</code> to silence polling in any non-debug mode.</li> <li>[Enhancement] Provide <code>metadata#message</code> to be able to retrieve message from metadata.</li> <li>[Enhancement] Include number of attempts prior to DLQ message being dispatched including the dispatch one (Pro).</li> <li>[Enhancement] Provide ability to decide how to dispatch from DLQ (sync / async).</li> <li>[Enhancement] Provide ability to decide how to mark as consumed from DLQ (sync / async).</li> <li>[Enhancement] Allow for usage of a custom Appsignal namespace when logging.</li> <li>[Enhancement] Do not run periodic jobs when LRJ job is running despite polling (LRJ can still start when Periodic runs).</li> <li>[Enhancement] Improve accuracy of periodic jobs and make sure they do not run too early after saturated work.</li> <li>[Enhancement] Introduce ability to async lock other subscription groups polling.</li> <li>[Enhancement] Improve shutdown when using long polling setup (high <code>max_wait_time</code>).</li> <li>[Enhancement] Provide <code>Karafka::Admin#read_lags_with_offsets</code> for ability to query lags and offsets of a given CG.</li> <li>[Enhancement] Allow direct assignments granular distribution in the Swarm (Pro).</li> <li>[Enhancement] Add a buffer to the supervisor supervision on shutdown to prevent a potential race condition when signal pass lags.</li> <li>[Enhancement] Provide ability to automatically generate and validate fingerprints of encrypted payload.</li> <li>[Enhancement] Support <code>enable.partition.eof</code> fast yielding.</li> <li>[Enhancement] Provide <code>#mark_as_consumed</code> and <code>#mark_as_consumed!</code> to the iterator.</li> <li>[Enhancement] Introduce graceful <code>#stop</code> to the iterator instead of recommending of usage of <code>break</code>.</li> <li>[Enhancement] Do not run jobs schedulers and other interval based operations on each job queue unlock.</li> <li>[Enhancement] Publish listeners status lifecycle events.</li> <li>[Enhancement] Use proxy wrapper for Admin metadata requests.</li> <li>[Enhancement] Use limited scope topic info data when operating on direct topics instead of full cluster queries.</li> <li>[Enhancement] No longer raise <code>Karafka::UnsupportedCaseError</code> for not recognized error types to support dynamic errors reporting.</li> <li>[Change] Do not create new proxy object to Rdkafka with certain low-level operations and re-use existing.</li> <li>[Change] Update <code>karafka.erb</code> template with a placeholder for waterdrop and karafka error instrumentation.</li> <li>[Change] Replace <code>statistics.emitted.error</code> error type with <code>callbacks.statistics.error</code> to align naming conventions.</li> <li>[Fix] Pro Swarm liveness listener can report incorrect failure when dynamic multiplexing scales down.</li> <li>[Fix] K8s liveness listener can report incorrect failure when dynamic multiplexing scales down.</li> <li>[Fix] Fix a case where connection conductor would not be released during manager state changes.</li> <li>[Fix] Make sure, that all <code>Admin</code> operations go through stabilization proxy.</li> <li>[Fix] Fix an issue where coordinator running jobs would not count periodic jobs and revocations.</li> <li>[Fix] Fix a case where critically crashed supervisor would raise incorrect error.</li> <li>[Fix] Re-raise critical supervisor errors before shutdown.</li> <li>[Fix] Fix a case when right-open (infinite) swarm matching would not pass validations.</li> <li>[Fix] Make <code>#enqueue_all</code> output compatible with <code>ActiveJob.perform_all_later</code> (oozzal)</li> <li>[Fix] Seek consumer group on a topic level is updating only recent partition.</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes", "title": "Upgrade Notes", "text": "<p>PLEASE MAKE SURE TO READ AND APPLY THEM!</p> <p>Available here.</p>"}, {"location": "Changelog-Karafka/#234-2024-04-11", "title": "2.3.4 (2024-04-11)", "text": "<ul> <li>[Fix] Seek consumer group on a topic level is updating only recent partition.</li> </ul>"}, {"location": "Changelog-Karafka/#233-2024-02-26", "title": "2.3.3 (2024-02-26)", "text": "<ul> <li>[Enhancement] Routing based topics allocation for swarm (Pro)</li> <li>[Enhancement] Publish the <code>-1</code> shutdown reason status for a non-responding node in swarm.</li> <li>[Enhancement] Allow for using the <code>distribution</code> mode for DataDog listener histogram reporting (Aerdayne).</li> <li>[Change] Change <code>internal.swarm.node_report_timeout</code> to 60 seconds from 30 seconds to compensate for long pollings.</li> <li>[Fix] Static membership routing evaluation happens too early in swarm.</li> <li>[Fix] Close producer in supervisor prior to forking and warmup to prevent invalid memory states.</li> </ul>"}, {"location": "Changelog-Karafka/#232-2024-02-16", "title": "2.3.2 (2024-02-16)", "text": "<ul> <li>[Feature] Provide swarm capabilities to OSS and Pro.</li> <li>[Feature] Provide ability to use complex strategies in DLQ (Pro).</li> <li>[Enhancement] Support using <code>:partition</code> as the partition key for ActiveJob assignments.</li> <li>[Enhancement] Expand Logger listener with swarm notifications.</li> <li>[Enhancement] Introduce K8s swarm liveness listener.</li> <li>[Enhancement] Use <code>Process.warmup</code> in Ruby 3.3+ prior to forks (in swarm) and prior to app start.</li> <li>[Enhancement] Provide <code>app.before_warmup</code> event to allow hooking code loading tools prior to final warmup.</li> <li>[Enhancement] Provide <code>Consumer#errors_tracker</code> to be able to get errors that occurred while doing complex recovery.</li> <li>[Fix] Infinite consecutive error flow with VPs and without DLQ can cause endless offsets accumulation.</li> <li>[Fix] Quieting mode causes too early unsubscribe.</li> </ul>"}, {"location": "Changelog-Karafka/#231-2024-02-08", "title": "2.3.1 (2024-02-08)", "text": "<ul> <li>[Refactor] Ensure that <code>Karafka::Helpers::Async#async_call</code> can run from multiple threads.</li> </ul>"}, {"location": "Changelog-Karafka/#230-2024-01-26", "title": "2.3.0 (2024-01-26)", "text": "<ul> <li>[Feature] Introduce Exactly-Once Semantics within consumers <code>#transaction</code> block (Pro)</li> <li>[Feature] Provide ability to multiplex subscription groups (Pro)</li> <li>[Feature] Provide <code>Karafka::Admin::Acl</code> for Kafka ACL management via the Admin APIs.</li> <li>[Feature] Periodic Jobs (Pro)</li> <li>[Feature] Offset Metadata storage (Pro)</li> <li>[Feature] Provide low-level listeners management API for dynamic resources scaling (Pro)</li> <li>[Enhancement] Improve shutdown process by allowing for parallel connections shutdown.</li> <li>[Enhancement] Introduce <code>non_blocking</code> routing API that aliases LRJ to indicate a different use-case for LRJ flow approach.</li> <li>[Enhancement] Allow to reset offset when seeking backwards by using the <code>reset_offset</code> keyword attribute set to <code>true</code>.</li> <li>[Enhancement] Alias producer operations in consumer to skip <code>#producer</code> reference.</li> <li>[Enhancement] Provide an <code>:independent</code> configuration to DLQ allowing to reset pause count track on each marking as consumed when retrying.</li> <li>[Enhancement] Remove no longer needed shutdown patches for <code>librdkafka</code> improving multi-sg shutdown times for <code>cooperative-sticky</code>.</li> <li>[Enhancement] Allow for parallel closing of connections from independent consumer groups.</li> <li>[Enhancement] Provide recovery flow for cases where DLQ dispatch would fail.</li> <li>[Change] Make <code>Kubernetes::LivenessListener</code> not start until Karafka app starts running.</li> <li>[Change] Remove the legacy \"inside of topics\" way of defining subscription groups names</li> <li>[Change] Update supported instrumentation to report on <code>#tick</code>.</li> <li>[Refactor] Replace <code>define_method</code> with <code>class_eval</code> in some locations.</li> <li>[Fix] Fix a case where internal Idle job scheduling would go via the consumption flow.</li> <li>[Fix] Make the Iterator <code>#stop_partition</code> work with karafka-rdkafka <code>0.14.6</code>.</li> <li>[Fix] Ensure Pro components are not loaded during OSS specs execution (not affecting usage).</li> <li>[Fix] Fix invalid action label for consumers in DataDog logger instrumentation.</li> <li>[Fix] Fix a scenario where <code>Karafka::Admin#seek_consumer_group</code> would fail because reaching not the coordinator.</li> <li>[Ignore] option --include-consumer-groups not working as intended after removal of \"thor\"</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_1", "title": "Upgrade Notes", "text": "<p>Available here.</p>"}, {"location": "Changelog-Karafka/#2214-2023-12-07", "title": "2.2.14 (2023-12-07)", "text": "<ul> <li>[Feature] Provide <code>Karafka::Admin#delete_consumer_group</code> and <code>Karafka::Admin#seek_consumer_group</code>.</li> <li>[Feature] Provide <code>Karafka::App.assignments</code> that will return real-time assignments tracking.</li> <li>[Enhancement] Make sure that the Scheduling API is thread-safe by default and allow for lock-less schedulers when schedulers are stateless.</li> <li>[Enhancement] \"Blockless\" topics with defaults</li> <li>[Enhancement] Provide a <code>finished?</code> method to the jobs for advanced reference based job schedulers.</li> <li>[Enhancement] Provide <code>client.reset</code> notification event.</li> <li>[Enhancement] Remove all usage of concurrent-ruby from Karafka</li> <li>[Change] Replace single #before_schedule with appropriate methods and events for scheduling various types of work. This is needed as we may run different framework logic on those and, second, for accurate job tracking with advanced schedulers.</li> <li>[Change] Rename <code>before_enqueue</code> to <code>before_schedule</code> to reflect what it does and when (internal).</li> <li>[Change] Remove not needed error catchers for strategies code. This code if errors, should be considered critical and should not be silenced.</li> <li>[Change] Remove not used notifications events.</li> </ul>"}, {"location": "Changelog-Karafka/#2213-2023-11-17", "title": "2.2.13 (2023-11-17)", "text": "<ul> <li>[Feature] Introduce low-level extended Scheduling API for granular control of schedulers and jobs execution [Pro].</li> <li>[Enhancement] Use separate lock for user-facing synchronization.</li> <li>[Enhancement] Instrument <code>consumer.before_enqueue</code>.</li> <li>[Enhancement] Limit usage of <code>concurrent-ruby</code> (plan to remove it as a dependency fully)</li> <li>[Enhancement] Provide <code>#synchronize</code> API same as in VPs for LRJs to allow for lifecycle events and consumption synchronization.</li> </ul>"}, {"location": "Changelog-Karafka/#2212-2023-11-09", "title": "2.2.12 (2023-11-09)", "text": "<ul> <li>[Enhancement] Rewrite the polling engine to update statistics and error callbacks despite longer non LRJ processing or long <code>max_wait_time</code> setups. This change provides stability to the statistics and background error emitting making them time-reliable.</li> <li>[Enhancement] Auto-update Inline Insights if new insights are present for all consumers and not only LRJ (OSS and Pro).</li> <li>[Enhancement] Alias <code>#insights</code> with <code>#inline_insights</code> and <code>#insights?</code> with <code>#inline_insights?</code></li> </ul>"}, {"location": "Changelog-Karafka/#2211-2023-11-03", "title": "2.2.11 (2023-11-03)", "text": "<ul> <li>[Enhancement] Allow marking as consumed in the user <code>#synchronize</code> block.</li> <li>[Enhancement] Make whole Pro VP marking as consumed concurrency safe for both async and sync scenarios.</li> <li>[Enhancement] Provide new alias to <code>karafka server</code>, that is: <code>karafka consumer</code>.</li> </ul>"}, {"location": "Changelog-Karafka/#2210-2023-11-02", "title": "2.2.10 (2023-11-02)", "text": "<ul> <li>[Enhancement] Allow for running <code>#pause</code> without specifying the offset (provide offset or <code>:consecutive</code>). This allows for pausing on the consecutive message (last received + 1), so after resume we will get last message received + 1 effectively not using <code>#seek</code> and not purging <code>librdafka</code> buffer preserving on networking. Please be mindful that this uses notion of last message passed from librdkafka, and not the last one available in the consumer (<code>messages.last</code>). While for regular cases they will be the same, when using things like DLQ, LRJs, VPs or Filtering API, those may not be the same.</li> <li>[Enhancement] Drastically improve network efficiency of operating with LRJ by using the <code>:consecutive</code> offset as default strategy for running LRJs without moving the offset in place and purging the data.</li> <li>[Enhancement] Do not \"seek in place\". When pausing and/or seeking to the same location as the current position, do nothing not to purge buffers and not to move to the same place where we are.</li> <li>[Fix] Pattern regexps should not be part of declaratives even when configured.</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_2", "title": "Upgrade Notes", "text": "<p>In the latest Karafka release, there are no breaking changes. However, please note the updates to #pause and #seek. If you spot any issues, please report them immediately. Your feedback is crucial.</p>"}, {"location": "Changelog-Karafka/#229-2023-10-24", "title": "2.2.9 (2023-10-24)", "text": "<ul> <li>[Enhancement] Allow using negative offset references in <code>Karafka::Admin#read_topic</code>.</li> <li>[Change] Make sure that WaterDrop <code>2.6.10</code> or higher is used with this release to support transactions fully and the Web-UI.</li> </ul>"}, {"location": "Changelog-Karafka/#228-2023-10-20", "title": "2.2.8 (2023-10-20)", "text": "<ul> <li>[Feature] Introduce Appsignal integration for errors and metrics tracking.</li> <li>[Enhancement] Expose <code>#synchronize</code> for VPs to allow for locks when cross-VP consumers work is needed.</li> <li>[Enhancement] Provide <code>#collapse_until!</code> direct consumer API to allow for collapsed virtual partitions consumer operations together with the Filtering API for advanced use-cases.</li> <li>[Refactor] Reorganize how rebalance events are propagated from <code>librdkafka</code> to Karafka. Replace <code>connection.client.rebalance_callback</code> with <code>rebalance.partitions_assigned</code> and <code>rebalance.partitions_revoked</code>. Introduce two extra events: <code>rebalance.partitions_assign</code> and <code>rebalance.partitions_revoke</code> to handle pre-rebalance future work.</li> <li>[Refactor] Remove <code>thor</code> as a CLI layer and rely on Ruby <code>OptParser</code></li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_3", "title": "Upgrade Notes", "text": "<ol> <li>Unless you were using <code>connection.client.rebalance_callback</code> which was considered private, nothing.</li> <li>None of the CLI commands should change but <code>thor</code> has been removed so please report if you find any bugs.</li> </ol>"}, {"location": "Changelog-Karafka/#227-2023-10-07", "title": "2.2.7 (2023-10-07)", "text": "<ul> <li>[Feature] Introduce Inline Insights to both OSS and Pro. Inline Insights allow you to get the Kafka insights/metrics from the consumer instance and use them to alter the processing flow. In Pro, there's an extra filter flow allowing to ensure, that the insights exist during consumption.</li> <li>[Enhancement] Make sure, that subscription groups ids are unique by including their consumer group id in them similar to how topics ids are handled (not a breaking change).</li> <li>[Enhancement] Expose <code>#attempt</code> method on a consumer to directly indicate number of attempt of processing given data.</li> <li>[Enhancement] Support Rails 7.1.</li> </ul>"}, {"location": "Changelog-Karafka/#226-2023-09-26", "title": "2.2.6 (2023-09-26)", "text": "<ul> <li>[Enhancement] Retry <code>Karafka::Admin#read_watermark_offsets</code> fetching upon <code>not_leader_for_partition</code> that can occur mostly on newly created topics in KRaft and after crashes during leader selection.</li> </ul>"}, {"location": "Changelog-Karafka/#225-2023-09-25", "title": "2.2.5 (2023-09-25)", "text": "<ul> <li>[Enhancement] Ensure, that when topic related operations end, the result is usable. There were few cases where admin operations on topics would finish successfully but internal Kafka caches would not report changes for a short period of time.</li> <li>[Enhancement] Stabilize cooperative-sticky early shutdown procedure.</li> <li>[Fix] use <code>#nil?</code> instead of <code>#present?</code> on <code>DataDog::Tracing::SpanOperation</code> (vitellochris)</li> <li>[Maintenance] Align connection clearing API with Rails 7.1 deprecation warning.</li> <li>[Maintenance] Make <code>#subscription_group</code> reference consistent in the Routing and Instrumentation.</li> <li>[Maintenance] Align the consumer pause instrumentation with client pause instrumentation by adding <code>subscription_group</code> visibility to the consumer.</li> </ul>"}, {"location": "Changelog-Karafka/#224-2023-09-13", "title": "2.2.4 (2023-09-13)", "text": "<ul> <li>[Enhancement] Compensate for potential Kafka cluster drifts vs consumer drift in batch metadata (#1611).</li> </ul>"}, {"location": "Changelog-Karafka/#223-2023-09-12", "title": "2.2.3 (2023-09-12)", "text": "<ul> <li>[Fix] Karafka admin time based offset lookup can break for one non-default partition.</li> </ul>"}, {"location": "Changelog-Karafka/#222-2023-09-11", "title": "2.2.2 (2023-09-11)", "text": "<ul> <li>[Feature] Provide ability to define routing defaults.</li> <li>[Maintenance] Require <code>karafka-core</code> <code>&gt;=</code> <code>2.2.2</code></li> </ul>"}, {"location": "Changelog-Karafka/#221-2023-09-01", "title": "2.2.1 (2023-09-01)", "text": "<ul> <li>[Fix] Fix insufficient validation of named patterns</li> <li>[Maintenance] Rely on <code>2.2.x</code> <code>karafka-core</code>.</li> </ul>"}, {"location": "Changelog-Karafka/#220-2023-09-01", "title": "2.2.0 (2023-09-01)", "text": "<ul> <li>[Feature] Introduce dynamic topic subscriptions based on patterns [Pro].</li> <li>[Enhancement] Allow for <code>Karafka::Admin</code> setup reconfiguration via <code>config.admin</code> scope.</li> <li>[Enhancement] Make sure that consumer group used by <code>Karafka::Admin</code> obeys the <code>ConsumerMapper</code> setup.</li> <li>[Fix] Fix a case where subscription group would not accept a symbol name.</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_4", "title": "Upgrade Notes", "text": "<p>As always, please make sure you have upgraded to the most recent version of <code>2.1</code> before upgrading to <code>2.2</code>.</p> <p>If you are not using Kafka ACLs, there is no action you need to take.</p> <p>If you are using Kafka ACLs and you've set up permissions for <code>karafka_admin</code> group, please note that this name has now been changed and is subject to Consumer Name Mapping.</p> <p>That means you must ensure that the new consumer group that by default equals <code>CLIENT_ID_karafka_admin</code> has appropriate permissions. Please note that the Web UI also uses this group.</p> <p><code>Karafka::Admin</code> now has its own set of configuration options available, and you can find more details about that here.</p> <p>If you want to maintain the <code>2.1</code> behavior, that is <code>karafka_admin</code> admin group, we recommend introducing this case inside your consumer mapper. Assuming you use the default one, the code will look as follows:</p> <pre><code>  class MyMapper\n    def call(raw_consumer_group_name)\n      # If group is the admin one, use as it was in 2.1\n      return 'karafka_admin' if raw_consumer_group_name == 'karafka_admin'\n\n      # Otherwise use default karafka strategy for the rest\n      \"#{Karafka::App.config.client_id}_#{raw_consumer_group_name}\"\n    end\n  end\n</code></pre>"}, {"location": "Changelog-Karafka/#2113-2023-08-28", "title": "2.1.13 (2023-08-28)", "text": "<ul> <li>[Feature] Introduce Cleaning API for much better memory management for iterative data processing [Pro].</li> <li>[Enhancement] Automatically free message resources after processed for ActiveJob jobs [Pro]</li> <li>[Enhancement] Free memory used by the raw payload as fast as possible after obtaining it from <code>karafka-rdkafka</code>.</li> <li>[Enhancement] Support changing <code>service_name</code> in DataDog integration.</li> </ul>"}, {"location": "Changelog-Karafka/#2112-2023-08-25", "title": "2.1.12 (2023-08-25)", "text": "<ul> <li>[Fix] Fix a case where DLQ + VP without intermediate marking would mark earlier message then the last one.</li> </ul>"}, {"location": "Changelog-Karafka/#2111-2023-08-23", "title": "2.1.11 (2023-08-23)", "text": "<ul> <li>[Enhancement] Expand the error handling for offset related queries with timeout error retries.</li> <li>[Enhancement] Allow for connection proxy timeouts configuration.</li> </ul>"}, {"location": "Changelog-Karafka/#2110-2023-08-21", "title": "2.1.10 (2023-08-21)", "text": "<ul> <li>[Enhancement] Introduce <code>connection.client.rebalance_callback</code> event for instrumentation of rebalances.</li> <li>[Enhancement] Introduce new <code>runner.before_call</code> monitor event.</li> <li>[Refactor] Introduce low level commands proxy to handle deviation in how we want to run certain commands and how rdkafka-ruby runs that by design.</li> <li>[Change] No longer validate excluded topics routing presence if patterns any as it does not match pattern subscriptions where you can exclude things that could be subscribed in the future.</li> <li>[Fix] do not report negative lag stored in the DD listener.</li> <li>[Fix] Do not report lags in the DD listener for cases where the assignment is not workable.</li> <li>[Fix] Do not report negative lags in the DD listener.</li> <li>[Fix] Extremely fast shutdown after boot in specs can cause process not to stop.</li> <li>[Fix] Disable <code>allow.auto.create.topics</code> for admin by default to prevent accidental topics creation on topics metadata lookups.</li> <li>[Fix] Improve the <code>query_watermark_offsets</code> operations by increasing too low timeout.</li> <li>[Fix] Increase <code>TplBuilder</code> timeouts to compensate for remote clusters.</li> <li>[Fix] Always try to unsubscribe short-lived consumers used throughout the system, especially in the admin APIs.</li> <li>[Fix] Add missing <code>connection.client.poll.error</code> error type reference.</li> </ul>"}, {"location": "Changelog-Karafka/#219-2023-08-06", "title": "2.1.9 (2023-08-06)", "text": "<ul> <li>[Feature] Introduce ability to customize pause strategy on a per topic basis (Pro).</li> <li>[Enhancement] Disable the extensive messages logging in the default <code>karafka.rb</code> template.</li> <li>[Change] Require <code>waterdrop</code> <code>&gt;= 2.6.6</code> due to extra <code>LoggerListener</code> API.</li> </ul>"}, {"location": "Changelog-Karafka/#218-2023-07-29", "title": "2.1.8 (2023-07-29)", "text": "<ul> <li>[Enhancement] Introduce <code>Karafka::BaseConsumer#used?</code> method to indicate, that at least one invocation of <code>#consume</code> took or will take place. This can be used as a replacement to the non-direct <code>messages.count</code> check for shutdown and revocation to ensure, that the consumption took place or is taking place (in case of running LRJ).</li> <li>[Enhancement] Make <code>messages#to_a</code> return copy of the underlying array to prevent scenarios, where the mutation impacts offset management.</li> <li>[Enhancement] Mitigate a librdkafka <code>cooperative-sticky</code> rebalance crash issue.</li> <li>[Enhancement] Provide ability to overwrite <code>consumer_persistence</code> per subscribed topic. This is mostly useful for plugins and extensions developers.</li> <li>[Fix] Fix a case where the performance tracker would crash in case of mutation of messages to an empty state.</li> </ul>"}, {"location": "Changelog-Karafka/#217-2023-07-22", "title": "2.1.7 (2023-07-22)", "text": "<ul> <li>[Enhancement] Always query for watermarks in the Iterator to improve the initial response time.</li> <li>[Enhancement] Add <code>max_wait_time</code> option to the Iterator.</li> <li>[Fix] Fix a case where <code>Admin#read_topic</code> would wait for poll interval on non-existing messages instead of early exit.</li> <li>[Fix] Fix a case where Iterator with per partition offsets with negative lookups would go below the number of available messages.</li> <li>[Fix] Remove unused constant from Admin module.</li> <li>[Fix] Add missing <code>connection.client.rebalance_callback.error</code> to the <code>LoggerListener</code> instrumentation hook.</li> </ul>"}, {"location": "Changelog-Karafka/#216-2023-06-29", "title": "2.1.6 (2023-06-29)", "text": "<ul> <li>[Enhancement] Provide time support for iterator</li> <li>[Enhancement] Provide time support for admin <code>#read_topic</code></li> <li>[Enhancement] Provide time support for consumer <code>#seek</code>.</li> <li>[Enhancement] Remove no longer needed locks for client operations.</li> <li>[Enhancement] Raise <code>Karafka::Errors::TopicNotFoundError</code> when trying to iterate over non-existing topic.</li> <li>[Enhancement] Ensure that Kafka multi-command operations run under mutex together.</li> <li>[Change] Require <code>waterdrop</code> <code>&gt;= 2.6.2</code></li> <li>[Change] Require <code>karafka-core</code> <code>&gt;= 2.1.1</code></li> <li>[Refactor] Clean-up iterator code.</li> <li>[Fix]  Improve performance in dev environment for a Rails app (juike)</li> <li>[Fix] Rename <code>InvalidRealOffsetUsage</code> to <code>InvalidRealOffsetUsageError</code> to align with naming of other errors.</li> <li>[Fix] Fix unstable spec.</li> <li>[Fix] Fix a case where automatic <code>#seek</code> would overwrite manual seek of a user when running LRJ.</li> <li>[Fix] Make sure, that user direct <code>#seek</code> and <code>#pause</code> operations take precedence over system actions.</li> <li>[Fix] Make sure, that <code>#pause</code> and <code>#resume</code> with one underlying connection do not race-condition.</li> </ul>"}, {"location": "Changelog-Karafka/#215-2023-06-19", "title": "2.1.5 (2023-06-19)", "text": "<ul> <li>[Enhancement] Drastically improve <code>#revoked?</code> response quality by checking the real time assignment lost state on librdkafka.</li> <li>[Enhancement] Improve eviction of saturated jobs that would run on already revoked assignments.</li> <li>[Enhancement] Expose <code>#commit_offsets</code> and <code>#commit_offsets!</code> methods in the consumer to provide ability to commit offsets directly to Kafka without having to mark new messages as consumed.</li> <li>[Enhancement] No longer skip offset commit when no messages marked as consumed as <code>librdkafka</code> has fixed the crashes there.</li> <li>[Enhancement] Remove no longer needed patches.</li> <li>[Enhancement] Ensure, that the coordinator revocation status is switched upon revocation detection when using <code>#revoked?</code></li> <li>[Enhancement] Add benchmarks for marking as consumed (sync and async).</li> <li>[Change] Require <code>karafka-core</code> <code>&gt;= 2.1.0</code></li> <li>[Change] Require <code>waterdrop</code> <code>&gt;= 2.6.1</code></li> </ul>"}, {"location": "Changelog-Karafka/#214-2023-06-06", "title": "2.1.4 (2023-06-06)", "text": "<ul> <li>[Fix] <code>processing_lag</code> and <code>consumption_lag</code> on empty batch fail on shutdown usage (#1475)</li> </ul>"}, {"location": "Changelog-Karafka/#213-2023-05-29", "title": "2.1.3 (2023-05-29)", "text": "<ul> <li>[Maintenance] Add linter to ensure, that all integration specs end with <code>_spec.rb</code>.</li> <li>[Fix] Fix <code>#retrying?</code> helper result value (Aerdayne).</li> <li>[Fix] Fix <code>mark_as_consumed!</code> raising an error instead of <code>false</code> on <code>unknown_member_id</code> (#1461).</li> <li>[Fix] Enable phantom tests.</li> </ul>"}, {"location": "Changelog-Karafka/#212-2023-05-26", "title": "2.1.2 (2023-05-26)", "text": "<ul> <li>Set minimum <code>karafka-core</code> on <code>2.0.13</code> to make sure correct version of <code>karafka-rdkafka</code> is used.</li> <li>Set minimum <code>waterdrop</code> on <code>2.5.3</code> to make sure correct version of <code>waterdrop</code> is used.</li> </ul>"}, {"location": "Changelog-Karafka/#211-2023-05-24", "title": "2.1.1 (2023-05-24)", "text": "<ul> <li>[Fix] Liveness Probe Doesn't Meet HTTP 1.1 Criteria - Causing Kubernetes Restarts (#1450)</li> </ul>"}, {"location": "Changelog-Karafka/#210-2023-05-22", "title": "2.1.0 (2023-05-22)", "text": "<ul> <li>[Feature] Provide ability to use CurrentAttributes with ActiveJob's Karafka adapter (federicomoretti).</li> <li>[Feature] Introduce collective Virtual Partitions offset management.</li> <li>[Feature] Use virtual offsets to filter out messages that would be re-processed upon retries.</li> <li>[Enhancement] No longer break processing on failing parallel virtual partitions in ActiveJob because it is compensated by virtual marking.</li> <li>[Enhancement] Always use Virtual offset management for Pro ActiveJobs.</li> <li>[Enhancement] Do not attempt to mark offsets on already revoked partitions.</li> <li>[Enhancement] Make sure, that VP components are not injected into non VP strategies.</li> <li>[Enhancement] Improve complex strategies inheritance flow.</li> <li>[Enhancement] Optimize offset management for DLQ + MoM feature combinations.</li> <li>[Change] Removed <code>Karafka::Pro::BaseConsumer</code> in favor of <code>Karafka::BaseConsumer</code>. (#1345)</li> <li>[Fix] Fix for <code>max_messages</code> and <code>max_wait_time</code> not having reference in errors.yml (#1443)</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_5", "title": "Upgrade Notes", "text": "<ol> <li>Upgrade to Karafka <code>2.0.41</code> prior to upgrading to <code>2.1.0</code>.</li> <li>Replace <code>Karafka::Pro::BaseConsumer</code> references to <code>Karafka::BaseConsumer</code>.</li> <li>Replace <code>Karafka::Instrumentation::Vendors::Datadog:Listener</code> with <code>Karafka::Instrumentation::Vendors::Datadog::MetricsListener</code>.</li> </ol>"}, {"location": "Changelog-Karafka/#2041-2023-04-19", "title": "2.0.41 (2023-04-19)", "text": "<ul> <li>[Feature] Provide <code>Karafka::Pro::Iterator</code> for anonymous topic/partitions iterations and messages lookups (#1389 and #1427).</li> <li>[Enhancement] Optimize topic lookup for <code>read_topic</code> admin method usage.</li> <li>[Enhancement] Report via <code>LoggerListener</code> information about the partition on which a given job has started and finished.</li> <li>[Enhancement] Slightly normalize the <code>LoggerListener</code> format. Always report partition related operations as followed: <code>TOPIC_NAME/PARTITION</code>.</li> <li>[Enhancement] Do not retry recovery from <code>unknown_topic_or_part</code> when Karafka is shutting down as there is no point and no risk of any data losses.</li> <li>[Enhancement] Report <code>client.software.name</code> and <code>client.software.version</code> according to <code>librdkafka</code> recommendation.</li> <li>[Enhancement] Report ten longest integration specs after the suite execution.</li> <li>[Enhancement] Prevent user originating errors related to statistics processing after listener loop crash from potentially crashing the listener loop and hanging Karafka process.</li> </ul>"}, {"location": "Changelog-Karafka/#2040-2023-04-13", "title": "2.0.40 (2023-04-13)", "text": "<ul> <li>[Enhancement] Introduce <code>Karafka::Messages::Messages#empty?</code> method to handle Idle related cases where shutdown or revocation would be called on an empty messages set. This method allows for checking if there are any messages in the messages batch.</li> <li>[Refactor] Require messages builder to accept partition and do not fetch it from messages.</li> <li>[Refactor] Use empty messages set for internal APIs (Idle) (so there always is <code>Karafka::Messages::Messages</code>)</li> <li>[Refactor] Allow for empty messages set initialization with -1001 and -1 on metadata (similar to <code>librdkafka</code>)</li> </ul>"}, {"location": "Changelog-Karafka/#2039-2023-04-11", "title": "2.0.39 (2023-04-11)", "text": "<ul> <li>[Feature] Provide ability to throttle/limit number of messages processed in a time unit (#1203)</li> <li>[Feature] Provide Delayed Topics (#1000)</li> <li>[Feature] Provide ability to expire messages (expiring topics)</li> <li>[Feature] Provide ability to apply filters after messages are polled and before enqueued. This is a generic filter API for any usage.</li> <li>[Enhancement] When using ActiveJob with Virtual Partitions, Karafka will stop if collectively VPs are failing. This minimizes number of jobs that will be collectively re-processed.</li> <li>[Enhancement] <code>#retrying?</code> method has been added to consumers to provide ability to check, that we're reprocessing data after a failure. This is useful for branching out processing based on errors.</li> <li>[Enhancement] Track active_job_id in instrumentation (#1372)</li> <li>[Enhancement] Introduce new housekeeping job type called <code>Idle</code> for non-consumption execution flows.</li> <li>[Enhancement] Change how a manual offset management works with Long-Running Jobs. Use the last message offset to move forward instead of relying on the last message marked as consumed for a scenario where no message is marked.</li> <li>[Enhancement] Prioritize in Pro non-consumption jobs execution over consumption despite LJF. This will ensure, that housekeeping as well as other non-consumption events are not saturated when running a lot of work.</li> <li>[Enhancement] Normalize the DLQ behaviour with MoM. Always pause on dispatch for all the strategies.</li> <li>[Enhancement] Improve the manual offset management and DLQ behaviour when no markings occur for OSS.</li> <li>[Enhancement] Do not early stop ActiveJob work running under virtual partitions to prevent extensive reprocessing.</li> <li>[Enhancement] Drastically increase number of scenarios covered by integration specs (OSS and Pro).</li> <li>[Enhancement] Introduce a <code>Coordinator#synchronize</code> lock for cross virtual partitions operations.</li> <li>[Fix] Do not resume partition that is not paused.</li> <li>[Fix] Fix <code>LoggerListener</code> cases where logs would not include caller id (when available)</li> <li>[Fix] Fix not working benchmark tests.</li> <li>[Fix] Fix a case where when using manual offset management with a user pause would ignore the pause and seek to the next message.</li> <li>[Fix] Fix a case where dead letter queue would go into an infinite loop on message with first ever offset if the first ever offset would not recover.</li> <li>[Fix] Make sure to resume always for all LRJ strategies on revocation.</li> <li>[Refactor] Make sure that coordinator is topic aware. Needed for throttling, delayed processing and expired jobs.</li> <li>[Refactor] Put Pro strategies into namespaces to better organize multiple combinations.</li> <li>[Refactor] Do not rely on messages metadata for internal topic and partition operations like <code>#seek</code> so they can run independently from the consumption flow.</li> <li>[Refactor] Hold a single topic/partition reference on a coordinator instead of in executor, coordinator and consumer.</li> <li>[Refactor] Move <code>#mark_as_consumed</code> and <code>#mark_as_consumed!</code>into <code>Strategies::Default</code> to be able to introduce marking for virtual partitions.</li> </ul>"}, {"location": "Changelog-Karafka/#2038-2023-03-27", "title": "2.0.38 (2023-03-27)", "text": "<ul> <li>[Enhancement] Introduce <code>Karafka::Admin#read_watermark_offsets</code> to get low and high watermark offsets values.</li> <li>[Enhancement] Track active_job_id in instrumentation (#1372)</li> <li>[Enhancement] Improve <code>#read_topic</code> reading in case of a compacted partition where the offset is below the low watermark offset. This should optimize reading and should not go beyond the low watermark offset.</li> <li>[Enhancement] Allow <code>#read_topic</code> to accept instance settings to overwrite any settings needed to customize reading behaviours.</li> </ul>"}, {"location": "Changelog-Karafka/#2037-2023-03-20", "title": "2.0.37 (2023-03-20)", "text": "<ul> <li>[Fix] Declarative topics execution on a secondary cluster run topics creation on the primary one (#1365)</li> <li>[Fix]  Admin read operations commit offset when not needed (#1369)</li> </ul>"}, {"location": "Changelog-Karafka/#2036-2023-03-17", "title": "2.0.36 (2023-03-17)", "text": "<ul> <li>[Refactor] Rename internal naming of <code>Structurable</code> to <code>Declaratives</code> for declarative topics feature.</li> <li>[Fix] AJ + DLQ + MOM + LRJ is pausing indefinitely after the first job (#1362)</li> </ul>"}, {"location": "Changelog-Karafka/#2035-2023-03-13", "title": "2.0.35 (2023-03-13)", "text": "<ul> <li>[Feature] Allow for defining topics config via the DSL and its automatic creation via CLI command.</li> <li>[Feature] Allow for full topics reset and topics repartitioning via the CLI.</li> </ul>"}, {"location": "Changelog-Karafka/#2034-2023-03-04", "title": "2.0.34 (2023-03-04)", "text": "<ul> <li>[Enhancement] Attach an <code>embedded</code> tag to Karafka processes started using the embedded API.</li> <li>[Change] Renamed <code>Datadog::Listener</code> to <code>Datadog::MetricsListener</code> for consistency. (#1124)</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_6", "title": "Upgrade Notes", "text": "<ol> <li>Replace <code>Datadog::Listener</code> references to <code>Datadog::MetricsListener</code>.</li> </ol>"}, {"location": "Changelog-Karafka/#2033-2023-02-24", "title": "2.0.33 (2023-02-24)", "text": "<ul> <li>[Feature] Support <code>perform_all_later</code> in ActiveJob adapter for Rails <code>7.1+</code></li> <li>[Feature] Introduce ability to assign and re-assign tags in consumer instances. This can be used for extra instrumentation that is context aware.</li> <li>[Feature] Introduce ability to assign and reassign tags to the <code>Karafka::Process</code>.</li> <li>[Enhancement] When using <code>ActiveJob</code> adapter, automatically tag jobs with the name of the <code>ActiveJob</code> class that is running inside of the <code>ActiveJob</code> consumer.</li> <li>[Enhancement] Make <code>::Karafka::Instrumentation::Notifications::EVENTS</code> list public for anyone wanting to re-bind those into a different notification bus.</li> <li>[Enhancement] Set <code>fetch.message.max.bytes</code> for <code>Karafka::Admin</code> to <code>5MB</code> to make sure that all data is fetched correctly for Web UI under heavy load (many consumers).</li> <li>[Enhancement] Introduce a <code>strict_topics_namespacing</code> config option to enable/disable the strict topics naming validations. This can be useful when working with pre-existing topics which we cannot or do not want to rename.</li> <li>[Fix] Karafka monitor is prematurely cached (#1314)</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_7", "title": "Upgrade Notes", "text": "<p>Since <code>#tags</code> were introduced on consumers, the <code>#tags</code> method is now part of the consumers API.</p> <p>This means, that in case you were using a method called <code>#tags</code> in your consumers, you will have to rename it:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      tags &lt;&lt; message.payload.tag\n    end\n\n    tags.each { |tags| puts tag }\n  end\n\n  private\n\n  # This will collide with the tagging API\n  # This NEEDS to be renamed not to collide with `#tags` method provided by the consumers API.\n  def tags\n    @tags ||= Set.new\n  end\nend\n</code></pre>"}, {"location": "Changelog-Karafka/#2032-2023-02-13", "title": "2.0.32 (2023-02-13)", "text": "<ul> <li>[Fix] Many non-existing topic subscriptions propagate poll errors beyond client</li> <li>[Enhancement] Ignore <code>unknown_topic_or_part</code> errors in dev when <code>allow.auto.create.topics</code> is on.</li> <li>[Enhancement] Optimize temporary errors handling in polling for a better backoff policy</li> </ul>"}, {"location": "Changelog-Karafka/#2031-2023-02-12", "title": "2.0.31 (2023-02-12)", "text": "<ul> <li>[Feature] Allow for adding partitions via <code>Admin#create_partitions</code> API.</li> <li>[Fix] Do not ignore admin errors upon invalid configuration (#1254)</li> <li>[Fix] Topic name validation (#1300) - CandyFet</li> <li>[Enhancement] Increase the <code>max_wait_timeout</code> on admin operations to five minutes to make sure no timeout on heavily loaded clusters.</li> <li>[Maintenance] Require <code>karafka-core</code> &gt;= <code>2.0.11</code> and switch to shared RSpec locator.</li> <li>[Maintenance] Require <code>karafka-rdkafka</code> &gt;= <code>0.12.1</code></li> </ul>"}, {"location": "Changelog-Karafka/#2030-2023-01-31", "title": "2.0.30 (2023-01-31)", "text": "<ul> <li>[Enhancement] Alias <code>--consumer-groups</code> with <code>--include-consumer-groups</code></li> <li>[Enhancement] Alias <code>--subscription-groups</code> with <code>--include-subscription-groups</code></li> <li>[Enhancement] Alias <code>--topics</code> with <code>--include-topics</code></li> <li>[Enhancement] Introduce <code>--exclude-consumer-groups</code> for ability to exclude certain consumer groups from running</li> <li>[Enhancement] Introduce <code>--exclude-subscription-groups</code> for ability to exclude certain subscription groups from running</li> <li>[Enhancement] Introduce <code>--exclude-topics</code> for ability to exclude certain topics from running</li> </ul>"}, {"location": "Changelog-Karafka/#2029-2023-01-30", "title": "2.0.29 (2023-01-30)", "text": "<ul> <li>[Enhancement] Make sure, that the <code>Karafka#producer</code> instance has the <code>LoggerListener</code> enabled in the install template, so Karafka by default prints both consumer and producer info.</li> <li>[Enhancement] Extract the code loading capabilities of Karafka console from the executable, so web can use it to provide CLI commands.</li> <li>[Fix] Fix for: running karafka console results in NameError with Rails (#1280)</li> <li>[Fix] Make sure, that the <code>caller</code> for async errors is being published.</li> <li>[Change] Make sure that WaterDrop <code>2.4.10</code> or higher is used with this release to support Web-UI.</li> </ul>"}, {"location": "Changelog-Karafka/#2028-2023-01-25", "title": "2.0.28 (2023-01-25)", "text": "<ul> <li>[Feature] Provide the ability to use Dead Letter Queue with Virtual Partitions.</li> <li>[Enhancement] Collapse Virtual Partitions upon retryable error to a single partition. This allows dead letter queue to operate and mitigate issues arising from work virtualization. This removes uncertainties upon errors that can be retried and processed. Affects given topic partition virtualization only for multi-topic and multi-partition parallelization. It also minimizes potential \"flickering\" where given data set has potentially many corrupted messages. The collapse will last until all the messages from the collective corrupted batch are processed. After that, virtualization will resume.</li> <li>[Enhancement] Introduce <code>#collapsed?</code> consumer method available for consumers using Virtual Partitions.</li> <li>[Enhancement] Allow for customization of DLQ dispatched message details in Pro (#1266) via the <code>#enhance_dlq_message</code> consumer method.</li> <li>[Enhancement] Include <code>original_consumer_group</code> in the DLQ dispatched messages in Pro.</li> <li>[Enhancement] Use Karafka <code>client_id</code> as kafka <code>client.id</code> value by default</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_8", "title": "Upgrade Notes", "text": "<p>If you want to continue to use <code>karafka</code> as default for kafka <code>client.id</code>, assign it manually:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other settings...\n    config.kafka = {\n      'client.id': 'karafka'\n    }\n</code></pre>"}, {"location": "Changelog-Karafka/#2027-2023-01-11", "title": "2.0.27 (2023-01-11)", "text": "<ul> <li>Do not lock Ruby version in Karafka in favour of <code>karafka-core</code>.</li> <li>Make sure <code>karafka-core</code> version is at least <code>2.0.9</code> to make sure we run <code>karafka-rdkafka</code>.</li> </ul>"}, {"location": "Changelog-Karafka/#2026-2023-01-10", "title": "2.0.26 (2023-01-10)", "text": "<ul> <li>[Feature] Allow for disabling given topics by setting <code>active</code> to false. It will exclude them from consumption but will allow to have their definitions for using admin APIs, etc.</li> <li>[Enhancement] Early terminate on <code>read_topic</code> when reaching the last offset available on the request time.</li> <li>[Enhancement] Introduce a <code>quiet</code> state that indicates that Karafka is not only moving to quiet mode but actually that it reached it and no work will happen anymore in any of the consumer groups.</li> <li>[Enhancement] Use Karafka defined routes topics when possible for <code>read_topic</code> admin API.</li> <li>[Enhancement] Introduce <code>client.pause</code> and <code>client.resume</code> instrumentation hooks for tracking client topic partition pausing and resuming. This is alongside of <code>consumer.consuming.pause</code> that can be used to track both manual and automatic pausing with more granular consumer related details. The <code>client.*</code> should be used for low level tracking.</li> <li>[Enhancement] Replace <code>LoggerListener</code> pause notification with one based on <code>client.pause</code> instead of <code>consumer.consuming.pause</code>.</li> <li>[Enhancement] Expand <code>LoggerListener</code> with <code>client.resume</code> notification.</li> <li>[Enhancement] Replace random anonymous subscription groups ids with stable once.</li> <li>[Enhancement] Add <code>consumer.consume</code>, <code>consumer.revoke</code> and <code>consumer.shutting_down</code> notification events and move the revocation logic calling to strategies.</li> <li>[Change] Rename job queue statistics <code>processing</code> key to <code>busy</code>. No changes needed because naming in the DataDog listener stays the same.</li> <li>[Fix] Fix proctitle listener state changes reporting on new states.</li> <li>[Fix] Make sure all files descriptors are closed in the integration specs.</li> <li>[Fix] Fix a case where empty subscription groups could leak into the execution flow.</li> <li>[Fix] Fix <code>LoggerListener</code> reporting so it does not end with <code>.</code>.</li> <li>[Fix] Run previously defined (if any) signal traps created prior to Karafka signals traps.</li> </ul>"}, {"location": "Changelog-Karafka/#2025-2023-01-10", "title": "2.0.25 (2023-01-10)", "text": "<ul> <li>Release yanked due to accidental release with local changes.</li> </ul>"}, {"location": "Changelog-Karafka/#2024-2022-12-19", "title": "2.0.24 (2022-12-19)", "text": "<ul> <li>[Feature] Provide out of the box encryption support for Pro.</li> <li>[Enhancement] Add instrumentation upon <code>#pause</code>.</li> <li>[Enhancement] Add instrumentation upon retries.</li> <li>[Enhancement] Assign <code>#id</code> to consumers similar to other entities for ease of debugging.</li> <li>[Enhancement] Add retries and pausing to the default <code>LoggerListener</code>.</li> <li>[Enhancement] Introduce a new final <code>terminated</code> state that will kick in prior to exit but after all the instrumentation and other things are done.</li> <li>[Enhancement] Ensure that state transitions are thread-safe and ensure state transitions can occur in one direction.</li> <li>[Enhancement] Optimize status methods proxying to <code>Karafka::App</code>.</li> <li>[Enhancement] Allow for easier state usage by introducing explicit <code>#to_s</code> for reporting.</li> <li>[Enhancement] Change auto-generated id from <code>SecureRandom#uuid</code> to <code>SecureRandom#hex(6)</code></li> <li>[Enhancement] Emit statistic every 5 seconds by default.</li> <li>[Enhancement] Introduce general messages parser that can be swapped when needed.</li> <li>[Fix] Do not trigger code reloading when <code>consumer_persistence</code> is enabled.</li> <li>[Fix] Shutdown producer after all the consumer components are down and the status is stopped. This will ensure, that any instrumentation related Kafka messaging can still operate.</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_9", "title": "Upgrade Notes", "text": "<p>If you want to disable <code>librdkafka</code> statistics because you do not use them at all, update the <code>kafka</code> <code>statistics.interval.ms</code> setting and set it to <code>0</code>:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other settings...\n    config.kafka = {\n      'statistics.interval.ms': 0\n    }\n  end\nend\n</code></pre>"}, {"location": "Changelog-Karafka/#2023-2022-12-07", "title": "2.0.23 (2022-12-07)", "text": "<ul> <li>[Maintenance] Align with <code>waterdrop</code> and <code>karafka-core</code></li> <li>[Enhancement] Provide <code>Admin#read_topic</code> API to get topic data without subscribing.</li> <li>[Enhancement] Upon an end user <code>#pause</code>, do not commit the offset in automatic offset management mode. This will prevent from a scenario where pause is needed but during it a rebalance occurs and a different assigned process starts not from the pause location but from the automatic offset that may be different. This still allows for using the <code>#mark_as_consumed</code>.</li> <li>[Fix] Fix a scenario where manual <code>#pause</code> would be overwritten by a resume initiated by the strategy.</li> <li>[Fix] Fix a scenario where manual <code>#pause</code> in LRJ would cause infinite pause.</li> </ul>"}, {"location": "Changelog-Karafka/#2022-2022-12-02", "title": "2.0.22 (2022-12-02)", "text": "<ul> <li>[Enhancement] Load Pro components upon Karafka require so they can be altered prior to setup.</li> <li>[Enhancement] Do not run LRJ jobs that were added to the jobs queue but were revoked meanwhile.</li> <li>[Enhancement] Allow running particular named subscription groups similar to consumer groups.</li> <li>[Enhancement] Allow running particular topics similar to consumer groups.</li> <li>[Enhancement] Raise configuration error when trying to run Karafka with options leading to no subscriptions.</li> <li>[Fix] Fix <code>karafka info</code> subscription groups count reporting as it was misleading.</li> <li>[Fix] Allow for defining subscription groups with symbols similar to consumer groups and topics to align the API.</li> <li>[Fix] Do not allow for an explicit <code>nil</code> as a <code>subscription_group</code> block argument.</li> <li>[Fix] Fix instability in subscription groups static members ids when using <code>--consumer_groups</code> CLI flag.</li> <li>[Fix] Fix a case in routing, where anonymous subscription group could not be used inside of a consumer group.</li> <li>[Fix] Fix a case where shutdown prior to listeners build would crash the server initialization.</li> <li>[Fix] Duplicated logs in development environment for Rails when logger set to <code>$stdout</code>.</li> </ul>"}, {"location": "Changelog-Karafka/#20021-2022-11-25", "title": "20.0.21 (2022-11-25)", "text": "<ul> <li>[Enhancement] Make revocation jobs for LRJ topics non-blocking to prevent blocking polling when someone uses non-revocation aware LRJ jobs and revocation happens.</li> </ul>"}, {"location": "Changelog-Karafka/#2020-2022-11-24", "title": "2.0.20 (2022-11-24)", "text": "<ul> <li>[Enhancement] Support <code>group.instance.id</code> assignment (static group membership) for a case where a single consumer group has multiple subscription groups (#1173).</li> </ul>"}, {"location": "Changelog-Karafka/#2019-2022-11-20", "title": "2.0.19 (2022-11-20)", "text": "<ul> <li>[Feature] Provide ability to skip failing messages without dispatching them to an alternative topic (DLQ).</li> <li>[Enhancement] Improve the integration with Ruby on Rails by preventing double-require of components.</li> <li>[Enhancement] Improve stability of the shutdown process upon critical errors.</li> <li>[Enhancement] Improve stability of the integrations spec suite.</li> <li>[Fix] Fix an issue where upon fast startup of multiple subscription groups from the same consumer group, a ghost queue would be created due to problems in <code>Concurrent::Hash</code>.</li> </ul>"}, {"location": "Changelog-Karafka/#2018-2022-11-18", "title": "2.0.18 (2022-11-18)", "text": "<ul> <li>[Feature] Support quiet mode via <code>TSTP</code> signal. When used, Karafka will finish processing current messages, run <code>shutdown</code> jobs, and switch to a quiet mode where no new work is being accepted. At the same time, it will keep the consumer group quiet, and thus no rebalance will be triggered. This can be particularly useful during deployments.</li> <li>[Enhancement] Trigger <code>#revoked</code> for jobs in case revocation would happen during shutdown when jobs are still running. This should ensure, we get a notion of revocation for Pro LRJ jobs even when revocation happening upon shutdown (#1150).</li> <li>[Enhancement] Stabilize the shutdown procedure for consumer groups with many subscription groups that have non-aligned processing cost per batch.</li> <li>[Enhancement] Remove double loading of Karafka via Rails railtie.</li> <li>[Fix] Fix invalid class references in YARD docs.</li> <li>[Fix] prevent parallel closing of many clients.</li> <li>[Fix] fix a case where information about revocation for a combination of LRJ + VP would not be dispatched until all VP work is done.</li> </ul>"}, {"location": "Changelog-Karafka/#2017-2022-11-10", "title": "2.0.17 (2022-11-10)", "text": "<ul> <li>[Fix] Few typos around DLQ and Pro DLQ Dispatch original metadata naming.</li> <li>[Fix] Narrow the components lookup to the appropriate scope (#1114)</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_10", "title": "Upgrade Notes", "text": "<ol> <li>Replace <code>original-*</code> references from DLQ dispatched metadata with <code>original_*</code></li> </ol> <pre><code># DLQ topic consumption\ndef consume\n  messages.each do |broken_message|\n    topic = broken_message.metadata['original_topic'] # was original-topic\n    partition = broken_message.metadata['original_partition'] # was original-partition\n    offset = broken_message.metadata['original_offset'] # was original-offset\n\n    Rails.logger.error \"This message is broken: #{topic}/#{partition}/#{offset}\"\n  end\nend\n</code></pre>"}, {"location": "Changelog-Karafka/#2016-2022-11-09", "title": "2.0.16 (2022-11-09)", "text": "<ul> <li>[Breaking] Disable the root <code>manual_offset_management</code> setting and require it to be configured per topic. This is part of \"topic features\" configuration extraction for better code organization.</li> <li>[Feature] Introduce Dead Letter Queue feature and Pro Enhanced Dead Letter Queue feature</li> <li>[Enhancement] Align attributes available in the instrumentation bus for listener related events.</li> <li>[Enhancement] Include consumer group id in consumption related events (#1093)</li> <li>[Enhancement] Delegate pro components loading to Zeitwerk</li> <li>[Enhancement] Include <code>Datadog::LoggerListener</code> for tracking logger data with DataDog (@bruno-b-martins)</li> <li>[Enhancement] Include <code>seek_offset</code> in the <code>consumer.consume.error</code> event payload (#1113)</li> <li>[Refactor] Remove unused logger listener event handler.</li> <li>[Refactor] Internal refactoring of routing validations flow.</li> <li>[Refactor] Reorganize how routing related features are represented internally to simplify features management.</li> <li>[Refactor] Extract supported features combinations processing flow into separate strategies.</li> <li>[Refactor] Auto-create topics in the integration specs based on the defined routing</li> <li>[Refactor] Auto-inject Pro components via composition instead of requiring to use <code>Karafka::Pro::BaseConsumer</code> (#1116)</li> <li>[Fix] Fix a case where routing tags would not be injected when given routing definition would not be used with a block</li> <li>[Fix] Fix a case where using <code>#active_job_topic</code> without extra block options would cause <code>manual_offset_management</code> to stay false.</li> <li>[Fix] Fix a case when upon Pro ActiveJob usage with Virtual Partitions, correct offset would not be stored</li> <li>[Fix] Fix a case where upon Virtual Partitions usage, same underlying real partition would be resumed several times.</li> <li>[Fix] Fix LRJ enqueuing pause increases the coordinator counter (#115)</li> <li>[Fix] Release <code>ActiveRecord</code> connection to the pool after the work in non-dev envs (#1130)</li> <li>[Fix] Fix a case where post-initialization shutdown would not initiate shutdown procedures.</li> <li>[Fix] Prevent Karafka from committing offsets twice upon shutdown.</li> <li>[Fix] Fix for a case where fast consecutive stop signaling could hang the stopping listeners.</li> <li>[Specs] Split specs into regular and pro to simplify how resources are loaded</li> <li>[Specs] Add specs to ensure, that all the Pro components have a proper per-file license (#1099)</li> </ul>"}, {"location": "Changelog-Karafka/#upgrade-notes_11", "title": "Upgrade Notes", "text": "<ol> <li>Remove the <code>manual_offset_management</code> setting from the main config if you use it:</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n\n    # This line needs to be removed:\n    config.manual_offset_management = true\n  end\nend\n</code></pre> <ol> <li>Set the <code>manual_offset_management</code> feature flag per each topic where you want to use it in the routing. Don't set it for topics where you want the default offset management strategy to be used.</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    consumer_group :group_name do\n      topic :example do\n        consumer ExampleConsumer\n        manual_offset_management true\n      end\n\n      topic :example2 do\n        consumer ExampleConsumer2\n        manual_offset_management true\n      end\n    end\n  end\nend\n</code></pre> <ol> <li>If you were using code to restart dead connections similar to this:</li> </ol> <pre><code>class ActiveRecordConnectionsCleaner\n  def on_error_occurred(event)\n    return unless event[:error].is_a?(ActiveRecord::StatementInvalid)\n\n    ::ActiveRecord::Base.clear_active_connections!\n  end\nend\n\nKarafka.monitor.subscribe(ActiveRecordConnectionsCleaner.new)\n</code></pre> <p>It should be removed. This code is no longer needed.</p>"}, {"location": "Changelog-Karafka/#2015-2022-10-20", "title": "2.0.15 (2022-10-20)", "text": "<ul> <li>Sanitize admin config prior to any admin action.</li> <li>Make messages partitioner outcome for virtual partitions consistently distributed in regards to concurrency.</li> <li>Improve DataDog/StatsD metrics reporting by reporting per topic partition lags and trends.</li> <li>Replace synchronous offset commit with async on resuming paused partition (#1087).</li> </ul>"}, {"location": "Changelog-Karafka/#2014-2022-10-16", "title": "2.0.14 (2022-10-16)", "text": "<ul> <li>Prevent consecutive stop signals from starting multiple supervision shutdowns.</li> <li>Provide <code>Karafka::Embedded</code> to simplify the start/stop process when running Karafka from within other process (Puma, Sidekiq, etc).</li> <li>Fix a race condition when un-pausing a long-running-job exactly upon listener resuming would crash the listener loop (#1072).</li> </ul>"}, {"location": "Changelog-Karafka/#2013-2022-10-14", "title": "2.0.13 (2022-10-14)", "text": "<ul> <li>Early exit upon attempts to commit current or earlier offset twice.</li> <li>Add more integration specs covering edge cases.</li> <li>Strip non producer related config when default producer is initialized (#776)</li> </ul>"}, {"location": "Changelog-Karafka/#2012-2022-10-06", "title": "2.0.12 (2022-10-06)", "text": "<ul> <li>Commit stored offsets upon rebalance revocation event to reduce number of messages that are re-processed.</li> <li>Support cooperative-sticky rebalance strategy.</li> <li>Replace offset commit after each batch with a per-rebalance commit.</li> <li>User instrumentation to publish internal rebalance errors.</li> </ul>"}, {"location": "Changelog-Karafka/#2011-2022-09-29", "title": "2.0.11 (2022-09-29)", "text": "<ul> <li>Report early on errors related to network and on max poll interval being exceeded to indicate critical problems that will be retries but may mean some underlying problems in the system.</li> <li>Fix support of Ruby 2.7.0 to 2.7.2 (#1045)</li> </ul>"}, {"location": "Changelog-Karafka/#2010-2022-09-23", "title": "2.0.10 (2022-09-23)", "text": "<ul> <li>Improve error recovery by delegating the recovery to the existing <code>librdkafka</code> instance.</li> </ul>"}, {"location": "Changelog-Karafka/#209-2022-09-22", "title": "2.0.9 (2022-09-22)", "text": "<ul> <li>Fix Singleton not visible when used in PORO (#1034)</li> <li>Divide pristine specs into pristine and poro. Pristine will still have helpers loaded, poro will have nothing.</li> <li>Fix a case where <code>manual_offset_management</code> offset upon error is not reverted to the first message in a case where there were no markings as consumed at all for multiple batches.</li> <li>Implement small reliability improvements around marking as consumed.</li> <li>Introduce a config sanity check to make sure Virtual Partitions are not used with manual offset management.</li> <li>Fix a possibility of using <code>active_job_topic</code> with Virtual Partitions and manual offset management (ActiveJob still can use due to atomicity of jobs).</li> <li>Move seek offset ownership to the coordinator to allow Virtual Partitions further development.</li> <li>Improve client shutdown in specs.</li> <li>Do not reset client on network issue and rely on <code>librdkafka</code> to do so.</li> <li>Allow for nameless (anonymous) subscription groups (#1033)</li> </ul>"}, {"location": "Changelog-Karafka/#208-2022-09-19", "title": "2.0.8 (2022-09-19)", "text": "<ul> <li>[Breaking change] Rename Virtual Partitions <code>concurrency</code> to <code>max_partitions</code> to avoid confusion  (#1023).</li> <li>Allow for block based subscription groups management (#1030).</li> </ul>"}, {"location": "Changelog-Karafka/#207-2022-09-05", "title": "2.0.7 (2022-09-05)", "text": "<ul> <li>[Breaking change] Redefine the Virtual Partitions routing DSL to accept concurrency</li> <li>Allow for <code>concurrency</code> setting in Virtual Partitions to extend or limit number of jobs per regular partition. This allows to make sure, we do not use all the threads on virtual partitions jobs</li> <li>Allow for creation of as many Virtual Partitions as needed, without taking global <code>concurrency</code> into consideration</li> </ul>"}, {"location": "Changelog-Karafka/#206-2022-09-02", "title": "2.0.6 (2022-09-02)", "text": "<ul> <li>Improve client closing.</li> <li>Fix for: Multiple LRJ topics fetched concurrently block ability for LRJ to kick in (#1002)</li> <li>Introduce a pre-enqueue sync execution layer to prevent starvation cases for LRJ</li> <li>Close admin upon critical errors to prevent segmentation faults</li> <li>Add support for manual subscription group management (#852)</li> </ul>"}, {"location": "Changelog-Karafka/#205-2022-08-23", "title": "2.0.5 (2022-08-23)", "text": "<ul> <li>Fix unnecessary double new line in the <code>karafka.rb</code> template for Ruby on Rails</li> <li>Fix a case where a manually paused partition would not be processed after rebalance (#988)</li> <li>Increase specs stability.</li> <li>Lower concurrency of execution of specs in Github CI.</li> </ul>"}, {"location": "Changelog-Karafka/#204-2022-08-19", "title": "2.0.4 (2022-08-19)", "text": "<ul> <li>Fix hanging topic creation (#964)</li> <li>Fix conflict with other Rails loading libraries like <code>gruf</code> (#974)</li> </ul>"}, {"location": "Changelog-Karafka/#203-2022-08-09", "title": "2.0.3 (2022-08-09)", "text": "<ul> <li>Update boot info on server startup.</li> <li>Update <code>karafka info</code> with more descriptive Ruby version info.</li> <li>Fix issue where when used with Rails in development, log would be too verbose.</li> <li>Fix issue where Zeitwerk with Rails would not load Pro components despite license being present.</li> </ul>"}, {"location": "Changelog-Karafka/#202-2022-08-07", "title": "2.0.2 (2022-08-07)", "text": "<ul> <li>Bypass issue with Rails reload in development by releasing the connection (https://github.com/rails/rails/issues/44183).</li> </ul>"}, {"location": "Changelog-Karafka/#201-2022-08-06", "title": "2.0.1 (2022-08-06)", "text": "<ul> <li>Provide <code>Karafka::Admin</code> for creation and destruction of topics and fetching cluster info.</li> <li>Update integration specs to always use one-time disposable topics.</li> <li>Remove no longer needed <code>wait_for_kafka</code> script.</li> <li>Add more integration specs for cover offset management upon errors.</li> </ul>"}, {"location": "Changelog-Karafka/#200-2022-08-05", "title": "2.0.0 (2022-08-05)", "text": "<p>This changelog describes changes between <code>1.4</code> and <code>2.0</code>. Please refer to appropriate release notes for changes between particular <code>rc</code> releases.</p> <p>Karafka 2.0 is a major rewrite that brings many new things to the table but also removes specific concepts that happened not to be as good as I initially thought when I created them.</p> <p>Please consider getting a Pro version if you want to support my work on the Karafka ecosystem!</p> <p>For anyone worried that I will start converting regular features into Pro: This will not happen. Anything free and fully OSS in Karafka 1.4 will forever remain free. Most additions and improvements to the ecosystem are to its free parts. Any feature that is introduced as a free and open one will not become paid.</p>"}, {"location": "Changelog-Karafka/#additions", "title": "Additions", "text": "<p>This section describes new things and concepts introduced with Karafka 2.0.</p> <p>Karafka 2.0:</p> <ul> <li>Introduces multi-threaded support for concurrent work consumption for separate partitions as well as for single partition work via Virtual Partitions.</li> <li>Introduces Active Job adapter for using Karafka as a jobs backend with Ruby on Rails Active Job.</li> <li>Introduces fully automatic integration end-to-end test suite that checks any case I could imagine.</li> <li>Introduces Virtual Partitions for ability to parallelize work of a single partition.</li> <li>Introduces Long-Running Jobs to allow for work that would otherwise exceed the <code>max.poll.interval.ms</code>.</li> <li>Introduces the Enhanced Scheduler that uses a non-preemptive LJF (Longest Job First) algorithm instead of a a FIFO (First-In, First-Out) one.</li> <li>Introduces Enhanced Active Job adapter that is optimized and allows for strong ordering of jobs and more.</li> <li>Introduces seamless Ruby on Rails integration via <code>Rails::Railte</code> without need for any extra configuration.</li> <li>Provides <code>#revoked</code> method for taking actions upon topic revocation.</li> <li>Emits underlying async errors emitted from <code>librdkafka</code> via the standardized <code>error.occurred</code> monitor channel.</li> <li>Replaces <code>ruby-kafka</code> with <code>librdkafka</code> as an underlying driver.</li> <li>Introduces official EOL policies.</li> <li>Introduces benchmarks that can be used to profile Karafka.</li> <li>Introduces a requirement that the end user code needs to be thread-safe.</li> <li>Introduces a Pro subscription with a commercial license to fund further ecosystem development.</li> </ul>"}, {"location": "Changelog-Karafka/#deletions", "title": "Deletions", "text": "<p>This section describes things that are no longer part of the Karafka ecosystem.</p> <p>Karafka 2.0:</p> <ul> <li>Removes topics mappers concept completely.</li> <li>Removes pidfiles support.</li> <li>Removes daemonization support.</li> <li>Removes support for using <code>sidekiq-backend</code> due to introduction of multi-threading.</li> <li>Removes the <code>Responders</code> concept in favour of WaterDrop producer usage.</li> <li>Removes completely all the callbacks in favour of finalizer method <code>#shutdown</code>.</li> <li>Removes single message consumption mode in favour of documentation on how to do it easily by yourself.</li> </ul>"}, {"location": "Changelog-Karafka/#changes", "title": "Changes", "text": "<p>This section describes things that were changed in Karafka but are still present.</p> <p>Karafka 2.0:</p> <ul> <li>Uses only instrumentation that comes from Karafka. This applies also to notifications coming natively from <code>librdkafka</code>. They are now piped through Karafka prior to being dispatched.</li> <li>Integrates WaterDrop <code>2.x</code> tightly with autoconfiguration inheritance and an option to redefine it.</li> <li>Integrates with the <code>karafka-testing</code> gem for RSpec that also has been updated.</li> <li>Updates <code>cli info</code> to reflect the <code>2.0</code> details.</li> <li>Stops validating <code>kafka</code> configuration beyond minimum as the rest is handled by <code>librdkafka</code>.</li> <li>No longer uses <code>dry-validation</code>.</li> <li>No longer uses <code>dry-monitor</code>.</li> <li>No longer uses <code>dry-configurable</code>.</li> <li>Lowers general external dependencies three heavily.</li> <li>Renames <code>Karafka::Params::BatchMetadata</code> to <code>Karafka::Messages::BatchMetadata</code>.</li> <li>Renames <code>Karafka::Params::Params</code> to <code>Karafka::Messages::Message</code>.</li> <li>Renames <code>#params_batch</code> in consumers to <code>#messages</code>.</li> <li>Renames <code>Karafka::Params::Metadata</code> to <code>Karafka::Messages::Metadata</code>.</li> <li>Renames <code>Karafka::Fetcher</code> to <code>Karafka::Runner</code> and align notifications key names.</li> <li>Renames <code>StdoutListener</code> to <code>LoggerListener</code>.</li> <li>Reorganizes monitoring and logging to match new concepts.</li> <li>Notifies on fatal worker processing errors.</li> <li>Contains updated install templates for Rails and no-non Rails.</li> <li>Changes how the routing style (<code>0.5</code>) behaves. It now builds a single consumer group instead of one per topic.</li> <li>Introduces changes that will allow me to build full web-UI in the upcoming <code>2.1</code>.</li> <li>Contains updated example apps.</li> <li>Standardizes error hooks for all error reporting (<code>error.occurred</code>).</li> <li>Changes license to <code>LGPL-3.0</code>.</li> <li>Introduces a <code>karafka-core</code> dependency that contains common code used across the ecosystem.</li> <li>Contains updated wiki on everything I could think of.</li> </ul>"}, {"location": "Changelog-Karafka/#older-releases", "title": "Older releases", "text": "<p>This changelog tracks Karafka <code>2.0</code> and higher changes.</p> <p>If you are looking for changes in the unsupported releases, we recommend checking the <code>1.4</code> branch of the Karafka repository.</p>"}, {"location": "Changelog-Rdkafka/", "title": "Rdkafka Changelog", "text": ""}, {"location": "Changelog-Rdkafka/#0220-unreleased", "title": "0.22.0 (Unreleased)", "text": "<ul> <li>[Feature] Add precompiled <code>x86_64-linux-gnu</code> setup.</li> <li>[Feature] Add precompiled <code>x86_64-linux-musl</code> setup.</li> <li>[Feature] Add precompiled <code>macos_arm64</code> setup.</li> <li>[Fix] Fix a case where using empty key on the <code>musl</code> architecture would cause a segfault.</li> <li>[Fix] Fix for null pointer reference bypass on empty string being too wide causing segfault.</li> <li>[Enhancement] Allow for producing to non-existing topics with <code>key</code> and <code>partition_key</code> present.</li> <li>[Enhancement] Replace TTL-based partition count cache with a global cache that reuses <code>librdkafka</code> statistics data when possible.</li> <li>[Enhancement] Support producing and consuming of headers with mulitple values (KIP-82).</li> <li>[Enhancement] Allow native Kafka customization poll time.</li> <li>[Enhancement] Roll out experimental jruby support.</li> <li>[Enhancement] Run all specs on each of the platforms with and without precompilation.</li> <li>[Enhancement] Support transactional id in the ACL API.</li> <li>[Fix] Fix issue where post-closed producer C topics refs would not be cleaned.</li> <li>[Fix] Fiber causes Segmentation Fault.</li> <li>[Change] Move to trusted-publishers and remove signing since no longer needed.</li> </ul> <p>Note: Precompiled extensions are a new feature in this release. While they significantly improve installation speed and reduce build dependencies, they should be thoroughly tested in your staging environment before deploying to production. If you encounter any issues with precompiled extensions, you can fall back to building from sources.</p>"}, {"location": "Changelog-Rdkafka/#0210-2025-02-13", "title": "0.21.0 (2025-02-13)", "text": "<ul> <li>[Enhancement] Bump librdkafka to <code>2.8.0</code></li> </ul>"}, {"location": "Changelog-Rdkafka/#0200-2025-01-07", "title": "0.20.0 (2025-01-07)", "text": "<ul> <li>[Breaking] Deprecate and remove <code>#each_batch</code> due to data consistency concerns.</li> <li>[Enhancement] Bump librdkafka to <code>2.6.1</code></li> <li>[Enhancement] Expose <code>rd_kafka_global_init</code> to mitigate macos forking issues.</li> <li>[Enhancement] Avoid clobbering LDFLAGS and CPPFLAGS if in a nix prepared environment (secobarbital).</li> <li>[Patch] Retire no longer needed cooperative-sticky patch.</li> </ul>"}, {"location": "Changelog-Rdkafka/#0190-2024-10-01", "title": "0.19.0 (2024-10-01)", "text": "<ul> <li>[Breaking] Drop Ruby 3.0 support</li> <li>[Enhancement] Update <code>librdkafka</code> to <code>2.5.3</code></li> <li>[Enhancement] Use default oauth callback if none is passed (bachmanity1)</li> <li>[Fix] Fix incorrectly behaving CI on failures. </li> <li>[Patch] Patch with \"Add forward declaration to fix compilation without ssl\" fix</li> </ul>"}, {"location": "Changelog-Rdkafka/#0180-2024-09-02", "title": "0.18.0 (2024-09-02)", "text": "<ul> <li>[Enhancement] Update <code>librdkafka</code> to <code>2.5.0</code></li> <li>[Enhancement] Do not release GVL on <code>rd_kafka_name</code> (ferrous26)</li> <li>[Patch] Patch cooperative-sticky assignments in librdkafka.</li> <li>[Fix] Mitigate a case where FFI would not restart the background events callback dispatcher in forks</li> <li>[Fix] Fix unused variable reference in producer (lucasmvnascimento)</li> </ul>"}, {"location": "Changelog-Rdkafka/#0170-2024-08-03", "title": "0.17.0 (2024-08-03)", "text": "<ul> <li>[Feature] Add <code>#seek_by</code> to be able to seek for a message by topic, partition and offset (zinahia)</li> <li>[Enhancement] Update <code>librdkafka</code> to <code>2.4.0</code></li> <li>[Enhancement] Support ability to release patches to librdkafka.</li> <li>[Change] Remove old producer timeout API warnings.</li> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Rdkafka/#0161-2024-07-10", "title": "0.16.1 (2024-07-10)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Rdkafka/#0160-2024-06-13", "title": "0.16.0 (2024-06-13)", "text": "<ul> <li>[Breaking] Retire support for Ruby 2.7.</li> <li>[Breaking] Messages without headers returned by <code>#poll</code> contain frozen empty hash.</li> <li>[Breaking] <code>HashWithSymbolKeysTreatedLikeStrings</code> has been removed so headers are regular hashes with string keys.</li> <li>[Feature] Support incremental config describe + alter API.</li> <li>[Feature] Oauthbearer token refresh callback (bruce-szalwinski-he)</li> <li>[Feature] Provide ability to use topic config on a producer for custom behaviors per dispatch.</li> <li>[Enhancement] Use topic config reference cache for messages production to prevent topic objects allocation with each message.</li> <li>[Enhancement] Provide <code>Rrdkafka::Admin#describe_errors</code> to get errors descriptions (mensfeld)</li> <li>[Enhancement] Replace time poll based wait engine with an event based to improve response times on blocking operations and wait (nijikon + mensfeld)</li> <li>[Enhancement] Allow for usage of the second regex engine of librdkafka by setting <code>RDKAFKA_DISABLE_REGEX_EXT</code> during build (mensfeld)</li> <li>[Enhancement] name polling Thread as <code>rdkafka.native_kafka#&lt;name&gt;</code> (nijikon)</li> <li>[Enhancement] Save two objects on message produced and lower CPU usage on message produced with small improvements.</li> <li>[Change] Allow for native kafka thread operations deferring and manual start for consumer, producer and admin.</li> <li>[Change] The <code>wait_timeout</code> argument in <code>AbstractHandle.wait</code> method is deprecated and will be removed in future versions without replacement. We don't rely on it's value anymore (nijikon)</li> <li>[Fix] Background logger stops working after forking causing memory leaks (mensfeld)</li> <li>[Fix] Fix bogus case/when syntax. Levels 1, 2, and 6 previously defaulted to UNKNOWN (jjowdy)</li> </ul>"}, {"location": "Changelog-Rdkafka/#0152-2024-07-10", "title": "0.15.2 (2024-07-10)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Rdkafka/#0151-2024-01-30", "title": "0.15.1 (2024-01-30)", "text": "<ul> <li>[Enhancement] Provide support for Nix OS (alexandriainfantino)</li> <li>[Enhancement] Replace <code>rd_kafka_offset_store</code> with <code>rd_kafka_offsets_store</code> (mensfeld)</li> <li>[Enhancement] Alias <code>topic_name</code> as <code>topic</code> in the delivery report (mensfeld)</li> <li>[Enhancement] Provide <code>label</code> producer handler and report reference for improved traceability (mensfeld)</li> <li>[Enhancement] Include the error when invoking <code>create_result</code> on producer handle (mensfeld)</li> <li>[Enhancement] Skip intermediate array creation on delivery report callback execution (one per message) (mensfeld).</li> <li>[Enhancement] Report <code>-1</code> instead of <code>nil</code> in case <code>partition_count</code> failure (mensfeld).</li> <li>[Fix] Fix return type on <code>#rd_kafka_poll</code> (mensfeld)</li> <li>[Fix] <code>uint8_t</code> does not exist on Apple Silicon (mensfeld)</li> <li>[Fix] Missing ACL <code>RD_KAFKA_RESOURCE_BROKER</code> constant reference (mensfeld)</li> <li>[Fix] Partition cache caches invalid nil result for <code>PARTITIONS_COUNT_TTL</code> (mensfeld)</li> <li>[Change] Rename <code>matching_acl_pattern_type</code> to <code>matching_acl_resource_pattern_type</code> to align the whole API (mensfeld)</li> </ul>"}, {"location": "Changelog-Rdkafka/#0150-2023-12-03", "title": "0.15.0 (2023-12-03)", "text": "<ul> <li>[Feature] Add <code>Admin#metadata</code> (mensfeld)</li> <li>[Feature] Add <code>Admin#create_partitions</code> (mensfeld)</li> <li>[Feature] Add <code>Admin#delete_group</code> utility (piotaixr)</li> <li>[Feature] Add Create and Delete ACL Feature To Admin Functions (vgnanasekaran)</li> <li>[Feature] Support <code>#assignment_lost?</code> on a consumer to check for involuntary assignment revocation (mensfeld)</li> <li>[Enhancement] Expose alternative way of managing consumer events via a separate queue (mensfeld)</li> <li>[Enhancement] Bump librdkafka to 2.3.0 (mensfeld)</li> <li>[Enhancement] Increase the <code>#lag</code> and <code>#query_watermark_offsets</code> default timeouts from 100ms to 1000ms. This will compensate for network glitches and remote clusters operations (mensfeld)</li> <li>[Change] Use <code>SecureRandom.uuid</code> instead of <code>random</code> for test consumer groups (mensfeld)</li> </ul>"}, {"location": "Changelog-Rdkafka/#0141-2024-07-10", "title": "0.14.1 (2024-07-10)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Rdkafka/#0140-2023-11-21", "title": "0.14.0 (2023-11-21)", "text": "<ul> <li>[Enhancement] Add <code>raise_response_error</code> flag to the <code>Rdkafka::AbstractHandle</code>.</li> <li>[Enhancement] Allow for setting <code>statistics_callback</code> as nil to reset predefined settings configured by a different gem (mensfeld)</li> <li>[Enhancement] Get consumer position (thijsc &amp; mensfeld)</li> <li>[Enhancement] Provide <code>#purge</code> to remove any outstanding requests from the producer (mensfeld)</li> <li>[Enhancement] Update <code>librdkafka</code> to <code>2.2.0</code> (mensfeld)</li> <li>[Enhancement] Introduce producer partitions count metadata cache (mensfeld)</li> <li>[Enhancement] Increase metadata timeout request from <code>250 ms</code> to <code>2000 ms</code> default to allow for remote cluster operations via <code>rdkafka-ruby</code> (mensfeld)</li> <li>[Enhancement] Introduce <code>#name</code> for producers and consumers (mensfeld)</li> <li>[Enhancement] Include backtrace in non-raised binded errors (mensfeld)</li> <li>[Fix] Reference to Opaque is not released when Admin, Consumer or Producer is closed (mensfeld)</li> <li>[Fix] Trigger <code>#poll</code> on native kafka creation to handle oauthbearer cb (mensfeld)</li> <li>[Fix] <code>#flush</code> does not handle the timeouts errors by making it return <code>true</code> if all flushed or <code>false</code> if failed. We do not raise an exception here to keep it backwards compatible (mensfeld)</li> <li>[Change] Remove support for Ruby 2.6 due to it being EOL and WeakMap incompatibilities (mensfeld)</li> <li>[Change] Update Kafka Docker with Confluent KRaft (mensfeld)</li> <li>[Change] Update librdkafka repo reference from edenhill to confluentinc (mensfeld)</li> </ul>"}, {"location": "Changelog-Rdkafka/#0131-2024-07-10", "title": "0.13.1 (2024-07-10)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Rdkafka/#0130-2023-07-24", "title": "0.13.0 (2023-07-24)", "text": "<ul> <li>Support cooperative sticky partition assignment in the rebalance callback (methodmissing)</li> <li>Support both string and symbol header keys (ColinDKelley)</li> <li>Handle tombstone messages properly (kgalieva)</li> <li>Add topic name to delivery report (maeve)</li> <li>Allow string partitioner config (mollyegibson)</li> <li>Fix documented type for DeliveryReport#error (jimmydo)</li> <li>Bump librdkafka to 2.0.2 (lmaia)</li> <li>Use finalizers to cleanly exit producer and admin (thijsc)</li> <li>Lock access to the native kafka client (thijsc)</li> <li>Fix potential race condition in multi-threaded producer (mensfeld)</li> <li>Fix leaking FFI resources in specs (mensfeld)</li> <li>Improve specs stability (mensfeld)</li> <li>Make metadata request timeout configurable (mensfeld)</li> <li>call_on_partitions_assigned and call_on_partitions_revoked only get a tpl passed in (thijsc)</li> </ul>"}, {"location": "Changelog-Rdkafka/#0121-2024-07-11", "title": "0.12.1 (2024-07-11)", "text": "<ul> <li>[Fix] Switch to local release of librdkafka to mitigate its unavailability.</li> </ul>"}, {"location": "Changelog-Rdkafka/#0120-2022-06-17", "title": "0.12.0 (2022-06-17)", "text": "<ul> <li>Bumps librdkafka to 1.9.0</li> <li>Fix crash on empty partition key (mensfeld)</li> <li>Pass the delivery handle to the callback (gvisokinskas)</li> </ul>"}, {"location": "Changelog-Rdkafka/#0110-2021-11-17", "title": "0.11.0 (2021-11-17)", "text": "<ul> <li>Upgrade librdkafka to 1.8.2</li> <li>Bump supported minimum Ruby version to 2.6</li> <li>Better homebrew path detection</li> </ul>"}, {"location": "Changelog-Rdkafka/#0100-2021-09-07", "title": "0.10.0 (2021-09-07)", "text": "<ul> <li>Upgrade librdkafka to 1.5.0</li> <li>Add error callback config</li> </ul>"}, {"location": "Changelog-Rdkafka/#090-2021-06-23", "title": "0.9.0 (2021-06-23)", "text": "<ul> <li>Fixes for Ruby 3.0</li> <li>Allow any callable object for callbacks (gremerritt)</li> <li>Reduce memory allocations in Rdkafka::Producer#produce (jturkel)</li> <li>Use queue as log callback to avoid unsafe calls from trap context (breunigs)</li> <li>Allow passing in topic configuration on create_topic (dezka)</li> <li>Add each_batch method to consumer (mgrosso)</li> </ul>"}, {"location": "Changelog-Rdkafka/#081-2020-12-07", "title": "0.8.1 (2020-12-07)", "text": "<ul> <li>Fix topic_flag behaviour and add tests for Metadata (geoff2k)</li> <li>Add topic admin interface (geoff2k)</li> <li>Raise an exception if @native_kafka is nil (geoff2k)</li> <li>Option to use zstd compression (jasonmartens)</li> </ul>"}, {"location": "Changelog-Rdkafka/#080-2020-06-02", "title": "0.8.0 (2020-06-02)", "text": "<ul> <li>Upgrade librdkafka to 1.4.0</li> <li>Integrate librdkafka metadata API and add partition_key (by Adithya-copart)</li> <li>Ruby 2.7 compatibility fix (by Geoff The\u0301)A</li> <li>Add error to delivery report (by Alex Stanovsky)</li> <li>Don't override CPPFLAGS and LDFLAGS if already set on Mac (by Hiroshi Hatake)</li> <li>Allow use of Rake 13.x and up (by Tomasz Pajor)</li> </ul>"}, {"location": "Changelog-Rdkafka/#070-2019-09-21", "title": "0.7.0 (2019-09-21)", "text": "<ul> <li>Bump librdkafka to 1.2.0 (by rob-as)</li> <li>Allow customizing the wait time for delivery report availability (by mensfeld)</li> </ul>"}, {"location": "Changelog-Rdkafka/#060-2019-07-23", "title": "0.6.0 (2019-07-23)", "text": "<ul> <li>Bump librdkafka to 1.1.0 (by Chris Gaffney)</li> <li>Implement seek (by breunigs)</li> </ul>"}, {"location": "Changelog-Rdkafka/#050-2019-04-11", "title": "0.5.0 (2019-04-11)", "text": "<ul> <li>Bump librdkafka to 1.0.0 (by breunigs)</li> <li>Add cluster and member information (by dmexe)</li> <li>Support message headers for consumer &amp; producer (by dmexe)</li> <li>Add consumer rebalance listener (by dmexe)</li> <li>Implement pause/resume partitions (by dmexe)</li> </ul>"}, {"location": "Changelog-Rdkafka/#042-2019-01-12", "title": "0.4.2 (2019-01-12)", "text": "<ul> <li>Delivery callback for producer</li> <li>Document list param of commit method</li> <li>Use default Homebrew openssl location if present</li> <li>Consumer lag handles empty topics</li> <li>End iteration in consumer when it is closed</li> <li>Add support for storing message offsets</li> <li>Add missing runtime dependency to rake</li> </ul>"}, {"location": "Changelog-Rdkafka/#041-2018-10-19", "title": "0.4.1 (2018-10-19)", "text": "<ul> <li>Bump librdkafka to 0.11.6</li> </ul>"}, {"location": "Changelog-Rdkafka/#040-2018-09-24", "title": "0.4.0 (2018-09-24)", "text": "<ul> <li>Improvements in librdkafka archive download</li> <li>Add global statistics callback</li> <li>Use Time for timestamps, potentially breaking change if you   rely on the previous behavior where it returns an integer with   the number of milliseconds.</li> <li>Bump librdkafka to 0.11.5</li> <li>Implement TopicPartitionList in Ruby so we don't have to keep   track of native objects.</li> <li>Support committing a topic partition list</li> <li>Add consumer assignment method</li> </ul>"}, {"location": "Changelog-Rdkafka/#035-2018-01-17", "title": "0.3.5 (2018-01-17)", "text": "<ul> <li>Fix crash when not waiting for delivery handles</li> <li>Run specs on Ruby 2.5</li> </ul>"}, {"location": "Changelog-Rdkafka/#034-2017-12-05", "title": "0.3.4 (2017-12-05)", "text": "<ul> <li>Bump librdkafka to 0.11.3</li> </ul>"}, {"location": "Changelog-Rdkafka/#033-2017-10-27", "title": "0.3.3 (2017-10-27)", "text": "<ul> <li>Fix bug that prevent display of <code>RdkafkaError</code> message</li> </ul>"}, {"location": "Changelog-Rdkafka/#032-2017-10-25", "title": "0.3.2 (2017-10-25)", "text": "<ul> <li><code>add_topic</code> now supports using a partition count</li> <li>Add way to make errors clearer with an extra message</li> <li>Show topics in subscribe error message</li> <li>Show partition and topic in query watermark offsets error message</li> </ul>"}, {"location": "Changelog-Rdkafka/#031-2017-10-23", "title": "0.3.1 (2017-10-23)", "text": "<ul> <li>Bump librdkafka to 0.11.1</li> <li>Officially support ranges in <code>add_topic</code> for topic partition list.</li> <li>Add consumer lag calculator</li> </ul>"}, {"location": "Changelog-Rdkafka/#030-2017-10-17", "title": "0.3.0 (2017-10-17)", "text": "<ul> <li>Move both add topic methods to one <code>add_topic</code> in <code>TopicPartitionList</code></li> <li>Add committed offsets to consumer</li> <li>Add query watermark offset to consumer</li> </ul>"}, {"location": "Changelog-Rdkafka/#020-2017-10-13", "title": "0.2.0 (2017-10-13)", "text": "<ul> <li>Some refactoring and add inline documentation</li> </ul>"}, {"location": "Changelog-Rdkafka/#01x-2017-09-10", "title": "0.1.x (2017-09-10)", "text": "<ul> <li>Initial working version including producing and consuming</li> </ul>"}, {"location": "Changelog-WaterDrop/", "title": "WaterDrop changelog", "text": ""}, {"location": "Changelog-WaterDrop/#285-2025-06-23", "title": "2.8.5 (2025-06-23)", "text": "<ul> <li>[Enhancement] Normalize topic + partition logs format (single place).</li> <li>[Fix] A producer is not idempotent unless the enable.idempotence config is <code>true</code> (ferrous26).</li> </ul>"}, {"location": "Changelog-WaterDrop/#284-2025-05-23", "title": "2.8.4 (2025-05-23)", "text": "<ul> <li>[Change] Require <code>karafka-rdkafka</code> <code>&gt;= 0.19.2</code> due to new partition count caching usage.</li> <li>[Change] Move to trusted-publishers and remove signing since no longer needed.</li> </ul>"}, {"location": "Changelog-WaterDrop/#283-2025-04-08", "title": "2.8.3 (2025-04-08)", "text": "<ul> <li>[Enhancement] Support producing messages with arrays of strings in headers (KIP-82).</li> <li>[Refactor] Introduce a <code>bin/verify_topics_naming</code> script to ensure proper test topics naming convention.</li> <li>[Refactor] Remove factory bot and active support requirement in tests/dev.</li> <li>[Change] Require <code>karafka-rdkafka</code> <code>&gt;= 0.19.1</code> due to KIP-82.</li> </ul>"}, {"location": "Changelog-WaterDrop/#282-2025-02-13", "title": "2.8.2 (2025-02-13)", "text": "<ul> <li>[Feature] Allow for tagging of producer instances similar to how consumers can be tagged.</li> <li>[Refactor] Ensure all test topics in the test suite start with \"it-\" prefix. </li> <li>[Refactor] Introduce a <code>bin/verify_kafka_warnings</code> script to clean Kafka from temporary test-suite topics.</li> </ul>"}, {"location": "Changelog-WaterDrop/#281-2024-12-26", "title": "2.8.1 (2024-12-26)", "text": "<ul> <li>[Enhancement] Raise <code>WaterDrop::ProducerNotTransactionalError</code> when attempting to use transactions on a non-transactional producer.</li> <li>[Fix] Disallow closing a producer from within a transaction.</li> <li>[Fix] WaterDrop should prevent opening a transaction using a closed producer.</li> </ul>"}, {"location": "Changelog-WaterDrop/#280-2024-09-16", "title": "2.8.0 (2024-09-16)", "text": "<p>This release contains BREAKING changes. Make sure to read and apply upgrade notes.</p> <ul> <li>[Breaking] Require Ruby <code>3.1+</code>.</li> <li>[Breaking] Remove ability to abort transactions using <code>throw(:abort)</code>. Please use <code>raise WaterDrop::Errors::AbortTransaction</code>.</li> <li>[Breaking] Disallow (similar to ActiveRecord) exiting transactions with <code>return</code>, <code>break</code> or <code>throw</code>.</li> <li>[Breaking] License changed from MIT to LGPL with an additional commercial option. Note: there is no commercial code in this repository. The commercial license is available for companies unable to use LGPL-licensed software for legal reasons.</li> <li>[Enhancement] Make variants fiber safe.</li> <li>[Enhancement] In transactional mode do not return any <code>dispatched</code> messages as none will be dispatched due to rollback.</li> <li>[Enhancement] Align the <code>LoggerListener</code> async messages to reflect, that messages are delegated to the internal queue and not dispatched.</li> <li>[Fix] Ensure, that <code>:dispatched</code> key for <code>#produce_many_sync</code> always contains delivery handles (final) and not delivery reports.</li> </ul>"}, {"location": "Changelog-WaterDrop/#274-2024-07-04", "title": "2.7.4 (2024-07-04)", "text": "<ul> <li>[Maintenance] Alias <code>WaterDrop::Errors::AbortTransaction</code> with <code>WaterDrop::AbortTransaction</code>.</li> <li>[Maintenance] Lower the precision reporting to 100 microseconds in the logger listener.</li> <li>[Fix] Consumer consuming error: Local: Erroneous state (state) post break flow in transaction.</li> <li>[Change] Require 'karafka-core' <code>&gt;= 2.4.3</code></li> </ul>"}, {"location": "Changelog-WaterDrop/#273-2024-06-09", "title": "2.7.3 (2024-06-09)", "text": "<ul> <li>[Enhancement] Introduce <code>reload_on_transaction_fatal_error</code> to reload the librdkafka after transactional failures</li> <li>[Enhancement] Flush on fatal transactional errors.</li> <li>[Enhancement] Add topic scope to <code>report_metric</code> (YadhuPrakash)</li> <li>[Enhancement] Cache middleware reference saving 1 object allocation on each message dispatch.</li> <li>[Enhancement] Provide <code>#idempotent?</code> similar to <code>#transactional?</code>.</li> <li>[Enhancement] Provide alias to <code>#with</code> named <code>#variant</code>.</li> <li>[Fix] Prevent from creating <code>acks</code> altering variants on idempotent producers.</li> </ul>"}, {"location": "Changelog-WaterDrop/#272-2024-05-09", "title": "2.7.2 (2024-05-09)", "text": "<ul> <li>[Fix] Fix missing requirement of <code>delegate</code> for non-Rails use-cases. Always require delegate for variants usage (samsm)</li> </ul>"}, {"location": "Changelog-WaterDrop/#271-2024-05-09", "title": "2.7.1 (2024-05-09)", "text": "<ul> <li>[Feature] Support context-base configuration with low-level topic settings alterations producer variants.</li> <li>[Enhancement] Prefix random default <code>SecureRandom.hex(6)</code> producers ids with <code>waterdrop-hex</code> to indicate type of object.</li> </ul>"}, {"location": "Changelog-WaterDrop/#270-2024-04-26", "title": "2.7.0 (2024-04-26)", "text": "<p>This release contains BREAKING changes. Make sure to read and apply upgrade notes.</p> <ul> <li>[Feature] Support custom OAuth providers.</li> <li>[Breaking] Drop Ruby <code>2.7</code> support.</li> <li>[Breaking] Change default timeouts so final delivery <code>message.timeout.ms</code> is less that <code>max_wait_time</code> so we do not end up with not final verdict.</li> <li>[Breaking] Update all the time related configuration settings to be in <code>ms</code> and not mixed.</li> <li>[Breaking] Remove no longer needed <code>wait_timeout</code> configuration option.</li> <li>[Breaking] Do not validate or morph (via middleware) messages added to the buffer prior to <code>flush_sync</code> or <code>flush_async</code>.</li> <li>[Enhancement] Provide <code>WaterDrop::Producer#transaction?</code> that returns only when producer has an active transaction running.</li> <li>[Enhancement] Introduce <code>instrument_on_wait_queue_full</code> flag (defaults to <code>true</code>) to be able to configure whether non critical (retryable) queue full errors should be instrumented in the error pipeline. Useful when building high-performance pipes with WaterDrop queue retry backoff as a throttler.</li> <li>[Enhancement] Protect critical <code>rdkafka</code> thread executable code sections.</li> <li>[Enhancement] Treat the queue size as a gauge rather than a cumulative stat (isturdy).</li> <li>[Fix] Fix a case where purge on non-initialized client would crash.</li> <li>[Fix] Middlewares run twice when using buffered produce.</li> <li>[Fix] Validations run twice when using buffered produce.</li> </ul>"}, {"location": "Changelog-WaterDrop/#2614-2024-02-06", "title": "2.6.14 (2024-02-06)", "text": "<ul> <li>[Enhancement] Instrument <code>producer.connected</code> and <code>producer.closing</code> lifecycle events.</li> </ul>"}, {"location": "Changelog-WaterDrop/#2613-2024-01-29", "title": "2.6.13 (2024-01-29)", "text": "<ul> <li>[Enhancement] Expose <code>#partition_count</code> for building custom partitioners that need to be aware of number of partitions on a given topic.</li> </ul>"}, {"location": "Changelog-WaterDrop/#2612-2024-01-03", "title": "2.6.12 (2024-01-03)", "text": "<ul> <li>[Enhancement] Provide ability to label message dispatches for increased observability.</li> <li>[Enhancement] Provide ability to commit offset during the transaction with a consumer provided.</li> <li>[Change] Change transactional message purged error type from <code>message.error</code> to <code>librdkafka.dispatch_error</code> to align with the non-transactional error type.</li> <li>[Change] Remove usage of concurrent ruby.</li> </ul>"}, {"location": "Changelog-WaterDrop/#2611-2023-10-25", "title": "2.6.11 (2023-10-25)", "text": "<ul> <li>[Enhancement] Return delivery handles and delivery report for both dummy and buffered clients with proper topics, partitions and offsets assign and auto-increment offsets per partition.</li> <li>[Fix] Fix a case where buffered test client would not accumulate messages on failed transactions</li> </ul>"}, {"location": "Changelog-WaterDrop/#2610-2023-10-24", "title": "2.6.10 (2023-10-24)", "text": "<ul> <li>[Improvement] Introduce <code>message.purged</code> event to indicate that a message that was not delivered to Kafka was purged. This most of the time refers to messages that were part of a transaction and were not yet dispatched to Kafka. It always means, that given message was not delivered but in case of transactions it is expected. In case of non-transactional it usually means <code>#purge</code> usage or exceeding <code>message.timeout.ms</code> so <code>librdkafka</code> removes this message from its internal queue. Non-transactional producers do not use this and pipe purges to <code>error.occurred</code>.</li> <li>[Fix] Fix a case where <code>message.acknowledged</code> would not have <code>caller</code> key.</li> <li>[Fix] Fix a bug where critical errors (like <code>IRB::Abort</code>) would not abort the ongoing transaction.</li> </ul>"}, {"location": "Changelog-WaterDrop/#269-2023-10-23", "title": "2.6.9 (2023-10-23)", "text": "<ul> <li>[Improvement] Introduce a <code>transaction.finished</code> event to indicate that transaction has finished whether it was aborted or committed.</li> <li>[Improvement] Use <code>transaction.committed</code> event to indicate that transaction has been committed.</li> </ul>"}, {"location": "Changelog-WaterDrop/#268-2023-10-20", "title": "2.6.8 (2023-10-20)", "text": "<ul> <li>[Feature] Introduce transactions support.</li> <li>[Improvement] Expand <code>LoggerListener</code> to inform about transactions (info level).</li> <li>[Improvement] Allow waterdrop to use topic as a symbol or a string.</li> <li>[Improvement] Enhance both <code>message.acknowledged</code> and <code>error.occurred</code> (for <code>librdkafka.dispatch_error</code>) with full delivery_report.</li> <li>[Improvement] Provide <code>#close!</code> that will force producer close even with outgoing data after the ma wait timeout.</li> <li>[Improvement] Provide <code>#purge</code> that will purge any outgoing data and data from the internal queues (both WaterDrop and librdkafka).</li> <li>[Fix] Fix the <code>librdkafka.dispatch_error</code> error dispatch for errors with negative code.</li> </ul>"}, {"location": "Changelog-WaterDrop/#267-2023-09-01", "title": "2.6.7 (2023-09-01)", "text": "<ul> <li>[Improvement] early flush data from <code>librdkafka</code> internal buffer before closing.</li> <li>[Maintenance] Update the signing cert as the old one expired.</li> </ul>"}, {"location": "Changelog-WaterDrop/#266-2023-08-03", "title": "2.6.6 (2023-08-03)", "text": "<ul> <li>[Improvement] Provide <code>log_messages</code> option to <code>LoggerListener</code> so the extensive messages data logging can disabled.</li> </ul>"}, {"location": "Changelog-WaterDrop/#265-2023-07-22", "title": "2.6.5 (2023-07-22)", "text": "<ul> <li>[Fix] Add cause to the errors that are passed into instrumentation (konalegi)</li> </ul>"}, {"location": "Changelog-WaterDrop/#264-2023-07-11", "title": "2.6.4 (2023-07-11)", "text": "<ul> <li>[Improvement] Use original error <code>#inspect</code> for <code>WaterDrop::Errors::ProduceError</code> and <code>WaterDrop::Errors::ProduceManyError</code> instead of the current empty string.</li> </ul>"}, {"location": "Changelog-WaterDrop/#263-2023-06-28", "title": "2.6.3 (2023-06-28)", "text": "<ul> <li>[Change] Use <code>Concurrent::AtomicFixnum</code> to track operations in progress to prevent potential race conditions on JRuby and TruffleRuby (not yet supported but this is for future usage).</li> <li>[Change] Require <code>karafka-rdkafka</code> <code>&gt;= 0.13.2</code>.</li> <li>[Change] Require 'karafka-core' <code>&gt;= 2.1.1</code></li> </ul>"}, {"location": "Changelog-WaterDrop/#262-2023-06-21", "title": "2.6.2 (2023-06-21)", "text": "<ul> <li>[Refactor] Introduce a counter-based locking approach to make sure, that we close the producer safely but at the same time not to limit messages production with producing lock.</li> <li>[Refactor] Make private methods private.</li> <li>[Refactor] Validate that producer is not closed only when attempting to produce.</li> <li>[Refactor] Improve one 5 minute long spec to run in 10 seconds.</li> <li>[Refactor] clear client assignment after closing.</li> </ul>"}, {"location": "Changelog-WaterDrop/#261-2023-06-19", "title": "2.6.1 (2023-06-19)", "text": "<ul> <li>[Refactor] Remove no longer needed patches.</li> <li>[Fix] Fork detection on a short lived processes seems to fail. Clear the used parent process client reference not to close it in the finalizer (#356).</li> <li>[Change] Require <code>karafka-rdkafka</code> <code>&gt;= 0.13.0</code>.</li> <li>[Change] Require 'karafka-core' <code>&gt;= 2.1.0</code></li> </ul>"}, {"location": "Changelog-WaterDrop/#260-2023-06-11", "title": "2.6.0 (2023-06-11)", "text": "<ul> <li>[Improvement] Introduce <code>client_class</code> setting for ability to replace underlying client with anything specific to a given env (dev, test, etc).</li> <li>[Improvement] Introduce <code>Clients::Buffered</code> useful for writing specs that do not have to talk with Kafka (id-ilych)</li> <li>[Improvement] Make <code>#produce</code> method private to avoid confusion and make sure it is not used directly (it is not part of the official API).</li> <li>[Change] Change <code>wait_on_queue_full</code> from <code>false</code> to <code>true</code> as a default.</li> <li>[Change] Rename <code>wait_on_queue_full_timeout</code> to <code>wait_backoff_on_queue_full</code> to match what it actually does.</li> <li>[Enhancement] Introduce <code>wait_timeout_on_queue_full</code> with proper meaning. That is, this represents time after which despite backoff the error will be raised. This should allow to raise an error in case the backoff attempts were insufficient. This prevents from a case, where upon never deliverable messages we would end up with an infinite loop.</li> <li>[Fix] Provide <code>type</code> for queue full errors that references the appropriate public API method correctly.</li> </ul>"}, {"location": "Changelog-WaterDrop/#253-2023-05-26", "title": "2.5.3 (2023-05-26)", "text": "<ul> <li>[Enhancement] Include topic name in the <code>error.occurred</code> notification payload.</li> <li>[Enhancement] Include topic name in the <code>message.acknowledged</code> notification payload.</li> <li>[Maintenance] Require <code>karafka-core</code> <code>2.0.13</code></li> </ul>"}, {"location": "Changelog-WaterDrop/#252-2023-04-24", "title": "2.5.2 (2023-04-24)", "text": "<ul> <li>[Fix] Require missing Pathname (#345)</li> </ul>"}, {"location": "Changelog-WaterDrop/#251-2023-03-09", "title": "2.5.1 (2023-03-09)", "text": "<ul> <li>[Feature] Introduce a configurable backoff upon <code>librdkafka</code> queue full (false by default).</li> </ul>"}, {"location": "Changelog-WaterDrop/#250-2023-03-04", "title": "2.5.0 (2023-03-04)", "text": "<ul> <li>[Feature] Pipe all the errors including synchronous errors via the <code>error.occurred</code>.</li> <li>[Improvement] Pipe delivery errors that occurred not via the error callback using the <code>error.occurred</code> channel.</li> <li>[Improvement] Introduce <code>WaterDrop::Errors::ProduceError</code> and <code>WaterDrop::Errors::ProduceManyError</code> for any inline raised errors that occur. You can get the original error by using the <code>#cause</code>.</li> <li>[Improvement] Include <code>#dispatched</code> messages handler in the <code>WaterDrop::Errors::ProduceManyError</code> error, to be able to understand which of the messages were delegated to <code>librdkafka</code> prior to the failure.</li> <li>[Maintenance] Remove the <code>WaterDrop::Errors::FlushFailureError</code> in favour of correct error that occurred to unify the error handling.</li> <li>[Maintenance] Rename <code>Datadog::Listener</code> to <code>Datadog::MetricsListener</code> to align with Karafka (#329).</li> <li>[Fix] Do not flush when there is no data to flush in the internal buffer.</li> <li>[Fix] Wait on the final data flush for short-lived producers to make sure, that the message is actually dispatched by <code>librdkafka</code> or timeout.</li> </ul>"}, {"location": "Changelog-WaterDrop/#2411-2023-02-24", "title": "2.4.11 (2023-02-24)", "text": "<ul> <li>Replace the local rspec locator with generalized core one.</li> <li>Make <code>::WaterDrop::Instrumentation::Notifications::EVENTS</code> list public for anyone wanting to re-bind those into a different notification bus.</li> </ul>"}, {"location": "Changelog-WaterDrop/#2410-2023-01-30", "title": "2.4.10 (2023-01-30)", "text": "<ul> <li>Include <code>caller</code> in the error instrumentation to align with Karafka.</li> </ul>"}, {"location": "Changelog-WaterDrop/#249-2023-01-11", "title": "2.4.9 (2023-01-11)", "text": "<ul> <li>Remove empty debug logging out of <code>LoggerListener</code>.</li> <li>Do not lock Ruby version in Karafka in favour of <code>karafka-core</code>.</li> <li>Make sure <code>karafka-core</code> version is at least <code>2.0.9</code> to make sure we run <code>karafka-rdkafka</code>.</li> </ul>"}, {"location": "Changelog-WaterDrop/#248-2023-01-07", "title": "2.4.8 (2023-01-07)", "text": "<ul> <li>Use monotonic time from Karafka core.</li> </ul>"}, {"location": "Changelog-WaterDrop/#247-2022-12-18", "title": "2.4.7 (2022-12-18)", "text": "<ul> <li>Add support to customizable middlewares that can modify message hash prior to validation and dispatch.</li> <li>Fix a case where upon not-available leader, metadata request would not be retried</li> <li>Require <code>karafka-core</code> 2.0.7.</li> </ul>"}, {"location": "Changelog-WaterDrop/#246-2022-12-10", "title": "2.4.6 (2022-12-10)", "text": "<ul> <li>Set <code>statistics.interval.ms</code> to 5 seconds by default, so the defaults cover all the instrumentation out of the box.</li> </ul>"}, {"location": "Changelog-WaterDrop/#245-2022-12-10", "title": "2.4.5 (2022-12-10)", "text": "<ul> <li>Fix invalid error scope visibility.</li> <li>Cache partition count to improve messages production and lower stress on Kafka when <code>partition_key</code> is on.</li> </ul>"}, {"location": "Changelog-WaterDrop/#244-2022-12-09", "title": "2.4.4 (2022-12-09)", "text": "<ul> <li>Add temporary patch on top of <code>rdkafka-ruby</code> to mitigate metadata fetch timeout failures.</li> </ul>"}, {"location": "Changelog-WaterDrop/#243-2022-12-07", "title": "2.4.3 (2022-12-07)", "text": "<ul> <li>Support for librdkafka 0.13</li> <li>Update Github Actions</li> <li>Change auto-generated id from <code>SecureRandom#uuid</code> to <code>SecureRandom#hex(6)</code></li> <li>Remove shared components that were moved to <code>karafka-core</code> from WaterDrop</li> </ul>"}, {"location": "Changelog-WaterDrop/#242-2022-09-29", "title": "2.4.2 (2022-09-29)", "text": "<ul> <li>Allow sending tombstone messages (#267)</li> </ul>"}, {"location": "Changelog-WaterDrop/#241-2022-08-01", "title": "2.4.1 (2022-08-01)", "text": "<ul> <li>Replace local statistics decorator with the one extracted to <code>karafka-core</code>.</li> </ul>"}, {"location": "Changelog-WaterDrop/#240-2022-07-28", "title": "2.4.0 (2022-07-28)", "text": "<ul> <li>Small refactor of the DataDog/Statsd listener to align for future extraction to <code>karafka-common</code>.</li> <li>Replace <code>dry-monitor</code> with home-brew notification layer (API compatible) and allow for usage with <code>ActiveSupport::Notifications</code>.</li> <li>Remove all the common code into <code>karafka-core</code> and add it as a dependency.</li> </ul>"}, {"location": "Changelog-WaterDrop/#233-2022-07-18", "title": "2.3.3 (2022-07-18)", "text": "<ul> <li>Replace <code>dry-validation</code> with home-brew validation layer and drop direct dependency on <code>dry-validation</code>.</li> <li>Remove indirect dependency on dry-configurable from DataDog listener (no changes required).</li> </ul>"}, {"location": "Changelog-WaterDrop/#232-2022-07-17", "title": "2.3.2 (2022-07-17)", "text": "<ul> <li>Replace <code>dry-configurable</code> with home-brew config and drop direct dependency on <code>dry-configurable</code>.</li> </ul>"}, {"location": "Changelog-WaterDrop/#231-2022-06-17", "title": "2.3.1 (2022-06-17)", "text": "<ul> <li>Update rdkafka patches to align with <code>0.12.0</code> and <code>0.11.1</code> support.</li> </ul>"}, {"location": "Changelog-WaterDrop/#230-2022-04-03", "title": "2.3.0 (2022-04-03)", "text": "<ul> <li>Rename StdoutListener to LoggerListener (#240)</li> </ul>"}, {"location": "Changelog-WaterDrop/#220-2022-02-18", "title": "2.2.0 (2022-02-18)", "text": "<ul> <li>Add Datadog listener for metrics + errors publishing</li> <li>Add Datadog example dashboard template</li> <li>Update Readme to show Dd instrumentation usage</li> <li>Align the directory namespace convention with gem name (waterdrop =&gt; WaterDrop)</li> <li>Introduce a common base for validation contracts</li> <li>Drop CI support for ruby 2.6</li> <li>Require all <code>kafka</code> settings to have symbol keys (compatibility with Karafka 2.0 and rdkafka)</li> </ul>"}, {"location": "Changelog-WaterDrop/#210-2022-01-03", "title": "2.1.0 (2022-01-03)", "text": "<ul> <li>Ruby 3.1 support</li> <li>Change the error notification key from <code>error.emitted</code> to <code>error.occurred</code>.</li> <li>Normalize error tracking and make all the places publish errors into the same notification endpoint (<code>error.occurred</code>).</li> <li>Start semantic versioning WaterDrop.</li> </ul>"}, {"location": "Changelog-WaterDrop/#207-2021-12-03", "title": "2.0.7 (2021-12-03)", "text": "<ul> <li>Source code metadata url added to the gemspec</li> <li>Replace <code>:producer</code> with <code>:producer_id</code> in events and update <code>StdoutListener</code> accordingly. This change aligns all the events in terms of not publishing the whole producer object in the events.</li> <li>Add <code>error.emitted</code> into the <code>StdoutListener</code>.</li> <li>Enable <code>StdoutLogger</code> in specs for additional integration coverage.</li> </ul>"}, {"location": "Changelog-WaterDrop/#206-2021-12-01", "title": "2.0.6 (2021-12-01)", "text": "<ul> <li>Fix some unstable specs.</li> </ul>"}, {"location": "Changelog-WaterDrop/#218-fixes-a-case-where-dispatch-of-callbacks-the-same-moment-a-new-producer-was-created-could-cause-a-concurrency-issue-in-the-manager", "title": "218 - Fixes a case, where dispatch of callbacks the same moment a new producer was created could cause a concurrency issue in the manager.", "text": ""}, {"location": "Changelog-WaterDrop/#205-2021-11-28", "title": "2.0.5 (2021-11-28)", "text": ""}, {"location": "Changelog-WaterDrop/#bug-fixes", "title": "Bug fixes", "text": "<ul> <li>Fixes an issue where multiple producers would emit stats of other producers causing the same stats to be published several times (as many times as a number of producers). This could cause invalid reporting for multi-kafka setups.</li> <li>Fixes a bug where emitted statistics would contain their first value as the first delta value for first stats emitted.</li> <li>Fixes a bug where decorated statistics would include a delta for a root field with non-numeric values.</li> </ul>"}, {"location": "Changelog-WaterDrop/#changes-and-features", "title": "Changes and features", "text": "<ul> <li>Introduces support for error callbacks instrumentation notifications with <code>error.emitted</code> monitor emitted key for tracking background errors that would occur on the producer (disconnects, etc).</li> <li>Removes the <code>:producer</code> key from <code>statistics.emitted</code> and replaces it with <code>:producer_id</code> not to inject whole producer into the payload</li> <li>Removes the <code>:producer</code> key from <code>message.acknowledged</code> and replaces it with <code>:producer_id</code> not to inject whole producer into the payload</li> <li>Cleanup and refactor of callbacks support to simplify the API and make it work with Rdkafka way of things.</li> <li>Introduces a callbacks manager concept that will also be within in Karafka <code>2.0</code> for both statistics and errors tracking per client.</li> <li>Sets default Kafka <code>client.id</code> to <code>waterdrop</code> when not set.</li> <li>Updates specs to always emit statistics for better test coverage.</li> <li>Adds statistics and errors integration specs running against Kafka.</li> <li>Replaces direct <code>RSpec.describe</code> reference with auto-discovery</li> <li>Patches <code>rdkafka</code> to provide functionalities that are needed for granular callback support.</li> </ul>"}, {"location": "Changelog-WaterDrop/#204-2021-09-19", "title": "2.0.4 (2021-09-19)", "text": "<ul> <li>Update <code>dry-*</code> to the recent versions and update settings syntax to match it</li> <li>Update Zeitwerk requirement</li> </ul>"}, {"location": "Changelog-WaterDrop/#203-2021-09-05", "title": "2.0.3 (2021-09-05)", "text": "<ul> <li>Remove rdkafka patch in favour of spec topic pre-creation</li> <li>Do not close client that was never used upon closing producer</li> </ul>"}, {"location": "Changelog-WaterDrop/#202-2021-08-13", "title": "2.0.2 (2021-08-13)", "text": "<ul> <li>Add support for <code>partition_key</code></li> <li>Switch license from <code>LGPL-3.0</code> to  <code>MIT</code></li> <li>Switch flushing on close to sync</li> </ul>"}, {"location": "Changelog-WaterDrop/#201-2021-06-05", "title": "2.0.1 (2021-06-05)", "text": "<ul> <li>Remove Ruby 2.5 support and update minimum Ruby requirement to 2.6</li> <li>Fix the <code>finalizer references object to be finalized</code> warning issued with 3.0</li> </ul>"}, {"location": "Changelog-WaterDrop/#200-2020-12-13", "title": "2.0.0 (2020-12-13)", "text": "<ul> <li>Redesign of the whole API (see <code>README.md</code> for the use-cases and the current API)</li> <li>Replace <code>ruby-kafka</code> with <code>rdkafka</code></li> <li>Switch license from <code>MIT</code> to <code>LGPL-3.0</code></li> <li> </li> <li>Global state removed</li> <li>Redesigned metrics that use <code>rdkafka</code> internal data + custom diffing</li> <li>Restore JRuby support</li> </ul>"}, {"location": "Changelog-WaterDrop/#113-add-some-basic-validations-of-the-kafka-scope-of-the-config-azdaroth", "title": "113 - Add some basic validations of the kafka scope of the config (Azdaroth)", "text": ""}, {"location": "Changelog-WaterDrop/#140-2020-08-25", "title": "1.4.0 (2020-08-25)", "text": "<ul> <li>Release to match Karafka 1.4 versioning.</li> </ul>"}, {"location": "Changelog-WaterDrop/#134-2020-02-17", "title": "1.3.4 (2020-02-17)", "text": "<ul> <li>Support for new <code>dry-configurable</code></li> </ul>"}, {"location": "Changelog-WaterDrop/#133-2019-01-06", "title": "1.3.3 (2019-01-06)", "text": ""}, {"location": "Changelog-WaterDrop/#119-support-exactly-once-delivery-and-transactional-messaging-kylekthompson", "title": "119 - Support exactly once delivery and transactional messaging (kylekthompson)", "text": ""}, {"location": "Changelog-WaterDrop/#119-support-delivery_boy-10-kylekthompson", "title": "119 - Support delivery_boy 1.0 (kylekthompson)", "text": ""}, {"location": "Changelog-WaterDrop/#132-2019-26-12", "title": "1.3.2 (2019-26-12)", "text": "<ul> <li>Ruby 2.7.0 support</li> <li>Fix missing <code>delegate</code> dependency on <code>ruby-kafka</code></li> </ul>"}, {"location": "Changelog-WaterDrop/#131-2019-10-21", "title": "1.3.1 (2019-10-21)", "text": "<ul> <li>Ruby 2.6.5 support</li> <li>Expose setting to optionally verify hostname on ssl certs #109 (tabdollahi)</li> </ul>"}, {"location": "Changelog-WaterDrop/#130-2019-09-09", "title": "1.3.0 (2019-09-09)", "text": "<ul> <li>Drop Ruby 2.4 support</li> </ul>"}, {"location": "Changelog-WaterDrop/#130rc1-2019-07-31", "title": "1.3.0.rc1 (2019-07-31)", "text": "<ul> <li>Drop Ruby 2.3 support</li> <li>Drop support for Kafka 0.10 in favor of native support for Kafka 0.11.</li> <li>Ruby 2.6.3 support</li> <li>Support message headers</li> <li><code>sasl_over_ssl</code> support</li> <li>Unlock Ruby Kafka + provide support for 0.7 only</li> <li> </li> <li>Drop support for Kafka 0.10 in favor of native support for Kafka 0.11.</li> <li>Support ruby-kafka 0.7</li> <li>Support message headers</li> <li><code>sasl_over_ssl</code> support</li> <li><code>ssl_client_cert_key_password</code> support</li> <li> </li> <li>Use Zeitwerk for gem code loading</li> <li> </li> <li> </li> <li>Bump delivery_boy (0.2.7 =&gt; 0.2.8)</li> </ul>"}, {"location": "Changelog-WaterDrop/#60-rename-listener-to-stdoutlistener", "title": "60 - Rename listener to StdoutListener", "text": ""}, {"location": "Changelog-WaterDrop/#87-make-stdout-listener-as-instance", "title": "87 - Make stdout listener as instance", "text": ""}, {"location": "Changelog-WaterDrop/#93-zstd-compression-support", "title": "93 - zstd compression support", "text": ""}, {"location": "Changelog-WaterDrop/#99-schemas-are-renamed-to-contracts", "title": "99 - schemas are renamed to contracts", "text": ""}, {"location": "Changelog-WaterDrop/#125", "title": "1.2.5", "text": "<ul> <li>Bump dependencies to match Karafka</li> <li>drop jruby support</li> <li>drop ruby 2.2 support</li> </ul>"}, {"location": "Changelog-WaterDrop/#124", "title": "1.2.4", "text": "<ul> <li>Due to multiple requests, unlock of 0.7 with an additional post-install message</li> </ul>"}, {"location": "Changelog-WaterDrop/#123", "title": "1.2.3", "text": "<ul> <li>Lock ruby-kafka to 0.6 (0.7 support targeted for WaterDrop 1.3)</li> </ul>"}, {"location": "Changelog-WaterDrop/#122", "title": "1.2.2", "text": ""}, {"location": "Changelog-WaterDrop/#55-codec-settings-unification-and-config-applier", "title": "55 - Codec settings unification and config applier", "text": ""}, {"location": "Changelog-WaterDrop/#121", "title": "1.2.1", "text": ""}, {"location": "Changelog-WaterDrop/#54-compression_codec-api-sync-with-king-konf-requirements", "title": "54 - compression_codec api sync with king-konf requirements", "text": ""}, {"location": "Changelog-WaterDrop/#120", "title": "1.2.0", "text": "<ul> <li>Ruby 2.5.0 support</li> <li>Gem bump to match Karafka framework versioning</li> <li> </li> <li> </li> </ul>"}, {"location": "Changelog-WaterDrop/#45-allow-specifying-a-create-time-for-messages", "title": "45 - Allow specifying a create time for messages", "text": ""}, {"location": "Changelog-WaterDrop/#47-support-scram-once-released", "title": "47 - Support SCRAM once released", "text": ""}, {"location": "Changelog-WaterDrop/#49-add-lz4-support-once-merged-and-released", "title": "49 - Add lz4 support once merged and released", "text": ""}, {"location": "Changelog-WaterDrop/#50-potential-message-loss-in-async-mode", "title": "50 - Potential message loss in async mode", "text": ""}, {"location": "Changelog-WaterDrop/#48-ssl_ca_certs_from_system", "title": "48 - ssl_ca_certs_from_system", "text": ""}, {"location": "Changelog-WaterDrop/#52-use-instrumentation-compatible-with-karafka-12", "title": "52 - Use instrumentation compatible with Karafka 1.2", "text": ""}, {"location": "Changelog-WaterDrop/#101", "title": "1.0.1", "text": "<ul> <li>Added high level retry on connection problems</li> </ul>"}, {"location": "Changelog-WaterDrop/#100", "title": "1.0.0", "text": "<ul> <li>Gem bump</li> <li>Ruby 2.4.2 support</li> <li>Raw ruby-kafka driver is now replaced with delivery_boy</li> <li>Sync and async producers</li> <li>Complete update of the API</li> <li>Much better validations for config details</li> <li>Complete API remodel - please read the new README</li> <li>Renamed send_messages to deliver</li> </ul>"}, {"location": "Changelog-WaterDrop/#37-ack-level-for-producer", "title": "37 - ack level for producer", "text": ""}, {"location": "Changelog-WaterDrop/#04", "title": "0.4", "text": "<ul> <li>Bump to match Karafka</li> <li>Renamed <code>hosts</code> to <code>seed_brokers</code></li> <li>Removed the <code>ssl</code> scoping for <code>kafka</code> config namespace to better match Karafka conventions</li> <li>Added <code>client_id</code> option on a root config level</li> <li>Added <code>logger</code> option on a root config level</li> <li>Auto Propagation of config down to ruby-kafka</li> </ul>"}, {"location": "Changelog-WaterDrop/#032", "title": "0.3.2", "text": "<ul> <li>Removed support for Ruby 2.1.*</li> <li>~~Ruby 2.3.3 as default~~</li> <li>Ruby 2.4.0 as default</li> <li>Gem dump x2</li> <li>Dry configurable config (#20)</li> <li>added .rspec for default spec helper require</li> <li>Added SSL capabilities</li> <li>Coditsu instead of PG dev tools for quality control</li> </ul>"}, {"location": "Changelog-WaterDrop/#031", "title": "0.3.1", "text": "<ul> <li>Dev tools update</li> <li>Gem update</li> <li>Specs updates</li> <li>File naming convention fix from waterdrop to water_drop + compatibility file</li> <li>Additional params (partition, etc) that can be passed into producer</li> </ul>"}, {"location": "Changelog-WaterDrop/#030", "title": "0.3.0", "text": "<ul> <li>Driver change from Poseidon (not maintained) to Ruby-Kafka</li> </ul>"}, {"location": "Changelog-WaterDrop/#020", "title": "0.2.0", "text": "<ul> <li>Version dump - this WaterDrop version no longer relies on Aspector to work</li> <li> </li> <li> </li> <li> </li> </ul>"}, {"location": "Changelog-WaterDrop/#17-logger-for-aspector-waterdrop-no-longer-depends-on-aspector", "title": "17 - Logger for Aspector - WaterDrop no longer depends on Aspector", "text": ""}, {"location": "Changelog-WaterDrop/#8-add-send-date-as-a-default-value-added-to-a-message-wont-fix-should-be-implemented-on-a-message-level-since-waterdrop-just-transports-messages-without-adding-additional-stuff", "title": "8 - add send date as a default value added to a message - wont-fix. Should be implemented on a message level since WaterDrop just transports messages without adding additional stuff.", "text": ""}, {"location": "Changelog-WaterDrop/#11-same-as-above", "title": "11 - same as above", "text": ""}, {"location": "Changelog-WaterDrop/#0113", "title": "0.1.13", "text": "<ul> <li>Resolved bug #15. When you use waterdrop in aspect way, message will be automatically parse to JSON.</li> </ul>"}, {"location": "Changelog-WaterDrop/#0112", "title": "0.1.12", "text": "<ul> <li>Removed default to_json casting because of binary/other data types incompatibility. This is an incompatibility. If you use WaterDrop, please add a proper casting method to places where you use it.</li> <li>Gem dump</li> </ul>"}, {"location": "Changelog-WaterDrop/#0111", "title": "0.1.11", "text": "<ul> <li>Poseidon options extractions and tweaks</li> </ul>"}, {"location": "Changelog-WaterDrop/#0110", "title": "0.1.10", "text": "<ul> <li>Switched raise_on_failure to ignore all StandardError failures (or not to), not just specific once</li> <li>Reloading inside connection pool connection that seems to be broken (one that failed) - this should prevent from multiple issues (but not from single one) that are related to the connection</li> </ul>"}, {"location": "Changelog-WaterDrop/#019", "title": "0.1.9", "text": "<ul> <li>Required acks and set to -1 (most secure but slower)</li> <li>Added a proxy layer to to producer so we could replace Kafka with other messaging systems</li> <li>Gem dump</li> </ul>"}, {"location": "Changelog-WaterDrop/#018", "title": "0.1.8", "text": "<ul> <li>proper poseidon clients names (not duplicated)</li> </ul>"}, {"location": "Changelog-WaterDrop/#017", "title": "0.1.7", "text": "<ul> <li>kafka_host, kafka_hosts and kafka_ports settings consistency fix</li> </ul>"}, {"location": "Changelog-WaterDrop/#016", "title": "0.1.6", "text": "<ul> <li>Added null-logger gem</li> </ul>"}, {"location": "Changelog-WaterDrop/#015", "title": "0.1.5", "text": "<ul> <li>raise_on_failure flag to ignore (if false) that message was not sent</li> </ul>"}, {"location": "Changelog-WaterDrop/#014", "title": "0.1.4", "text": "<ul> <li>Renamed WaterDrop::Event to WaterDrop::Message to follow Apache Kafka naming convention</li> </ul>"}, {"location": "Changelog-WaterDrop/#013", "title": "0.1.3", "text": "<ul> <li>Gems cleanup</li> <li>Requirements fix</li> </ul>"}, {"location": "Changelog-WaterDrop/#012", "title": "0.1.2", "text": "<ul> <li>Initial gem release</li> </ul>"}, {"location": "Components/", "title": "Components", "text": "<p>Karafka is a framework for producing and consuming messages using Kafka. It is built out of a few components:</p> <ul> <li>Karafka (Consumer) - responsible for consuming messages from Kafka</li> <li>WaterDrop (Producer) - messages producer integrated with Karafka out of the box</li> <li>Karafka-Web (UI) - User interface for the Karafka framework</li> <li>Karafka-Rdkafka (Driver) - A customized fork of <code>rdkafka-ruby</code> providing additional functionalities and extended stability</li> <li>Rdkafka-Ruby (Driver base) - The original librdkafka bindings also maintained by us.</li> </ul>"}, {"location": "Components/#producer", "title": "Producer", "text": "<p>The producer can run in any Ruby process, allowing you to produce Kafka messages.</p> <pre><code># Fast, non-blocking, recommended\nKarafka.producer.produce_async(topic: 'events', payload: Events.last.to_json)\n\n# Slower, blocking\nKarafka.producer.produce_sync(topic: 'events', payload: Events.last.to_json)\n\nKarafka.producer.class #=&gt; WaterDrop::Producer\n</code></pre> <p>Karafka uses WaterDrop to produce messages. It is a standalone Karafka framework component that can also be used in applications that only produce messages.</p> <p>Please refer to WaterDrop documentation for more details.</p>"}, {"location": "Components/#consumer-server", "title": "Consumer / server", "text": "<p>Each Karafka server process pulls messages from Kafka topics and processes them. The server will instantiate consumers and deliver messages from desired topics. Everything else is up to your code.</p> <p>Example consumer printing payload of fetched messages:</p> <pre><code>class PrintingConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      puts message.payload\n    end\n  end\nend\n</code></pre>"}, {"location": "Components/#karafka-web", "title": "Karafka Web", "text": "<p>Karafka Web UI is a user interface for the Karafka framework. The Web UI provides a convenient way for developers to monitor and manage their Karafka-based applications without the need to use the command line or third-party software. It does not require any additional database beyond Kafka itself.</p> <p> </p>"}, {"location": "Components/#karafka-rdkafka", "title": "Karafka-Rdkafka", "text": "<p>Karafka uses its fork of the <code>rdkafka-ruby</code>. It is done to ensure that each release is complete, stable, and tested against the Karafka ecosystem. Providing our driver layer ensures that upgrades are safe and reliable.</p>"}, {"location": "Components/#rdkafka-ruby", "title": "Rdkafka-ruby", "text": "<p>Modern and performant Kafka client library for Ruby based on librdkafka. It acts as a base for the <code>karafka-rdkafka</code> gem and is also maintained by us. It was created and developed by AppSignal until a handover at the end of 2023.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "Concurrency-and-Multithreading/", "title": "Concurrency and multithreading", "text": "<p>Karafka uses native Ruby threads to achieve concurrent processing in four scenarios:</p> <ul> <li>for concurrent processing of messages from different topics partitions.</li> <li>for concurrent processing of messages from same topic different partitions (via Multiplexing or Non-Blocking Jobs). </li> <li>for concurrent processing of messages from a single partition when using the Virtual Partitions feature.</li> <li>to handle consumer groups management (each consumer group defined will be managed by a separate thread).</li> </ul> <p>Additionally, Karafka supports Swarm Mode for enhanced concurrency. This mode forks independent processes to optimize CPU utilization, leveraging Ruby's Copy-On-Write (CoW) and process supervision for improved throughput and scalability in processing Kafka messages.</p> <p>Separate Resources Management Documentation</p> <p>Please be aware that detailed information on how Karafka manages resources such as threads and TCP connections can be found on a separate documentation page titled Resources Management. This page provides comprehensive insights into the allocation and optimization of system resources by Karafka components.</p>"}, {"location": "Concurrency-and-Multithreading/#parallel-messages-processing", "title": "Parallel Messages Processing", "text": "<p>After messages are fetched from Kafka, Karafka will split incoming messages into separate jobs. Those jobs will then be put on a queue from which a poll of workers can consume. All the ordering warranties will be preserved.</p> <p>You can control the number of workers you want to start by using the <code>concurrency</code> setting:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Run two processing threads\n    config.concurrency = 2\n    # Other settings here...\n  end\nend\n</code></pre>"}, {"location": "Concurrency-and-Multithreading/#parallel-processing-of-multiple-topicspartitions", "title": "Parallel Processing of Multiple Topics/Partitions", "text": "<p>Karafka uses multiple threads to process messages coming from different topics and partitions.</p> <p>Using multiple threads for IO intense work can bring great performance improvements to your system \"for free.\"</p> <p> </p> <p> *This example illustrates performance difference for IO intense jobs. </p> <p>Example of work distribution amongst two workers:</p> <p> </p> <p>Please keep in mind that if you scale horizontally and end up with one Karafka process being subscribed only to a single topic partition, you can still process data from it in parallel using the Virtual Partitions feature.</p>"}, {"location": "Concurrency-and-Multithreading/#parallel-kafka-connections-within-a-single-consumer-group-subscription-groups", "title": "Parallel Kafka connections within a Single Consumer Group (Subscription Groups)", "text": "<p>Karafka uses a concept called <code>subscription groups</code> to organize topics into groups that can be subscribed to Kafka together. This aims to preserve resources to achieve as few connections to Kafka as possible.</p> <p>This grouping strategy has certain downsides, as with one connection, in case of a lag, you may get messages from a single topic partition for an extended time. This may prevent you from utilizing multiple threads to achieve better performance.</p> <p>If you expect scenarios like this to occur, you may want to manually control the number of background connections from Karafka to Kafka. You can define a <code>subscription_group</code> block for several topics, and topics within the same <code>subscription_group</code> will be grouped and will share a separate connection to the cluster. By default, all the topics are grouped within a single subscription group.</p> <p>Each subscription group connection operates independently in a separate background thread. They do, however, share the workers' poll for processing.</p> <p>Below you can find an example of how routing translates into subscription groups and Kafka connections:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    subscription_group 'a' do\n      topic :A do\n        consumer ConsumerA\n      end\n\n      topic :B do\n        consumer ConsumerB\n      end\n\n      topic :D do\n        consumer ConsumerD\n      end\n    end\n\n    subscription_group 'b' do\n      topic :C do\n        consumer ConsumerC\n      end\n    end\n  end\nend\n</code></pre> <p> </p> <p> *This example illustrates how Karafka routing translates into subscription groups and their underlying connections to Kafka.    </p> <p>This example is a simplification. Depending on other factors, Karafka may create more subscription groups to manage the resources better. It will, however, never group topics together that are within different subscription groups.</p> <p>Subscription groups are a different concept than consumer groups. It is an internal Karafka concept; you can have many subscription groups in one consumer group.</p> <p>If you are interested in how <code>librdkafka</code> fetches messages, please refer to this documentation.</p>"}, {"location": "Concurrency-and-Multithreading/#parallel-processing-of-a-single-topic-partition-virtual-partitions", "title": "Parallel Processing Of a Single Topic Partition (Virtual Partitions)", "text": "<p>Karafka allows you to parallelize further processing of data from a single partition of a single topic via a feature called Virtual Partitions.</p> <p>Virtual Partitions allow you to parallelize the processing of data from a single partition. This can drastically increase throughput when IO operations are involved.</p> <p> </p> <p> *This example illustrates the throughput difference for IO intense work, where the IO cost of processing a single message is 1ms.    </p> <p>You can read more about this feature here.</p>"}, {"location": "Concurrency-and-Multithreading/#parallel-consuming-and-processing-of-the-same-topic-partitions", "title": "Parallel Consuming and Processing of the Same Topic Partitions", "text": "<p>Parallel consumption and processing of the same topic partitions in Karafka can be achieved through Multiplying the connections or using Non-Blocking Jobs feature. Multiplexing creates connections to Kafka, allowing for concurrent consumption, which is ideal for different operations or scaling needs. Non-Blocking Jobs, conversely, utilize the same connection but employ a sophisticated pausing strategy to handle processing efficiently. The choice between these approaches depends on the specific requirements, such as operation type and scale. For a comprehensive understanding, visiting the dedicated documentation pages for Multiplexing and Non-Blocking Jobs is recommended.</p>"}, {"location": "Concurrency-and-Multithreading/#consumer-group-multi-threading", "title": "Consumer Group Multi-Threading", "text": "<p>Since each consumer group requires a separate connection and a thread, we do this concurrently.</p> <p>It means that for each consumer group, you will have one additional thread running. For high-load topics, there is always an IO overhead on transferring data from and to Kafka. This approach allows you to consume data concurrently.</p>"}, {"location": "Concurrency-and-Multithreading/#work-saturation", "title": "Work Saturation", "text": "<p>Karafka is designed to efficiently handle a high volume of Kafka messages by leveraging a pool of worker threads. These workers can run in parallel, each processing messages independently. This parallelization allows Karafka to achieve high throughput by distributing the work of processing messages across multiple threads. However, this concurrent processing model can sometimes encounter a phenomenon known as work saturation or job saturation.</p> <p>Work saturation, in the context of multi-threaded processing, occurs when there are more jobs in the queue than the worker threads. As a result, an increasingly large backlog of jobs accumulates in the queue. The implications of work saturation can be significant, and the growing backlog leads to increased latency between when a job is enqueued and when it's eventually processed. Persistent saturation can also lead to a strain on system resources, possibly exhausting memory or rendering the system unable to accept new jobs.</p> <p>Recognizing the potential challenge of job saturation, Karafka provides monitoring capabilities to measure job execution latency. This allows you to track the time from when a job enters the queue to when it's processed. Rising latency can indicate that the system is nearing saturation and isn't processing jobs as quickly as it should.</p> <p>There are a few ways to measure the saturation in Karafka:</p> <ul> <li>You can look at the <code>Enqueued</code> value in the Web-UI. This value indicates the total number of jobs waiting in the internal queues of all the Karafka processes. The high value there indicates increased saturation.</li> <li>You can log the <code>messages.metadata.processing_lag</code> value, which describes how long a batch had to wait before it was picked up by one of the workers.</li> <li>If you are using our Datadog integration, it contains the <code>processing_lag</code> metrics.</li> </ul> <p>Job saturation in Karafka isn't inherently critical, but it may lead to increased consumption lag, resulting in potential delays in processing tasks. This is because when the system is overloaded with jobs, it takes longer to consume and process new incoming data. Moreover, heavily saturated processes can create an additional issue; they may exceed the max.poll.interval.ms configuration parameter. This parameter sets the maximum delay allowed before the Kafka broker considers the consumer unresponsive and reassigns its partitions. In such a scenario, maintaining an optimal balance in job saturation becomes crucial for ensuring efficient message processing.</p>"}, {"location": "Concurrency-and-Multithreading/#subscription-group-blocking-polls", "title": "Subscription Group Blocking Polls", "text": "<p>In the Karafka framework, the default operation mode favors minimal connections to Kafka, typically using just one connection per process for all assigned topic partitions. This approach is generally efficient and ensures even workload distribution when data is evenly spread across partitions. However, this model might lead to reduced parallelism in scenarios where significant lag and many messages accumulate in a few partitions. This happens because polling under such circumstances retrieves many messages from the lagging partitions, concentrating work there rather than distributing it.</p> <p>Karafka's default compliance with the <code>max.poll.interval.ms</code> setting exacerbates this issue. The framework will initiate a new poll once all data from the previous poll is processed, potentially leading to periods where no further messages are being consumed because the system is busy processing a backlog from one or a few partitions. This behavior can dramatically reduce the system's overall concurrency and throughput in extreme cases of lag across many partitions.</p> <p>To address these challenges and enhance parallel processing, Karafka offers several strategies:</p> <ul> <li> <p>Optimizing Polling: Adjusting the <code>fetch.message.max.bytes</code> setting can reduce the number of messages fetched per partition in each poll. This ensures a more effective round-robin distribution of work and prevents any single partition from overwhelming the system.</p> </li> <li> <p>Non-Blocking Jobs: This approach allows the same connection to fetch new messages while previous messages are still being processed. It uses a sophisticated pausing strategy to manage the flow, ensuring processing stays caught up in polling.</p> </li> <li> <p>Virtual Partitions: By subdividing actual Kafka partitions into smaller, virtual ones, Karafka can parallelize the processing of messages from a single physical partition. This can significantly increase the processing throughput and efficiency.</p> </li> <li> <p>Connection Multiplexing: Establishing multiple connections for the same consumer group allows for independent polling and processing. Each connection handles a subset of partitions, ensuring that a lag in one doesn't halt the polling of others.</p> </li> <li> <p>Subscription Groups: Organizes topics into groups for parallel data polling from multiple topics, mitigating lag effects and improving performance by utilizing multiple threads.</p> </li> </ul> <p>Choosing the right strategy (or combination of strategies) depends on your system's specific characteristics and requirements, including the nature of your workload, the scale of your operations, and your performance goals. Each approach offers different benefits and trade-offs regarding complexity, resource usage, and potential throughput. Therefore, carefully analyzing your system's behavior under various conditions is crucial to determining the most effective path to improved parallelism and performance. For detailed guidance and best practices on implementing these strategies, consulting the dedicated documentation pages on Multiplexing, Non-Blocking Jobs, and other relevant sections of the Karafka documentation is highly recommended.</p>"}, {"location": "Concurrency-and-Multithreading/#swarm-mode", "title": "Swarm Mode", "text": "<p>Karafka provides an advanced operation mode known as Swarm, designed to optimize the framework's performance by leveraging Ruby's Copy-On-Write (CoW) mechanism and multi-process architecture. This mode significantly enhances Karafka's ability to utilize multiple CPU cores more efficiently, thus improving the throughput and scalability of your Ruby applications that process Kafka messages.</p> <p>In Swarm Mode, Karafka forks multiple independent processes, each capable of running concurrently. This approach allows the framework to manage and supervise these processes effectively, ensuring high availability and resilience. By doing so, Karafka can better distribute the workload across available CPU cores, minimizing bottlenecks and maximizing processing speed.</p> <p>Swarm has its own section. You can read about it here.</p>"}, {"location": "Concurrency-and-Multithreading/#setting-thread-priority", "title": "Setting Thread Priority", "text": "<p>Karafka supports explicit thread priority configuration. Adjusting thread priorities can mitigate performance issues caused by mixed workloads, particularly by reducing latency when running IO-bound and CPU-bound tasks concurrently.</p> <p>Karafka processing threads have a default priority set to <code>-1</code>. Lowering this priority further can significantly reduce tail latency for IO-bound tasks, ensuring more balanced resource allocation, especially in scenarios with CPU-intensive workloads that could monopolize the Global VM Lock (GVL).</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Lower worker thread priority to prevent CPU-bound tasks from starving IO-bound threads\n    config.worker_thread_priority = -3\n  end\nend\n</code></pre> <p>Lowering thread priority (e.g., negative values like <code>-1</code>, <code>-3</code>) can significantly reduce tail latency for IO-bound tasks. This ensures more balanced resource allocation, especially in scenarios with CPU-intensive workloads that could monopolize the Global VM Lock (GVL).</p> <p>Thread Priority and GVL</p> <p>Ruby employs a Global VM Lock (GVL) that ensures only one thread executes Ruby code at a time. The Ruby VM switches threads roughly every 100ms (thread quantum) unless explicitly released (such as during IO operations). CPU-intensive tasks holding the GVL for the entire quantum period can significantly increase latency for other threads, especially those performing quick IO tasks. Adjusting thread priority mitigates this issue by influencing the scheduling decisions and allowing shorter, IO-bound threads more frequent access to the CPU.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Configuration/", "title": "Configuration", "text": "<p>Karafka contains multiple configuration options. To keep everything organized, all the configuration options were divided into two groups:</p> <ul> <li> <p>root <code>karafka</code> options - options directly related to the Karafka framework and its components.</p> </li> <li> <p>kafka scoped <code>librdkafka</code> options - options related to librdkafka</p> </li> </ul> <p>To apply all those configuration options, you need to use the <code>#setup</code> method from the <code>Karafka::App</code> class:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n    # librdkafka configuration options need to be set as symbol values\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092'\n    }\n  end\nend\n</code></pre> <p>Karafka allows you to redefine some of the settings per each topic, which means that you can have a specific custom configuration that might differ from the default one configured at the app level. This allows you for example, to connect to multiple Kafka clusters.</p> <p>kafka <code>client.id</code> is a string passed to the server when making requests. This is to track the source of requests beyond just IP/port by allowing a logical application name to be included in server-side request logging. Therefore the <code>client_id</code> should be shared across multiple instances in a cluster or horizontally scaled application but distinct for each application.</p>"}, {"location": "Configuration/#karafka-configuration-options", "title": "Karafka configuration options", "text": "<p>A list of all the karafka configuration options with their details and defaults can be found here.</p>"}, {"location": "Configuration/#librdkafka-driver-configuration-options", "title": "librdkafka driver configuration options", "text": "<p>A list of all the configuration options related to <code>librdkafka</code> with their details and defaults can be found here.</p>"}, {"location": "Configuration/#external-components-configurators", "title": "External components configurators", "text": "<p>For additional setup and/or configuration tasks, you can use the <code>app.initialized</code> event hook. It is executed once per process, right after all the framework components are ready (including those dynamically built). It can be used, for example, to configure some external components that need to be based on Karafka internal settings.</p> <p>Because of how the Karafka framework lifecycle works, this event is triggered after the <code>#setup</code> is done. You need to subscribe to this event before that happens, either from the <code>#setup</code> block or before.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # All the config magic\n\n    # Once everything is configured and done, assign Karafka app logger as a MyComponent logger\n    # @note This example does not use config details, but you can use all the config values\n    #   to setup your external components\n    config.monitor.subscribe('app.initialized') do\n      MyComponent::Logging.logger = Karafka::App.logger\n    end\n  end\nend\n</code></pre>"}, {"location": "Configuration/#environment-variables-settings", "title": "Environment variables settings", "text": "<p>There are several env settings you can use with Karafka. They are described under the Env Variables section of this Wiki.</p>"}, {"location": "Configuration/#messages-compression", "title": "Messages compression", "text": "<p>Kafka lets you compress your messages as they travel over the wire. By default, producer messages are sent uncompressed.</p> <p>Karafka producer (WaterDrop) supports following compression types:</p> <ul> <li><code>gzip</code></li> <li><code>zstd</code></li> <li><code>lz4</code></li> <li><code>snappy</code></li> </ul> <p>You can enable the compression by using the <code>compression.codec</code> and <code>compression.level</code> settings:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      # Other kafka settings...\n      'compression.codec': 'gzip',\n      'compression.level': '12'\n    }\n  end\nend\n</code></pre> <p>In order to use <code>zstd</code>, you need to install <code>libzstd-dev</code>:</p> <p><code>bash apt-get install -y libzstd-dev</code></p>"}, {"location": "Configuration/#types-of-configuration-in-karafka", "title": "Types of Configuration in Karafka", "text": "<p>When working with Karafka, it is crucial to understand the different configurations available, as these settings directly influence how Karafka interacts with your application code and the underlying Kafka infrastructure.</p>"}, {"location": "Configuration/#root-configuration-in-the-setup-block", "title": "Root Configuration in the Setup Block", "text": "<p>The root configuration within the <code>setup</code> block of Karafka pertains directly to the Karafka framework and its components. This includes settings that influence the behavior of your Karafka application at a fundamental level, such as client identification, logging preferences, and consumer groups details.</p> <p>Example of root configuration:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n    config.initial_offset = 'latest'\n  end\nend\n</code></pre>"}, {"location": "Configuration/#kafka-scoped-librdkafka-options", "title": "Kafka Scoped <code>librdkafka</code> Options", "text": "<p>librdkafka configuration options are specified within the same setup block but scoped specifically under the <code>kafka</code> key. These settings are passed directly to the librdkafka library, the underlying Kafka client library that Karafka uses. This includes configurations for Kafka connections, such as bootstrap servers, SSL settings, and timeouts.</p> <p>Example of <code>librdkafka</code> scoped options:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'ssl.ca.location': '/etc/ssl/certs'\n    }\n  end\nend\n</code></pre>"}, {"location": "Configuration/#admin-configs-api", "title": "Admin Configs API", "text": "<p>Karafka also supports the Admin Configs API, which is designed to view and manage configurations at the Kafka broker and topic levels. These settings are different from the client configurations (root and Kafka scoped) as they pertain to the infrastructure level of Kafka itself rather than how your application interacts with it.</p> <p>Examples of these settings include:</p> <ul> <li> <p>Broker Configurations: Like log file sizes, message sizes, and default retention policies.</p> </li> <li> <p>Topic Configurations: Such as partition counts, replication factors, and topic-level overrides for retention.</p> </li> </ul> <p>To put it in perspective, these configurations can be likened to those in a database. Just as a database has client, database, and table configurations, Kafka has its own set of configurations at different levels.</p> <ul> <li> <p>Client Configurations: Similar to client-specific settings in SQL databases, such as query timeouts or statement timeouts.</p> </li> <li> <p>Database Configurations: Analogous to database-level settings such as database encoding, connection limits, or default transaction isolation levels.</p> </li> <li> <p>Table Configurations: Similar to table-specific settings like storage engine choices or per-table cache settings in a database.</p> </li> </ul> <p>These infrastructural settings are crucial for managing Kafka more efficiently. They ensure that the Kafka cluster is optimized for both performance and durability according to the needs of the applications it supports.</p> <p>Managing Topics Configuration with Declarative Topics API</p> <p>If you want to manage topic configurations more effectively, we recommend using Karafka's higher-level API, Declarative Topics. This API simplifies defining and managing your Kafka topics, allowing for clear and concise topic configurations within your application code. For detailed usage and examples, refer to our comprehensive guide on Declarative Topics.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "Consuming-Messages/", "title": "Consuming Messages", "text": "<p>Consumers should inherit from the ApplicationConsumer. You need to define a <code>#consume</code> method that will execute your business logic code against a batch of messages.</p> <p>Karafka fetches and consumes messages in batches by default.</p>"}, {"location": "Consuming-Messages/#consuming-messages", "title": "Consuming Messages", "text": "<p>Karafka framework has a long-running server process responsible for fetching and consuming messages.</p> <p>To start the Karafka server process, use the following CLI command:</p> <pre><code>bundle exec karafka server\n</code></pre>"}, {"location": "Consuming-Messages/#in-batches", "title": "In Batches", "text": "<p>Data fetched from Kafka is accessible using the <code>#messages</code> method. The returned object is an enumerable containing received data and additional information that can be useful during the processing.</p> <p>To access the payload of your messages, you can use the <code>#payload</code> method available for each received message:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    # Print all the payloads one after another\n    messages.each do |message|\n      puts message.payload\n    end\n  end\nend\n</code></pre> <p>You can also access all the payloads together to elevate things like batch DB operations available for some of the ORMs:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    # Insert all the events at once with a single query\n    Event.insert_all messages.payloads\n  end\nend\n</code></pre>"}, {"location": "Consuming-Messages/#one-at-a-time", "title": "One At a Time", "text": "<p>While we encourage you to process data in batches to elevate in-memory computation and many DBs batch APIs, you may want to process messages one at a time.</p> <p>You can achieve this by defining a base consumer with such a capability:</p> <pre><code>class SingleMessageBaseConsumer &lt; Karafka::BaseConsumer\n  attr_reader :message\n\n  def consume\n    messages.each do |message|\n      @message = message\n      consume_one\n\n      mark_as_consumed(message)\n    end\n  end\nend\n\nclass Consumer &lt; SingleMessageBaseConsumer\n  def consume_one\n    puts \"I received following message: #{message.payload}\"\n  end\nend\n</code></pre>"}, {"location": "Consuming-Messages/#accessing-topic-details", "title": "Accessing Topic Details", "text": "<p>If, in any case, your logic is dependent on some routing details, you can access them from the consumer using the <code>#topic</code> method. You could use it, for example, in case you want to perform a different logic within a single consumer based on the topic from which your messages come:</p> <pre><code>class UsersConsumer &lt; ApplicationConsumer\n  def consume\n    send(:\"topic_#{topic.name}\")\n  end\n\n  def topic_a\n    # do something\n  end\n\n  def topic_b\n    # do something else if it's a \"b\" topic\n  end\nend\n</code></pre> <p>If you're interested in all the details that are stored in the topic, you can extract all of them at once, by using the <code>#to_h</code> method:</p> <pre><code>class UsersConsumer &lt; ApplicationConsumer\n  def consume\n    puts topic.to_h #=&gt; { name: 'x', ... }\n  end\nend\n</code></pre>"}, {"location": "Consuming-Messages/#consuming-from-earliest-or-latest-offset", "title": "Consuming From Earliest or Latest Offset", "text": "<p>Karafka, by default, will start consuming messages from the earliest it can reach. You can, however configure it to start consuming from the latest message by setting the <code>initial_offset</code> value as a default:</p> <pre><code># This will start from the earliest (default)\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.initial_offset = 'earliest'\n  end\nend\n\n# This will make Karafka start consuming from the latest message on a given topic\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.initial_offset = 'latest'\n  end\nend\n</code></pre> <p>or on a per-topic basis:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :events do\n      consumer EventsConsumer\n      # Start from earliest for this specific topic\n      initial_offset 'earliest'\n    end\n\n    topic :notifications do\n      consumer NotificationsConsumer\n      # Start from latest for this specific topic\n      initial_offset 'latest'\n    end\n  end\nend\n</code></pre> <p>This setting applies only to the first execution of a Karafka process. All following executions will pick up from the last offset where the process ended previously.</p>"}, {"location": "Consuming-Messages/#detecting-revocation-midway", "title": "Detecting Revocation Midway", "text": "<p>When working with a distributed system like Kafka, partitions of a topic can be distributed among different consumers in a consumer group for processing. However, there might be cases where a partition needs to be taken away from a consumer and reassigned to another consumer. This is referred to as a partition revocation.</p> <p>Partition revocation can be voluntary, where the consumer willingly gives up the partition after it is done processing the current batch, or it can be involuntary. An involuntary partition revocation is typically due to a rebalance triggered by consumer group changes or a failure in the consumer, which causes it to become unresponsive. It is important to remember that involuntary revocations can occur during data processing. You may not want to continue processing messages when you know the partition has been taken away. This is where the <code>#revoked?</code> method is beneficial.</p> <p>By monitoring the status of the <code>#revoked?</code> method, your application can detect that your process no longer owns a partition you are operating on. In such scenarios, you can choose to stop any ongoing, expensive processing. This can help you save resources and limit the number of potential reprocessings.</p> <pre><code>def consume\n  messages.each do |message|\n    Message.create!(message)\n\n    mark_as_consumed(message)\n\n    return if revoked?\n  end\nend\n</code></pre> <p>It is worth, however, keeping in mind that under normal operating conditions, Karafka will complete all ongoing processing before a rebalance occurs. This includes finishing the processing of all messages already fetched. Karafka has built-in mechanisms to handle voluntary partition revocations and rebalances, ensuring that no messages are lost or unprocessed during such events. Hence <code>#revoked?</code> is especially useful for involuntary revocations.</p> <p>In most cases, especially if you do not use Long-Running Jobs, the Karafka default offset management strategy should be more than enough. It ensures that after batch processing as well as upon rebalances, before partition reassignment, all the offsets are committed. In a healthy system with stable deployment procedures and without frequent short-lived consumer generations, the number of re-processings should be close to zero.</p> <p>You do not need to mark the message as consumed for the <code>#revoked?</code> method result to change.</p> <p>When using the Long-Running Jobs feature, <code>#revoked?</code> result also changes independently from marking messages.</p>"}, {"location": "Consuming-Messages/#consumer-persistence", "title": "Consumer Persistence", "text": "<p>Karafka consumer instances are persistent by default. This means that a single consumer instance will \"live\" as long as a given process instance consumes a given topic partition. This means you can elevate in-memory processing and buffering to achieve better performance.</p> <p>Karafka consumer instance for a given topic partition will be re-created in case a given partition is lost and re-assigned.</p> <p>If you decide to utilize such techniques, you may be better with manual offset management.</p> <pre><code># A consumer that will buffer messages in memory until it reaches 1000 of them. Then it will flush\n# and commit the offset.\nclass EventsConsumer &lt; ApplicationConsumer\n  # Flush every 1000 messages\n  MAX_BUFFER_SIZE = 1_000\n\n  def initialized\n    @buffer = []\n  end\n\n  def consume\n    # Print all the payloads one after another\n    @buffer += messages.payloads\n\n    return if @buffer.size &lt; MAX_BUFFER_SIZE\n\n    flush\n  end\n\n  private\n\n  def flush\n    Event.insert_all @buffer\n\n    mark_as_consumed @buffer.last\n\n    @buffer.clear!\n  end\nend\n</code></pre>"}, {"location": "Consuming-Messages/#shutdown-and-partition-revocation-handlers", "title": "Shutdown and Partition Revocation Handlers", "text": "<p>Karafka consumer, aside from the <code>#consume</code> method, allows you to define two additional methods that you can use to free any resources that you may be using upon certain events. Those are:</p> <ul> <li><code>#revoked</code> - will be executed when there is a rebalance resulting in the given partition being revoked from the current process.</li> <li><code>#shutdown</code> - will be executed when the Karafka process is being shutdown.</li> </ul> <pre><code>class LogsConsumer &lt; ApplicationConsumer\n  def initialized\n    @log = File.open('log.txt', 'a')\n  end\n\n  def consume\n    messages.each do |message|\n      @log &lt;&lt; message.raw_payload\n    end\n  end\n\n  def shutdown\n    @log.close\n  end\n\n  def revoked\n    @log.close\n  end\nend\n</code></pre> <p>Please note that when using <code>#shutdown</code> with the filtering API or Delayed Topics, there are scenarios where <code>#shutdown</code> and <code>#revoked</code> may be invoked without prior <code>#consume</code> running and the <code>#messages</code> batch may be empty.</p>"}, {"location": "Consuming-Messages/#initial-state-setup", "title": "Initial State Setup", "text": "<p>Karafka consumers provide a special <code>#initialized</code> method called automatically after the consumer instance is fully prepared and initialized.</p> <p>This method can be used to set up any additional state, resources, or connections your consumer may need during its lifecycle. Karafka's consumer instance is not entirely bootstrapped during the <code>#initialize</code> method. This means crucial details, like routing information, topic details, and more, may not yet be available. Using <code>#initialize</code> to set up dependencies might result in incomplete or incorrect configurations. On the other hand, <code>#initialized</code> is executed once the consumer is fully ready and contains all the details it might need. By default, <code>#initialized</code> does nothing. Still, you can override it to include custom setup logic for your consumer:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def initialized\n    # Any setup logic you want to perform once the consumer is fully ready\n    @connection = establish_db_connection\n    puts \"Consumer is initialized with topic: #{topic.name}\"\n  end\n\n  def consume\n    messages.each do |message|\n      # Process messages using the setup done in #initialized\n      puts message.payload\n    end\n  end\n\n  private\n\n  def establish_db_connection\n    # Custom logic to establish a database connection\n  end\nend\n</code></pre> <p>Using <code>#initialized</code> allows access to the full context of the consumer, as it is called when the consumer has been fully set up. This provides several benefits, such as establishing database connections, setting up loggers, or initializing API clients that require topic-specific information. By deferring resource setup to <code>#initialized</code>, you avoid potential issues arising when certain resources or states are unavailable during the construction phase.</p>"}, {"location": "Consuming-Messages/#enablepartitioneof-early-yield", "title": "<code>enable.partition.eof</code> Early Yield", "text": "<p>In typical Karafka consumption scenarios, when a consumer reaches the end of a partition, it might still wait for new messages to arrive. This behavior is governed by settings such as <code>max_wait_time</code> or <code>max_messages</code>, which dictate how long a consumer should wait for new data before timing out or moving on. While this can benefit continuous data streams, it may introduce unnecessary latency in scenarios where real-time data processing and responsiveness are critical.</p> <p>The <code>enable.partition.eof</code> configuration option changes how Karafka responds when the end of a partition is reached during message consumption. By default, when Karafka encounters the end of a partition, it waits for more messages until either <code>max_wait_time</code> or <code>max_messages</code> limits are reached. However, if <code>enable.partition.eof</code> is set for a subscription group to <code>true</code>, Karafka will immediately delegate already accumulated messages (if any) for processing, even if neither <code>max_wait_time</code> nor <code>max_messages</code> has been reached.</p>"}, {"location": "Consuming-Messages/#benefits-of-early-yield", "title": "Benefits of Early Yield", "text": "<ul> <li> <p>Reduced Latency: Immediate message yielding upon reaching the end of a partition can significantly reduce latency. This is particularly beneficial in environments where data must be processed and acted upon quickly.</p> </li> <li> <p>Increased Responsiveness: Systems that require high responsiveness will benefit from not having to wait for the timeout conditions (<code>max_wait_time</code> or <code>max_messages</code>) to be met, allowing subsequent processing steps to commence without delay.</p> </li> <li> <p>Efficient Resource Utilization: By avoiding unnecessary waiting times, system resources can be better utilized for processing rather than idling, potentially leading to cost optimizations and improved throughput.</p> </li> </ul>"}, {"location": "Consuming-Messages/#downsides-of-early-yield", "title": "Downsides of Early Yield", "text": "<ul> <li> <p>Potential for Increased CPU Usage: In highly active systems where new messages are frequently published, constantly checking for the end of partition could lead to increased CPU utilization. This is because the system needs to manage and check state transitions more frequently.</p> </li> <li> <p>Complexity in Batch Processing: For applications that are optimized for batch processing, this setting might disrupt the batching logic, as messages could be processed in smaller batches, potentially leading to inefficiencies.</p> </li> </ul>"}, {"location": "Consuming-Messages/#configuring-enablepartitioneof", "title": "Configuring <code>enable.partition.eof</code>", "text": "<p>The <code>enable.partition.eof</code> is one of the <code>kafka</code> scoped options and can be set for all subscription groups or on a per-subscription group basis, depending on your use case.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'enable.partition.eof': true\n    }\n  end\n\n  # You can also do it per topics in a subscription group\n  routes.draw do\n    subscription_group :fast do\n      topic 'events' do\n        consumer EventsConsumer\n        kafka(\n          'bootstrap.servers': '127.0.0.1:9092',\n          'enable.partition.eof': true\n        )\n      end\n    end\n  end\nend\n</code></pre> <p>This configuration ensures that as soon as the end of a partition is reached, any accumulated messages are immediately processed, enhancing the system's responsiveness and efficiency.</p>"}, {"location": "Consuming-Messages/#inline-api-based-consumption", "title": "Inline API Based Consumption", "text": "<p>Karafka Pro provides the Iterator API that allows you to subscribe to topics and to perform lookups from Rake tasks, custom scripts, Rails console, or any other Ruby processes.</p> <pre><code># Note: you still need to configure your settings using `karafka.rb`\n\n# Select all the events of user with id 5 from last 10 000 messages of\n# each partition of the topic `users_events`\n\nuser_5_events = []\n\niterator = ::Karafka::Pro::Iterator.new(\n  { 'users_events' =&gt; -1000 }\n)\n\niterator.each do |message|\n  # Cast to integer because headers are always strings or arrays of strings\n  next unless message.headers['user-id'].to_i == 5\n\n  user_5_events &lt;&lt; message\nend\n\nputs \"There were #{user_5_events.count} messages\"\n</code></pre> <p>You can read more about it here.</p>"}, {"location": "Consuming-Messages/#avoiding-unintentional-overwriting-of-the-consumer-instance-variables", "title": "Avoiding Unintentional Overwriting of the Consumer Instance Variables", "text": "<p>When working with Karafka consumers, it is crucial to be aware of and avoid unintentionally overwriting certain instance variables used by the consumer instances. Overwriting these variables can lead to critical processing errors and result in issues such as <code>worker.process.error</code> visible in the web UI. Below are the primary instance variables used in consumers that you need to be cautious about:</p> <ul> <li><code>@id</code>: Represents the ID of the current consumer.</li> <li><code>@messages</code>: Stores the messages for the topic to which a given consumer is subscribed.</li> <li><code>@client</code>: Refers to the Kafka connection client.</li> <li><code>@coordinator</code>: Handles coordination of message processing.</li> <li><code>@producer</code>: Holds the instance of the producer.</li> </ul> <p>Accidentally overwriting any of these instance variables can disrupt the normal functioning of the consumer, leading to:</p> <ul> <li>Inability to correctly process or retrieve messages.</li> <li>Loss of connection to the Kafka server.</li> <li>Failure in coordinating message processing.</li> <li>Inability to produce messages properly.</li> </ul> <p>Such disruptions often manifest as \"worker.process.error\" in the web UI, indicating critical processing failures.</p>"}, {"location": "Consuming-Messages/#reaching-the-end-of-a-partition-eof", "title": "Reaching the End of a Partition (EOF)", "text": "<p>Karafka includes dedicated handling for end-of-partition (EOF) scenarios, allowing you to execute specific logic when the end of a partition is reached. For this feature to work, you must enable the <code>enable.partition.eof</code> kafka setting in your configuration.    </p>"}, {"location": "Consuming-Messages/#enabling-eof-handling", "title": "Enabling EOF Handling", "text": "<p>To use EOF features, ensure that both the <code>enable.partition.eof</code> option and the <code>eofed</code> setting are configured properly:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'enable.partition.eof': true\n    }\n  end\n\n  routes.draw do\n    topic 'events' do\n      consumer EventsConsumer\n      # Ensure EOF handling is activated\n      eofed true\n    end\n  end\nend\n</code></pre>"}, {"location": "Consuming-Messages/#implementing-eof-handling", "title": "Implementing EOF Handling", "text": "<p>EOF signaling can happen in two ways:</p> <ul> <li>Via <code>#eofed</code> Method: This method is triggered when no more messages are polled.</li> <li>Alongside <code>#consume</code> Method: EOF can be signaled together with messages if some messages were polled.</li> </ul> <p>Full Coverage of EOF</p> <p>To ensure full coverage of EOF scenarios, both the <code>#eofed</code> method and the <code>#eofed?</code> method should be used. This ensures that EOF is handled whether it occurs with or without new messages.</p>"}, {"location": "Consuming-Messages/#eofed-method", "title": "<code>#eofed</code> Method", "text": "<p>Define the <code>#eofed</code> method in your consumer to handle cases where no more messages are polled alongside the EOF information:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      # Process each message\n      puts message.payload\n    end\n\n    # Check if EOF was signaled alongside messages\n    if eofed?\n      puts \"Reached the end of the partition with messages.\"\n      # Implement any additional logic needed when EOF is reached with messages\n    end\n  end\n\n  def eofed\n    puts \"Reached the end of the partition with no more messages.\"\n    # Implement any additional logic needed when EOF is reached\n  end\nend\n</code></pre>"}, {"location": "Consuming-Messages/#handling-eof-in-consume-method", "title": "Handling EOF in <code>#consume</code> Method", "text": "<p>If EOF is signaled together with messages, the <code>#eofed</code> method will not be triggered. In such cases, Karafka provides a <code>#eofed?</code> method that can be used to detect that EOF has been signaled alongside the messages.</p> <p>The <code>#eofed?</code> method allows you to detect EOF within the <code>#consume</code> method:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      # Process each message\n      puts message.payload\n    end\n\n    # Check if EOF was signaled alongside messages\n    if eofed?\n      puts \"Reached the end of the partition with messages.\"\n      # Implement any additional logic needed when EOF is reached with messages\n    end\n  end\nend\n</code></pre>"}, {"location": "Consuming-Messages/#use-cases-for-eof-handling", "title": "Use Cases for EOF Handling", "text": "<p>Knowing when a partition has reached EOF can be helpful in several scenarios:</p> <ul> <li> <p>Batch Processing Completion: When processing data in batches, knowing when you have processed all available data can be beneficial. This allows you to finalize batch operations, such as committing transactions or aggregating results.</p> </li> <li> <p>Data Synchronization: In cases where you need to synchronize data between different systems, knowing the EOF can signal that all current data has been consumed, and it's safe to start a new synchronization cycle.</p> </li> <li> <p>Resource Cleanup: After reaching the end of a partition, you may want to release or reallocate resources that are no longer needed, optimizing your application's performance.</p> </li> <li> <p>Logging and Monitoring: Logging EOF events can be useful for monitoring data consumption and detecting when there are no more messages to process, which can help debugging and performance tuning.</p> </li> <li> <p>Triggering Downstream Processes: EOF can be a signal to trigger downstream processes that depend on the completion of data consumption, ensuring that subsequent operations only start once all relevant data has been processed.</p> </li> </ul>"}, {"location": "Consuming-Messages/#wrapping-the-execution-flow", "title": "Wrapping the Execution Flow", "text": "<p>Karafka's design includes the ability to \"wrap\" the execution flow, which allows to execute custom logic before and after the message processing cycle. This functionality is particularly valuable for scenarios where additional setup, teardown, or contextual operations (such as selecting a transactional producer from a pool) are needed.</p> <p>The <code>#wrap</code> method surrounds the entire operational flow of the consumer, not just the user's business logic. This includes:</p> <ol> <li>User-Defined Logic: The custom message processing logic implemented in all the actions such as <code>#consume</code>, <code>#revoked</code>, etc. methods.</li> <li>Framework-Level Operations: Core functionalities such as offset management, message acknowledgment, and internal state synchronization.</li> <li>Error Handling and Recovery: Ensures proper transactional rollbacks or retries in case of failures.</li> </ol> <p>To implement <code>#wrap</code>, override it in your consumer class. The method should ensure that <code>yield</code> is always invoked, regardless of any failures or conditions. This is critical because skipping <code>yield</code> can disrupt Karafka's ability to execute its internal processes, leading to inconsistencies or data loss.</p> <p>Here's an example of a <code>#wrap</code> implementation:</p> <pre><code>class CustomConsumer &lt; ApplicationConsumer\n  def consume\n    # This will cause a backoff if no producer was available\n    raise @wrap_error if @wrap_error\n\n    # Your logic here\n  end\n\n  def wrap(_action_name)\n    default_producer = producer\n\n    begin\n      # Attempt to select a producer from the pool\n      PRODUCERS.with do |transactional_producer|\n        self.producer = transactional_producer\n        yield\n      end\n    rescue ProducerUnavailableError =&gt; e\n      # Handle scenarios where a producer isn't available\n      @wrap_error = e\n      yield # Ensure framework operations still execute\n    ensure\n      @wrap_error = false\n      # Restore the original producer after execution\n      self.producer = default_producer\n    end\n  end\nend\n</code></pre> <p>Last modified: 2025-05-16 21:07:08</p>"}, {"location": "Dead-Letter-Queue/", "title": "Dead Letter Queue", "text": "<p>The Dead Letter Queue feature provides a systematic way of dealing with persistent consumption errors that may require a different handling approach while allowing you to continue processing.</p> <p>While consuming data, not everything may go as intended. When an error occurs in Karafka, by default, the framework will apply a back-off strategy and will try again and again. Some errors, however, may be non-recoverable. For example, a broken JSON payload will not be fixed by parsing it again. Messages with non-recoverable errors can be safely moved and analyzed later without interrupting the flow of other valid messages.</p> <p>And this is where the Dead Letter Queue pattern shines.</p> <p>A Dead Letter Queue in Karafka is a feature that, when enabled, will transfer problematic messages into a separate topic allowing you to continue processing even upon errors.</p> <p> </p> <p> *This example illustrates a simple DLQ flow with a processing retry.    </p>"}, {"location": "Dead-Letter-Queue/#using-dead-letter-queue", "title": "Using Dead Letter Queue", "text": "<p>The only thing you need to add to your setup is the <code>dead_letter_queue</code> definition for topics for which you want to enable it:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        # Name of the target topic where problematic messages should be moved to\n        topic: 'dead_messages',\n        # How many times we should retry processing with a back-off before\n        # moving the message to the DLQ topic and continuing the work\n        #\n        # If set to zero, will not retry at all.\n        max_retries: 2,\n        # Apply the independent approach for the DLQ recovery. More in the docs below\n        # It is set to false by default\n        independent: false,\n        # Should the offset of dispatched DLQ message be marked as consumed\n        # This is true by default except when using manual offset management\n        mark_after_dispatch: true\n      )\n    end\n  end\nend\n</code></pre> <p>Once enabled, after the defined number of retries, problematic messages will be moved to a separate topic unblocking the processing.</p> <p>Advanced DLQ Management in Karafka Pro</p> <p>If you're looking for advanced error handling and message recovery capabilities, Karafka Pro's Enhanced DLQ offers complex, context-aware strategies and additional DLQ-related features for superior message integrity and processing precision.</p> <p>Default Behavior with <code>manual_offset_management</code></p> <p>When <code>manual_offset_management</code> is enabled, the <code>mark_after_dispatch</code> option is set to <code>false</code> by default. This means that messages moved to the Dead Letter Queue (DLQ) will not have their offsets automatically marked as consumed. You need to handle offset marking manually or set <code>mark_after_dispatch</code> to <code>true</code> explicitly to ensure proper message acknowledgment and avoid reprocessing the same message repeatedly.</p>"}, {"location": "Dead-Letter-Queue/#dlq-configuration-options", "title": "DLQ Configuration Options", "text": "<p>The table below contains options the <code>#dead_letter_queue</code> routing method accepts.</p> Name Type Description <code>max_retries</code> Integer Defines the number of retries before moving a message to the DLQ. <code>topic</code> String Specifies the DLQ topic name for problematic messages. <code>independent</code> Boolean Treats each message independently with its own error counter. <code>dispatch_method</code> Symbol (<code>:produce_async</code> or <code>:produce_sync</code>) Describes whether dispatch on dlq should be sync or async (async by default). <code>marking_method</code> Symbol (<code>:mark_as_consumed</code> or <code>:mark_as_consumed!</code>) Describes whether marking on DLQ should be async or sync (async by default). <code>mark_after_dispatch</code> Boolean Controls whether the message offset is marked as consumed after it's moved to the DLQ. When <code>true</code> (default for non-MOM), the offset is committed, ensuring smooth continuation of message processing. By default, it is set to <code>false</code> when <code>manual_offset_management(true)</code> is used."}, {"location": "Dead-Letter-Queue/#independent-error-counting", "title": "Independent Error Counting", "text": "<p>In standard operations, Karafka, while processing messages, does not make assumptions about the processing strategy employed by the user. Whether it\u2019s individual message processing or batch operations, Karafka remains agnostic. This neutrality in the processing strategy becomes particularly relevant during the DLQ recovery phases.</p> <p>Under normal circumstances, Karafka treats a batch of messages as a collective unit during DLQ recovery. For example, consider a batch of messages labeled <code>0</code> through <code>9</code>, where message <code>4</code> is problematic. Messages <code>0</code> to <code>3</code> are processed successfully, but message <code>4</code> causes a crash. Karafka then enters the DLQ flow, attempting to reprocess message <code>4</code> multiple times before eventually moving it to the DLQ and proceeding to message <code>5</code> after a brief backoff period. This approach is based on the presumption that the entire batch might be problematic, possibly due to issues like batch upserts. Hence, if a subsequent message in the same batch (say, message <code>7</code>) fails after message <code>4</code> has recovered, Karafka will move message <code>7</code> to the DLQ without resetting the counter and will restart processing from message <code>8</code>.</p> <p>This collective approach might not align with specific use cases with independent message processing. In such cases, the failure of one message does not necessarily imply a problem with the entire batch.</p> <p>Karafka's independent flag introduces a nuanced approach to DLQ recovery, treating each message as an individual entity with its error counter. When enabled, this flag allows Karafka to reset the error count for each message as soon as it is successfully consumed, ensuring that each message is processed on its own merits, independent of the batch. This feature is especially beneficial in scenarios where messages are not interdependent, providing a more targeted and efficient error-handling process for each message within a batch.</p> <p>To enable it, add <code>independent: true</code> to your DLQ topic definition:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: 'dead_messages',\n        max_retries: 3,\n        independent: true\n      )\n    end\n  end\nend\n</code></pre> <p>And make sure your consumer is marking each message as successfully consumed:</p> <pre><code>class OrdersStatesConsumer &lt; Karafka::BaseConsumer\n  def consume\n    messages.each do |message|\n      puts message.payload\n\n      mark_as_consumed(message)\n    end\n  end\nend\n</code></pre> <p>The following diagrams compare DLQ flows in Karafka: the first without the independent flag and the second with it enabled, demonstrating the operational differences between these two settings.</p> <p> </p> <p> *The diagram shows DLQ retry behavior without the independent flag: each error in a batch adds to the error counter until the DLQ dispatch takes place on the last erroneous message.    </p> <p> </p> <p> *The diagram shows DLQ retry behavior with the independent flag active: the error counter resets after each message is successfully processed, avoiding DLQ dispatch if all messages recover.    </p>"}, {"location": "Dead-Letter-Queue/#delaying-the-dlq-data-processing", "title": "Delaying the DLQ Data Processing", "text": "<p>In some cases, it can be beneficial to delay the processing of messages dispatched to a Dead Letter Queue (DLQ) topic. This can be useful when a message has failed to be processed multiple times, and you want to avoid overwhelming the system with repeated processing attempts. By delaying the processing of these messages, you can avoid consuming valuable resources and prevent potential system failures or downtime.</p> <p>Another benefit of delaying the processing of messages dispatched to a DLQ topic is that it can allow developers to investigate the underlying issues that caused the message to fail in the first place. With delayed processing, you can give your team time to investigate and address the root cause of the issue rather than simply reprocessing the message repeatedly and potentially compounding the problem.</p> <p>Overall, delaying the processing of messages dispatched to a DLQ topic can help ensure the stability and reliability of your system while also giving your team the time and resources needed to address underlying issues and prevent future failures.</p> <p>You can read more about the Karafka Delayed Topics feature here.</p>"}, {"location": "Dead-Letter-Queue/#disabling-retries", "title": "Disabling Retries", "text": "<p>If you do not want to retry processing at all upon errors, you can set the <code>max_retries</code> value to <code>0</code>:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: 'dead_messages',\n        max_retries: 0\n      )\n    end\n  end\nend\n</code></pre> <p>Messages will never be re-processed with the following settings and will be moved without retries to the DLQ topic.</p>"}, {"location": "Dead-Letter-Queue/#disabling-dispatch", "title": "Disabling Dispatch", "text": "<p>For some use cases, you may want to skip messages after retries without dispatching them to an alternative topic.</p> <p>This functionality is available in Karafka Pro, and you can read about it here.</p>"}, {"location": "Dead-Letter-Queue/#dispatch-warranties", "title": "Dispatch Warranties", "text": "<p>Messages dispatched to the DLQ topic preserve both <code>payload</code> and <code>headers</code>. They do not follow any partitioning strategy and will be distributed randomly.</p> <p>The original offset, partition, and topic information will not be preserved. If you need those, we recommend you use the Enhanced Dead Letter Queue.</p> <p>If you need messages dispatched to the DLQ topic to preserve order, you either need to use a DLQ topic with a single partition, or you need to use the Enhanced Dead Letter Queue implementation.</p>"}, {"location": "Dead-Letter-Queue/#manual-dlq-dispatch", "title": "Manual DLQ Dispatch", "text": "<p>When the Dead Letter Queue is enabled, Karafka will provide you with an additional method called <code>#dispatch_to_dlq</code> that you can use to transfer messages to the DLQ topic. You can use it if you encounter messages you do not want to deal with but do not want to raise an exception:</p> <pre><code>class OrdersStatesConsumer\n  def consume\n    messages.each do |message|\n      if EventContract.valid?(message.payload)\n        EventsImporter.import(message.payload)\n      else\n        # Skip on messages that are not valid without raising an exception\n        dispatch_to_dlq(message)\n      end\n\n      mark_as_consumed(message)\n    end\n  end\nend\n</code></pre>"}, {"location": "Dead-Letter-Queue/#monitoring-and-instrumentation", "title": "Monitoring and Instrumentation", "text": "<p>Each time a message is moved to the Dead Letter Queue topic, it will emit a <code>dead_letter_queue.dispatched</code> with a <code>message</code> key.</p> <p>You can use it to enrich your instrumentation and monitoring:</p> <pre><code>Karafka.monitor.subscribe('dead_letter_queue.dispatched') do |event|\n  message = event[:message]\n\n  topic = message.topic\n  partition = message.partition\n  offset = message.offset\n\n  puts \"Oh no! We gave up on this flunky message: #{topic}/#{partition}/#{offset}\"\nend\n</code></pre>"}, {"location": "Dead-Letter-Queue/#batch-processing-limitations", "title": "Batch Processing Limitations", "text": "<p>At the moment, DLQ does not have the ability to skip whole batches. For scenarios where the collective outcome of messages operations is causing errors, Karafka will skip one after another. This means that you may encounter \"flickering\", where seemingly valid messages are being moved to the DLQ before reaching the corrupted one.</p> <p>If skipping batches is something you would utilize, please get in touch with us so we can understand your use cases and possibly introduce this functionality.</p>"}, {"location": "Dead-Letter-Queue/#compacting-limitations", "title": "Compacting Limitations", "text": "<p>Karafka does not publish the <code>key</code> value for DLQ messages. This means that if you set your <code>log.cleanup.policy</code> to <code>compact</code>, newer messages will overwrite the older ones when the log compaction process kicks in.</p> <p>Karafka Pro sets the <code>key</code> value based on the errored message partition to ensure the same partition delivery for consecutive errors from the same original partition.</p> <p>We recommend either:</p> <ul> <li>Enhancing the DLQ messages with a proper <code>key</code> value using the Enhanced Dead Letter Queue custom details feature.</li> <li>Not using a <code>compact</code> policy and relying on <code>log.retention.ms</code> instead to make sure that the given DLQ topic does not grow beyond expectations.</li> <li>Enhancing the DLQ dispatched message by forking Karafka and making needed enhancements to the code.</li> </ul>"}, {"location": "Dead-Letter-Queue/#using-dead-letter-queue-with-a-multi-cluster-setup", "title": "Using Dead Letter Queue with a Multi-Cluster Setup", "text": "<p>When working with a DLQ pattern and using Karafka multi-cluster support, please remember that by default, all the messages dispatched to the DLQ topic will go to the main cluster as the <code>#producer</code> uses the default cluster settings.</p> <p>You can alter this by overriding the <code>#producer</code> consumer method and providing your cluster-specific producer instance.</p> <p>Do not create producer instances per consumer but one per cluster. Karafka producer is thread-safe and can operate from multiple consumers simultaneously.</p> <pre><code># In an initializer, before usage\nPRODUCERS_FOR_CLUSTERS = {\n  primary: Karafka.producer,\n  secondary: ::WaterDrop::Producer.new do |p_config|\n    p_config.kafka = {\n      'bootstrap.servers': 'localhost:9095',\n      'request.required.acks': 1\n    }\n  end\n}\n\nclass ClusterXConsumer\n  def consume\n    # logic + DLQ setup in routes\n  end\n\n  private\n\n  # Make this consumer always write all data to the secondary cluster\n  # by making this method return your desired producer and not the default\n  # producer\n  def producer\n    PRODUCERS_FOR_CLUSTERS.fetch(:secondary)\n  end\nend\n</code></pre> <p>You can read more about producing to multiple clusters here.</p>"}, {"location": "Dead-Letter-Queue/#dispatch-and-marking-warranties", "title": "Dispatch and Marking Warranties", "text": "<p>When using the Dead Letter Queue (DLQ) feature in Karafka, messages are handled with specific dispatch and marking behaviors critical for understanding how message failures are managed. By default, Karafka employs asynchronous dispatch and non-blocking marking as consumed. However, these can be configured to behave synchronously for stricter processing guarantees.</p> <p>For environments where message processing integrity is critical, you should configure dispatch and marking to operate synchronously. This ensures that each message is not only sent to the DLQ but also acknowledged by Kafka before proceeding and, similarly, that a message is confirmed as consumed before moving on.</p> <p>To configure synchronous dispatch, you can set the <code>dispatch_method</code> option to <code>:produce_sync</code>. This setting ensures that the producer waits for a response from Kafka, confirming that the message has been received and stored before it returns control to the application.</p> <p>Similarly, to configure blocking marking, you can set the <code>marking_method</code> to <code>:mark_as_consumed!</code>. This ensures that the message is marked as consumed in your application when Kafka confirms that it has been committed, reducing the risk of losing the message's consumption state.</p> <p>Here is an example configuration that uses synchronous dispatch and blocking marking for messages sent to the DLQ:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: 'dead_messages',\n        max_retries: 2,\n        dispatch_method: :produce_sync,\n        marking_method: :mark_as_consumed!\n      )\n    end\n  end\nend\n</code></pre> <p>Risks of Async DLQ Dispatch</p> <p>When configuring the Dead Letter Queue (DLQ) in Karafka with asynchronous dispatch (<code>dispatch_method: :produce_async</code>), messages are immediately moved to a background queue and considered dispatched as soon as the action is triggered. This can create a potential risk where the application assumes a message has been delivered to the DLQ when, in reality, it may still be pending dispatch or could fail. In this case:</p> <ul> <li> <p>Edge Case: If there's an error in the background dispatch (e.g., network failure or broker downtime), the application won't be aware immediately, and retry mechanisms might not handle the failed message correctly.</p> </li> <li> <p>Risk: This can lead to undetected message loss or delayed delivery to the DLQ, causing inconsistency in how failures are handled.</p> </li> </ul> <p>For critical systems where message integrity is essential, it's recommended to use synchronous dispatch (<code>dispatch_method: :produce_sync</code>), ensuring the DLQ message is successfully acknowledged by Kafka before continuing.</p>"}, {"location": "Dead-Letter-Queue/#dlq-topic-configuration-management", "title": "DLQ Topic Configuration Management", "text": "<p>The Dead Letter Queue (DLQ) topics in Karafka are Kafka topics like any other. Managing their configuration requires separate route definitions with specific configuration settings. Paying attention to these details is crucial as they allow you to fully control and customize the DLQ topics to suit your needs.</p> <p>To manage the configuration of the DLQ topic, you need to define a separate route for it, specifying the desired configurations, such as the number of partitions, replication factor, retention policies, and more.</p> <p>Here's an example of how to set up a DLQ topic with specific configurations:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: 'dead_messages',\n        max_retries: 2\n      )\n    end\n\n    # Separate route definition for the DLQ topic with specific configurations\n    topic :dead_messages do\n      # Indicate that we do not consume from this topic\n      active(false)\n      config(\n        partitions: 3,\n        replication_factor: 2,\n        'retention.ms': 604_800_000, # 7 days in milliseconds\n        'cleanup.policy': 'compact'\n      )\n    end\n  end\nend\n</code></pre> <p>This approach ensures that you can manage the DLQ topic configurations independently of the main topic, providing greater flexibility and control over how problematic messages are handled and stored.</p>"}, {"location": "Dead-Letter-Queue/#pro-enhanced-dead-letter-queue", "title": "Pro Enhanced Dead Letter Queue", "text": "<p>We highly recommend you check out the Enhanced Dead Letter Queue, especially if you:</p> <ul> <li>expect a higher quantity of messages being moved to the DLQ topic,</li> <li>need to preserve original topic, partition, and offset,</li> <li>need to preserve the ordering of messages,</li> <li>need an ability to write complex context and error-type aware strategies for error handling,</li> <li>want to have a DLQ topic with several partitions.</li> <li>want to alter <code>payload</code>, <code>headers</code>, <code>key</code> or any other attributes of the DLQ message.</li> <li>want to delay processing of data dispatched to the DLQ topic.</li> </ul>"}, {"location": "Dead-Letter-Queue/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Payment processing: In payment processing systems, DLQs can be used to capture failed payment transactions due to network issues, invalid payment information, or other issues. These transactions can be reviewed and processed later, ensuring no payment is lost.</p> </li> <li> <p>Email delivery: In email delivery systems, DLQs can be used to capture email messages that fail to deliver due to invalid email addresses, network issues, or other issues. These messages can be later reviewed and resent, ensuring that important emails are not lost.</p> </li> <li> <p>Order processing: In e-commerce systems, DLQs can be used to capture orders that fail to process due to system errors, payment failures, or other issues. These orders can be reviewed and processed later, ensuring no orders are lost, and customer satisfaction is maintained.</p> </li> <li> <p>Data pipeline processing: In data pipeline systems, DLQs can be used to capture data events that fail to process due to data schema issues, data quality issues, or other issues. These events can be later reviewed and processed, ensuring that important data is not lost and data integrity is maintained.</p> </li> <li> <p>Fraud detection: In financial systems, DLQs can be used to capture suspicious transactions that fail to process due to system errors, network issues, or other issues.</p> </li> <li> <p>Gaming platforms: In gaming systems, DLQs can be used to capture game events that fail to process due to connectivity issues, data quality problems, or other issues.</p> </li> </ul> <p>The Karafka Dead Letter Queue is worth using because it provides a way to handle messages that cannot be processed for any reason without losing data. The DLQ allows the failed messages to be reviewed and processed later, reducing the risk of data loss and providing greater reliability and resilience to the system. The DLQ in Karafka is easy to set up and can be configured to handle different scenarios, including retry mechanisms, error handling, and version incompatibilities. This makes it an essential feature for businesses that want to ensure the reliability of their messaging system and avoid data loss in case of errors.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Debugging/", "title": "Debugging", "text": "<p>This document was created in response to recurring community questions and confusion around Karafka's double-processing or missed message behaviors. While it's understandable given the complexity of message-driven systems, it's important to clarify up front:</p> <ul> <li> <p>There are no confirmed bugs in the most recent version of Karafka (at the time of writing) that cause unintended double-processing or skipped messages. If you observe such behavior and can reliably reproduce it, please open an issue with a minimal test case.</p> </li> <li> <p>Karafka includes a comprehensive integration test suite, which features a dedicated assertion layer ensuring:</p> <ul> <li>Messages are not fetched more than once.</li> <li>A single consumer instance never processes messages from different topics or partitions.</li> <li>Kafka protocol semantics (like ordering and delivery guarantees) are respected.</li> </ul> </li> </ul> <p>Despite this, users sometimes report what they believe is double-processing or missing messages. In every confirmed case to date except one, the root cause has been either:</p> <ul> <li>Misunderstanding of consumption semantics.</li> <li>Improper offset management.</li> <li>Non-thread-safe or unsafe consumer code.</li> <li>Non-idempotent producer with network issues.</li> <li>Misconfigured deployments (e.g., multiple consumer groups unintentionally).</li> </ul> <p>This guide aims to help you systematically identify, debug, and resolve these issues.</p>"}, {"location": "Debugging/#overview", "title": "Overview", "text": "<p>Karafka defaults to at-least-once delivery semantics, which means that under normal operating conditions and proper usage, each message will be delivered and processed once. This assumes no crashes, correct offset handling, and thread-safe consumer logic.</p> <p>Karafka also supports other Kafka delivery semantics:</p> <ul> <li>Exactly-once semantics (EOS) \u2014 via Kafka transactions, which ensure atomicity between offset commits and message production.</li> <li>At-most-once semantics \u2014 by committing offsets before processing. This guarantees no duplication but may lead to message loss if a crash occurs during processing.</li> </ul> <p>When used correctly and under healthy conditions (no ungraceful termination), Karafka, with at least one semantics, will process each message once and only once, even without transactions.</p>"}, {"location": "Debugging/#architecture-and-lifecycle", "title": "Architecture And Lifecycle", "text": "<p>Introductory Overview</p> <p>This section provides a high-level introduction to Karafka's architecture and lifecycle. For more comprehensive details on Karafka's internal mechanisms, configuration options, and operational best practices, please refer to other sections of this documentation where these concepts are explored in greater depth.</p> <p>Karafka is a multi-threaded Kafka consumer framework for Ruby/Rails applications. Understanding its internals will help pinpoint where duplicates arise:</p> <ul> <li> <p>Consumer Group and Partitions: Karafka consumers typically run as a separate process (via <code>karafka server</code>). They join a Kafka consumer group, dividing topic partitions among instances. If you run multiple Karafka processes (for scaling or high availability) under the same group ID, Kafka will assign each partition to only one process at a time (preventing duplicates). If processes use different group IDs, each group will receive all messages (causing intentional duplicates). Always ensure all instances use the same group for load-balanced consumption.</p> </li> <li> <p>Threads and Concurrency: Within a Karafka process, multiple Ruby threads may be spawned to fetch and consume messages in parallel (controlled amongst others by <code>Karafka::App.config.concurrency</code>). By default, Karafka uses threads to process different partitions and topics concurrently. Each partition's messages are processed in order, and Karafka preserves partition ordering by not processing multiple messages from the same partition at the same time (unless using advanced features like Virtual Partitions). This means concurrency &gt; 1 mainly improves throughput when consuming multiple partitions or topics simultaneously. All code must be thread-safe, as Karafka shares the process among threads much like a Puma or Sidekiq. Non-thread-safe code can cause race conditions and unpredictable behavior (including potential double processing or missed acknowledgments).</p> </li> <li> <p>Message Fetching and Batching: Karafka fetches messages from Kafka (often in batches) and hands them to your consumer class's <code>#consume</code> method. By default, Karafka may buffer a certain number of messages (controlled by settings like <code>max_messages</code>). Batch processing means your <code>#consume</code> method will receive an array of messages you typically iterate over. Karafka's default is to fetch as many as possible up to <code>max_messages</code> or within <code>max_wait_time</code> to maximize throughput.</p> </li> <li> <p>Offset Acknowledgement (Commit): After processing messages, the consumer must commit the offsets of those messages back to Kafka's broker to mark them as processed (so the group doesn't redeliver them on the next session). Karafka auto-commits offsets by default at regular intervals (every 5 seconds via Kafka's <code>auto.commit.interval.ms</code>) and at graceful shutdown or right before a rebalance occurs. Essentially, Karafka will periodically checkpoint how far it has read. If the process crashes or a rebalance happens before an offset is committed, those messages may be redelivered to this or another consumer - hence, duplicates. Karafka's design mitigates this by also committing to orderly rebalances and shutdowns, but unexpected crashes or forced terminations can still result in uncommitted messages being reprocessed.</p> </li> <li> <p>Exactly-Once Option: For critical workflows that absolutely cannot tolerate duplicates, Karafka supports Kafka transactions (Exactly-Once Semantics). This ties the offset commit to producing a result in one atomic operation. However, using transactions is more complex and beyond the scope of most consumer-only scenarios.</p> </li> </ul> <p>Understanding this lifecycle, we can see that double-processing usually indicates something went wrong between message receipt and offset commit \u2013 often due to errors, timeouts, or miscoordination in this flow.</p>"}, {"location": "Debugging/#common-causes-of-double-message-processing", "title": "Common Causes of Double Message Processing", "text": "<p>Several issues (usually user or configuration errors) can interrupt the normal flow and lead to a message being processed twice. Below, we detail each cause and how it arises, using real-world examples.</p>"}, {"location": "Debugging/#improper-offset-committing-or-acknowledgement", "title": "Improper Offset Committing or Acknowledgement", "text": "<p>If offsets aren\u2019t committed at the correct time or in the proper way, Kafka may think messages haven\u2019t been processed and resend them.</p>"}, {"location": "Debugging/#misusing-manual-offset-management", "title": "Misusing Manual Offset Management", "text": "<p>Karafka allows turning off auto-offset commits (<code>manual_offset_management(true)</code> per topic) so you can call <code>mark_as_consumed</code> or <code>mark_as_consumed!</code> in your code at precise points. This is powerful but dangerous if forgotten.</p> <p>Example: You disable auto commits to implement a custom flow but forget to call <code>mark_as_consumed</code> after processing. The consumer will never commit those messages. All those messages will be delivered again on the next restart, causing duplicates. Always mark messages as consumed (or re-enable auto commits) when using manual mode. If you use <code>mark_as_consumed</code> (non-bang), remember it\u2019s asynchronous (just flags for later commit); using <code>mark_as_consumed!</code> commits immediately but at a performance cost.</p> <pre><code># This example illustrates incorrect setup\nclass KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders do\n      consumer OrdersConsumer\n      # Auto marking disabled\n      manual_offset_management(true)\n    end\n  end\nend\n\nclass OrdersConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      process(message)\n      # mark_as_consumed is missing \u2014 offsets will never be committed!\n    end\n  end\nend\n</code></pre>"}, {"location": "Debugging/#committing-offsets-too-early-or-out-of-sync", "title": "Committing Offsets Too Early or Out of Sync", "text": "<p>The offset should be committed after a message (or batch) is fully processed. If one mistakenly commits an offset before processing (or commits a higher offset while some messages are still in progress), and then the app crashes during processing, those in-flight messages won\u2019t be reprocessed (resulting in lost messages, not duplicates). Conversely, committing too late (or not at all) leads to replays.</p> <p>Example: A developer manually calls <code>mark_as_consumed</code> before processing, causing some messages to be marked as consumed before processing potentially. This can confuse which messages are processed. The rule is to commit only after successful processing, never before. If unsure, rely on Karafka\u2019s automatic commits, which are designed to happen after batch processing is complete.</p> <pre><code># This example illustrates incorrect setup\nclass KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders do\n      consumer OrdersConsumer\n      manual_offset_management(true)\n    end\n  end\nend\n\nclass OrdersConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      # This marks the message as consumed *before* it's processed.\n      mark_as_consumed(message)\n\n      # If the process crashes here, the message is lost - offset was already committed\n      process(message)\n    end\n  end\nend\n</code></pre>"}, {"location": "Debugging/#unhandled-exceptions-in-consumer-code", "title": "Unhandled Exceptions in Consumer Code", "text": "<p>Runtime errors in your consumer logic are a very common cause of double-processing. Karafka has built-in retry/backoff behavior: if your consume method raises an exception, Karafka will not commit the offsets for that batch (since it didn't complete successfully) and will retry the message(s) after a pause\u200b from the last committed offset. This is by design: it prevents data loss but means the failed message (and potentially others in the same batch) will be processed again.</p>"}, {"location": "Debugging/#exceptions-on-individual-messages-with-automatic-marking", "title": "Exceptions on Individual Messages With Automatic Marking", "text": "<p>If you process messages in a loop without per-message marking and one message triggers an error, Karafka treats the whole batch as failed by default.</p> <p>Example: Your consumer does <code>messages.each { |m| handle(m) }</code> and <code>#handle</code> throws an error on the 3rd message. The first two messages were processed, but their offsets haven't been committed yet (since Karafka commits them after the batch). Karafka catches the error, logs it, and will retry from the 1st message's offset on the next attempt. Result: The first two messages will be delivered again along with the third, causing duplicates for those two. To mitigate this, make your processing robust per message. You can rescue exceptions around the single message to ensure the batch continues for others, or use <code>#mark_as_consumed</code> as you go to commit offsets for messages that succeeded before the error. Karafka Pro's Virtual Partitions feature even handles this scenario by skipping already consumed messages on retry to avoid duplicates.</p> <pre><code># This example illustrates incorrect setup\nclass OrdersConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      # will crash on e.g. 3rd message\n      raise 'Boom' if message.payload == 'fail'  # simulate a broken message\n    end\n  end\nend\n</code></pre> <p>Use Karafka Monitoring Hooks to Debug Consumer Failures</p> <p>Use Karafka's monitoring hooks to debug these scenarios. Subscribe to the <code>error.occurred</code> event to get detailed info whenever a consumer error happens.</p>"}, {"location": "Debugging/#non-thread-safe-code-or-shared-resource-issues", "title": "Non-Thread-Safe Code or Shared Resource Issues", "text": "<p>Because Karafka runs concurrently, any code that isn\u2019t safe under multi-threading can inadvertently lead to double-processing symptoms. While Karafka itself ensures a single message is handled by one thread at a time, user code might introduce duplication in a few ways.</p>"}, {"location": "Debugging/#shared-global-state", "title": "Shared Global State", "text": "<p>Multiple threads can interfere if you use class variables, singletons, or another globally shared mutable state to track processing.</p> <p>Example: Suppose you have a global hash to ensure you don't process the same item twice or a global counter for deduplication. If two threads (processing different partitions) access it, they might see stale or conflicting data. One thread might reset or change a flag that causes the other thread to re-process something. Always protect shared state with mutexes or, better, avoid it. Use local variables or consumer instance local storage if needed, and remember each Karafka consumer instance is tied to a partition and persists for that partition's lifetime so you can store state in instance variables safely per partition.</p> <pre><code># This example illustrates incorrect setup\n# Incorrect: Using Global State Across Threads\nclass OrdersConsumer &lt; ApplicationConsumer\n  # Global shared state across all threads and partitions\n  @@processed_orders = Set.new\n\n  def consume\n    messages.each do |message|\n      order_id = message.payload['order_id']\n      unless @@processed_orders.include?(order_id)\n        process(message)\n        @@processed_orders &lt;&lt; order_id\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Debugging/#external-services-not-thread-safe", "title": "External Services not Thread-Safe", "text": "<p>Ensure libraries you call (HTTP clients, database drivers, etc.) are thread-safe or use separate connections per thread. For instance, Rails ActiveRecord is thread-safe if you use its connection pooling properly. A gem that is not thread-safe might mishandle requests when called in parallel. This could manifest as duplicate actions. For example, a non-thread-safe cache library might erroneously replay a write operation from two threads.</p>"}, {"location": "Debugging/#manual-thread-management-in-consumer", "title": "Manual Thread Management in Consumer", "text": "<p>Sometimes, users try to spawn their threads within consume to parallelize work. This is not recommended - Karafka already handles parallelism. If you do this, be very careful with offset commits. For example, if you spawn a background thread to process a message and immediately mark the message as consumed in the main thread, the background thread might still work when Karafka commits and moves on. If that thread raises an error, you've already acknowledged the message, so Karafka won't retry it \u2013 you just lost it (not a duplicate, but data loss). Conversely, suppose you delay offset commit until threads join. In that case, you might not commit in time, causing Kafka to redeliver messages processed successfully by threads (duplicate processing). In short, avoid inventing your threading on top of Karafka's.</p> <pre><code># This example illustrates incorrect setup\nclass OrdersConsumer &lt; ApplicationConsumer\n  self.manual_offset_management = true\n\n  def consume\n    messages.each do |message|\n      Thread.new do\n        process(message)  # background thread still running...\n      end\n\n      mark_as_consumed(message)  # main thread marks as consumed immediately\n    end\n  end\nend\n</code></pre>"}, {"location": "Debugging/#threadworker-mismanagement-and-concurrency-settings", "title": "Thread/Worker Mismanagement and Concurrency Settings", "text": "<p>This category is related to thread safety but involves how you configure and deploy Karafka.</p>"}, {"location": "Debugging/#karafka-swarm-mode-multi-process", "title": "Karafka Swarm Mode (Multi-Process)", "text": "<p>Karafka supports forking multiple worker processes (similar to Puma workers) to overcome MRI GIL limits\u200b. If you use swarm mode, ensure each forked process still has a unique consumer group member identity. Karafka handles this under the hood, but if you manually run multiple Karafka processes (e.g. via a Procfile or multiple containers), make sure they share the same <code>group.id</code> in config. A mistake here is running two Karafka processes with the same topics but different group names, which means both processes will independently consume all messages (duplicating everything).</p> <p>The correct approach to scaling consumers is to run multiple processes all configured as one group (or use Karafka's built-in swarm mode). In Rails deployments, it's common to have an independent Karafka process for each app instance (all using the same group), which is fine.</p> <pre><code># This example illustrates incorrect setup\nclass KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders do\n      # different group name for each process\n      # this will start processing from beginning each time new\n      # karafka process starts\n      consumer_group \"app_group_#{Process.pid}\" do\n        topic :orders do \n          consumer OrdersConsumer\n        end\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Debugging/#multiple-karafka-apps-on-same-topics", "title": "Multiple Karafka Apps on Same Topics", "text": "<p>Another scenario is that you might have two different Karafka apps (maybe two services) subscribing to the same Kafka topic. If they are meant to handle the same data (like two separate consumers for different purposes), that's not an error \u2013 it's expected duplicates (each group processes independently). But you might inadvertently double-consume if they were not supposed to overlap (e.g., a copy-paste of the app running by mistake). Double-check which services consume which topics, and use distinct group IDs only when you intend multiple independent consumptions.</p> <p>Case Study \u2013 Competing Consumers: Imagine running Karafka in Kubernetes with an HPA (Horizontal Pod Autoscaler). You set it to scale up to 5 replicas on high load. However, you accidentally left the consumer group name as the default (which might include a random component or environment-specific name). When new pods start, they form their group instead of joining the existing one because the <code>group.id</code> was misconfigured per pod. Now, all pods consume the same topic independently \u2013 leading to each message being processed 5 times (once per pod). The fix: define a consistent <code>consumer_group</code> name in your Karafka routing config so all pods join the same group. After that, messages will properly partition among pods with no duplicates.</p> <pre><code># This example illustrates incorrect setup\n# Both apps use the same default consumer group (app) and subscribe\n# to the same topic.\n\n# Application A\nclass KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders do\n      consumer OrdersConsumer\n    end\n  end\nend\n\n# Application B\nclass KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders do\n      consumer NotificationsConsumer\n    end\n  end\nend\n</code></pre>"}, {"location": "Debugging/#kafka-rebalancing-side-effects-slow-consumers-timeouts", "title": "Kafka Rebalancing Side Effects (Slow Consumers, Timeouts)", "text": "<p>Kafka's consumer group rebalancing can cause message reprocessing if not managed correctly. Rebalancing happens whenever consumers join or leave the group (e.g. scaling up/down or a crash) or occasionally due to coordinator decisions. During a rebalance, partitions may move between consumers. Karafka tries to handle this gracefully.</p>"}, {"location": "Debugging/#graceful-rebalances", "title": "Graceful Rebalances", "text": "<p>Karafka will, whenever possible, complete the processing of any in-flight batch and commit offsets before relinquishing a partition on rebalance. In an ideal case, when you scale up or down, no message is left uncommitted during the transition, so the new consumer starts at the right spot (no duplicates).</p>"}, {"location": "Debugging/#revocation-mid-processing", "title": "Revocation Mid-Processing", "text": "<p>If a rebalance occurs while a consumer is still processing a batch (maybe a long-running batch), Karafka provides a <code>#revoked?</code> method so your code can detect it and stop. If you ignore it, one of two things might happen:</p> <ul> <li>If Karafka waits, you finish the batch and commit, which is fine (no duplicate).</li> <li>If Kafka forcibly revokes (in case of a timeout or crash), the partition ownership is lost mid-processing. Any messages processed but not committed by the time of revocation will be reassigned and reprocessed by another consumer, causing duplicates.</li> </ul>"}, {"location": "Debugging/#session-timeouts-and-max-poll-interval", "title": "Session Timeouts and Max Poll Interval", "text": "<p>The Kafka broker uses a <code>session.timeout.ms</code> (and <code>max.poll.interval.ms</code> for polling heartbeat) to decide if a consumer is dead or stuck. Suppose your consumer takes longer than this timeout to process a batch without polling Kafka. In that case, the broker assumes it's down and will trigger a rebalance, assigning its partitions to another consumer. Karafka uses an internal heartbeat thread (via librdkafka) to keep the session alive during long processing. However, if the processing exceeds <code>max.poll.interval.ms</code>, Kafka will still consider it failed. If your processing logic takes a lot of time, consider looking into Karafka's Long-Running Jobs feature.</p> <p>For example, if <code>max.poll.interval.ms</code> is 300 seconds (default for Kafka clients) and your consumer takes 600 seconds to handle a huge batch or a slow operation, Kafka may kick it out. Then, another consumer (or a newly started instance) will take over that partition and re-read from the last committed offset (which was before the long batch). Now, those messages will be processed again on the new consumer. This looks like a mysterious duplicate: two processes handled the same messages. It's one message, two different consumers, due to a timeout.</p>"}, {"location": "Debugging/#slow-consumer-scenario", "title": "Slow Consumer Scenario", "text": "<p>A real-world case was reported where two Karafka pods were each getting all messages. The logs showed <code>Kafka::UnknownMemberId</code> errors and frequent rebalances. The cause was a consumer that was too slow (processing took longer than the session timeout). Kafka kept ejecting it from the group and redistributing its partition to the other process. That process would also eventually get ejected as it hit the timeout, causing a ping-pong of partition ownership. Each time, some messages were reprocessed on the other side. The solution was to tune the consumer settings: either turn off large batch processing or increase the <code>max.poll.interval.ms</code> so that the consumer had enough time to finish without being considered dead.</p> <p>In Karafka, you can adjust <code>max_wait_time</code> and <code>max_messages</code> to fetch smaller batches (process messages more frequently in smaller chunks). Essentially, if you have long processing tasks:</p> <ul> <li>Make sure Kafka's timeouts are higher than the worst-case processing time.</li> <li>Use strategies to break the work into smaller pieces.</li> <li>Consider using the Long-Running Jobs feature.</li> </ul>"}, {"location": "Debugging/#memory-usage-memory-leaks", "title": "Memory Usage / Memory Leaks", "text": "<p>As of now, Karafka components have no known memory leaks. We take each report extremely seriously. Before reporting a potential memory leak, please follow these steps:</p> <ol> <li> <p>Upgrade to the Latest Version: Ensure you use the most recent versions of all Karafka ecosystem gems. Issues might have already been fixed in newer releases.</p> </li> <li> <p>Check for External Dependencies: Limit the use of non-default gems to eliminate issues that might arise from other libraries.</p> </li> <li> <p>Simplify Concurrency: Set the <code>concurrency</code> value to <code>1</code> to simplify the processing flow and identify if the issue is related to multi-threading.</p> </li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.concurrency = 1\n  end\nend\n</code></pre> <ol> <li>Use a Single Topic and Partition: Test with a single topic and partition to reduce complexity and isolate the issue.</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders do\n      consumer OrdersConsumer\n    end\n  end\nend\n</code></pre> <ol> <li> <p>Monitor Memory Usage: Use tools like <code>memory_profiler</code> or <code>derailed_benchmarks</code> to monitor and profile memory usage in your Karafka application.</p> </li> <li> <p>Check Configuration: Verify your Karafka configuration for any unusual settings that might cause excessive memory usage.</p> </li> <li> <p>Review Logs: Check your logs for any warnings or errors that might indicate a problem with memory management.</p> </li> <li> <p>Isolate the Problem: Reproduce the issue in a controlled environment. Use minimal configuration and isolate the components one by one.</p> </li> <li> <p>Garbage Collection: Force garbage collection and monitor if the memory usage drops. This can help determine if the issue is with Ruby's garbage collector.</p> </li> </ol> <pre><code>GC.start\n</code></pre> <ol> <li>Collect Diagnostic Data: Gather detailed diagnostic data, including heap dumps and backtraces. This information will be crucial for debugging the issue.</li> </ol> <pre><code>Process.kill('TTIN', Process.pid)\n</code></pre>"}, {"location": "Debugging/#understanding-memory-usage-and-leaks", "title": "Understanding Memory Usage and Leaks", "text": "<p>Karafka is designed to be efficient with memory, but many factors can contribute to increased memory usage or leaks:</p> <ol> <li> <p>Memory Bloat: This occurs when your process's memory size keeps increasing over time, even if it is not actively processing a higher load. Common causes include:    - Ruby gem issues or memory fragmentation.    - Unreleased resources or objects being held in memory longer than necessary.</p> </li> <li> <p>Garbage Collection: Ruby uses a garbage collector (GC) to manage memory. Sometimes, tweaking GC settings can help manage memory usage more effectively. You can experiment with environment variables like <code>RUBY_GC_HEAP_GROWTH_FACTOR</code>, <code>RUBY_GC_MALLOC_LIMIT</code>, and <code>RUBY_GC_OLDMALLOC_LIMIT</code> to optimize memory use.</p> </li> <li> <p>External Dependencies: Libraries and gems that your application depends on might have their own memory issues. Regularly update and monitor all dependencies.</p> </li> <li> <p>Profiling Tools: Use memory profiling tools to identify potential leaks or bloat. Tools like <code>memory_profiler</code>, <code>derailed_benchmarks</code>, and <code>stackprof</code> can help pinpoint memory issues in your application.</p> </li> <li> <p>Heap Dumps: Collecting and analyzing heap dumps can provide insights into memory allocation and help identify objects that are using excessive memory.</p> </li> <li> <p>Code Review: Regularly review your code for inefficient memory usage patterns, such as large data structures or extensive caching without expiration policies.</p> </li> </ol> <p>If you have followed these steps and still believe there is a memory leak in Karafka, please report it through one of the following channels:</p> <ul> <li>The Karafka official Slack channel</li> <li>Open a GitHub issue</li> </ul>"}, {"location": "Debugging/#recommendations-for-managing-memory-in-karafka", "title": "Recommendations for Managing Memory in Karafka", "text": "<ol> <li> <p>Set <code>MALLOC_ARENA_MAX=2</code>: This environment variable is the closest thing to a silver bullet if you are using Linux/glibc in production. Setting <code>MALLOC_ARENA_MAX=2</code> limits the number of memory arenas, which can significantly reduce memory fragmentation and overall memory usage.</p> <p><code>sh export MALLOC_ARENA_MAX=2</code></p> <p>On Heroku, you can set this configuration by running:</p> <p><code>sh heroku config:set MALLOC_ARENA_MAX=2</code></p> <p>By default, glibc can create multiple memory arenas to improve concurrency for multithreaded applications. However, this can lead to high memory usage due to fragmentation. Limiting the number of arenas helps to manage memory more efficiently.</p> </li> <li> <p>Switch to <code>jemalloc</code>: <code>jemalloc</code> is a memory allocator that works well with Ruby, particularly Ruby 3.0 and later. It is designed to reduce fragmentation and improve memory management, leading to more stable memory usage patterns.</p> <p>To install <code>jemalloc</code>, follow these steps:</p> <p><code>sh sudo apt-get install libjemalloc-dev</code></p> <p>Then, compile your Ruby with <code>jemalloc</code> support.</p> </li> </ol>"}, {"location": "Debugging/#systematic-debugging-of-processing-issues-in-karafka", "title": "Systematic Debugging of Processing Issues in Karafka", "text": "<p>When experiencing issues with Karafka, a systematic approach to debugging can save time and help pinpoint the root cause. This guide provides a structured methodology for identifying and resolving common Karafka processing problems.</p>"}, {"location": "Debugging/#confirm-the-symptom", "title": "Confirm the Symptom", "text": "<p>First, verify that the same message (the same Kafka partition and offset or the same unique key in the payload) is processed more than once. Add logging in your consumer to print the message's topic/partition/offset or any unique ID. This will help distinguish true Kafka-level duplicates from logical issues (like accidentally performing an action twice in your code).</p>"}, {"location": "Debugging/#ensure-youre-using-current-versions", "title": "Ensure You're Using Current Versions", "text": "<p>Before diving into debugging always verify you're using the most recent versions of all Karafka ecosystem gems. The issue you're experiencing may have already been fixed in a newer version.</p>"}, {"location": "Debugging/#create-a-minimal-reproduction-environment", "title": "Create a Minimal Reproduction Environment", "text": "<p>To isolate and identify issues:</p> <ul> <li>Use as few non-default gems as possible to eliminate interference from other libraries</li> <li>Simplify your processing flow by reducing <code>concurrency</code>:</li> </ul> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.concurrency = 1\n  end\nend\n</code></pre> <ul> <li>Reduce topics and partitions to minimize variables:</li> </ul> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other configuration\n  end\n\n  routes.draw do\n    # Disable other topics for debug...\n    # topic :shippings do\n    #   consumer ShippingsConsumer\n    # end\n\n    topic :orders do\n      consumer OrdersConsumer\n    end\n  end\nend\n</code></pre>"}, {"location": "Debugging/#enable-enhanced-logging", "title": "Enable Enhanced Logging", "text": "<p>Karafka uses the info log level by default. To get more detailed information:</p> <ul> <li>Set your logger to debug level:</li> </ul> <pre><code>Karafka::App.logger.level = Logger::DEBUG\n</code></pre> <ul> <li>Enable librdkafka debug flags for detailed internal information:</li> </ul> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap. servers': '127.0.0.1:9092',\n      # other settings...\n      debug: 'all'\n    }\n  end\nend\n</code></pre> <p>Available debug flags include: <code>generic</code>, <code>broker</code>, <code>topic</code>, <code>metadata</code>, <code>feature</code>, <code>queue</code>, <code>msg</code>, <code>protocol</code>, <code>cgrp</code>, <code>security</code>, <code>fetch</code>, <code>interceptor</code>, <code>plugin</code>, <code>consumer</code>, <code>admin</code>, <code>eos</code>, <code>mock</code>, <code>assignor</code>, <code>conf</code>, <code>all</code></p> <p>Debug Mode Usage Caution</p> <p>Using debug mode extensively, especially in production, may impact performance and generate large log files. Ensure you revert to regular settings once your issue is resolved.</p> <p>When debug mode is configured correctly, Karafka will generate detailed logs to help you troubleshoot issues. These logs are printed whether or not Karafka can connect to the Kafka cluster, as part of them are generated during the pre-connection establishment phase.</p> <p>Below is an example of what these debug logs might look like:</p> <pre><code>rdkafka: [thrd:app]: 127.0.0.1:9092/bootstrap: Enabled low-latency ops queue wake-ups\nrdkafka: [thrd:app]: 127.0.0.1:9092/bootstrap: Added new broker with NodeId -1\nrdkafka: [thrd:app]: 127.0.0.1:9092/bootstrap: Selected for cluster connection: bootstrap servers added (broker has 0 connection attempt(s))\nrdkafka: [thrd::0/internal]: :0/internal: Enter main broker thread\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Enter main broker thread\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Received CONNECT op\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Broker changed state INIT -&gt; TRY_CONNECT\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: Broadcasting state change\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: broker in state TRY_CONNECT connecting\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Broker changed state TRY_CONNECT -&gt; CONNECT\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: Broadcasting state change\nrdkafka: [thrd:app]: librdkafka v2.3.0 (0x20300ff) example_app#producer-1 initialized\nrdkafka: [thrd:app]: Client configuration:\nrdkafka: [thrd:app]:   client.id = example_app\nrdkafka: [thrd:app]:   client.software.version = 2.3.0\nrdkafka: [thrd:app]:   metadata.broker.list = 127.0.0.1:9092\nrdkafka: [thrd:app]:   topic.metadata.refresh.interval.ms = 5000\nrdkafka: [thrd:app]:   debug = generic,broker,topic,metadata,feature,queue,msg,protocol,cgrp,security,fetch,interceptor,plugin,consumer,admin,eos,mock,assignor,conf,all\nrdkafka: [thrd:app]:   statistics.interval.ms = 0\nrdkafka: [thrd:app]:   error_cb = 0x7fb0111b0000\nrdkafka: [thrd:app]:   stats_cb = 0x7fb011791000\nrdkafka: [thrd:app]:   log_cb = 0x7fb011792000\nrdkafka: [thrd:app]:   log.queue = true\nrdkafka: [thrd:app]:   background_event_cb = 0x7fb0111b2000\nrdkafka: [thrd:app]:   opaque = 0x2b34\nrdkafka: [thrd:app]:   api.version.request = true\nrdkafka: [thrd:app]:   allow.auto.create.topics = false\nrdkafka: [thrd:app]:   oauthbearer_token_refresh_cb = 0x7fb011220000\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Connecting to ipv4#127.0.0.1:9092 (plaintext) with socket 11\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Connected to ipv4#127.0.0.1:9092\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Connected (#1)\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Broker changed state CONNECT -&gt; APIVERSION_QUERY\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: Broadcasting state change\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Sent ApiVersionRequest (v3, 44 bytes @ 0, CorrId 1)\nrdkafka: [thrd:app]: Not selecting any broker for cluster connection: still suppressed for 48ms: application metadata request\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Received ApiVersionResponse (v3, 453 bytes, CorrId 1, rtt 6.05ms)\nrdkafka: [thrd:127.0.0.1:9092/bootstrap]: 127.0.0.1:9092/bootstrap: Broker API support:\n</code></pre>"}, {"location": "Debugging/#disable-custom-instrumentation-and-monitors", "title": "Disable Custom Instrumentation and Monitors", "text": "<p>While useful for observability, custom instrumentation, and monitors can inadvertently affect message processing when implemented incorrectly. Historically, some cases of message duplication have been traced to custom monitors (particularly those added for distributed tracing) that interfered with Karafka's internal operations.</p> <p>If you're experiencing duplicate processing, temporarily disabling all custom monitors can help isolate whether your instrumentation contributes to the issue. Once confirmed, carefully review your monitor implementations to ensure they operate as passive observers without side effects on Karafka's core processing logic.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Disable any custom monitors and instrumentation\n    # Use defaults when investigating\n\n    # config.monitor = MonitorWithOpenTelemetry.new\n  end\nend\n</code></pre>"}, {"location": "Debugging/#capture-thread-states-for-hanging-processes", "title": "Capture Thread States for Hanging Processes", "text": "<p>If Karafka seems frozen or is not progressing:</p> <ul> <li> <p>Send <code>SIGTTIN</code> to the Karafka process. It will print backtraces of all threads to stdout/log.</p> </li> <li> <p>This shows you what each thread is doing \u2014 e.g., stuck waiting on IO, DB, mutex, or sleeping. Works only when LoggerListener is enabled in your monitor setup (enabled by default).</p> </li> </ul> <pre><code>kill -TTIN &lt;karafka_pid&gt;\n</code></pre> <p>Useful to detect:</p> <ul> <li>Deadlocks</li> <li>Long blocking operations</li> <li>Consumers stuck on external services</li> </ul>"}, {"location": "Debugging/#check-for-errors-and-retries", "title": "Check for Errors and Retries", "text": "<p>Review logs and monitor hooks to spot retry loops or failures:</p> <ul> <li>Check for <code>consumer.consume.error</code> events - these will show unhandled exceptions during consume.</li> <li>Look for repeated processing of the same offset - this is often a sign of crash or retry behavior.</li> <li>The presence of <code>retrying?</code> in logs or monitor events</li> </ul> <pre><code>def consume\n  messages.each do |message|\n    logger.info(\"retry attempt: #{message.attempt}\") if message.retrying?\n  end\nend\n</code></pre>"}, {"location": "Debugging/#simplify-the-environment", "title": "Simplify the Environment", "text": "<p>To isolate the issue, try:</p> <ul> <li>Single-threaded Mode: Set <code>concurrency = 1</code></li> <li>Single Instance: Run only one Karafka process</li> <li>Small Batch Size: Reduce <code>max_messages</code></li> <li>Test with a Controlled Topic: Create a test topic with a single partition</li> </ul>"}, {"location": "Debugging/#inspect-kafka-logs-and-metrics", "title": "Inspect Kafka Logs and Metrics", "text": "<p>Look for:</p> <ul> <li>Consumer group events in broker logs</li> <li>\"Member xyz was removed from the group due to timeout\" messages</li> <li>Rebalance metrics</li> </ul>"}, {"location": "Debugging/#consider-pro-support", "title": "Consider Pro Support", "text": "<p>If after following all the steps above, you're still unable to isolate or resolve the issue, or if you're dealing with a production-critical incident and need deeper Kafka/Ruby insight, consider reaching out for Pro assistance.</p> <p>Karafka Pro offers:</p> <ul> <li>Private Slack channel access</li> <li>Direct help from the author</li> <li>Assistance with debugging, architecture, rebalancing, upgrade strategies, and more</li> </ul> <p>You can contact us at <code>contact@karafka.io</code> or via the private Slack channel if you're a Pro customer.</p> <p>Don't hesitate to get in touch if you still need clarification after following this guide. We're happy to help.</p> <p>Last modified: 2025-04-07 14:56:33</p>"}, {"location": "Declarative-Topics/", "title": "Declarative Topics", "text": "<p>Karafka allows you to manage your topics in three ways:</p> <ul> <li>Using the built-in Declarative Topics routing + CLI functionality (recommended)</li> <li>Directly via the Admin API</li> <li>From the Pro Web UI via the Topics Management feature</li> </ul> <p>Karafka considers your topics setup (retention, partitions, etc.) as part of your business logic. You can describe them in the routing and make Karafka ensure their consistency across all the environments using the appropriate CLI commands. Thanks to that, you can make sure that everything is described as code.</p> <p>Default Cluster Limitation</p> <p>All admin operations in Karafka always run on the default cluster. To run admin operations on multiple clusters, you need separate Karafka boot files for each cluster. For more details, visit the Admin Multi-Cluster Setup section.</p> <p>Keeping Kafka topics configuration as code has several benefits:</p> <ul> <li> <p>Version Control: By keeping the topic settings as code, you can track changes over time and easily understand historical changes related to the topics. This is particularly important in a production environment where changes need to be carefully managed.</p> </li> <li> <p>Reproducibility: When you define Kafka topics settings as code, you can easily recreate the same topic with the same settings in multiple environments. This ensures that your development, staging, and production environments are consistent, which can help prevent unexpected issues and bugs.</p> </li> <li> <p>Automation: If you use code to define Kafka topics settings, you can automate the process of creating and updating topics. This can save time and reduce the risk of human error.</p> </li> <li> <p>Collaboration: When you keep Kafka topics settings as code, you can collaborate with other developers on the configuration. You can use tools like Git to manage changes and merge different configurations.</p> </li> <li> <p>Documentation: Code is self-documenting, meaning anyone can look at the configuration and understand what is happening. This can make it easier for new team members to get up to speed and help troubleshoot issues.</p> </li> </ul> <p>Overall, keeping Kafka topics settings as code can make it easier to manage, automate, and collaborate on Kafka topics, saving time and reducing the risk of errors.</p> <p>Karafka routing allows you to do that via per topic <code>#config</code> method that you can use to describe your Kafka topic configuration.</p> <p>This configuration is used by a set of Karafka CLI commands you can invoke to operate on your application's topics.</p> <p>There are the following commands supported:</p> Command Description <code>karafka topics create</code> creates topics with appropriate settings. <code>karafka topics delete</code> deletes all the topics defined in the routes. <code>karafka topics repartition</code> adds additional partitions to topics with fewer partitions than expected. <code>karafka topics reset</code> deletes and re-creates all the topics. <code>karafka topics plan</code> plans the migration process and prints what changes are going to be applied if migration runs. <code>karafka topics align</code> aligns configuration of all the declarative topics that exist based on the declarative topics definitions. <code>karafka topics migrate</code> creates missing topics, repartitions existing to match expected partitions count and aligns the configuration. <p>The below example illustrates the usage of the <code>migrate</code> command to align the number of partitions and to add one additional topic:</p>      Note: Asciinema videos are not visible when viewing this wiki on GitHub. Please use our     online     documentation instead."}, {"location": "Declarative-Topics/#defining-topic-configuration", "title": "Defining Topic Configuration", "text": "<p>All the configuration for a given topic needs to be defined using the topic scope <code>#config</code> method.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :a do\n      config(\n        partitions: 6,\n        replication_factor: 3,\n        'retention.ms': 86_400_000, # 1 day in ms\n        'cleanup.policy': 'delete'\n      )\n\n      consumer ConsumerA\n    end\n\n    topic :b do\n      config(\n        partitions: 2,\n        replication_factor: 3\n        # The rest will be according to the cluster defaults\n      )\n\n      consumer ConsumerB\n    end\n  end\nend\n</code></pre> <p>If not invoked, the default config looks as followed:</p> <pre><code>config(\n  partitions: 1,\n  replication_factor: 1\n)\n</code></pre>"}, {"location": "Declarative-Topics/#excluding-topics-from-the-topics-management", "title": "Excluding Topics from the Topics Management", "text": "<p>If you want to manage only part of your topics using Karafka, you can set the <code>active</code> flag for a given topic configuration to false.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :a do\n      config(active: false)\n    end\n  end\nend\n</code></pre> <p>This will effectively ignore this topic from being altered in any way by Karafka. Karafka will ignore this topic together in all the CLI topics related operations.</p> <p>Keep in mind that setting <code>active</code> to false inside the <code>#config</code> is not equivalent to disabling the topic consumption using the <code>active</code> method.</p> <p>You can use Karafka to manage topics that you do not consume from as well by defining their config and making them inactive at the same time:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :a do\n      config(\n        partitions: 2,\n        replication_factor: 3\n      )\n\n      active false\n    end\n  end\nend\n</code></pre> <p>Setting such as above will allow Karafka to manage the topic while instructing Karafka not to try to consume it. A configuration like this is helpful in a multi-app environment where you want Karafka to manage topics, but their consumption belongs to other applications.</p>"}, {"location": "Declarative-Topics/#production-usage", "title": "Production Usage", "text": "<p>The topics management CLI never performs any destructive actions except the <code>delete</code> and <code>reset</code> commands. This means you can safely include the <code>karafka topics migrate</code> in your deployment pipelines if you wish to delegate topics management to Karafka.</p> <p>Please keep in mind that topics management API does not provide any means of concurrency locking when CLI commands are being executed.</p>"}, {"location": "Declarative-Topics/#strict-declarative-topics-validation", "title": "Strict Declarative Topics Validation", "text": "<p>Karafka provides an optional configuration flag, <code>config.strict_declarative_topics</code>, that ensures all topics, including Dead Letter Queue (DLQ), are declared via the definitions of the declarative topics. When set to <code>true</code>, this flag enforces validation during routing to confirm that all topics are properly defined as declarative topics, even if they are inactive.</p> <p>This setting is particularly useful if you want to ensure that all topics in the routing are managed and controlled through declarative definitions, enhancing consistency and preventing unintentional topic omissions. By using this flag, you can be confident that your entire topics setup is defined and managed as part of your configuration, reducing the chances of configuration drift across different environments.</p> <p>You can enable this validation by adding the following to your <code>karafka.rb</code>:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.strict_declarative_topics = true\n  end\nend\n</code></pre> <p>With this setting enabled, Karafka will fail to start if any topics in the routing, including DLQ topics, are missing declarative definitions, ensuring strict adherence to the declarative topics management strategy.</p>"}, {"location": "Declarative-Topics/#detailed-exit-codes", "title": "Detailed Exit Codes", "text": "<p>When managing Kafka topics via Karafka's CLI commands, the <code>--detailed-exitcode</code> option can be configured. This option alters the exit codes based on the operation's result. This option provides more granular information, making integrating Karafka's topic management into automated systems like CI/CD pipelines easier. </p> <p>When the <code>--detailed-exitcode</code> flag is enabled, the <code>topic</code> related commands exit codes will work as follows:</p> <ul> <li><code>0</code>: No changes were made. This means that all topics are already aligned with the desired state, and no operations were required.</li> <li><code>1</code>: An error occurred during the operation. This indicates a failure or issue that needs to be addressed before continuing.</li> <li><code>2</code>: Changes were either present or successfully applied. This code is returned when topics were created, updated, or deleted as part of the operation.</li> </ul> <p>This behavior allows you to differentiate between successful operations with no changes, successful operations with changes, and errors, providing more control over how you handle Karafka's CLI topic management results.</p> <p>You can enable this functionality by passing the <code>--detailed-exitcode</code> flag when invoking any topic-related command:</p> <pre><code>karafka topics plan --detailed-exitcode\n</code></pre> <p>This will ensure the correct exit code is returned based on the operation's outcome, enabling better automation and monitoring of topic changes.</p>"}, {"location": "Declarative-Topics/#limitations-and-other-info", "title": "Limitations and Other Info", "text": "<ul> <li>Topics management is enabled by default but will not be used unless any CLI commands are invoked.</li> <li><code>migrate</code> does not wait for a confirmation. Use <code>plan</code> command to check the changes that would be applied.</li> <li>If a topic is used by several consumer groups defined in one application, only the first <code>config</code> defined will be used.</li> <li>Topics management API does not support the management of multiple independent Kafka clusters. Only the primary one will be managed.</li> <li>Topics management API does not provide any means of concurrency locking when CLI commands are being executed. This means it is up to you to ensure that two topic CLI commands are not running in parallel during the deployments.</li> <li>Topics commands are not transactional. It means that the state application may be partial in case of errors.</li> <li>Topics commands are idempotent. Broken set of operations can be retried after fixes without worry.</li> <li>Karafka will never alter any topics that are not defined in the routing.</li> <li><code>replication_factor</code> can be set only during the topic creation. It will not be altered for existing topics.</li> <li><code>repartition</code> will not downscale the number of topic partitions and will ignore such configuration.</li> </ul> <p>Last modified: 2025-06-16 13:33:42</p>"}, {"location": "Deployment/", "title": "Deployment", "text": "<p>Karafka is currently being used in production with the following deployment methods:</p> <ul> <li>systemd (+ Capistrano)</li> <li>Docker</li> <li>AWS + MSK (Fully Managed Apache Kafka)</li> <li>Heroku</li> <li>Kubernetes</li> <li>Confluent Cloud</li> <li>Custom OAuth Token Providers</li> </ul> <p>Since the only thing that is long-running is the Karafka server, it shouldn't be hard to make it work with other deployment and CD tools.</p>"}, {"location": "Deployment/#systemd-capistrano", "title": "systemd (+ Capistrano)", "text": "<p>You can easily manage Karafka applications with <code>systemd</code>. Here's an example <code>.service</code> file that you can use.</p> <pre><code># Move to /lib/systemd/system/karafka.service\n# Run: systemctl enable karafka\n\n[Unit]\nDescription=karafka\nAfter=syslog.target network.target\n\n[Service]\nType=simple\n\nWorkingDirectory=/opt/current\nExecStart=/bin/bash -lc 'bundle exec karafka server'\nUser=deploy\nGroup=deploy\nUMask=0002\n\nRestartSec=1\nRestart=on-failure\n\n# output goes to /var/log/syslog\nStandardOutput=syslog\nStandardError=syslog\n\n# This will default to \"bundler\" if we don't specify it\nSyslogIdentifier=karafka\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>If you want to use <code>systemd</code> based solution together with Capistrano, you don't need the <code>capistrano-karafka</code> gem. Instead, you can use this simple Capistrano <code>.cap</code> file:</p> <pre><code># frozen_string_literal: true\n\nafter 'deploy:starting', 'karafka:stop'\nafter 'deploy:published', 'karafka:start'\nafter 'deploy:failed', 'karafka:restart'\n\nnamespace :karafka do\n  task :start do\n    on roles(:app) do\n      execute :sudo, :systemctl, :start, 'karafka'\n    end\n  end\n\n  task :stop do\n    on roles(:app) do\n      execute :sudo, :systemctl, :stop, 'karafka'\n    end\n  end\n\n  task :restart do\n    on roles(:app) do\n      execute :sudo, :systemctl, :restart, 'karafka'\n    end\n  end\n\n  task :status do\n    on roles(:app) do\n      execute :sudo, :systemctl, :status, 'karafka'\n    end\n  end\nend\n</code></pre> <p>If you need to run several processes of a given type, please refer to <code>template unit files</code>.</p>"}, {"location": "Deployment/#docker", "title": "Docker", "text": "<p>Karafka can be dockerized as any other Ruby/Rails app. To execute <code>karafka server</code> command in your Docker container, just put this into your Dockerfile:</p> <pre><code>ENV KARAFKA_ENV production\nCMD bundle exec karafka server\n</code></pre>"}, {"location": "Deployment/#aws-msk-fully-managed-apache-kafka", "title": "AWS + MSK (Fully Managed Apache Kafka)", "text": "<p>First of all, it is worth pointing out that Karafka, similar to librdkafka does not support SASL mechanism for AWS MSK IAM that allows Kafka clients to handle authentication and authorization with MSK clusters through AWS IAM. This mechanism is a proprietary idea that is not part of Kafka.</p> <p>Karafka does, however, support:</p> <ul> <li>Standard SASL + SSL mechanisms.</li> <li>Custom OAuth Token Providers flow.</li> </ul> <p>Please follow the below instructions for both cluster initialization and Karafka configuration or go to the Custom Oauth Token Providers section.</p> <p>AWS Integration with Custom OAuth Token Providers</p> <p>While Karafka can be deployed on AWS using the Custom OAuth Token provider flow, additional code or gems may be required to fetch the tokens when necessary. This code is not included in the standard Karafka setup, so you must implement or integrate it based on your authentication provider's requirements.</p>"}, {"location": "Deployment/#aws-msk-cluster-setup", "title": "AWS MSK cluster setup", "text": "<ol> <li>Navigate to the AWS MSK page and press the <code>Create cluster</code> button.</li> <li>Select <code>Custom create</code> and <code>Provisioned</code> settings.</li> </ol> <ol> <li>Use custom config and set <code>auto.create.topics.enable</code> to <code>true</code> unless you want to create topics using Kafka API. You can change it later, and in general, it is recommended to disallow auto-topic creation (typos, etc.), but this can be useful for debugging.</li> </ol> <ol> <li>Setup your VPC and networking details.</li> <li>Make sure that you disable the <code>Unauthenticated access</code> option. With it enabled, there won't be any authentication beyond those imposed by your security groups and VPC.</li> <li>Disable <code>IAM role-based authentication</code>.</li> <li>Enable <code>SASL/SCRAM authentication</code></li> </ol> <ol> <li>Provision your cluster.</li> <li>Make sure your cluster is accessible from your machines. You can test it by using the AWS VPC Reachability Analyzer.</li> </ol> <ol> <li>Visit your cluster <code>Properties</code> page and copy the <code>Endpoints</code> addresses.</li> </ol> <ol> <li>Log in to any of your machines and run a <code>telnet</code> session to any of the brokers:</li> </ol> <pre><code>telnet your-broker.kafka.us-east-1.amazonaws.com 9096\n\nTrying 172.31.22.230...\nConnected to your-broker.kafka.us-east-1.amazonaws.com.\nEscape character is '^]'.\n^Connection closed by foreign host.\n</code></pre> <p>If you can connect, your settings are correct, and your cluster is visible from your instance.</p> <ol> <li>Go to the AWS Secret Manager and create a key starting with <code>AmazonMSK_</code> prefix. Select <code>Other type of secret</code> and <code>Plaintext</code> and provide the following value inside of the text field:</li> </ol> <p> </p> <ol> <li>In the <code>Encryption key</code> section, press the <code>Add new key</code>.</li> </ol> <p> </p> <ol> <li>Create a <code>Symmetric</code> key with <code>Encrypt and decrypt</code> as a usage pattern.</li> </ol> <p> </p> <ol> <li>Select your key in the <code>Encryption key</code> section and press <code>Next</code>.</li> <li>Provide a secret name and description and press <code>Next</code> until you reach the <code>Store</code> button.</li> <li>Store your secret.</li> <li>Go back to the AWS MSK and select your cluster.</li> <li>Navigate to the <code>Associated secrets from AWS Secrets Manager</code> section and press <code>Associate secrets</code></li> </ol> <p> </p> <ol> <li>Press the <code>Choose secrets</code> and select the previously created secret.</li> </ol> <p> </p> <ol> <li>Press <code>Associate secrets</code>. It will take AWS a while to do it.</li> <li>Congratulations, you just configured everything needed to make it work with Karafka.</li> </ol>"}, {"location": "Deployment/#karafka-configuration-for-aws-msk-sasl-ssl", "title": "Karafka configuration for AWS MSK SASL + SSL", "text": "<p>Provide the following details to the <code>kafka</code> section:</p> <pre><code>config.kafka = {\n  'bootstrap.servers': 'yourcluster-broker1.amazonaws.com:9096,yourcluster-broker2.amazonaws.com:9096',\n  'security.protocol': 'SASL_SSL',\n  'sasl.username': 'username',\n  'sasl.password': 'password',\n  'sasl.mechanisms': 'SCRAM-SHA-512'\n}\n</code></pre> <p>After that, you should be good to go.</p>"}, {"location": "Deployment/#troubleshooting-aws-msk", "title": "Troubleshooting AWS MSK", "text": ""}, {"location": "Deployment/#local-authentication-failure", "title": "Local: Authentication failure", "text": "<pre><code>ERROR -- : rdkafka: [thrd:sasl_ssl://broker1.kafka.us-east-1.amazonaws.]:\nsasl_ssl://broker1.us-east-1.amazonaws.com:9096/bootstrap: SASL authentication error:\nAuthentication failed during authentication due to invalid credentials with SASL mechanism SCRAM-SHA-512\n(after 312ms in state AUTH_REQ, 1 identical error(s) suppressed)\n\nERROR -- : librdkafka internal error occurred: Local: Authentication failure (authentication)\n\n</code></pre> <p>It may mean two things:</p> <ul> <li>Your credentials are wrong</li> <li>AWS MSK did not yet refresh its allowed keys, and you need to wait. Despite AWS reporting cluster as <code>Active</code> with no pending changes, it may take a few minutes for the credentials to start working.</li> </ul>"}, {"location": "Deployment/#connection-setup-timed-out-in-state-connect", "title": "Connection setup timed out in state CONNECT", "text": "<pre><code>rdkafka: [thrd:sasl_ssl://broker1.kafka.us-east-1.amazonaws.]:\nsasl_ssl://broker1.us-east-1.amazonaws.com:9092/bootstrap:\nConnection setup timed out in state CONNECT (after 30037ms in state CONNECT)\n</code></pre> <p>This means Kafka is unreachable. Check your brokers' addresses and ensure you use a proper port: <code>9096</code> with SSL or <code>9092</code> when plaintext. Also, make sure your instance can access AWS MSK at all.</p>"}, {"location": "Deployment/#connection-failures-and-timeouts", "title": "Connection failures and timeouts", "text": "<p>Please make sure that your instances can reach Kafka. Keep in mind that security group updates can have a certain lag in propagation.</p>"}, {"location": "Deployment/#rdkafkardkafkaerror-broker-invalid-replication-factor-invalid_replication_factor", "title": "Rdkafka::RdkafkaError (Broker: Invalid replication factor (invalid_replication_factor))", "text": "<p>Please make sure your custom setting <code>default.replication.factor</code> value matches what you have declared as <code>Number of zones</code> in the <code>Brokers</code> section:</p> <p> </p>"}, {"location": "Deployment/#rdkafkardkafkaerror-broker-topic-authorization-failed-topic_authorization_failed", "title": "Rdkafka::RdkafkaError: Broker: Topic authorization failed (topic_authorization_failed)", "text": "<p>This error occurs in case you enabled Kafka ACL but did not grant proper ACL permissions to your users. It often happens when you make your AWS MSK public.</p> <p>Please note that <code>allow.everyone.if.no.acl.found</code> <code>false</code> superseeds <code>auto.create.topics.enable</code>. This means that despite <code>auto.create.topics.enable</code> being set to <code>true</code>, you will not be able to auto-create topics as the ACL will block this.</p> <p>We recommend creating all the needed topics before making the cluster public and assigning proper permissions via Kafka ACL.</p> <p>If you want to verify that this is indeed an ACL issue, try running <code>::Karafka::Admin.cluster_info</code>. If you get cluster info and no errors, you can connect to the cluster, but ACL blocks any usage.</p> <pre><code>::Karafka::Admin.cluster_info =&gt;\n#&lt;Rdkafka::Metadata:0x00007fea8e3a43c0                                           \n @brokers=[{:broker_id=&gt;1001, :broker_name=&gt;\"your-kafka-host\", :broker_port=&gt;9092}],   \n @topics=[]\n&gt;\n</code></pre> <p>You can also use this ACL command to give all operations access for the brokers on all the topics to a given user:</p> <pre><code>./bin/kafka-acls.sh \\\n  --authorizer-properties zookeeper.connect=&lt;ZOOKEEPER_CONNECTION_STRING&gt; \\\n  --add \\\n  --allow-principal User:&lt;USER_NAME&gt; \\\n  --allow-host=* \\\n  --operation All \\\n  --topic=* \\\n  --group=*\n</code></pre> <p>The above command must be run from a client machine with Java + Kafka installation, and the machine should also be able to communicate with the zookeeper nodes.</p>"}, {"location": "Deployment/#heroku", "title": "Heroku", "text": "<p>Karafka works with the Heroku Kafka add-on, but it requires some extra configuration and understanding of how the Heroku Kafka add-on works.</p> <p> </p> <p>Details about how Kafka for Heroku works can also be found here:</p> <ul> <li>https://devcenter.heroku.com/articles/kafka-on-heroku</li> <li>https://devcenter.heroku.com/articles/multi-tenant-kafka-on-heroku</li> <li>https://devcenter.heroku.com/articles/kafka-addon-migration</li> </ul>"}, {"location": "Deployment/#heroku-kafka-prefix-convention", "title": "Heroku Kafka Prefix Convention", "text": "<p>This section only applies to the Multi-Tenant add-on mode.</p> <p>All Kafka Basic topics and consumer groups begin with a unique prefix associated with your add-on. This prefix is accessible via the <code>KAFKA_PREFIX</code> environment variable.</p> <p>That means that in the multi-tenant mode, you must remember always to prefix all the topic names and all the consumer group names with the <code>KAFKA_PREFIX</code> environment variable value.</p> <p>To make it work you need to follow few steps:</p> <ol> <li>Change the <code>group_id</code> setting to match your <code>KAFKA_PREFIX</code>.</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # other config options...\n\n    # Inject the prefix automatically to the default consumer group\n    config.group_id = \"#{ENV['KAFKA_PREFIX']}app\"\n  end\nend\n</code></pre> <ol> <li>If you use explicit consumer groups, ensure they are prefixed with <code>KAFKA_PREFIX</code>.</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    consumer_group \"#{ENV['KAFKA_PREFIX']}my-group1\" do\n      topic :example do\n        consumer ExampleConsumer\n      end\n\n      topic :example2 do\n        consumer ExampleConsumer2\n      end\n    end\n\n    consumer_group \"#{ENV['KAFKA_PREFIX']}my-group2\" do\n      topic :example3 do\n        consumer Example2Consumer3\n      end\n    end\n  end\nend\n</code></pre> <ol> <li>Create all the consumer groups before using them via the Heroku CLI.</li> </ol> <pre><code>heroku kafka:consumer-groups:create CONSUMER_GROUP_NAME\n</code></pre> <p>The value of <code>KAFKA_PREFIX</code> typically is like <code>smoothboulder-1234.</code> which would make the consumer group in Karafka <code>smoothboulder-1234.app</code>. Kafka itself does not need to know the prefix when creating the consumer group.</p> <p>This means that the Heroku CLI command needs to look as follows:</p> <pre><code>heroku kafka:consumer-groups:create app\n</code></pre> <p>This allows Heroku's multi-tenant setup to route <code>smoothboulder-1234.app</code> to your cluster correctly.</p> <ol> <li>When consuming, you always need to use the prefixed topic name:</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  # ...\n  routes.draw do\n    topic \"#{ENV['KAFKA_PREFIX']}users_events\" do\n      consumer UsersEventsConsumer\n    end\n  end\nend\n</code></pre> <ol> <li>When producing, you always need to use the prefixed topic name:</li> </ol> <pre><code>Karafka.producer.produce_async(\n  topic: \"#{ENV['KAFKA_PREFIX']}users_events\",\n  payload: {\n    user_id: user.id,\n    event: 'user.deleted'\n  }.to_json\n)\n</code></pre> <ol> <li>When using <code>Karafka::Admin</code> and <code>Karafka::Web</code> please make sure to create appropriate consumer groups as well.</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # other config options...\n\n    # Make Karafka admin use such a group ID\n    config.admin.group_id = \"#{ENV['KAFKA_PREFIX']}karafka-admin-extra\"\n  end\nend\n\nKarafka::Web.setup do |config|\n  # other config options...\n  config.group_id = \"#{ENV['KAFKA_PREFIX']}karafka-web-ui\"\nend\n</code></pre> <pre><code># Since both are set to karafka_admin by default, if you did not change those values,\n# you can just run:\nheroku kafka:consumer-groups:create karafka_admin\n\n# If you did change them according to the above example, create all the groups needed:\n\n# Create the admin consumer group\nheroku kafka:consumer-groups:create karafka-admin-extra\n\n# Create the web ui consumer group\nheroku kafka:consumer-groups:create karafka-web-ui\n</code></pre> <p>You will need to configure your topics in Kafka before they can be used. This can be done in the Heroku UI or via the CLI provided by Heroku. Be sure to name your topics without the KAFKA_PREFIX, e.g. <code>heroku kafka:topics:create users_events --partitions 3</code>.</p>"}, {"location": "Deployment/#configuring-karafka-to-work-with-heroku-ssl", "title": "Configuring Karafka to Work With Heroku SSL", "text": "<p>When you turn on the add-on, Heroku exposes a few environment variables within which important details are stored. You need to use them to configure Karafka as follows:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      # ...\n      'security.protocol': 'ssl',\n      # KAFKA_URL has the protocol that we do not need as we define the protocol separately\n      'bootstrap.servers': ENV['KAFKA_URL'].gsub('kafka+ssl://', ''),\n      'ssl.certificate.pem': ENV['KAFKA_CLIENT_CERT'],\n      'ssl.key.pem': ENV['KAFKA_CLIENT_CERT_KEY'],\n      'ssl.ca.pem': ENV['KAFKA_TRUSTED_CERT']\n    }\n\n    # ... other config options\n  end\nend\n</code></pre>"}, {"location": "Deployment/#heroku-retention-policy-impact-on-the-web-ui", "title": "Heroku Retention Policy Impact on the Web UI", "text": "<p>Heroku's one-day default retention policy for Kafka may affect Karafka Web UI's functionality and reliability, leading to a lack of data and operational issues. The Web UI depends on continuous data flow and requires access to historical data for analytics. A one-day policy is insufficient for Web UI topics. To improve stability and reliability, we recommend configuring your Web UI topics according to our recommended defaults, which can be found here.</p>"}, {"location": "Deployment/#troubleshooting", "title": "Troubleshooting", "text": "<p>There are few problems you may encounter when configuring things for Heroku:</p>"}, {"location": "Deployment/#unsupported-protocol-kafkassl", "title": "Unsupported protocol \"KAFKA+SSL\"", "text": "<pre><code>parse error: unsupported protocol \"KAFKA+SSL\"\n</code></pre> <p>Solution: Make sure you strip off the <code>kafka+ssl://</code> component from the <code>KAFKA_URL</code> env variable content.</p>"}, {"location": "Deployment/#disconnected-while-requesting-apiversion", "title": "Disconnected while requesting ApiVersion", "text": "<pre><code>Disconnected while requesting ApiVersion: might be caused by incorrect security.protocol configuration\n(connecting to a SSL listener?)\n</code></pre> <p>Solution: Make sure all the settings are configured exactly as presented in the configuration section.</p>"}, {"location": "Deployment/#topic-authorization-failed", "title": "Topic authorization failed", "text": "<pre><code>Broker: Topic authorization failed (topic_authorization_failed) (Rdkafka::RdkafkaError)\n</code></pre> <p>Solution: Make sure to namespace all the topics and consumer groups with the <code>KAFKA_PREFIX</code> environment value.</p>"}, {"location": "Deployment/#messages-are-not-being-consumed", "title": "Messages are not being consumed", "text": "<pre><code>DEBUG -- : [3732873c8a74] Polled 0 messages in 1000ms\nDEBUG -- : [3732873c8a74] Polling messages...\nDEBUG -- : [3732873c8a74] Polled 0 messages in 1000ms\nDEBUG -- : [3732873c8a74] Polling messages...\nDEBUG -- : [3732873c8a74] Polled 0 messages in 1000ms\n</code></pre> <p>Solution 1: Basic multi-tenant Kafka plans require a prefix on topics and consumer groups. Make sure that both your topics and consumer groups are prefixed.</p> <p>Solution 2: Make sure you've created appropriate consumer groups prior to them being used via the Heroku CLI.</p>"}, {"location": "Deployment/#missing-information-or-initial-consumers-state-missing-notice-after-a-while", "title": "Missing Information or \"Initial Consumers State Missing\" Notice After a While", "text": "<p>Please read the Heroku Retention Policy Impact on the Web UI section and apply correct Web UI topics configuration.</p>"}, {"location": "Deployment/#kubernetes", "title": "Kubernetes", "text": "<p>Karafka can be easily deployed using Kubernetes. Since Karafka is often used for mission-critical applications that handle a high volume of messages, it's vital to ensure that the application stays healthy and responsive. Fortunately, Karafka supports liveness checks, which can be used to verify that the application is running correctly. With Kubernetes, it's easy to define liveness probes that periodically check the status of a Karafka application and restart the container if necessary. By using Kubernetes to deploy Karafka and configuring liveness probes, you can ensure that their mission-critical applications stay up and running, even in the face of unexpected failures.</p>"}, {"location": "Deployment/#basic-deployment-spec", "title": "Basic deployment spec", "text": "<p>Below you can find the basic deployment spec for a Karafka consumer process:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: karafka-deployment\n  labels:\n    app: app-name\nspec:\n  replicas: 5 # Number of processes you want to have\n  selector:\n    matchLabels:\n      app: app-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: app-name\n    spec:\n      containers:\n        - name: app-name\n          image: app-docker-image\n          command: [\"bundle\", \"exec\", \"karafka\", \"server\"]\n          env:\n            - name: KARAFKA_ENV\n              value: production\n</code></pre> <p>When deploying Karafka consumers using Kubernetes, it's generally not recommended to use strategies other than <code>Recreate</code>. This is because other strategies, such as <code>RollingUpdate</code> may cause extensive rebalancing among the consumer processes. This can lead to slow deployments and double-processing of messages, which can be a significant problem.</p> <p>For larger deployments with many consumer processes, it's especially important to be mindful of the rebalancing issue.</p> <p>Overall, when deploying Karafka consumers using Kubernetes, it's important to consider the deployment strategy carefully and to choose a strategy that will minimize the risk of rebalancing issues. By using the <code>Recreate</code> strategy and configuring Karafka static group memberships and <code>cooperative.sticky</code> rebalance strategy settings, you can ensure that your Karafka application stays reliable and performant, even during large-scale deployments.</p>"}, {"location": "Deployment/#liveness", "title": "Liveness", "text": "<p>There are many ways to define a liveness probe for a Kubernetes deployment, and the best approach depends on the application's specific requirements. We recommend using the HTTP liveness probe, as this is the most common type of liveness probe, which checks if the container is alive by sending an HTTP request to a specific endpoint. Karafka provides a base listener that starts a minimal HTTP server exposing basic health information about the running process using following HTTP codes:</p> <ul> <li><code>200</code> - Everything works as expected, with detailed JSON status information.</li> <li><code>500</code> - Karafka process is not behaving as expected and should be restarted.</li> </ul> <p>Karafka Kubernetes liveness listener can be initialized with two important thresholds: <code>consuming_ttl</code> and <code>polling_ttl</code>. These thresholds are used to determine if Karafka or the user code consuming from Kafka hangs for an extended time.</p> <p>If the <code>consuming_ttl</code> threshold is exceeded, it suggests that the user code consuming from Kafka is taking too long. Similarly, if the <code>polling_ttl</code> is exceeded, this means that the polling does not happen often enough. In both cases, when either of these thresholds is surpassed, the liveness endpoint configured in Karafka will respond with an HTTP 500 status code.</p> <p>This configuration allows you to handle scenarios where Karafka hangs, or the user code consuming from Kafka becomes unresponsive. By setting appropriate values for <code>consuming_ttl</code> and <code>polling_ttl</code>, you can tailor the liveness probe to detect and handle these situations effectively.</p> <p>Important Note on Liveness Probes in Swarm Mode</p> <p>The standard Karafka Kubernetes liveness listener is not suitable for Swarm Mode. In Swarm Mode, the default listener will cause Kubernetes to inaccurately mark the Karafka process as dead due to its inability to assess the health of individual swarm nodes correctly. Karafka offers a specialized liveness listener for Swarm Mode to ensure accurate health checks and prevent unnecessary restarts. Ensure you use the correct listener for Swarm Mode deployments to maintain your application's reliability in a Kubernetes environment.</p> <p>Below you can find an example of how to require, configure and connect the liveness HTTP listener.</p> <ol> <li>Put following code at the end of your <code>karafka.rb</code> file:</li> </ol> <pre><code>require 'karafka/instrumentation/vendors/kubernetes/liveness_listener'\n\nlistener = ::Karafka::Instrumentation::Vendors::Kubernetes::LivenessListener.new(\n  # If hostname not specified or nil, will bind to all the interfaces\n  hostname: '192.168.1.100',\n  port: 3000,\n  # Make sure polling happens at least once every 5 minutes\n  polling_ttl: 300_000,\n  # Make sure that consuming does not hang and does not take more than 1 minute\n  consuming_ttl: 60_000\n)\n\nKarafka.monitor.subscribe(listener)\n</code></pre> <ol> <li>Expand your deployment spec with the following <code>livenessProbe</code> section:</li> </ol> <pre><code>livenessProbe:\n  httpGet:\n    path: /\n    port: 3000\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 5\n</code></pre> <p>The provided liveness listener is a generic implementation designed to handle common scenarios. However, it may not address all cases specific to your application's requirements. Fortunately, the listener serves as a solid foundation that can be customized and extended to create a more complex and tailored solution.</p> <p>By using the provided listener as a starting point, you can have the flexibility to build your liveness probe that accommodates your unique needs. This can involve adding additional checks, implementing custom logic, or integrating with other monitoring systems to create a more comprehensive and sophisticated liveness solution.</p>"}, {"location": "Deployment/#response-format", "title": "Response Format", "text": "<p>The liveness listener returns detailed health information in JSON format:</p> <p>Healthy Response (200 OK):</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"timestamp\": 1717251446,\n  \"port\": 3000,\n  \"process_id\": 12345,\n  \"errors\": {\n    \"polling_ttl_exceeded\": false,\n    \"consumption_ttl_exceeded\": false,\n    \"unrecoverable\": false\n  }\n}\n\n**Unhealthy Response (500 Internal Server Error):**\n\n```json\n{\n  \"status\": \"unhealthy\",\n  \"timestamp\": 1717251500,\n  \"port\": 3000,\n  \"process_id\": 12345,\n  \"errors\": {\n    \"polling_ttl_exceeded\": true,\n    \"consumption_ttl_exceeded\": false,\n    \"unrecoverable\": false\n  }\n}\n</code></pre> <p>This response format allows for more granular monitoring and debugging while maintaining compatibility with existing Kubernetes liveness probe configurations that check for HTTP 2xx status codes.</p>"}, {"location": "Deployment/#extending-liveness-with-healthy", "title": "Extending Liveness with <code>#healthy?</code>", "text": "<p>The <code>#healthy?</code> method in the liveness listener is a public method that can be expanded to include additional application-specific health checks. By default, this method verifies whether the Karafka process operates as expected. However, you can override or extend it to include custom checks tailored to your application's requirements.</p> <p>For example, if your application depends on external services (like a database or an API), you can extend the <code>#healthy?</code> method to ensure these services are also reachable. If any of these checks fail, you can return a <code>500</code> status, prompting Kubernetes to restart the container, thereby increasing the resilience of your deployment.</p> <p>This flexibility allows you to go beyond the default liveness check, adding layers of health verification specific to your application's architecture and dependencies.</p> <pre><code>class LivenessListener &lt; ::Karafka::Instrumentation::Vendors::Kubernetes::LivenessListener\n  # Fail if redis that is required is down\n  def healthy?\n    return false unless super\n    return false unless redis_alive?\n\n    true\n  end\nend\n</code></pre>"}, {"location": "Deployment/#liveness-in-the-swarm-mode", "title": "Liveness In the Swarm Mode", "text": "<p>Karafka provides a specialized Kubernetes liveness listener for applications operating in the Swarm Mode. This adaptation ensures accurate health monitoring and management of the supervisor process within the swarm</p> <p>The <code>SwarmLivenessListener</code> is tailored to supervise the health of the Karafka supervisor process in Swarm Mode, addressing unique operational dynamics. It provides:</p> <ul> <li> <p>Controlling TTL: A configurable time-to-live (TTL) for supervising thread activity, ensuring the supervisor actively manages child nodes. Set this with consideration for normal and shutdown states to avoid false positives.</p> </li> <li> <p>Minimal HTTP Server: Similar to the standard listener, it runs an HTTP server for health checks, responding with:</p> </li> <li><code>200</code>: The supervisor is active and controlling, as expected, with detailed JSON status information.</li> <li><code>500</code>: Supervisor activity is below the controlling_ttl, indicating potential issues.</li> </ul> <p>To integrate the <code>SwarmLivenessListener</code> into your Karafka application, follow these steps:</p> <ol> <li>Embed the listener setup at the end of your <code>karafka.rb</code> file to initialize and subscribe the listener to your Karafka monitoring system:</li> </ol> <pre><code>require 'karafka/instrumentation/vendors/kubernetes/swarm_liveness_listener'\n\nlistener = ::Karafka::Instrumentation::Vendors::Kubernetes::SwarmLivenessListener.new(\n  # Optional: Specify to bind to a specific interface\n  hostname: '192.168.1.100',\n  # TCP port for the HTTP server\n  port: 3000,\n  # TTL for supervisor control checks (in milliseconds)\n  controlling_ttl: 60_000\n)\n\nKarafka.monitor.subscribe(listener)\n</code></pre> <ol> <li>Adjust your deployment spec to include the liveness probe, targeting the listener's port and path.</li> </ol> <pre><code>livenessProbe:\n  httpGet:\n    path: /\n    port: 3000\n  initialDelaySeconds: 30\n  periodSeconds: 10\n  timeoutSeconds: 5\n</code></pre> <p>By using the <code>SwarmLivenessListener</code>, you leverage a tool crafted explicitly for the complexities of Swarm Mode, ensuring that Kubernetes accurately reflects the health of your distributed Karafka application, thus safeguarding against premature process restarts and enhancing overall system reliability.</p>"}, {"location": "Deployment/#additional-processes-inside-the-same-pod", "title": "Additional processes inside the same pod", "text": "<p>The Liveness listener is configured to associate itself only with a running <code>karafka server</code> process in your container. This implies that if you intend to run a separate process like <code>rails console</code> within an active container, the listener will not attempt to associate with a port already in use.</p> <pre><code>require 'karafka/instrumentation/vendors/kubernetes/liveness_listener'\n\nlistener = ::Karafka::Instrumentation::Vendors::Kubernetes::LivenessListener.new(\n  # config goes here...\n)\n\nKarafka.monitor.subscribe(listener)\n</code></pre>"}, {"location": "Deployment/#confluent-cloud", "title": "Confluent Cloud", "text": "<p>Deploying Karafka on Confluent Cloud offers a streamlined way to manage  Kafka infrastructure with less overhead on physical or cloud infrastructure management. Here are the detailed steps and considerations for setting up Karafka with Confluent Cloud.</p> <ol> <li>Create a Confluent Cloud Account</li> </ol> <p>Begin by registering for a Confluent Cloud account at Confluent.io. After registration, you can manage your Kafka clusters directly from the Confluent UI.</p> <ol> <li>Create a Kafka Cluster</li> </ol> <ul> <li>Navigate to the Confluent Cloud dashboard.</li> <li>Click on \"Add Cluster\" to configure a new Kafka cluster.</li> <li>Select the appropriate cloud provider and region that fits your application needs.</li> <li>Choose a Basic or Dedicated cluster plan depending on your scale requirements.</li> </ul> <p>Region Selection</p> <p>Ensure that the region selected has low latency to your application servers to reduce the message delivery time.</p> <ol> <li>Configure Kafka Topics</li> </ol> <ul> <li>Once your cluster is active, go to the 'Topics' tab and create the necessary topics that your application will use.</li> </ul> <p> </p> <ul> <li>Set appropriate partitions and retention policies based on your expected workload and data retention needs.</li> </ul> <p> </p> <ol> <li>Configure Web UI Topics</li> </ol> <p>Since Confluent Cloud does not support automatic topic creation, you must ensure that all necessary Karafka Web UI topics are created before using the Web UI to monitor and manage your Kafka setup.</p> <p>The detailed list and settings of all required Web UI topics are in the Karafka documentation under the Web UI Getting Started guide, specifically in the section on manual web UI topic management.</p> <ol> <li>Integration with Karafka</li> </ol> <p>In your Karafka application, configure the Kafka client to connect to your Confluent Cloud cluster. You will need to specify several configurations provided by Confluent Cloud:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'allow.auto.create.topics': false,\n      'bootstrap.servers': 'your-confluent-bootstrap-server.confluent.cloud:9092',\n      'security.protocol': 'SASL_SSL',\n      'sasl.username': 'CONFLUENT_USERNAME',\n      'sasl.password': 'CONFLUENT_PASSWORD',\n      'sasl.mechanisms': 'PLAIN'\n    }\n\n    # Other settings...\n  end\nend\n</code></pre>"}, {"location": "Deployment/#custom-oauth-token-providers", "title": "Custom OAuth Token Providers", "text": "<p>Karafka and its companion gem, WaterDrop, can integrate with any custom OAuth token provider, enabling secure communication with a Kafka cluster. This capability is instrumental in cloud environments like AWS, where securing access and credentials is crucial.</p> <p>Karafka provides flexibility through a listener that allows integration with any OAuth token provider. This means that, even if your token provider is not part of the Karafka ecosystem, you can still use it by writing custom integration code.</p> <p>Karafka and WaterDrop producers accept <code>config.oauth.token_provider_listener</code>, executed whenever tokens must be refreshed. Inside of the <code>event</code> object, there will be a <code>:bearer</code> on which you should invoke either:</p> <ul> <li><code>#oauthbearer_set_token</code> with <code>:token</code>, <code>:lifetime_ms</code> and <code>:principal_name</code>.</li> <li><code>#oauthbearer_set_token_failure</code> with a string reason explaining why the token was not obtained.</li> </ul> <p>Default Producer Auto-Configuration</p> <p>The default Karafka producer (<code>Karafka.producer</code>) does not need a separate configuration as it inherits the Kafka settings directly from the Karafka application configuration. However, if you use custom-initialized WaterDrop producers, remember that they require individual configuration.</p> <p>Below is an example of how you can set up Karafka with AWS using a custom token refresher and how you can configure a custom WaterDrop producer instance:</p> <pre><code>class OAuthTokenRefresher\n  # Refresh OAuth tokens when required by the Karafka connection lifecycle\n  def on_oauthbearer_token_refresh(event)\n    # Note that such library does not exist, it is an example\n    signer = AwsAbstractTokenGenerator.new\n    token = signer.generate_auth_token\n\n    if token\n      event[:bearer].oauthbearer_set_token(\n        token: token.token,\n        lifetime_ms: token.expiration_time_ms,\n        principal_name: 'kafka-cluster'\n      )\n    else\n      event[:bearer].oauthbearer_set_token_failure(\n        token.failure_reason\n      )\n    end\n  end\nend\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other config options...\n\n    config.kafka = {\n      'bootstrap.servers': 'your-kafka-server:9098',\n      'security.protocol': 'sasl_ssl',\n      'sasl.mechanisms': 'OAUTHBEARER'\n    }\n\n    config.oauth.token_provider_listener = OAuthTokenRefresher.new\n  end\nend\n\n# If you only use Karafka.producer, it will be auto-configured\nCUSTOM_PRODUCER = WaterDrop::Producer.new do |config|\n  config.kafka = {\n    'bootstrap.servers': 'your-kafka-server:9098',\n    'security.protocol': 'sasl_ssl',\n    'sasl.mechanisms': 'OAUTHBEARER'\n  }\n\n  config.oauth.token_provider_listener = OAuthTokenRefresher.new\nend\n</code></pre> <p>Last modified: 2025-06-11 13:58:20</p>"}, {"location": "Deserialization/", "title": "Deserialization", "text": "<p>Karafka provides extensive support for custom deserialization processes that accommodate various data representations in your Kafka messages. This guide outlines how Karafka facilitates payload deserialization and extends to deserializing message keys and headers, offering broad flexibility in handling Kafka data. Additionally, it introduces the concept of lazy deserialization, which optimizes performance by delaying the deserialization process until the data is actually needed.</p>"}, {"location": "Deserialization/#deserializers-for-payload-key-and-headers", "title": "Deserializers for Payload, Key, and Headers", "text": "<p>Deserializers transform the raw data of Kafka messages (payload, key, and headers) into a format your application can process. You can configure default deserializers or specify custom deserializers for each topic within your routes configuration.</p> <p>Each type of deserializer in Karafka accepts different parameters within the <code>#call</code> method and is expected to return a specific type of deserialized data. Below is a brief description of what each deserializer receives and is expected to return:</p> Deserializer Type Receives Expects Return Input Data Method Payload Deserializer <code>Karafka::Messages::Message</code> Deserialized Payload <code>#raw_payload</code> Key Deserializer <code>Karafka::Messages::Metadata</code> Deserialized Key <code>#raw_key</code> Headers Deserializer <code>Karafka::Messages::Metadata</code> Deserialized Headers Hash <code>#raw_headers</code> <p>Below you can find an example of a payload XML deserializer:</p> <pre><code>class XmlDeserializer\n  def call(message)\n    # nil case is for tombstone messages\n    return nil if message.raw_payload.nil?\n\n    Hash.from_xml(message.raw_payload)\n  end\nend\n</code></pre>"}, {"location": "Deserialization/#default-deserializers", "title": "Default Deserializers", "text": "<p>Karafka makes specific assumptions about incoming data format, setting defaults for how payloads, keys, and headers are handled unless explicitly overridden by custom deserializers. Here are the default behaviors:</p> <ul> <li> <p>Payload: Karafka assumes the message payload is in JSON format. This default deserializer automatically parses the raw payload from JSON into a Ruby hash, catering to common data interchange practices and supporting the tombstone event format.</p> </li> <li> <p>Key: By default, the key remains as a raw string. This approach is practical for most applications where the key is used primarily as an identifier or a partitioning token within Kafka.</p> </li> <li> <p>Headers: Headers are also kept in their original format by default. Karafka treats headers as a hash with string keys and string values, which is typical for transmitting metadata associated with the message.</p> </li> </ul>"}, {"location": "Deserialization/#configuring-deserializers", "title": "Configuring Deserializers", "text": ""}, {"location": "Deserialization/#setting-defaults", "title": "Setting Defaults", "text": "<p>In Karafka, you can configure default deserializers for all topics by utilizing the <code>#defaults</code> block within your routing configuration. This is particularly useful if your application generally handles messages in a specific format and you wish to apply a consistent deserialization approach across multiple topics.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  defaults do\n    deserializers(\n      payload: JsonDeserializer.new,\n      key: StringDeserializer.new,\n      headers: HashDeserializer.new\n    )\n  end\nend\n</code></pre> <p>Suppose a specific deserializer is not set for a given element (payload, key, or headers). In that case, Karafka will revert to using the predefined defaults: JSON for payloads, raw strings for keys, and unchanged hashes for headers.</p>"}, {"location": "Deserialization/#custom-per-topic-deserializers", "title": "Custom Per-Topic Deserializers", "text": "<p>While setting default deserializers is a reliable way to maintain consistency across an application, Karafka's true power lies in its flexibility. It allows for detailed customization by configuring deserializers for individual topics, a feature that becomes invaluable when dealing with topics that require specific data handling procedures or when integrating with external systems that use varied data formats.</p> <p>To set deserializers for a specific topic, you use the deserializers method within the topic configuration block in your routing setup. This allows you to define unique deserialization logic for the payload, key, and headers of messages consumed from that topic.</p> <p>Here's an example of how to configure deserializes for individual topics:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :financial_transactions do\n      consumer TransactionsConsumer\n\n      deserializers(\n        payload: AvroDeserializer.new,\n        key: IntegerDeserializer.new,\n        headers: JsonDeserializer.new\n      )\n    end\n\n    topic :system_logs do\n      consumer LogsConsumer\n      deserializers(\n        payload: TextDeserializer.new\n        # Uses the default framework deserializers\n        # for headers and key\n      )\n    end\n  end\nend\n</code></pre> <p>Per-Topic Deserializer Overrides</p> <p>When you configure deserializers for a specific topic using the <code>#deserializers</code> method and do not include deserializers for all components (payload, key, headers), be aware that Karafka treats the <code>#deserializers</code> block as atomic. This means that for any component not explicitly defined within the topic's <code>#deserializers</code> block, Karafka will revert to using the framework's built-in defaults, not the overrides specified in the <code>#defaults</code> block.</p>"}, {"location": "Deserialization/#context-aware-deserialization", "title": "Context Aware Deserialization", "text": "<p>In more complex messaging environments, a message's content and format can vary significantly based on its context, such as the topic or specific headers associated with it. Karafka supports context-aware deserialization, where the deserialization logic can adjust dynamically based on additional information from the message itself, enhancing flexibility and robustness in message processing.</p> <p>Deserializers in Karafka can access the full context of a message, including its headers and the topic it belongs to. This capability allows for dynamic adjustments in the deserialization process based on this context, such as selecting a specific schema or method for decoding the message data. An everyday use case for this is with formats like Avro, where a message header may indicate the schema needed to decode a message.</p> <pre><code>class AvroDeserializer\n  attr_reader :avro\n\n  def initialize(avro: avro)\n    @avro = avro\n  end\n\n  def call(message)\n    avro.decode(message.raw_payload, subject: message.headers['message_type'])\n  end\nend\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :binary_video_details do\n      consumer Videos::DetailsConsumer\n\n      deserializers(\n        payload: AvroDeserializer.new(avro: AvroTurf.new)\n      )\n    end\n  end\nend\n</code></pre>"}, {"location": "Deserialization/#lazy-deserialization", "title": "Lazy Deserialization", "text": "<p>In Karafka, lazy deserialization extends beyond the payload to include both the message key and headers. This feature enhances performance by delaying the conversion of raw message data into a usable format until it is explicitly required. This approach is especially beneficial for operations such as metadata-based filtering or when dealing with large datasets where minimizing processing overhead is crucial.</p> <p>Karafka defers the deserialization for the payload, key, and headers. Here's how each component is handled:</p> <ul> <li> <p>Payload: Deserialization occurs when the <code>Karafka::Messages::Message#payload</code> method is invoked for the first time. The deserialized data is cached, so subsequent accesses do not trigger re-deserialization.</p> </li> <li> <p>Key: Similar to payloads, keys are deserialized on the first invocation of the <code>Karafka::Messages::Message#key</code> method, with the result cached for future accesses.</p> </li> <li> <p>Headers: Headers are deserialized upon the first call to <code>Karafka::Messages::Message#headers</code>. The deserialized headers are stored to prevent redundant processing.</p> </li> </ul> <p>Access to the raw data for each component is also provided without triggering deserialization:</p> <ul> <li><code>Karafka::Messages::Message#raw_payload</code> for payloads,</li> <li><code>Karafka::Messages::Message#raw_key</code> for keys,</li> <li><code>Karafka::Messages::Message#raw_headers</code> for headers.</li> </ul> <p>Here's an expanded example that illustrates the use of lazy deserialization across all components of a message. This example filters messages based on the content of the raw payload and headers, then processes and prints data from the deserialized payload and key:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    messages\n      # Limit data amount using raw payload string based scanning\n      .select { _1.raw_payload.include?('signature') }\n      # Deserialize\n      .map(&amp;:payload)\n      # Look for data with particular key\n      .select { _1.keys.include?('signature') }\n      # extract what you were looking for\n      .map { _1.fetch('signature') }\n      # Print only those\n      .each { puts _1 }\n  end\nend\n</code></pre> <p>Lazy deserialization provides following benefits:</p> <ul> <li> <p>Performance Optimization: By avoiding unnecessary deserialization, Karafka minimizes CPU usage and speeds up processing times, especially in filter-heavy applications.</p> </li> <li> <p>Resource Efficiency: Memory usage is optimized as only necessary data is processed and stored after deserialization.</p> </li> <li> <p>Flexibility and Control: Developers have more control over when and how data is processed, allowing for customized handling based on the content of the messages.</p> </li> </ul> <p>Lazy deserialization in Karafka provides a robust mechanism for managing data processing in a Kafka-based messaging environment. It ensures that applications remain efficient and responsive even as data volume and complexity grow.</p>"}, {"location": "Deserialization/#handling-of-tombstone-messages", "title": "Handling of Tombstone Messages", "text": "<p>In Apache Kafka, tombstone messages are specific messages with the message key present, but the payload is null. These messages serve a critical role in Kafka's log compaction feature, which reduces the size of the log by removing outdated records. A tombstone message indicates that any previous messages with the same key should be considered deleted or obsolete, allowing Kafka to maintain only the most current data state for each key.</p> <p>Even if your current application logic does not specifically handle or generate tombstone messages, it is important to design your systems to accommodate them. This ensures that your application can correctly interpret and react to data streams that might include tombstone messages, particularly when integrating with other systems or when changes to data handling policies are implemented.</p> <p>When creating custom deserializers, you should explicitly manage the possibility of encountering tombstone messages. The deserializer should be able to gracefully handle <code>nil</code> payloads without causing errors or unintended behavior in the application. Here is how you can implement this in a custom XML deserializer:</p> <pre><code>class XmlDeserializer\n  def call(message)\n    # Check for tombstone messages where the payload is nil\n    return nil if message.raw_payload.nil?\n\n    # Proceed with deserialization if the payload is not nil\n    Hash.from_xml(message.raw_payload)\n  end\nend\n</code></pre> <p>By properly managing tombstone messages in your Kafka consumers, you can ensure that your application remains stable and consistent, even when dealing with evolving data states facilitated by Kafka\u2019s log compaction feature.</p>"}, {"location": "Deserialization/#dynamic-deserialization-based-on-topic-or-message-metadata", "title": "Dynamic Deserialization Based on Topic or Message Metadata", "text": "<p>In scenarios where messages originate from various topics and no explicit consumers are set up, you may want to dynamically resolve the deserializer based on some condition, like the topic name or metadata. </p> <p>Instead of explicitly defining a deserializer for every topic, Karafka allows you to configure a \"smarter\" default deserializer that adapts dynamically.</p> <p>You can implement a custom deserializer that evaluates each message and applies the appropriate deserialization strategy based on the message's topic or other metadata (e.g., headers). Karafka will then use this dynamic deserializer when using the Admin API, Iterator API, and the Web UI Explorer.</p> <p>Here's an example of a dynamic deserializer setup:</p> <pre><code># Dynamic deserializer that decides what format to use\n# based on the topic name\nclass DynamicDeserializer\n  def call(message)\n    case message.topic\n    when 'producers'\n      JSON.parse(message.raw_payload)\n    else\n      Xml.parse(message.raw_payload)\n    end\n  end\nend\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # Set a dynamic resolver for all the payloads of all the topics even\n    # when they are not defined in the routing explicitly\n    defaults do\n      deserializers(\n        payload: DynamicDeserializer.new\n      )\n    end\n\n    topic :example do\n      consumer ExampleConsumer\n      # Since dynamic defaults are used, for topics that require explicit\n      # deserializer, you need to set it yourself\n      deserializers(\n        payload: JsonDeserializer.new\n      )\n    end\n  end\nend\n</code></pre> <p>This approach simplifies the management of topics and makes the deserialization process more flexible without requiring configuration for each topic that will not be consumed.</p>"}, {"location": "Deserialization/#apache-avro", "title": "Apache Avro", "text": "<p>Apache Avro is a data serialization system developed by the Apache Foundation, used widely in the Big Data and cloud computing field. It provides a compact, fast, binary data format with rich data structures. Its schema evolution capability allows for flexibility in data reading and writing, with old software being able to read new data and vice versa. This language-agnostic system aids efficient data interchange between programs and supports a seamless and efficient way of data storage, encoding, and decoding.</p> <p>From the perspective of the Karafka framework, Avro is a serialization and deserialization layer, and there are no special things that are required to use it aside from making sure that you both serialize and deserialize your data using it.</p> <p>In other words, Avro is used to convert complex data structures into a format that can be easily transported over the network or stored, and later be converted back into the original data structure. This is crucial when working with Kafka, as data often needs to be sent between different services or stored for later use.</p> <p>The Ruby ecosystem offers an excellent gem for working with Avro called <code>avro_turf</code>. This gem provides a simple and effective way to encode and decode data using Avro, and it's compatible with both local filesystem schemas and schema registries.</p> <p>Let's explain these two concepts:</p> <ol> <li> <p>Local Filesystem Schemas: When using Avro, you define how your data should be structured using schemas. These schemas are often stored as <code>.avsc</code> files on your local filesystem. <code>avro_turf</code> can read these schema files and use them to encode your data into Avro's binary format or decode binary data back into its original form.</p> </li> <li> <p>Schema Registry: A Schema Registry is a server that stores Avro schemas in a central location. It is handy in a microservice architecture where many services may need to share and access common schemas. Keeping your Avro schemas in a Schema Registry allows different services to look up the schema associated with a particular version of data, ensuring that data can be correctly decoded even as your schemas evolve.</p> </li> </ol>"}, {"location": "Deserialization/#serialization-using-avro", "title": "Serialization using Avro", "text": ""}, {"location": "Deserialization/#local-filesystem-schemas", "title": "Local Filesystem Schemas", "text": "<p>To serialize data with Avro and <code>avro_turf</code> for use with Karafka, you'll first need to define an Avro schema for the data you want to send. Once you have the schema, you need to create an Avro reference object, point it to your schema and ensure that during deserialization, Karafka knows which schema to use:</p> <pre><code>avro = AvroTurf.new(schemas_path: 'app/schemas/')\nmessage = avro.encode(\n  { 'full_name' =&gt; 'Jane Doe', 'age' =&gt; 28 },\n  schema_name: 'person',\n  validate: true\n)\n\nKarafka.producer.produce_async(\n  topic: 'people',\n  payload: message,\n  # indicate type of schema in the message headers\n  headers: { message_type: 'person' }\n)\n</code></pre>"}, {"location": "Deserialization/#schema-registry", "title": "Schema Registry", "text": "<p>In case of a schema registry, you also need to connect to it and select the expected subject of serialization:</p> <pre><code>avro = AvroTurf::Messaging.new(registry_url: 'http://0.0.0.0:8081')\n\nmessage = avro.encode(\n  { 'title' =&gt; 'hello, world' },\n  subject: 'greeting',\n  version: 1\n)\n\nKarafka.producer.produce_async(\n  topic: 'greeting',\n  payload: message\n)\n</code></pre> <p>When working with a schema registry, there is no need for a <code>message_type</code> definition, as it will be auto-detected.</p>"}, {"location": "Deserialization/#abstracting-avro-serialization", "title": "Abstracting Avro serialization", "text": "<p>If you frequently use Avro for serialization and deserialization with Karafka, creating a wrapper around the Karafka producer can be beneficial. This wrapper can handle the Avro serialization before producing messages to Kafka, reducing redundancy and making your code more concise and manageable:</p> <pre><code>class AvroProducer\n  # Point this to your schemas location\n  AVRO = AvroTurf.new(schemas_path: 'app/schemas/')\n\n  class &lt;&lt; self\n    def produce_async(topic, data, schema)\n      message = AVRO.encode(\n        data,\n        schema_name: schema_name,\n        validate: true\n      )\n\n      Karafka.producer.produce_async(\n        topic: topic,\n        payload: message,\n        headers: { message_type: schema }\n      )\n    end\n  end\nend\n\nAvroProducer.produce_async(\n  'people',\n  { 'full_name' =&gt; 'Jane Doe', 'age' =&gt; 28 },\n  'person'\n)\n</code></pre>"}, {"location": "Deserialization/#deserialization-using-avro", "title": "Deserialization using Avro", "text": "<p>When receiving Avro-encoded messages in a Karafka consumer, you'll need to deserialize these messages back into a usable form. One efficient way to handle this is by creating a custom Karafka deserializer.</p>"}, {"location": "Deserialization/#local-filesystem-schemas_1", "title": "Local Filesystem Schemas", "text": "<p>Create your Avro deserializer:</p> <pre><code>class AvroLocalDeserializer\n  AVRO = AvroTurf.new(schemas_path: 'app/schemas/')\n\n  def call(message)\n    AVRO.decode(\n      message.raw_payload,\n      schema_name: message.headers['message_type']\n    )\n  end\nend\n</code></pre> <p>And indicate that a given topic contains Avro data in your routing setup:</p> <pre><code>topic :person do\n  consumer PersonConsumer\n  deserializers(\n    payload: AvroLocalDeserializer.new\n  )\nend\n</code></pre>"}, {"location": "Deserialization/#schema-registry_1", "title": "Schema Registry", "text": "<p>Create your Avro deserializer:</p> <pre><code>class AvroRegistryDeserializer\n  # Note, that in a production system you may want to pass authorized Avro reference\n  AVRO = AvroTurf::Messaging.new(registry_url: 'http://0.0.0.0:8081')\n\n  def call(message)\n    AVRO.decode(\n      message.raw_payload\n    )\n  end\nend\n</code></pre> <p>And indicate that a given topic contains Avro data in your routing setup:</p> <pre><code>topic :person do\n  consumer PersonConsumer\n  deserializers(\n    payload: AvroRegistryDeserializer.new\n  )\nend\n</code></pre> <p>Last modified: 2024-10-17 11:11:06</p>"}, {"location": "Development-Gems-Publishing/", "title": "Karafka Ecosystem Gems Release Process", "text": "<p>This document outlines the process for releasing gems in the Karafka ecosystem using GitHub's trusted publishing workflow.</p>"}, {"location": "Development-Gems-Publishing/#overview", "title": "Overview", "text": "<p>Our release process uses GitHub Actions with RubyGems trusted publishing for secure, automated gem releases. This eliminates the need for long-lived API keys and provides better security through short-lived OIDC tokens.</p>"}, {"location": "Development-Gems-Publishing/#prerequisites", "title": "Prerequisites", "text": "<p>Before you can make releases, ensure you have:</p> <ul> <li>Repository access: Push access to the Karafka repository</li> <li>Release permissions: Ability to create GitHub releases</li> <li>Workflow approval rights: Permission to approve GitHub Actions workflows</li> </ul>"}, {"location": "Development-Gems-Publishing/#trusted-publishing-setup", "title": "Trusted Publishing Setup", "text": ""}, {"location": "Development-Gems-Publishing/#how-trusted-publishing-works", "title": "How Trusted Publishing Works", "text": "<p>Trusted publishing uses OpenID Connect (OIDC) to establish trust between GitHub Actions and RubyGems without storing long-lived credentials. Here's the flow:</p> <ol> <li>GitHub Actions generates a short-lived OIDC token during workflow execution</li> <li>RubyGems validates the token against the configured trusted publisher settings</li> <li>If validation passes, RubyGems allows the gem to be published</li> <li>The token expires automatically after the workflow completes</li> </ol>"}, {"location": "Development-Gems-Publishing/#configuration", "title": "Configuration", "text": "<p>The trusted publishing configuration is already set up in the Karafka GitHub environments and RubyGems settings. Each gem has:</p> <ul> <li>GitHub Environment: Named <code>deployment</code> with RubyGems trusted publisher configured</li> <li>RubyGems Trusted Publisher: Configured with repository details, workflow path, and environment name</li> <li>Workflow: <code>.github/workflows/push.yml</code> that triggers on version tags</li> </ul>"}, {"location": "Development-Gems-Publishing/#release-process", "title": "Release Process", "text": ""}, {"location": "Development-Gems-Publishing/#step-1-prepare-the-release", "title": "Step 1: Prepare the Release", "text": "<ol> <li> <p>Create version branch: Create a new branch with the naming pattern <code>v\"VERSION\"</code> (e.g., <code>v1.2.3</code>, <code>v2.0.0.beta.1</code>)</p> </li> <li> <p>Update version files: Update the gem version in the appropriate files (usually <code>lib/*/version.rb</code>)</p> </li> <li> <p>Update changelog: Document changes in <code>CHANGELOG.md</code> and update the release date to the current date</p> </li> <li> <p>Update README: If needed, update README.md with any new features, changes, or version-specific information</p> </li> <li> <p>Test locally: Run the full test suite and ensure everything works as expected</p> </li> <li> <p>Create PR: Submit changes via pull request with the version branch (<code>v\"VERSION\"</code>) and get it reviewed/merged    - PR title should be clear (e.g., \"Release v1.2.3\")    - Include a summary of changes in the PR description</p> </li> </ol>"}, {"location": "Development-Gems-Publishing/#step-2-create-github-release", "title": "Step 2: Create GitHub Release", "text": "<p>Important: Only proceed after the version PR has been merged to the master/main branch.</p> <ol> <li>Navigate to Releases: Go to the repository's \"Releases\" section</li> </ol> <p> </p> <ol> <li> <p>Create New Release: Click \"Draft a new release\"</p> </li> <li> <p>Set Tag: Create a new tag that exactly matches your version branch name (e.g., <code>v1.2.3</code>, <code>v2.0.0.beta.1</code>)</p> </li> </ol> <ul> <li>Critical: The tag must start with <code>v</code> to trigger the workflow</li> <li>The tag should match the version you just merged</li> </ul> <ol> <li>Fill Release Details:</li> </ol> <ul> <li>Release title: Usually the same as the tag (e.g., \"v1.2.3\")</li> <li>Description: Copy relevant sections from the updated changelog</li> <li>Pre-release: Check if this is a pre-release version</li> </ul> <p> </p> <ol> <li>Publish Release: Click \"Publish release\" - this will trigger the push workflow</li> </ol>"}, {"location": "Development-Gems-Publishing/#step-3-approve-and-monitor-workflow", "title": "Step 3: Approve and Monitor Workflow", "text": "<p>After publishing the GitHub release, the push workflow will be triggered and require approval:</p> <ol> <li>Check Actions Tab: Immediately navigate to the \"Actions\" tab in the repository</li> </ol> <p> </p> <ol> <li> <p>Find Workflow Run: Look for the newly triggered \"Push Gem\" workflow run that corresponds to your release tag</p> </li> <li> <p>Approve the Workflow: The workflow will be waiting for approval    - Click on the workflow run    - Click \"Review deployments\"     - Select the \"deployment\" environment    - Click \"Approve and deploy\"</p> </li> </ol> <p> </p> <ol> <li>Monitor Execution: Watch the workflow execution in real-time    - Monitor each step for successful completion    - Check logs if any step fails    - The entire process should take a few minutes</li> </ol> <p> </p>"}, {"location": "Development-Gems-Publishing/#step-4-verify-release", "title": "Step 4: Verify Release", "text": "<ol> <li>Check RubyGems: Visit the gem's page on https://rubygems.org to confirm the new version is published</li> <li>Test Installation: Try installing the gem locally: <code>gem install gem_name -v new_version</code></li> <li>Automatic Dependencies Update: There is no need for manual dependency updating since Renovate will do it automatically within 24 hours.</li> </ol>"}, {"location": "Development-Gems-Publishing/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "Development-Gems-Publishing/#common-issues", "title": "Common Issues", "text": "<p>Workflow doesn't trigger:</p> <ul> <li>Ensure tag starts with <code>v</code></li> <li>Check that the tag was created properly</li> <li>Verify you have push access to the repository</li> <li>Check if the run was approved</li> </ul> <p>Trusted publisher error:</p> <ul> <li>Verify the GitHub environment name matches RubyGems configuration</li> <li>Check that the workflow path is correct in RubyGems settings</li> <li>Ensure the repository name matches exactly</li> </ul> <p>Permission denied:</p> <ul> <li>Confirm you have release permissions for the repository</li> <li>Verify the <code>github.repository_owner == 'karafka'</code> condition</li> </ul> <p>Gem already exists:</p> <ul> <li>Check if the version was already released</li> <li>Ensure the version was properly bumped before creating the release</li> <li>Consider using a patch version if needed</li> </ul>"}, {"location": "Development-Gems-Publishing/#security-notes", "title": "Security Notes", "text": "<ul> <li>No API keys needed: Trusted publishing eliminates the need for long-lived RubyGems API keys</li> <li>Scoped access: OIDC tokens are automatically scoped to the specific repository and workflow</li> <li>Audit trail: All releases are tracked through GitHub Actions with full logs</li> </ul> <p>Last modified: 2025-05-27 11:21:53</p>"}, {"location": "Development-Naming-Conventions/", "title": "Karafka Naming Conventions", "text": "<p>This document establishes comprehensive naming conventions for the Karafka ecosystem. Consistent naming improves code readability, maintainability, and team collaboration while ensuring alignment with broader Kafka community practices.</p>"}, {"location": "Development-Naming-Conventions/#overview", "title": "Overview", "text": "<p>Naming conventions serve several critical purposes:</p> Purpose Benefit Consistency Predictable patterns across the codebase Clarity Self-documenting code that's easier to understand Maintainability Easier refactoring and debugging Community Alignment Following established Kafka ecosystem patterns Scalability Conventions that work as systems grow"}, {"location": "Development-Naming-Conventions/#topic-partition-naming", "title": "Topic-Partition Naming", "text": ""}, {"location": "Development-Naming-Conventions/#core-principles", "title": "Core Principles", "text": "Principle Description Consistency First Use the same format consistently within each context Community Alignment Follow established Kafka ecosystem conventions Context Awareness Different contexts may require different formats Future-Proof Choose formats that scale well as systems grow"}, {"location": "Development-Naming-Conventions/#single-topic-partition-references", "title": "Single Topic-Partition References", "text": ""}, {"location": "Development-Naming-Conventions/#dash-format-topic-partition", "title": "Dash Format: <code>topic-partition</code>", "text": "<p>Use the dash format when referring to a specific topic-partition pair as a unique identifier.</p> <p>When to Use:</p> <ul> <li>Log file references</li> <li>Metric names and identifiers</li> <li>Database keys or unique identifiers</li> <li>Internal system references</li> <li>Directory names</li> <li>Cache keys</li> </ul> <p>Examples:</p> <pre><code>my-events-0\nuser-activity-5\norder-updates-12\nsystem-metrics-0\n</code></pre> <p>Code Examples:</p> <pre><code># Metric collection\nmetrics[\"#{topic}-#{partition}.lag\"] = lag_value\n\n# Log file naming\nlog_file = \"logs/#{topic}-#{partition}.log\"\n\n# Cache key generation\ncache_key = \"offset:#{topic}-#{partition}\"\n</code></pre>"}, {"location": "Development-Naming-Conventions/#bracket-format-topic-partition", "title": "Bracket Format: <code>topic-[partition]</code>", "text": "<p>Use the bracket format when the context suggests this is part of a collection, even if currently only one partition.</p> <p>When to Use:</p> <ul> <li>Consumer assignments (even single partition)</li> <li>Partition lists in configuration</li> <li>Assignment displays in UI</li> <li>Contexts where partitions might be added/removed</li> </ul> <p>Examples:</p> <pre><code>my-events-[0]\nuser-activity-[5]\norder-updates-[12]\n</code></pre>"}, {"location": "Development-Naming-Conventions/#multiple-topic-partition-references", "title": "Multiple Topic-Partition References", "text": ""}, {"location": "Development-Naming-Conventions/#multiple-partitions-topic-partitionpartition", "title": "Multiple Partitions: <code>topic-[partition,partition,...]</code>", "text": "<p>Use square brackets with comma-separated partition numbers for multiple partitions of the same topic.</p> <p>Examples:</p> <pre><code>my-events-[0,1,2]\nuser-activity-[0,1,2,3,4,5]\norder-updates-[0,1,2,3,4,5,6,7,8,9,10,11]\n</code></pre> <p>UI Display Examples:</p> <pre><code>Consumer Group: my-consumer-group\n\u251c\u2500\u2500 my-events-[0,1,2]\n\u251c\u2500\u2500 user-activity-[3,4,5]\n\u2514\u2500\u2500 notifications-[0]\n</code></pre>"}, {"location": "Development-Naming-Conventions/#range-notation-topic-start-end", "title": "Range Notation: <code>topic-[start-end]</code>", "text": "<p>For consecutive partition ranges, you may use range notation for brevity.</p> <p>Examples:</p> <pre><code>my-events-[0-2]        # Equivalent to my-events-[0,1,2]\nuser-activity-[0-5]    # Equivalent to user-activity-[0,1,2,3,4,5]\nlarge-topic-[0-99]     # For topics with many consecutive partitions\n</code></pre> <p>When to Use Range Notation:</p> <ul> <li>Consecutive partitions only</li> <li>When list would be very long (10+ partitions)</li> <li>Configuration files where brevity helps readability</li> </ul>"}, {"location": "Development-Naming-Conventions/#cross-topic-references", "title": "Cross-Topic References", "text": ""}, {"location": "Development-Naming-Conventions/#multiple-topics-with-partitions", "title": "Multiple Topics with Partitions", "text": "<p>When showing assignments across multiple topics, use consistent formatting for each topic.</p> <p>Examples:</p> <pre><code>orders-[0,1,2], payments-[0,1], notifications-[0]\nevents-[0-9], metrics-[0-4], logs-[0]\n</code></pre>"}, {"location": "Development-Naming-Conventions/#topic-lists-without-partition-details", "title": "Topic Lists Without Partition Details", "text": "<p>When partition information isn't relevant, use simple topic names.</p> <p>Examples:</p> <pre><code>Topics: orders, payments, notifications\nSubscribed to: events, metrics, logs\n</code></pre>"}, {"location": "Development-Naming-Conventions/#context-specific-conventions", "title": "Context-Specific Conventions", "text": ""}, {"location": "Development-Naming-Conventions/#consumer-group-assignments", "title": "Consumer Group Assignments", "text": "<p>Use bracket notation to clearly show which partitions each consumer is handling.</p> <pre><code>Consumer Group: order-processing-group\nConsumer 1: orders-[0,2,4], payments-[0]\nConsumer 2: orders-[1,3,5], payments-[1]\nConsumer 3: notifications-[0]\n</code></pre>"}, {"location": "Development-Naming-Conventions/#rebalancing-logs", "title": "Rebalancing Logs", "text": "<p>Use bracket notation in rebalancing scenarios to show partition movements.</p> <pre><code>[INFO] Rebalancing started\n[INFO] Revoking: orders-[0,2,4], payments-[0]\n[INFO] Assigning: orders-[1,3], payments-[1], notifications-[0]\n[INFO] Rebalancing completed\n</code></pre>"}, {"location": "Development-Naming-Conventions/#monitoring-and-metrics", "title": "Monitoring and Metrics", "text": "<p>Use dash notation for metric names and identifiers.</p> <p>Metric Names:</p> <pre><code>kafka.consumer.lag.orders-0\nkafka.consumer.lag.orders-1\nkafka.producer.rate.events-5\n</code></pre> <p>Monitoring Displays:</p> <pre><code>Topic-Partition    | Current Offset | Lag\norders-0           | 15,432         | 0\norders-1           | 12,891         | 3\norders-2           | 18,765         | 1\n</code></pre>"}, {"location": "Development-Naming-Conventions/#error-messages-and-logging", "title": "Error Messages and Logging", "text": "<p>Use appropriate format based on context - dash for specific references, brackets for assignments.</p> <p>Error Message Examples:</p> <pre><code># Specific partition error\nlogger.error \"Failed to process message from orders-3\"\n\n# Assignment context error  \nlogger.error \"Cannot assign orders-[0,1,2] to consumer: already assigned\"\n\n# Rebalancing context\nlogger.info \"Consumer assigned: orders-[1,3,5], payments-[0]\"\n</code></pre>"}, {"location": "Development-Naming-Conventions/#api-responses-and-data-structures", "title": "API Responses and Data Structures", "text": "<p>Use bracket format for displaying assignments. For structured data, nest partitions within topic hashes following librdkafka conventions.</p> <p>JSON API Response:</p> <pre><code>{\n  \"consumer_id\": \"consumer-123\",\n  \"assigned_partitions\": \"orders-[0,1,2], payments-[0]\",\n  \"partition_details\": {\n    \"orders\": {\n      \"0\": { \"offset\": 1542, \"lag\": 0 },\n      \"1\": { \"offset\": 1123, \"lag\": 2 },\n      \"2\": { \"offset\": 1876, \"lag\": 1 }\n    },\n    \"payments\": {\n      \"0\": { \"offset\": 892, \"lag\": 0 }\n    }\n  }\n}\n</code></pre>"}, {"location": "Development-Naming-Conventions/#advanced-scenarios", "title": "Advanced Scenarios", "text": ""}, {"location": "Development-Naming-Conventions/#dead-letter-queues", "title": "Dead Letter Queues", "text": "<p>Use consistent naming for DLQ topic-partition references.</p> <pre><code>Original: orders-0\nDLQ: orders-dlq-0\n\nOriginal: user-events-[0,1,2]\nDLQ: user-events-dlq-[0,1,2]\n</code></pre>"}, {"location": "Development-Naming-Conventions/#compacted-topics", "title": "Compacted Topics", "text": "<p>Apply same conventions regardless of topic configuration.</p> <pre><code>user-profiles-0      # Single partition compacted topic\nuser-profiles-[0,1,2] # Multi-partition compacted topic\n</code></pre>"}, {"location": "Development-Naming-Conventions/#schema-registry-integration", "title": "Schema Registry Integration", "text": "<p>Use dash format for schema subject naming that includes partition info.</p> <pre><code>orders-0-value\norders-0-key\nuser-events-5-value\n</code></pre>"}, {"location": "Development-Naming-Conventions/#implementation-guidelines", "title": "Implementation Guidelines", "text": "Context Format Example Single specific reference <code>topic-partition</code> (dash) <code>orders-0</code> Single in assignment context <code>topic-[partition]</code> (brackets) <code>orders-[0]</code> Multiple partitions <code>topic-[partition,partition,...]</code> (brackets with commas) <code>orders-[0,1,2]</code> Ranges <code>topic-[start-end]</code> (brackets with dash for range) <code>orders-[0-9]</code> Metrics/IDs Always use dash format <code>kafka.lag.orders-0</code> Assignments/UI Always use bracket format <code>orders-[0,1,2]</code> <p>Key Guidelines:</p> <ul> <li>Be consistent within each context</li> <li>Choose format based on whether it's an identifier or a collection</li> </ul>"}, {"location": "Development-Naming-Conventions/#integration-and-unit-test-topics", "title": "Integration and Unit Test Topics", "text": "<p>All topics used in integration and unit tests across the Karafka ecosystem should follow a consistent naming pattern to enable easy cleanup and maintenance of test environments.</p>"}, {"location": "Development-Naming-Conventions/#test-topic-prefix-convention", "title": "Test Topic Prefix Convention", "text": "<p>All test topics must use the <code>it-</code> prefix (short for \"integration test\").</p> <p>This naming convention allows for simple cleanup operations using regex matching.</p>"}, {"location": "Development-Naming-Conventions/#standard-test-topic-format", "title": "Standard Test Topic Format", "text": ""}, {"location": "Development-Naming-Conventions/#primary-pattern-it-uuid", "title": "Primary Pattern: <code>it-UUID</code>", "text": "<p>Most test topics should follow the UUID-based naming pattern for uniqueness and isolation:</p> <pre><code># Good: UUID-based test topics\ntest_topic = \"it-#{SecureRandom.uuid}\"\n# Example: \"it-f47ac10b-58cc-4372-a567-0e02b2c3d479\"\n\n# RSpec example\nRSpec.describe SomeConsumer do\n  let(:topic_name) { \"it-#{SecureRandom.uuid}\" }\n\n  before do\n    create_topic(topic_name)\n  end\nend\n</code></pre> <p>Benefits of UUID-based naming:</p> <ul> <li>Guaranteed uniqueness across parallel test runs</li> <li>No conflicts between different test suites</li> <li>Safe for concurrent execution</li> <li>Easy to identify and clean up</li> </ul>"}, {"location": "Development-Naming-Conventions/#secondary-pattern-it-descriptive-name", "title": "Secondary Pattern: <code>it-descriptive-name</code>", "text": "<p>When tests require specific, predictable topic names, use descriptive names with the <code>it-</code> prefix:</p> <pre><code># Acceptable: When specific topic names are needed\ntest_topic = \"it-user-events-compacted\"\ntest_topic = \"it-orders-partitioned-by-region\"\ntest_topic = \"it-dead-letter-queue-testing\"\n\n# Configuration testing example\nRSpec.describe TopicConfiguration do\n  let(:compacted_topic) { \"it-user-profiles-compacted\" }\n  let(:partitioned_topic) { \"it-events-high-throughput\" }\nend\n</code></pre> <p>When to use descriptive names:</p> <ul> <li>Testing specific topic configurations (compaction, partitioning strategies)</li> <li>Integration tests that verify topic-specific behavior</li> <li>Tests that need to reference topics by predictable names</li> <li>Cross-component integration tests</li> </ul>"}, {"location": "Development-Naming-Conventions/#implementation-examples", "title": "Implementation Examples", "text": "<p>Test Setup Patterns:</p> <pre><code># Pattern 1: UUID-based (preferred)\nRSpec.describe MessageProcessor do\n  let(:input_topic) { \"it-#{SecureRandom.uuid}\" }\n  let(:output_topic) { \"it-#{SecureRandom.uuid}\" }\n  let(:dlq_topic) { \"it-#{SecureRandom.uuid}-dlq\" }\nend\n\n# Pattern 2: Descriptive names when needed\nRSpec.describe SchemaEvolution do\n  let(:versioned_topic) { \"it-schema-evolution-v2\" }\n  let(:backward_compat_topic) { \"it-schema-backward-compat\" }\nend\n\n# Pattern 3: Mixed approach\nRSpec.describe RebalancingBehavior do\n  let(:base_uuid) { SecureRandom.uuid }\n  let(:consumer_topic_1) { \"it-#{base_uuid}-consumer-1\" }\n  let(:consumer_topic_2) { \"it-#{base_uuid}-consumer-2\" }\nend\n</code></pre>"}, {"location": "Development-Naming-Conventions/#special-cases-and-exceptions", "title": "Special Cases and Exceptions", "text": "<p>Limited Exceptions</p> <p>There are a few existing special cases in the Karafka ecosystem where the <code>it-</code> prefix is not used. These exceptions are legacy patterns and should not be replicated in new code:</p> <ul> <li>Some specific integration tests may use different prefixes</li> <li>Certain benchmarking or performance tests may have custom naming</li> <li>Cross-platform compatibility tests might use specific formats</li> </ul> <p>Important Guidelines:</p> <ul> <li>Do not introduce new exceptions without explicit approval</li> <li>All new test code should follow the <code>it-</code> prefix convention</li> <li>When refactoring existing tests, migrate to the standard convention when possible</li> <li>Document any unavoidable exceptions with clear justification</li> </ul>"}, {"location": "Development-Naming-Conventions/#best-practices-summary", "title": "Best Practices Summary", "text": "Practice Description Always use <code>it-</code> prefix For all new test topics Prefer UUID-based naming For maximum isolation and uniqueness Use descriptive names sparingly Only when testing requires specific topic characteristics Clean up regularly Using regex-based deletion commands Integrate cleanup Into CI/CD pipelines Avoid creating new exceptions To the naming convention Document any unavoidable exceptions With clear reasoning Consider environment isolation For additional safety in shared development environments <p>This convention ensures that test topics can be easily identified, managed, and cleaned up across the entire Karafka ecosystem while maintaining clear separation from production topics.</p> <p>Last modified: 2025-06-10 15:19:33</p>"}, {"location": "Development-Native-Extensions/", "title": "Native Extensions: Precompiled rdkafka-ruby", "text": "<p>Karafka uses the <code>rdkafka-ruby</code> gem, which includes a native C extension that wraps the librdkafka library. To provide faster and more reliable installation, we distribute native extensions as precompiled gems (also called \"native gems\") for major platforms.</p> <p>This eliminates the need to compile C extensions during installation, resulting in significantly faster gem installation and removing build dependency requirements.</p>"}, {"location": "Development-Native-Extensions/#what-are-native-extensions", "title": "What Are Native Extensions?", "text": "<p>Native extensions are platform-specific compiled binaries that contain:</p> <ul> <li>Pre-compiled librdkafka libraries with all dependencies statically linked</li> <li>Self-contained binaries that don't require system dependencies</li> <li>Cryptographically verified dependencies with SHA256 checksums for supply chain security</li> <li>Full feature support including SSL/TLS, SASL, Kerberos/GSSAPI, and compression</li> </ul>"}, {"location": "Development-Native-Extensions/#supported-platforms", "title": "Supported Platforms", "text": "<p>rdkafka-ruby ships native extensions for the following platforms:</p> <ul> <li>Linux: <code>x86_64-linux-gnu</code>, <code>x86_64-linux-musl</code></li> <li>macOS: <code>arm64-darwin</code> (Apple Silicon)</li> </ul> <p>To check if your platform is supported, run:</p> <pre><code>ruby -e 'puts Gem::Platform.local.to_s'\n</code></pre>"}, {"location": "Development-Native-Extensions/#installation-benefits", "title": "Installation Benefits", "text": "<p>Benefits:</p> <ul> <li>10-100x faster installation (seconds instead of minutes)</li> <li>More reliable - no compilation failures</li> <li>No build dependencies required - works in minimal containers</li> <li>Enhanced security - all dependencies cryptographically verified</li> <li>Cloud-ready - perfect for Docker, AWS Lambda, etc.</li> </ul>"}, {"location": "Development-Native-Extensions/#without-native-extensions-fallback", "title": "Without Native Extensions (Fallback)", "text": "<pre><code>$ gem install rdkafka --platform=ruby\n# Downloads source gem and compiles during installation\n# Requires: build tools, librdkafka, OpenSSL, SASL, Kerberos, etc.\n</code></pre>"}, {"location": "Development-Native-Extensions/#whats-included-in-native-extensions", "title": "What's Included in Native Extensions", "text": "<p>Each native extension includes a self-contained librdkafka library with:</p>"}, {"location": "Development-Native-Extensions/#core-dependencies-statically-linked", "title": "Core Dependencies (Statically Linked)", "text": "<ul> <li>librdkafka - The core Kafka client library</li> <li>OpenSSL - SSL/TLS encryption support</li> <li>Cyrus SASL - Authentication mechanisms (PLAIN, SCRAM, GSSAPI)</li> <li>MIT Kerberos - Kerberos/GSSAPI authentication for enterprise environments</li> <li>zlib - Standard compression</li> <li>ZStd - High-performance compression</li> </ul>"}, {"location": "Development-Native-Extensions/#build-process-and-security", "title": "Build Process and Security", "text": ""}, {"location": "Development-Native-Extensions/#supply-chain-security", "title": "Supply Chain Security", "text": "<p>All dependencies are verified with SHA256 checksums during the build process:</p> <pre><code># Example from build process\n[SECURITY] Verifying checksum for openssl-3.0.16.tar.gz...\n[SECURITY] \u2705 Checksum verified for openssl-3.0.16.tar.gz\n[SECURITY] \ud83d\udd12 SECURITY VERIFICATION COMPLETE\n[SECURITY] All dependencies downloaded and verified with SHA256 checksums\n</code></pre>"}, {"location": "Development-Native-Extensions/#automated-build-pipeline", "title": "Automated Build Pipeline", "text": "<p>Native extensions are built using GitHub Actions with:</p> <ol> <li>Dependency Download: All dependencies downloaded from official sources</li> <li>Checksum Verification: SHA256 verification for supply chain security  </li> <li>Static Compilation: All dependencies statically linked</li> <li>Self-Contained Packaging: No external dependencies required</li> <li>Automated Testing: Comprehensive test suite across Ruby versions</li> <li>Trusted Publishing: Cryptographic attestation via RubyGems</li> </ol>"}, {"location": "Development-Native-Extensions/#docker-and-container-usage", "title": "Docker and Container Usage", "text": "<p>Native extensions are ideal for containerized applications:</p> <pre><code>FROM ruby:3.4-slim\n\n# No build dependencies needed!\nRUN gem install karafka\n\nCOPY . /app\nWORKDIR /app\n</code></pre>"}, {"location": "Development-Native-Extensions/#before-with-compilation", "title": "Before (with compilation)", "text": "<pre><code>FROM ruby:3.4\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    librdkafka-dev \\\n    libsasl2-dev \\\n    libssl-dev\n\nRUN gem install karafka  # 2-15 minutes\n</code></pre>"}, {"location": "Development-Native-Extensions/#after-with-native-extensions", "title": "After (with native extensions)", "text": "<pre><code>FROM ruby:3.4-slim\n\nRUN gem install karafka  # 3-10 seconds\n</code></pre>"}, {"location": "Development-Native-Extensions/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "Development-Native-Extensions/#force-native-extension-installation", "title": "Force Native Extension Installation", "text": "<pre><code># Explicitly request native extension\ngem install rdkafka --platform=x86_64-linux-gnu\n\n# Or in Gemfile\ngem 'rdkafka', platforms: [:x86_64_linux_gnu]\n</code></pre>"}, {"location": "Development-Native-Extensions/#fallback-to-source-compilation", "title": "Fallback to Source Compilation", "text": "<p>If native extensions don't work for your platform:</p> <pre><code># Force source compilation\ngem install rdkafka --platform=ruby\n\n# Or in Gemfile\ngem 'rdkafka', platforms: [:ruby]\n</code></pre>"}, {"location": "Development-Native-Extensions/#source-compilation-fallback", "title": "Source Compilation (Fallback)", "text": "<p>If you need to use source compilation instead of native extensions (e.g., for custom configurations or unsupported platforms):</p>"}, {"location": "Development-Native-Extensions/#force-source-compilation", "title": "Force Source Compilation", "text": "<pre><code># Force source compilation\ngem install rdkafka --platform=ruby\n\n# Or in Gemfile\ngem 'rdkafka', platforms: [:ruby]\n</code></pre>"}, {"location": "Development-Native-Extensions/#migration-from-source-compilation", "title": "Migration from Source Compilation", "text": "<p>If you're currently using source compilation:</p> <ol> <li>Remove build dependencies from your Dockerfile/CI</li> <li>Update Gemfile to allow native extensions:    <code>ruby    # Remove platform restrictions    gem 'rdkafka'  # Will automatically use native extensions</code></li> <li>Rebuild your containers - they'll be much faster!</li> </ol> <p>Last modified: 2025-07-02 12:10:36</p>"}, {"location": "Development-Precompilation/", "title": "Precompilation of Rdkafka - Mission Accomplished!", "text": "<p>As of 2025, native extensions (precompiled binaries) are now available for the rdkafka gem! This page documents the successful completion of one of the most challenging tasks in the Karafka ecosystem.</p>"}, {"location": "Development-Precompilation/#the-challenge-solved", "title": "The Challenge (Solved)", "text": "<p>The <code>rdkafka</code> gem previously required 60-90 seconds of compilation during installation. Here's the before and after:</p> Area Before After Docker build times \u274c 60-90 seconds compilation \u2705 3-10 seconds installation Development setup \u274c Requires build dependencies \u2705 No build dependencies needed Deployment processes \u274c Compilation failures possible \u2705 Reliable installation Developer productivity \u274c Slow, error-prone installs \u2705 10-100x faster installation"}, {"location": "Development-Precompilation/#why-this-was-complex", "title": "Why This Was Complex", "text": ""}, {"location": "Development-Precompilation/#abi-compatibility-hell", "title": "ABI Compatibility Hell", "text": "<p>Unlike simpler gems, rdkafka has a complex dependency web that created ABI (Application Binary Interface) compatibility challenges:</p> <p>System Library Dependencies:</p> <ul> <li>Different glibc/musl versions across Linux distributions</li> <li>OpenSSL versions (1.1.x vs 3.0+) with breaking changes</li> <li>Compression libraries (zlib, lz4, zstd, snappy) with varying versions</li> <li>SASL libraries for authentication mechanisms</li> <li>Different regex engines</li> </ul> <p>Platform-Specific Variations:</p> <ul> <li>Package manager differences (<code>apt</code>, <code>yum</code>, <code>apk</code>, <code>brew</code>, <code>nix</code>)</li> <li>Custom library installations in non-standard locations</li> <li>FIPS compliance requirements in enterprise environments</li> </ul>"}, {"location": "Development-Precompilation/#platform-matrix-explosion", "title": "Platform Matrix Explosion", "text": "<p>Supporting precompiled gems properly required:</p> <ul> <li>Ruby versions: All actively maintained versions</li> <li>Primary platforms: <ul> <li>x86_64-linux-gnu (Ubuntu/Debian/RHEL)</li> <li>x86_64-linux-musl (Alpine)</li> <li>arm64-darwin (Apple Silicon Mac)</li> </ul> </li> </ul>"}, {"location": "Development-Precompilation/#security-considerations", "title": "Security Considerations", "text": "<p>Precompiled native extensions presented legitimate security concerns:</p> <p>Supply Chain Security:</p> <ul> <li>Users cannot easily inspect compiled binaries</li> <li>Trust must be placed in the build process</li> <li>Potential for malicious code injection during compilation</li> <li>FIPS compliance requirements in regulated industries</li> </ul>"}, {"location": "Development-Precompilation/#what-was-accomplished", "title": "What Was Accomplished", "text": "Feature Description \u2705 Native extensions Available for major platforms (Linux, macOS, Windows) \u2705 Self-contained libraries All dependencies statically linked \u2705 Supply chain security SHA256 verification for all dependencies \u2705 Enterprise features Kerberos, SASL, SSL/TLS included \u2705 Automatic fallback Source compilation when needed \u2705 Cryptographic attestation RubyGems Trusted Publishing"}, {"location": "Development-Precompilation/#project-status", "title": "Project Status", "text": "Phase Description Status Phase 1 Foundation (Trusted Publishing, Build Infrastructure) \u2705 COMPLETED Phase 2 Core Platform Support (Linux, macOS) \u2705 COMPLETED Phase 3 Extended Platform Support (ARM64, Additional Variants) \u2705 COMPLETED <p>For complete documentation on using native extensions, see: Native Extensions</p> <p>Note: This represents the successful completion of a multi-month effort to solve one of Ruby's most complex native extension challenges. The Karafka ecosystem now provides installation speeds comparable to pure Ruby gems while maintaining full native library functionality.</p> <p>Last modified: 2025-07-02 12:10:36</p>"}, {"location": "Development-vs-Production/", "title": "Development vs Production", "text": "<p>When working with Karafka and Kafka, it's essential to understand the nuances between development (<code>development</code> and <code>test</code> environments) and production. Awareness of these differences ensures a smoother work experience and optimal system performance. Here's a detailed breakdown of some of the crucial considerations to keep in mind:</p>"}, {"location": "Development-vs-Production/#avoid-using-karafkas-reload-mode-in-production", "title": "Avoid Using Karafka's Reload Mode in Production", "text": "<p>While Karafka offers a reload mode, which can be very helpful during development, it's crucial not to use this in a production environment. This mode can impact the performance and stability of your system. Always ensure that this mode is disabled before deploying to production.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other settings...\n\n    # Recreate consumers with each batch. This will allow Rails code reload to work in the\n    # development mode. Otherwise Karafka process would not be aware of code changes\n    # It is recommended to have persistence turned on for any non-dev environment\n    config.consumer_persistence = !Rails.env.development?\n  end\nend\n</code></pre>"}, {"location": "Development-vs-Production/#pre-create-necessary-topics-in-the-production-kafka-cluster", "title": "Pre-create Necessary Topics in the Production Kafka Cluster", "text": "<p>Kafka topics act as communication channels for your messages. It is best to create all the required topics in your production Kafka cluster upfront. Doing so ensures no interruptions or issues when your application starts sending or receiving messages and that your topics have the desired number of partitions. You can use Declarative Topics functionality for that.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :system_events do\n      config(\n        partitions: 6,\n        replication_factor: 3,\n        'retention.ms': 86_400_000 # 1 day in ms,\n        'cleanup.policy': 'delete',\n        'compression.codec': 'gzip'\n      )\n\n      consumer EventsConsumer\n    end\n  end\nend\n</code></pre>"}, {"location": "Development-vs-Production/#disable-automatic-topic-creation-in-production", "title": "Disable Automatic Topic Creation in Production", "text": "<p>When set to true, the <code>allow.auto.create.topics</code> setting enables Kafka to create topics automatically. However, it's recommended not to rely on this feature in a production environment. It's more controlled and predictable to manually set up your topics, ensuring they are configured correctly for your production needs.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'allow.auto.create.topics': !Rails.env.production?\n    }\n  end\nend\n</code></pre>"}, {"location": "Development-vs-Production/#lock-your-topics-list-in-development-after-stabilization", "title": "Lock Your Topics List in Development After Stabilization", "text": "<p>As you develop and test, you may often modify your list of Kafka topics and their settings. However, once you stabilize your topic list, it's a good idea to lock it. Typos and minor errors can easily be overlooked, leading to potential issues propagating to production.</p>"}, {"location": "Development-vs-Production/#be-cautious-with-the-default-single-partition-for-auto-created-topics", "title": "Be Cautious with the Default Single Partition for Auto-created Topics", "text": "<p>Topics that are automatically created because of <code>allow.auto.create.topics</code> are assigned just one partition by default. While this may suffice for development purposes, production environments often require multiple partitions for better performance and scalability. Ensure you configure your topics' appropriate number of partitions before deploying to production.</p>"}, {"location": "Development-vs-Production/#consider-the-impact-of-rolling-deployments-on-rebalances", "title": "Consider the Impact of Rolling Deployments on Rebalances", "text": "<p>Whenever you do a rolling deployment of <code>N</code> processes, expect <code>N</code> rebalances to occur. Rebalances can impact the performance and stability of your Kafka cluster. However, using the <code>cooperative-sticky</code> rebalance strategy can mitigate some of these issues.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'partition.assignment.strategy': 'cooperative-sticky'\n    }\n  end\nend\n</code></pre>"}, {"location": "Development-vs-Production/#manual-topic-creation-and-consumer-starting-sequence", "title": "Manual Topic Creation and Consumer Starting Sequence", "text": "<p>Creating a topic manually or by sending the first message and then initiating a consumer is recommended. While Karafka does refresh cluster metadata information to detect new topics, this process can sometimes take over five minutes. Ensuring that the topic exists before starting a consumer reduces potential delays.</p>"}, {"location": "Development-vs-Production/#adjust-topic-metadata-refresh-interval-for-production", "title": "Adjust Topic Metadata Refresh Interval for Production", "text": "<p>In the development environment, the <code>topic.metadata.refresh.interval.ms</code> setting defaults to 5 seconds. This means Karafka quickly discovers any topic created after starting the Karafka service. However, in production, this short interval is not recommended. The default value for a production environment should be 5 minutes to reduce unnecessary overhead.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'topic.metadata.refresh.interval.ms': 5 * 60 * 1_000\n    }\n  end\nend\n</code></pre>"}, {"location": "Development-vs-Production/#opt-for-the-cooperative-sticky-rebalance-strategy-in-production", "title": "Opt for the cooperative-sticky Rebalance Strategy in Production", "text": "<p>The <code>cooperative-sticky</code> rebalance strategy set via the <code>partition.assignment.strategy</code> configuration is highly recommended for production environments. It offers better performance and stability compared to other rebalance strategies.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'partition.assignment.strategy': 'cooperative-sticky'\n    }\n  end\nend\n</code></pre>"}, {"location": "Development-vs-Production/#set-the-compressioncodec-for-both-topicbroker-settings-and-karafka", "title": "Set the <code>compression.codec</code> for Both Topic/Broker Settings and Karafka", "text": "<p>The <code>compression.codec</code> parameter in Kafka's configuration allows you to specify the compression algorithm to be used for messages. Kafka supports multiple compression algorithms like GZIP, LZ4, and Snappy. Karafka also honors the compression settings.</p> <p>There are several reasons why you should configure compression for your production environments and why it needs to be set on both Kafka and Karafka levels:</p> <ul> <li> <p>Network Traffic Volume Reduction: One of the main benefits of compression is to reduce the amount of data transmitted over the network. When producers send compressed data to the broker, and consumers receive it, it reduces the bandwidth utilized. Remember that compression needs to be set for both Kafka topics and Karafka to ensure data is being compressed before it is sent over the wire. Otherwise, the compression will occur only on the broker, and no network traffic savings will occur.</p> </li> <li> <p>Consistency: Keeping the compression setting consistent between producers, consumers, and brokers ensures the data is uniformly compressed throughout its lifecycle. This minimizes issues related to unsupported compression formats or mismatched compression expectations.</p> </li> <li> <p>Performance &amp; Storage: Compressed data is typically smaller, leading to better storage efficiency on the broker side and quicker transmission times.</p> </li> </ul> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'compression.codec': 'gzip'\n    }\n  end\nend\n</code></pre>"}, {"location": "Development-vs-Production/#avoid-rolling-upgrades-for-partitionassignmentstrategy-changes", "title": "Avoid Rolling Upgrades for <code>partition.assignment.strategy</code> Changes", "text": "<p>The <code>partition.assignment.strategy</code> in Kafka determines how topic partitions are allocated amongst the consumers in a consumer group. Adjusting this strategy can influence the distribution of partitions and, thus, the performance and efficiency of your consumers.</p> <p>When you switch between assignment strategies, be aware that:</p> <ol> <li> <p>Deployment Concerns: Direct strategy shifts using rolling upgrades can result in conflicts. Running consumers with distinct assignment strategies within the same group will trigger an \"Inconsistent group protocol\" error, \"assignors must have the same protocol type\" error or similar</p> </li> <li> <p>Performance Variations: Different strategies can lead to diverse load distributions, influencing the processing efficiency of individual consumers.</p> </li> <li> <p>Potential for Uneven Distribution: Some strategies might result in specific consumers being assigned a larger share of partitions, leading to uneven work distribution.</p> </li> <li> <p>Compatibility Concerns: Ensure the chosen strategy is compatible with your Kafka broker version. Some strategies might be exclusive to specific Kafka versions.</p> </li> </ol> <p>To ensure a smooth transition when adjusting the assignment strategy, follow these steps:</p> <ol> <li> <p>Backup Configuration: Initiate the process by backing up your existing Kafka and Karafka configurations. This creates a recovery point in case complications arise.</p> </li> <li> <p>Test in a Non-Production Environment: Before rolling out changes in a live setting, validate the new strategy in a controlled, non-production environment.</p> </li> <li> <p>Shutdown Consumers: To sidestep the \"Inconsistent group protocol\" error and other critical issues, stop all consumers in the consumer group before enacting the change.</p> </li> <li> <p>Update Strategy &amp; Restart: Modify the <code>partition.assignment.strategy</code> with all consumers offline. Once adjusted, you can bring all consumers back online.</p> </li> <li> <p>Monitor Behavior: Post-transition, maintain rigorous oversight of the consumer behaviors. Specifically, observe for unexpected rebalances or any imbalances in partition assignments.</p> </li> </ol> <p>To recap, while modifying <code>partition.assignment.strategy</code> in Karafka may promise enhanced consumer efficiency, the transition demands solid planning and execution. With the insights and procedure outlined above, you're equipped to undertake the shift methodically and with minimal disruption.</p>"}, {"location": "Development-vs-Production/#messages-from-the-future-time-drift-problem", "title": "Messages from the Future / Time Drift Problem", "text": "<p>In Kafka, every message (or record) produced to a topic carries metadata, including a timestamp. This timestamp is set by the producer or the broker when the message gets appended to the log. Consumers then use the timestamp to understand the chronological order of messages.</p> <p>Under regular operations, this system works seamlessly. However, problems arise when there's a time drift between the Kafka cluster nodes and the consumer. Essentially, if the Kafka broker believes it's 2:00 PM, while the consumer thinks it's 1:50 PM, the messages produced in that 10-minute interval by the broker will appear as if they're coming from the \"future\" when consumed.</p> <p>The time synchronization issue usually boils down to the Network Time Protocol (NTP). NTP is a protocol used to synchronize the clocks of computers to some time reference, which can be an atomic clock, GPS, or another reliable source.</p> <ul> <li> <p>NTP Not Installed: If NTP isn't installed on the machines running Kafka or the consumer application, they rely on their internal clocks. Over time, even minor discrepancies between internal clocks can add up, leading to significant drifts.</p> </li> <li> <p>NTP Malfunctions: Even with NTP installed, there might be cases where it's not working correctly. This can happen for various reasons, like network issues, software bugs, or misconfigurations.</p> </li> </ul>"}, {"location": "Development-vs-Production/#consequences-of-time-drift", "title": "Consequences of Time Drift", "text": "<p>When Kafka and the consumer drift apart in time, it doesn't just result in the odd phenomenon of messages from the future. It can:</p> <ul> <li> <p>Impact consumer logic that relies on time-based processing.</p> </li> <li> <p>Affect windowed operations in stream processing applications.</p> </li> <li> <p>Cause retention policies based on time to behave unpredictably.</p> </li> <li> <p>Cause other problems in Karafka Web UI tracking and reporting.</p> </li> </ul>"}, {"location": "Development-vs-Production/#how-to-prevent-time-drift", "title": "How to Prevent Time Drift", "text": "<ol> <li> <p>Ensure NTP is Installed: Always ensure that NTP is installed on all machines running Kafka brokers and consumer applications.</p> </li> <li> <p>Monitor NTP Status: Regularly monitor the NTP status to ensure it's running and is in sync with its time sources.</p> </li> <li> <p>Configure Alerts: Set alerts for any significant time drift between the servers. This can provide early warnings before time drift becomes a problem.</p> </li> <li> <p>Synchronize Frequently: Reduce the time between synchronization intervals to ensure that even minor drifts are corrected promptly.</p> </li> </ol> <p>In conclusion, while Kafka is a powerful tool, it's essential to remember the importance of time synchronization to ensure the reliable delivery and consumption of messages. Regularly monitoring and ensuring the correct functioning of NTP can prevent time drift issues, providing a smoother Kafka experience.</p>"}, {"location": "Development-vs-Production/#impact-on-karafka", "title": "Impact on Karafka", "text": "<p>Karafka and Karafka Web UI internal operations starting from <code>2.2.4</code> are resilient to this issue, as Karafka normalizes the time for internal computation. While this will not crash your operations, please note that time-sensitive metrics may not be accurate.</p>"}, {"location": "Development-vs-Production/#configure-your-brokers-offsetsretentionminutes-policy", "title": "Configure your brokers' <code>offsets.retention.minutes</code> policy", "text": "<p><code>offsets.retention.minutes</code> in Apache Kafka is a configuration setting that determines how long the Kafka broker will retain the offsets of consumer groups. The offset is a crucial piece of information that records the position of a consumer in a topic, essentially marking which messages have been processed.</p> <p>When a consumer is not running longer than the <code>offsets.retention.minutes</code> value, the following impacts can occur:</p> <ul> <li> <p>Loss of Offset Data: Once the retention period is exceeded, the offsets for that consumer group are deleted. If the consumer starts again after this period, it won't have a record of where it left off in processing the messages.</p> </li> <li> <p>Reprocessing of Messages: Without the offset information, the consumer might start reading messages from the beginning of the log or the latest offset, depending on its configuration. This can lead to reprocessing messages (if they start from the beginning) or missing out on messages (if they start from the latest).</p> </li> <li> <p>Data Duplication or Loss: The impact on data processing depends on the consumer's configuration and the nature of the data. It could result in data duplication or data loss if not managed properly.</p> </li> </ul> <p>It's essential to set the <code>offsets.retention.minutes</code> value considering your consumer applications' most extended expected downtime to avoid these issues. Setting a longer retention period for offsets can be crucial for systems where consumers might be down for extended periods.</p> <p>It's important to note that while the <code>offsets.retention.minutes</code> setting in Kafka might not seem particularly relevant in a development environment, it becomes crucial in a production setting.</p> <p>In development, consumer downtimes are generally short, and losing offsets might not have significant consequences as the data is often test data, and reprocessing might not be a concern. However, in a production environment, consumer downtime can be more impactful if a consumer is down for a time exceeding the <code>offsets.retention.minutes</code> value. The loss of offset data can lead to significant issues like message reprocessing or missing out on unprocessed messages. This can affect data integrity and processing efficiency.</p>"}, {"location": "Development-vs-Production/#topics-metadata-propagation-during-their-creation-and-removal", "title": "Topics Metadata Propagation During their Creation and Removal", "text": "<p>Apache Kafka, a distributed streaming platform, handles topic creation asynchronously. This means that when a new topic is created, there's a delay before it's recognized across all brokers in the Kafka cluster. This delay can cause temporary issues where the topic appears non-existent, leading to <code>unknown_topic</code> errors. To mitigate this, clients delay flagging a topic as non-existent for a default period of 30 seconds (configurable through <code>topic.metadata.propagation.max.ms</code>). This wait allows time for the topic metadata to propagate across the cluster.</p> <p>In Karafka it has specific implications:</p> <ul> <li> <p>Delayed Writing to Newly Created Topics: In Karafka, it's advisable not to start producing messages for a topic immediately after its creation using the Karafka Admin API. Due to Kafka's asynchronous nature, the topic might not be fully recognized across the cluster, leading to message delivery issues. Messages sent to these \"in-limbo\" topics get queued and might fail if the topic becomes unavailable within the propagation time.</p> </li> <li> <p>Topic Replication and Usability Timeframe: New topics must be fully replicated and usable across the cluster. This replication time varies depending on the cluster's size and configuration. In Karafka applications, developers should account for this delay and design their message-producing logic accordingly, allowing sufficient time for topics to stabilize within the Kafka ecosystem before initiating message production.</p> </li> <li> <p>Handling Topic Resets: Resetting topics in Karafka, especially in a production environment, should be approached with caution. Resetting a topic (deleting and recreating it) may lead to immediate marking of the topic as non-existent, as the propagation time does not apply in this scenario. This can cause significant disruptions in message flow and processing. We do not recommend removing and recreating topics on running systems. Always stop your producers and consumers before attempting to do so.</p> </li> <li> <p>Topic Auto-Creation Considerations: While Kafka and, by extension, Karafka support automatic topic creation, it's generally not recommended for consumer applications. Automatic topic creation can lead to issues where consumers attempt to consume from auto-created topics without producers, resulting in empty message sets.</p> </li> </ul> <p>In summary, when working with Kafka through Karafka, it's crucial to understand the asynchronous nature of Kafka's topic management. Developers should plan for propagation delays, be cautious with topic resets, and manage auto-creation settings judiciously to ensure a robust and reliable streaming application.</p>"}, {"location": "Development-vs-Production/#zstd-support-issues-on-macos", "title": "<code>zstd</code> Support Issues on macOS", "text": "<p>When using <code>rdkafka</code> or <code>karafka-rdkafka</code> on macOS, <code>zstd</code> support may break on macOS development machines. Users have encountered the following error:</p> <pre><code>Karafka::Errors::InvalidConfigurationError:\n\nUnsupported value \"zstd\" for configuration property \"compression.codec\": libzstd not available at build time\n</code></pre> <p>This issue occurs because of karafka-rdkafka not being linked against <code>libzstd</code>, even if <code>brew install zstd</code> was previously used to provide <code>zstd</code> support.</p> <p>To resolve this issue, ensure that <code>pkg-config</code> is installed on your macOS machine. The absence of <code>pkg-config</code> can prevent <code>librdkafka</code> from finding <code>libzstd</code> during the build process.</p> <pre><code>brew install pkg-config\ngem uninstall karafka-rdkafka\nbundle install\n</code></pre> <p>The need for <code>pkg-config</code> might not have been apparent in older versions due to changes in macOS or dependencies over time.</p>"}, {"location": "Development-vs-Production/#challenges-with-puma-worker-mode-on-macos", "title": "Challenges with Puma Worker Mode on macOS", "text": "<p>Forking processes on macOS, especially from macOS High Sierra (10.13) onwards, can introduce significant challenges due to changes in how macOS handles system calls in forked processes. These challenges often manifest as errors such as:</p> <ul> <li> <p><code>[NSCharacterSet initialize] may have been in progress in another thread when fork()</code></p> </li> <li> <p>Segmentation faults similar to: <code>[BUG] Segmentation fault at 0x0000000000000110</code></p> </li> </ul> <p>You can find an extensive explanation of Karafka ecosystem components forking support here.</p>"}, {"location": "Development-vs-Production/#be-aware-of-waterdrop-default-producer-middleware-modifications", "title": "Be Aware of WaterDrop Default Producer Middleware Modifications", "text": "<p>When applying middleware in <code>Karafka.producer</code> that modifies payloads or topics (like adding prefixes), you must consider that the Web UI also utilizes this producer. Any topic name changes must be applied across all environments and tools, including the Karafka Web UI. This ensures alignment between produced messages and what the Web UI expects. Alternatively, you can configure an independent Web UI with only a dedicated producer and not apply the middleware.</p> <p>For example, when applying such a middleware:</p> <pre><code>class NamespacerMiddleware\n  def call(message)\n    message[:topic] = \"my_prefix.#{message[:topic]}\"\n    message\n  end\nend\n\nKarafka.producer.middleware.append(NamespacerMiddleware.new)\n</code></pre> <p>Your Web UI topics configuration should look as follows:</p> <pre><code>Karafka::Web.setup do |config|\n  config.topics.errors = \"my_prefix.karafka_errors\"\n  config.topics.consumers.reports = \"my_prefix.karafka_consumers_reports\"\n  config.topics.consumers.states = \"my_prefix.karafka_consumers_states\"\n  config.topics.consumers.metrics = \"my_prefix.karafka_consumers_metrics\"\n  config.topics.consumers.commands = \"my_prefix.karafka_consumers_commands\"\nend\n</code></pre>"}, {"location": "Development-vs-Production/#consider-splitting-consumer-groups-to-improve-stability-and-performance", "title": "Consider Splitting Consumer Groups To Improve Stability and Performance", "text": "<p>When working with Karafka and Kafka, it is crucial to understand how rebalancing works, especially as you scale your deployment. A common pitfall occurs when a single consumer group subscribes to multiple topics, with individual consumers within the group only consuming subsets of those topics. This configuration can lead to significant inefficiencies during rebalances, affecting all consumers in the group regardless of the specific topics they are consuming and the assignment strategy.</p>"}, {"location": "Development-vs-Production/#rebalance-mechanism", "title": "Rebalance Mechanism", "text": "<p>Consumers within the same consumer group will undergo the same rebalance cycle, even if they are subscribed to different topics. Here's a detailed explanation of how this process works:</p> <ul> <li> <p>Rebalance Trigger: Rebalances can be triggered by several events, such as a new consumer joining the group, an existing consumer leaving, changes in subscription patterns, or changes in the number of partitions in the topics.</p> </li> <li> <p>Subscription and Assignment: Each consumer can subscribe to different topics in a consumer group. However, during a rebalance, Kafka assigns partitions to consumers based on the collective subscription of the entire group, not individual consumer subscriptions.</p> </li> <li> <p>Partition Assignment: Kafka's coordinator assigns partitions from the subscribed topics to the consumers within the group. Consumers receive partitions only from the topics to which they are subscribed, but the rebalance cycle affects all consumers in the group.</p> </li> </ul>"}, {"location": "Development-vs-Production/#example-scenario", "title": "Example Scenario", "text": "<p>Consider a consumer group A with two consumers:</p> <ul> <li>Consumer 1 (C1) subscribes to Topic Y.</li> <li>Consumer 2 (C2) subscribes to Topic X.</li> </ul> <p>During a rebalance:</p> <ul> <li>Kafka will assign partitions from Topic Y to C1.</li> <li>Kafka will assign partitions from Topic X to C2.</li> </ul> <p>Despite C1 and C2 subscribing to different topics, both consumers are affected by the rebalance cycle.</p>"}, {"location": "Development-vs-Production/#considerations", "title": "Considerations", "text": "<p>For small-scale development environments, having a single consumer group with multiple topic subscriptions might be manageable. However, in larger deployments, particularly those with more than 10 processes, this approach can be sub-optimal. The key reasons are:</p> <ul> <li> <p>Performance Impact: Frequent rebalances can temporarily degrade performance as partitions are reassigned, and consumers may experience brief downtimes.</p> </li> <li> <p>Coordination Overhead: The coordination and rebalance logic applies to the entire group, increasing the complexity and potential for inefficiencies.</p> </li> <li> <p>Rebalance Timeouts: In heterogeneous deployments, where consumers subscribe to different topics, the rebalance process can become more complex and prolonged. This complexity can lead to rebalance timeouts, where the rebalance cannot be completed within the time limitations imposed by Kafka settings, such as max.poll.interval.ms and session.timeout.ms.</p> </li> </ul>"}, {"location": "Development-vs-Production/#recommendation", "title": "Recommendation", "text": "<p>For larger deployments, organizing your consumers and topics is advisable so that each consumer group subscribes to a smaller, more focused set of topics. This reduces the scope and impact of rebalances, leading to more stable and performant applications.</p>"}, {"location": "Development-vs-Production/#configure-shutdown_timeout-for-cooperative-sticky-strategy-in-large-deployments", "title": "Configure <code>shutdown_timeout</code> for Cooperative-Sticky Strategy in Large Deployments", "text": "<p>When deploying Kafka with the <code>cooperative-sticky</code> rebalance strategy in environments with many consumers and partitions, setting the <code>shutdown_timeout</code> to an appropriately high value is crucial. This ensures that the rebalance and shutdown processes are completed smoothly without causing consumer disruptions.</p>"}, {"location": "Development-vs-Production/#why-set-a-high-shutdown_timeout", "title": "Why Set a High <code>shutdown_timeout</code>?", "text": "<p>The <code>shutdown_timeout</code> configuration defines the maximum time consumers can shut down gracefully. In larger deployments with many partitions, rebalances can take longer due to the complexity of ensuring minimal partition movement and maintaining a balanced load. A higher <code>shutdown_timeout</code> helps in:</p> <ul> <li> <p>Ensuring Graceful Shutdowns: Allows consumers sufficient time to process in-flight messages and commit offsets, reducing the risk of data loss or reprocessing.</p> </li> <li> <p>Reducing Rebalance Interruptions: Prevents premature shutdowns during rebalances, which can cause additional rebalances and increase system instability.</p> </li> <li> <p>Maintaining Consumer Health: Gives consumers more time to handle their state transitions, ensuring a smoother rebalance process.</p> </li> </ul> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other configuration options...\n    config.shutdown_timeout = 90_000 # 90 seconds\n  end\nend\n</code></pre>"}, {"location": "Development-vs-Production/#choosing-the-right-shutdown_timeout-value", "title": "Choosing the Right <code>shutdown_timeout</code> Value", "text": "<p>The appropriate value for shutdown_timeout depends on your specific deployment characteristics:</p> <ul> <li> <p>Consumer Group Size: Larger groups with more consumers may need a higher timeout to accommodate the increased coordination required.</p> </li> <li> <p>Partition Count: More partitions mean more work during rebalances, necessitating a higher timeout.</p> </li> <li> <p>Workload Characteristics: If your consumers process messages quickly, a lower timeout might suffice. For slower processing, increase the timeout accordingly.</p> </li> </ul> <p>We recommend setting the <code>shutdown_timeout</code> to at least 30 seconds. A timeout of at least 90 seconds is advisable for larger deployments to ensure a smooth and stable rebalance process.</p>"}, {"location": "Development-vs-Production/#static-group-membership-usage", "title": "Static Group Membership Usage", "text": "<p>In addition to configuring <code>shutdown_timeout,</code> consider using static group membership if possible. Static group membership offers several benefits that can enhance the stability and efficiency of your consumer groups:</p> <ul> <li> <p>Minimized Rebalance Impact: Static members maintain their identity across rebalances, reducing the need for frequent reassignments of partitions and improving overall group stability.</p> </li> <li> <p>Faster Rebalance Process: With static membership, the rebalance process can be completed more quickly as the coordinator has a clearer picture of group membership.</p> </li> <li> <p>Improved Resource Utilization: Static membership improves resource utilization by reducing the churn of consumer instances and minimizes the overhead associated with consumer state transitions.</p> </li> </ul> <p>To enable static group membership, set the <code>group.instance.id</code> configuration for each consumer instance in your <code>karafka.rb</code>:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Primary cluster\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      # Unique value per consumer group\n      'group.instance.id': \"consumer_instance_#{ENV['HOSTNAME']}\"\n    }\n\n    # Other settings...\n  end\nend\n</code></pre> <p>Static Group Membership and Multiplexing in Dynamic Mode</p> <p>We do not recommend using static group membership with Multiplexing operating in Dynamic mode. Multiplexing in Dynamic mode involves frequent changes in group composition, which conflicts with the nature of static group membership that relies on stable consumer identities. This can lead to increased complexity and more prolonged assignment lags.</p> <p>However, Multiplexing can be used without issues if Dynamic mode is not enabled. In this configuration, consumers maintain a more predictable group composition, which aligns well with the principles of static group membership and ensures a more stable and efficient operation.</p> <p>Last modified: 2025-05-16 21:07:08</p>"}, {"location": "Embedding/", "title": "Embedding", "text": "<p>Karafka can be embedded within another process so you do not need to run a separate process.</p> <p>This is called embedding.</p>"}, {"location": "Embedding/#usage", "title": "Usage", "text": "<p>To use embedding you need to:</p> <ol> <li>Configure Karafka as if it would be running independently as a separate process (standard configuration).</li> <li>Connect Karafka embedding API events to your primary process lifecycle flow.</li> </ol> <p>There are two embedding API calls that you need to connect to your main process lifecycle:</p> <ul> <li><code>::Karafka::Embedded.start</code> - Starts Karafka without process supervision and ownership of signals in a background thread. This method is non-blocking, and it won't interrupt other things running</li> <li><code>::Karafka::Embedded.stop</code> - Stops Karafka in a blocking fashion. It waits for all the current work to be done and then shuts down all the threads, connections, etc.</li> </ul> <p>Safe and Unsafe Trap Context Usage</p> <p>It is safe to use both <code>#quiet</code> and <code>#stop</code> from the trap context of a process that controls the execution in embedded mode, but it is not safe to run <code>#start</code> from the trap context.</p>"}, {"location": "Embedding/#usage-with-puma", "title": "Usage with Puma", "text": "<p>In a cluster mode:</p> <pre><code># config/puma.rb \n\nworkers 2\nthreads 1, 3\n\npreload_app!\n\non_worker_boot do\n  ::Karafka::Embedded.start\nend\n\non_worker_shutdown do\n  ::Karafka::Embedded.stop\nend\n</code></pre> <p>In a single node mode:</p> <pre><code># config/puma.rb \n\npreload_app!\n\n@config.options[:events].on_booted do\n  ::Karafka::Embedded.start\nend\n\n# There is no `on_worker_shutdown` equivalent for single mode\n@config.options[:events].on_stopped do\n  ::Karafka::Embedded.stop\nend\n</code></pre>"}, {"location": "Embedding/#usage-with-sidekiq", "title": "Usage with Sidekiq", "text": "<pre><code># config/initializers/sidekiq.rb\n\nSidekiq.configure_server do |config|\n  config.on(:startup) do\n    ::Karafka::Embedded.start\n  end\n\n  config.on(:quiet) do\n    # You may or may not want to have it here on quiet, depending on your use-case.\n    ::Karafka::Embedded.stop\n  end\n\n  config.on(:shutdown) do\n    ::Karafka::Embedded.stop\n  end\nend\n</code></pre>"}, {"location": "Embedding/#usage-with-passenger", "title": "Usage with Passenger", "text": "<pre><code>PhusionPassenger.on_event(:starting_worker_process) do\n  ::Karafka::Embedded.start\nend\n\nPhusionPassenger.on_event(:stopping_worker_process) do\n  ::Karafka::Embedded.stop\nend\n</code></pre>"}, {"location": "Embedding/#limitations", "title": "Limitations", "text": ""}, {"location": "Embedding/#long-living-processes-requirement", "title": "Long-living Processes Requirement", "text": "<p>Karafka is not designed to be periodically started and stopped within the same process. You might encounter unexpected behavior or errors if you attempt to do so. This design decision aligns with the nature of long-living processes in applications and services like Puma or Sidekiq. If you want to embed Karafka in your process, ensure it's persistent and long-living.</p>"}, {"location": "Embedding/#signal-handling", "title": "Signal Handling", "text": "<p>If your process captures signals, know Karafka won't intercept or handle them. This means actions like stopping the process using Ctrl-C, sending a TERM signal, or any other signals won't be managed by Karafka. The responsibility for signal handling lies entirely with the process owner. Properly managing these signals is crucial to avoid abrupt terminations or unforeseen consequences. Karafka won't react to Ctrl-C, TERM, or any other signal.</p>"}, {"location": "Embedding/#code-reload", "title": "Code Reload", "text": "<p>When Karafka is embedded in another process, you might find that code reloading doesn't function as you'd expect or might not work altogether. This can be particularly problematic during development when code changes are frequent.</p>"}, {"location": "Embedding/#concurrency-settings", "title": "Concurrency Settings", "text": "<p>Maintaining a conservative approach when setting concurrency levels with Karafka in the Embedded mode is advisable. A high concurrency setting might overtax your system resources, leading to potential slowdowns or bottlenecks. By keeping your concurrency settings on the lower side, you ensure that all tasks and responsibilities of your process can effectively access and utilize the resources they need without causing undue strain.</p>"}, {"location": "Embedding/#preloadingeager-loading", "title": "Preloading/Eager Loading", "text": "<p>Before you initiate the embedded Karafka server, your application code must be preloaded or eager loaded. This ensures that all necessary components, classes, and modules are available and loaded into memory when Karafka starts. Please do this to avoid missing dependencies or unexpected errors during runtime.</p>"}, {"location": "Embedding/#critical-error-handling", "title": "Critical Error Handling", "text": "<p>When operating Karafka in Embedded mode, it's crucial to understand that certain critical errors might be silently overlooked if the supervising process for Karafka Embedding does not correctly signal those errors. While Karafka might recognize and attempt to raise an error and notify about it via its instrumentation pipeline, the supervising process might not propagate or report this, leading to potential silent failures or unnoticed issues. For robust and reliable production deployments, it's critical to ensure that any errors Karafka might produce are not only correctly signaled by the supervising process but also reported and monitored. </p>"}, {"location": "Embedding/#partialsilent-crashes", "title": "Partial/Silent Crashes", "text": "<p>When utilizing Karafka in an embedded mode, it's vital to be aware of Partial or Silent Crash scenarios. These refer to situations where the Karafka process encounters a critical error and decides to halt its operations, but the overarching process in which Karafka runs continues to operate. This behavior can lead to situations where critical components have failed silently, but the system appears to be running, potentially leading to undetected issues or data loss.</p> <p>Certain critical errors, such as incompatible changes to the <code>partition.assignment.strategy</code>, can cause the embedded Karafka process to emit an error and terminate. However, this termination is isolated to Karafka itself, and may not propagate to the parent or supervising process.</p> <p>For example, when running Karafka within a Puma worker in the event of a critical Karafka crash, the Puma worker will remain unaffected. This means the HTTP server, despite the Karafka crash, will continue to accept and process messages. While this ensures that your HTTP server remains responsive, it also poses a risk since Karafka, a crucial component for processing, is no longer operational.</p> <p>To ensure system resilience and reliability:</p> <ul> <li> <p>Monitoring: Implement comprehensive monitoring tools that can detect and alert on both Karafka-specific errors and general system anomalies.</p> </li> <li> <p>Error Propagation: Ensure critical errors from embedded processes like Karafka are reported.</p> </li> <li> <p>Regular Testing: Periodically simulate critical errors in non-production environments to understand the system's response and to improve recovery mechanisms.</p> </li> </ul> <p>In conclusion, while embedding Karafka within larger processes can be efficient, knowing the potential for Partial or Silent Crashes is crucial. By understanding their implications and implementing mitigation strategies, you can ensure a more robust and resilient system.</p>"}, {"location": "Embedding/#process-termination", "title": "Process Termination", "text": "<p>When Karafka operates in an Embedded mode, it is essential to recognize that the Karafka supervisor does not have the final say regarding the termination of the entire process. In practice, if your surrounding process has a shutdown timeout shorter than Karafka's, there is a risk that Karafka could be forcefully terminated before it has had a chance to dispatch and delegate all work and states properly. While this might not pose an issue due to how offsets are managed, it can affect monitoring and management tools. For instance, Karafka Web UI interface monitoring Karafka might not capture the final state transition from \"stopping\" to \"stopped\". Instead, it may give an impression that the Karafka process is perpetually in the \"stopping\" phase, which can be misleading and make diagnostics more challenging.</p> <p>Always ensure you account for this behavior when integrating Karafka in an Embedded mode, especially if you rely on external tools or interfaces to monitor and manage your processes. Adjusting your surrounding process's shutdown timeout or ensuring it respects Karafka's requirements can help avoid such discrepancies.</p>"}, {"location": "Embedding/#web-ui-limitations-in-embedding-mode", "title": "Web UI Limitations in Embedding Mode", "text": "<p>When using Karafka in embedding mode, the Karafka Pro Web UI controlling feature will be limited. This is because, in embedding mode, Karafka does not have control over the entire Ruby process. As a result, some process management and control functionalities may not be fully available or operational. To leverage the full capabilities of the Karafka Pro Web UI, it is recommended that Karafka be run as a standalone application that can maintain complete control over the Ruby process.</p>"}, {"location": "Embedding/#thread-priority-management", "title": "Thread Priority Management", "text": "<p>When embedding Karafka within other processes like Puma or Sidekiq, thread priorities are crucial in balancing CPU time between Karafka's background processing and the host application's primary responsibilities. Unlike the pure priority concept, where threads with different priorities must compete for CPU time, Ruby's thread priority controls the thread scheduling quantum - how much GVL (Global VM Lock) time a thread gets before yielding to others.</p> <p>Ruby's thread priority is calculated as bit shifts of the default 100ms quantum:</p> <ul> <li>priority 0 = 100ms</li> <li>priority -1 = 50ms</li> <li>priority -2 = 25ms</li> <li>priority -3 = 12.5ms</li> </ul> <p>This mechanism determines how frequently a thread releases the GVL, which is critical when mixing CPU-bound background processing with IO-bound request handling.</p> <p>When a background processing thread has normal priority (0), it holds the GVL for 100ms between network I/O operations. Meanwhile, request handler threads waiting to serve quick operations (like cached value lookups) must wait for these 100ms slices to complete. This can transform a 10ms request into a much longer operation, explaining the importance of proper priority tuning in embedded mode:</p> <pre><code>Karafka.setup do |config|\n  # Worker thread priority (default: -1 = 50ms quantum)\n  config.worker_thread_priority = -2  # 25ms quantum for embedded mode\n\n  # Listener thread priority remains internal\n  # config.internal.connection.listener_thread_priority = 0\nend\n</code></pre> <p>The default worker thread priority is -1 (50ms quantum) to prevent CPU-intensive message processing from dominating the GVL. For embedded environments, lowering to -2 or -3 allows web requests to interleave more frequently with Kafka message processing, reducing tail latency while having minimal impact on background processing throughput.</p> <p>Here's the recommended configuration for different scenarios:</p> <pre><code># Puma configuration\non_worker_boot do\n  Karafka.setup do |config|\n    # Lower quantum for better request responsiveness\n    config.worker_thread_priority = -2\n  end\n\n  ::Karafka::Embedded.start\nend\n\n# Sidekiq configuration  \nSidekiq.configure_server do |config|\n  config.on(:startup) do\n    Karafka.setup do |config|\n      # Sidekiq's own processing may benefit from less aggressive priority\n      config.worker_thread_priority = -1\n    end\n\n    ::Karafka::Embedded.start\n  end\nend\n</code></pre> <p>The listener thread priority (internal setting <code>internal.connection.listener_thread_priority</code>, default 0) should not be modified unless necessary. Listener threads efficiently release the GVL while waiting for poll results, making the standard 100ms quantum appropriate for their workload.</p> <p>Performance Trade-offs</p> <p>Lower priorities reduce GVL time per quantum, which can slightly increase message processing latency. However, this trade-off usually improves overall system responsiveness. Monitor your specific workload and adjust priorities accordingly - the practical range is -3 to 3, with -3 providing the minimum 20ms quantum in practice due to Ruby's internal tick system.</p> <p>Last modified: 2025-05-05 15:29:38</p>"}, {"location": "Env-Variables/", "title": "Env Variables", "text": "<p>Karafka's behaviour can be altered with the following environment variables:</p> Name Description KARAFKA_ROOT_DIR Root dir of the Karafka application. Defaults to the directory in which Bundler was executed (<code>BUNDLE_GEMFILE</code>) KARAFKA_ENV Karafka app expected environment. If not defined, autodetected based on <code>RAILS_ENV</code>, <code>RACK_ENV</code> with a fallback to <code>development</code>. KARAFKA_BOOT_FILE Location of Karafka boot file (<code>karafka.rb</code>) or <code>false</code>. Defaults to <code>karafka.rb</code> in the project root directory. May be set to <code>false</code> in case you want to fully control Karafka boot process. KARAFKA_REQUIRE_RAILS Determines if Rails should be required when present in the Gemfile. If set to <code>false</code>, Karafka can run without Rails even if both are in the same Gemfile. <p>Last modified: 2024-08-08 18:15:29</p>"}, {"location": "Error-handling-and-back-off-policy/", "title": "Error Handling and back off policy", "text": "<p>Karafka's behavior upon errors is pretty predictable. There are three stages in which the Karafka server can be:</p> <ul> <li>Initialization</li> <li>Runtime</li> <li>Shutdown</li> </ul> <p>Depending on the state, Karafka behaves differently upon encountering exceptions.</p>"}, {"location": "Error-handling-and-back-off-policy/#initialization", "title": "Initialization", "text": "<p>Any error that occurs during the <code>initialization</code> phase of the <code>karafka server</code> will crash it immediately. This also includes critical configuration errors:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Client id must always be present\n    config.client_id = nil\n  end\nend\n</code></pre> <pre><code>bundle exec karafka server\nbundler: failed to load command: karafka\nKarafka::Errors::InvalidConfiguration: {:client_id=&gt;[\"must be filled\"]}\n</code></pre> <p>If a situation like that occurs, Karafka will exit with exit code 1.</p>"}, {"location": "Error-handling-and-back-off-policy/#runtime", "title": "Runtime", "text": "<p>Karafka has a couple of isolation layers that prevent it from being affected by any errors or exceptions from the application code.</p> <p>In any case, as long as system resources (like memory) are available, the Karafka process will never crash upon application errors. Also, threads for particular consumer groups and workers are isolated, so as long as you don't do any cross-consumer group work, they won't impact each other in any way.</p> <p>When processing messages from a Kafka topic, your code may raise any exception inherited from <code>StandardError</code>. The cause is typically because of one of the following reasons:</p> <ul> <li>Your business logic does not behave as you think it should.</li> <li>The message being processed is somehow malformed or is in an invalid format.</li> <li>You're using external resources such as a database or a network API that are temporarily unavailable.</li> </ul> <p>Your exception will propagate to the framework if not caught and handled within your application code. Karafka will stop processing messages from this topic partition, back off, and wait for a given time defined by the <code>pause_timeout</code> setting. This allows the consumer to continue processing messages from other partitions that may not be impacted by the problem while still making sure not to drop the original message. After that time, it will retry, processing the same message again. Single Kafka topic partition messages must be processed in order. That's why Karafka will never skip any messages.</p>"}, {"location": "Error-handling-and-back-off-policy/#retryable-methods", "title": "Retryable Methods", "text": "<p>It's crucial to understand how Karafka handles retries for different methods in the context of error handling and retries. This understanding is essential for effectively managing error scenarios in your Karafka applications. The framework's behavior varies depending on the method invoked:</p> Method Retryable Description <code>#consume</code> Yes Retries occur as Karafka implements a back-off policy for this method, suitable for handling transient issues in message processing. <code>#revoked</code> No No retries, as this method is called when a partition is lost due to reassignment. Retrying in this context is not logical. <code>#shutdown</code> No Retries are not applicable, as this method indicates the stopping of the process. Retrying during shutdown doesn't align with its purpose. <code>#tick</code> No Since this method is invoked frequently, retries are unnecessary and could lead to inefficiencies and redundancy. <code>#eofed</code> No This method is triggered when the end of a partition is reached. Since it's a normal part of processing when a partition is fully read, retries are unnecessary and not applicable. <code>#wrap</code> No Wraps the entire consumption lifecycle, including framework-level operations and user-defined logic. Must always call <code>yield</code> to ensure that all processing, synchronization, and cleanup actions are executed, even in error scenarios. <p>It's important to note that crashes or exceptions in all these methods, including <code>#consume</code>, <code>#revoked</code>, <code>#shutdown</code>, <code>#tick</code> and <code>#eofed</code>, are reported through Karafka's error notifications system. However, only errors occurring in the <code>#consume</code> method are considered retryable. </p> <p>Errors in the other methods (<code>#revoked</code>, <code>#shutdown</code>, <code>#tick</code> and <code>#eofed</code>) are not subject to retries. They are reported for logging and monitoring purposes, but aside from this notification, they do not disrupt or halt the ongoing processing of messages. This distinction is crucial for understanding how Karafka manages its resilience and stability in the face of errors.</p>"}, {"location": "Error-handling-and-back-off-policy/#altering-the-consumer-behaviour-upon-reprocessing", "title": "Altering the Consumer Behaviour upon Reprocessing", "text": "<p>The Karafka consumer <code>#retrying?</code> method is designed to detect whether we are in retry mode after an error has occurred. This method can be helpful in a variety of use cases where you need to alter the behavior of your application when a message is being retried. For example, you might want to send an alert or notification when a message is being retried, or you might want to branch out and perform a different action based on the fact that the message is being retried. By detecting whether a message is being retried or not, you can gain better control over your application's behavior and make sure that it is able to handle errors in a way that is appropriate for your specific use case.</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      if retrying?\n        puts 'We operate after an error'\n        puts message.payload\n      else\n        puts message.payload\n      end\n    end\n  end\nend\n</code></pre> <p>Please note that <code>retrying?</code> indicates that an error occurred previously, but you may receive fewer or more messages and previously.</p> <p>In addition to detecting retry scenarios with <code>#retrying?</code>, Karafka provides the <code>#attempt</code> method for more nuanced control. This method indicates the current attempt, offering opportunities for specific actions or alerts based on the number of retries. This advanced functionality allows tailored behavior adjustments during message processing retries, enhancing error-handling strategies.</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    # Just move on if not possible to fix error after 10 attempts\n    # This is just an example, you probably want a more sophisticated flow\n    if attempt &gt; 10\n      messages.each do |message|\n        begin\n          Processor.call(message)\n        rescue\n          nil\n        end\n      end\n    else\n      # Do not silence errors so there is a retry\n      messages.each do |message|\n        Processor.call(message)\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Error-handling-and-back-off-policy/#error-tracking", "title": "Error Tracking", "text": "<p>Karafka, in the runtime stage, publishes sync and async errors (any that would occur in background threads) to the monitor on an <code>error.occurred</code> channel. This allows you to connect any type of error logging or instrumentation by yourself:</p> <pre><code>Karafka.monitor.subscribe 'error.occurred' do |event|\n  type = event[:type]\n  error = event[:error]\n  details = (error.backtrace || []).join(\"\\n\")\n\n  puts \"Oh no! An error: #{error} of type: #{type} occurred!\"\n  puts details\nend\n</code></pre>"}, {"location": "Error-handling-and-back-off-policy/#dead-letter-queue", "title": "Dead Letter Queue", "text": "<p>Karafka provides out-of-the-box Dead Letter Queue pattern implementation that can be used to move failing messages to a separate topic.</p> <p>You can read about it here.</p>"}, {"location": "Error-handling-and-back-off-policy/#finding-the-failing-message", "title": "Finding the Failing Message", "text": "<p>Whenever a <code>consumer.consume.error</code> error occurs, Karafka will publish the <code>seek_offset</code> alongside other things. It contains the offset of the first uncommitted message in the <code>messages</code> batch.</p> <pre><code>Karafka.monitor.subscribe 'error.occurred' do |event|\n  type = event[:type]\n  error = event[:error]\n\n  # Skip any other error types\n  next unless type == 'consumer.consume.error'\n\n  messages = event[:caller].messages\n  seek_offset = event[:seek_offset]\n\n  failing_message = messages.find { |message| message.offset == seek_offset }\n\n  puts \"We have failed while processing message with offset: #{failing_message.offset}\"\nend\n</code></pre> <p>When doing batch operations, this message may not be the exact cause of the processing error.</p>"}, {"location": "Error-handling-and-back-off-policy/#exponential-backoff", "title": "Exponential Backoff", "text": "<p>Karafka is configured with <code>pause_with_exponential_backoff</code> enabled (<code>true</code>) by default. This configuration doubles the timeout period after each pause until a message from the partition is processed successfully. To prevent the timeout from extending indefinitely, <code>pause_max_timeout</code> can be set to your preferred maximum duration. By default, this maximum timeout is set to 30 seconds.</p> <p>Regardless of the error's nature, the Monitoring and Logging feature can track any issues encountered during operation.</p> <p>A monitoring and logging layer is strongly recommended to ensure prompt notification of errors that arise while processing Kafka messages.</p>"}, {"location": "Error-handling-and-back-off-policy/#pause-offset-selection", "title": "Pause Offset Selection", "text": "<p>Karafka keeps track of the last committed offset alongside Kafka when you mark a message as consumed. This means that after the pause, Karafka will start back from the failed message, not from the first message from the batch. This approach severely reduces the number of messages that must be reprocessed upon errors.</p> <p> </p> <p>This behavior is different in the case of Virtual Partitions. Please refer to this Wiki section for more details.</p>"}, {"location": "Error-handling-and-back-off-policy/#shutdown", "title": "Shutdown", "text": "<p>Karafka will wait for <code>shutdown_timeout</code> milliseconds before forcefully stopping in case of errors or problems during the shutdown process. If this value is not set, Karafka will wait indefinitely for consumers to finish processing given messages.</p> <p>Setting this value high enough is highly recommended so that Karafka won't stop itself in the middle of some non-transactional partially finished operations.</p>"}, {"location": "Error-handling-and-back-off-policy/#internal-framework-errors", "title": "Internal Framework Errors", "text": "<p>Karafka handles framework and Kafka-related errors on several layers, ensuring robust and reliable message processing. Most errors are either recovered automatically or retried based on predefined strategies.</p>"}, {"location": "Error-handling-and-back-off-policy/#error-recovery-and-retry-mechanisms", "title": "Error Recovery and Retry Mechanisms", "text": "<p>Karafka employs multiple layers of error handling to manage issues seamlessly:</p> <ul> <li>Framework-Level Recovery: Errors related to Karafka's internal operations are handled within the framework. This includes automatic retries and recovery procedures to maintain the stability of the message processing flow.</li> <li>Kafka-Related Errors: Issues originating from Kafka, such as connectivity problems or message fetching errors, are managed through retries and connection resets. Karafka ensures that these errors do not disrupt the overall processing pipeline.</li> </ul>"}, {"location": "Error-handling-and-back-off-policy/#final-recovery-strategy", "title": "Final Recovery Strategy", "text": "<p>In cases of unexpected errors within the listeners' loops, Karafka applies a final recovery strategy. This strategy involves resetting the client connection and restarting all associated resources. While this mechanism ensures continued operation, it is considered the last layer of defense.</p> <p>User Responsibility for Listener Errors</p> <p>Users should not rely on the final recovery strategy as a primary error-handling method. Any errors originating from listeners should be deeply investigated and resolved to prevent recurring issues. Proper error management at the listener level is crucial for maintaining the efficiency and reliability of your Karafka application.</p> <p>Last modified: 2025-05-16 21:07:08</p>"}, {"location": "Exit-codes/", "title": "Exit codes", "text": "<p>Each Karafka process defines exit statuses to indicate how the process terminated, offering clarity on operational outcome. Below, you can find the meaning behind each of the exit codes used:</p> Exit Code Description <code>0</code> Smooth shutdown: all work completed as expected, consumers stopped, etc. <code>1</code> Ruby exit code for syntax errors or other boot problems. <code>2</code> Forceful shutdown: not all consumers finished their work due to exceeding <code>shutdown_timeout</code>. <code>3</code> Exit code for an orphaned Swarm node. <p>Last modified: 2024-02-16 18:50:26</p>"}, {"location": "FAQ/", "title": "FAQ", "text": "<ol> <li>Does Karafka require Ruby on Rails?</li> <li>Why there used to be an ApplicationController mentioned in the Wiki and some articles?</li> <li>Does Karafka require Redis and/or Sidekiq to work?</li> <li>Could an HTTP controller also consume a fetched message through the Karafka router?</li> <li>Does Karafka require a separate process running?</li> <li>Can I start Karafka process with only particular consumer groups running for given topics?</li> <li>Can I use <code>#seek</code> to start processing topics partition from a certain point?</li> <li>Why Karafka does not pre-initializes consumers prior to first message from a given topic being received?</li> <li>Does Karafka restart dead PG connections?</li> <li>Does Karafka require gems to be thread-safe?</li> <li>When Karafka is loaded via railtie in test env, SimpleCov does not track code changes</li> <li>Can I use Thread.current to store data in between batches?</li> <li>Why Karafka process does not pick up newly created topics until restarted?</li> <li>Why is Karafka not doing work in parallel when I started two processes?</li> <li>Can I remove a topic while the Karafka server is running?</li> <li>What is a forceful Karafka stop?</li> <li>Can I use AWS MSK Serverless with IAM authentication?</li> <li>Why can't I connect to Kafka from another Docker container?</li> <li>How can I configure multiple bootstrap servers?</li> <li>Why, when using <code>cooperative-sticky</code> rebalance strategy, all topics get revoked on rebalance?</li> <li>What will happen with uncommitted offsets during a rebalance?</li> <li>Can I use Karafka with Ruby on Rails as a part of an internal gem?</li> <li>Can I skip messages on errors?</li> <li>What does static consumer fenced by other consumer with same group.instance.id mean?</li> <li>Why, in the Long-Running Jobs case, <code>#revoked</code> is executed even if <code>#consume</code> did not run because of revocation?</li> <li>Why am I seeing <code>Rdkafka::RdkafkaError (Local: Timed out (timed_out)</code> error when producing larger quantities of messages?</li> <li>Do I need to use <code>#revoked?</code> when not using Long-Running jobs?</li> <li>Can I consume from more than one Kafka cluster at the same time?</li> <li>Why Karafka uses <code>karafka-rdkafka</code> instead of <code>rdkafka</code> directly?</li> <li>Why am I seeing an <code>Implement this in a subclass</code> error?</li> <li>What is Karafka <code>client_id</code> used for?</li> <li>How can I increase Kafka and Karafka max message size?</li> <li>Why do DLQ messages in my system keep disappearing?</li> <li>What is the optimal number of threads to use?</li> <li>Can I use several producers with different configurations with Karafka?</li> <li>What is the Unsupported value \"SSL\" for configuration property \"security.protocol\": OpenSSL not available at build time?</li> <li>Can Karafka ask Kafka to list available topics?</li> <li>Why Karafka prints some of the logs with a time delay?</li> <li>Why is increasing <code>concurrency</code> not helping upon a sudden burst of messages?</li> <li>Why am I seeing a \"needs to be consistent namespacing style\" error?</li> <li>Why, despite setting <code>initial_offset</code> to <code>earliest</code>, Karafka is not picking up messages from the beginning?</li> <li>Should I TSTP, wait a while, then send TERM or set a longer <code>shutdown_timeout</code> and only send a TERM signal?</li> <li>Why am I getting <code>error:0A000086:SSL routines::certificate verify failed</code> after upgrading Karafka?</li> <li>Why am I seeing a <code>karafka_admin</code> consumer group with a constant lag present?</li> <li>Can I consume the same topic independently using two consumers within the same application?</li> <li>Why am I seeing Broker failed to validate record (invalid_record) error?</li> <li>How can I make polling faster?</li> <li>Can I dynamically add consumer groups and topics to a running Karafka process?</li> <li>Can a consumer instance be called multiple times from multiple threads?</li> <li>Can multiple threads reuse a single consumer instance?</li> <li>What does <code>Broker: Unknown topic or partition</code> error mean?</li> <li>Why some of consumer subscriptions are not visible in the Web UI?</li> <li>Is there a way to run Karafka in a producer-only mode?</li> <li>Why am I getting the <code>can't alloc thread (ThreadError)</code> error from the producer?</li> <li>Can I create all the topics needed by the Web UI manually?</li> <li>Can I consume messages from a Rake task?</li> <li>Do you provide an upgrade support when upgrading from EOL versions?</li> <li>Why there are so many Karafka strategies in the codebase?</li> <li>Why am I having problems running Karafka and Karafka Web with remote Kafka?</li> <li>Why after moving from Racecar to Karafka, my Confluent Datadog integration stopped working?</li> <li>Why am I getting <code>env: can't execute 'bash'</code> when installing Karafka in an Alpine Docker?</li> <li>Can I intercept WaterDrop messages in tests?</li> <li>Does Karafka Expiring Messages remove messages from Kafka?</li> <li>Can you actively ping the cluster from Karafka to check the cluster availability?</li> <li>How do I specify Karafka's environment?</li> <li>How can I configure WaterDrop with SCRAM?</li> <li>Why am I getting a <code>Local: Broker transport failure (transport)</code> error with the <code>Disconnected</code> info?</li> <li>Why am I getting a <code>All broker connections are down (all_brokers_down)</code> error together with the <code>Disconnected</code> info?</li> <li>What is the difference between <code>partition_key</code> and <code>key</code> in the WaterDrop gem?</li> <li>How can I set up WaterDrop with SCRAM?</li> <li>Is there a way to mark messages as consumed in bulk?</li> <li>How can I consume all the messages from a Kafka topic without a consumer process?</li> <li>What does <code>Broker: Invalid message (invalid_msg)</code> error mean?</li> <li>Is there an option in Karafka to re-consume all the messages from a topic even though all were already consumed?</li> <li>How can I make sure, that <code>Karafka.producer</code> does not block/delay my processing?</li> <li>Can <code>at_exit</code> be used to close the WaterDrop producer?</li> <li>Why, when DLQ is used with <code>max_retries</code> set to <code>0</code>, Karafka also applies a back-off?</li> <li>Can I use <code>rdkafka</code> and <code>karafka-rdkafka</code> together in the same project?</li> <li>Does using consumer <code>#seek</code> resets the committed offset?</li> <li>Is it recommended to use public consumer methods from outside the consumer?</li> <li>Why do I see <code>SASL authentication error</code> after AWS MSK finished the <code>Heal cluster</code> operation?</li> <li>Why Karafka and WaterDrop are behaving differently than <code>rdkafka</code>?</li> <li>Why am I seeing <code>Inconsistent group protocol</code> in Karafka logs?</li> <li>What is the difference between WaterDrop's <code>max_payload_size</code> and librdkafka's <code>message.max.bytes</code>?</li> <li>What are consumer groups used for?</li> <li>Why am I getting the <code>all topic names within a single consumer group must be unique</code> error?</li> <li>Why am I getting <code>WaterDrop::Errors::ProduceError</code>, and how can I know the underlying cause?</li> <li>Can extra information be added to the messages dispatched to the DLQ?</li> <li>Why does WaterDrop hang when I attempt to close it?</li> <li>Why Karafka commits offsets on rebalances and librdkafka does not?</li> <li>What is Karafka's assignment strategy for topics and partitions?</li> <li>Why can't I see the assignment strategy/protocol for some Karafka consumer groups?</li> <li>What can be done to log why the <code>produce_sync</code> has failed?</li> <li>Can I password-protect Karafka Web UI?</li> <li>Can I use a Karafka producer without setting up a consumer?</li> <li>What will happen when a message is dispatched to a dead letter queue topic that does not exist?</li> <li>Why do Karafka reports lag when processes are not overloaded and consume data in real-time?</li> <li>Does Kafka guarantee message processing orders within a single partition for single or multiple topics? And does this mean Kafka topics consumption run on a single thread?</li> <li>Why can I produce messages to my local Kafka docker instance but cannot consume?</li> <li>What is the release schedule for Karafka and its components?</li> <li>Can I pass custom parameters during consumer initialization?</li> <li>Where can I find producer idempotence settings?</li> <li>How can I control or limit the number of PostgreSQL database connections when using Karafka?</li> <li>Why is my Karafka application consuming more memory than expected?</li> <li>How can I optimize memory usage in Karafka?</li> <li>Why am I getting <code>No such file or directory - ps (Errno::ENOENT)</code> from the Web UI?</li> <li>Can I retrieve all records produced in a single topic using Karafka?</li> <li>How can I get the total number of messages in a topic?</li> <li>Why am I getting <code>Broker: Group authorization failed (group_authorization_failed)</code> when using Admin API or the Web UI?</li> <li>Why am I getting an <code>ArgumentError: undefined class/module YAML::Syck</code> when trying to install <code>karafka-license</code>?</li> <li>Are Virtual Partitions effective in case of not having IO or not having a lot of data?</li> <li>Is the \"one process per one topic partition\" recommendation in Kafka also applicable to Karafka?</li> <li>Does running <code>#mark_as_consumed</code> increase the processing time?</li> <li>Does it make sense to have multiple worker threads when operating on one partition in Karafka?</li> <li>Why don't Virtual Partitions provide me with any performance benefits?</li> <li>What are Long Running Jobs in Kafka and Karafka, and when should I consider using them?</li> <li>What can I do to optimize the latency in Karafka?</li> <li>What is the maximum recommended concurrency value for Karafka?</li> <li>Are there concerns about having unused worker threads for a Karafka consumer process?</li> <li>How can you effectively scale Karafka during busy periods?</li> <li>What are the benefits of using Virtual Partitions (VPs) in Karafka?</li> <li>What's the difference between increasing topic partition count and using VPs in terms of concurrency?</li> <li>How do VPs compare to multiple subscription groups regarding performance?</li> <li>What is the principle of strong ordering in Kafka and its implications?</li> <li>Why do I see <code>Rdkafka::Config::ClientCreationError</code> when changing the <code>partition.assignment.strategy</code>?</li> <li>Is it recommended to add the <code>waterdrop</code> gem to the Gemfile, or just <code>karafka</code> and <code>karafka-testing</code>?</li> <li>Can I use <code>Karafka.producer</code> to produce messages that will then be consumed by ActiveJob jobs?</li> <li>Why am I getting the <code>Broker: Policy violation (policy_violation)</code> error?</li> <li>Why am I getting a <code>Error querying watermark offsets for partition 0 of karafka_consumers_states</code> error?</li> <li>Why Karafka is consuming the same message multiple times?</li> <li>Why do Karafka Web UI topics contain binary/Unicode data instead of text?</li> <li>Can I use same Karafka Web UI topics for multiple environments like production and staging?</li> <li>Does Karafka plan to submit metrics via a supported Datadog integration, ensuring the metrics aren't considered custom metrics?</li> <li>How can I make Karafka not retry processing, and what are the implications?</li> <li>We faced downtime due to a failure in updating the SSL certs. How can we retrieve messages that were sent during this downtime?</li> <li>How can the retention policy of Kafka affect the data sent during the downtime?</li> <li>Is it possible to fetch messages per topic based on a specific time period in Karafka?</li> <li>Where can I find details on troubleshooting and debugging for Karafka?</li> <li>Does the open-source (OSS) version of Karafka offer time-based offset lookup features?</li> <li>I see a \"JoinGroup error: Broker: Invalid session timeout\" error. What does this mean, and how can I resolve it?</li> <li>The \"Producer Network Latency\" metric in DD seems too high. Is there something wrong with it?</li> <li>What is the purpose of the <code>karafka_consumers_reports</code> topic?</li> <li>Can I use <code>Karafka.producer</code> from within ActiveJob jobs running in the karafka server?</li> <li>Do you recommend using the singleton producer in Karafka for all apps/consumers/jobs in a system?</li> <li>Is it acceptable to declare short-living producers in each app/jobs as needed?</li> <li>What are the consequences if you call a <code>#produce_async</code> and immediately close the producer afterward?</li> <li>Is it problematic if a developer creates a new producer, calls <code>#produce_async</code>, and then closes the producer whenever they need to send a message?</li> <li>Could the async process remain open somewhere, even after the producer has been closed?</li> <li>Could a single producer be saturated, and if so, what kind of max rate of message production would be the limit?</li> <li>How does the batching process in WaterDrop works?</li> <li>Can you control the batching process in WaterDrop?</li> <li>Is it possible to exclude <code>karafka-web</code> related reporting counts from the web UI dashboard?</li> <li>Can I log errors in Karafka with topic, partition, and other consumer details?</li> <li>Why did our Kafka consumer start from the beginning after a 2-week downtime, but resumed correctly after a brief stop and restart?</li> <li>Why am I experiencing a load error when using Karafka with Ruby 2.7, and how can I fix it?</li> <li>Why am I getting <code>+[NSCharacterSet initialize] may have been in progress in another thread when fork()</code> error when forking on macOS?</li> <li>How does Karafka handle messages with undefined topics, and can they be routed to a default consumer?</li> <li>What happens if an error occurs while consuming a message in Karafka? Will the message be marked as not consumed and automatically retried?</li> <li>What does setting the <code>initial_offset</code> to <code>earliest</code> mean in Karafka? Does it mean the consumer starts consuming from the earliest message that has not been consumed yet?</li> <li>Why is the \"Dead\" tab in Web UI empty in my Multi App setup?</li> <li>What causes a \"Broker: Policy violation (policy_violation)\" error when using Karafka, and how can I resolve it?</li> <li>Why do I see hundreds of repeat exceptions with <code>pause_with_exponential_backoff</code> enabled?</li> <li>Does Karafka store the Kafka server address anywhere, and are any extra steps required to make it work after changing the server IP/hostname?</li> <li>What should I do if I encounter a loading issue with Karafka after upgrading Bundler to version <code>2.3.22</code>?</li> <li>Is there a good way to quiet down <code>bundle exec karafka server</code> extensive logging in development?</li> <li>How can I namespace messages for producing in Karafka?</li> <li>Why am I getting the <code>all topic names within a single consumer group must be unique</code> error when changing the location of the boot file using <code>KARAFKA_BOOT_FILE</code>?</li> <li>Why Is Kafka Using Only 7 Out of 12 Partitions Despite Specific Settings?</li> <li>Why does the Dead Letter Queue (DLQ) use the default deserializer instead of the one specified for the original topic in Karafka?</li> <li>What should I consider when manually dispatching messages to the DLQ in Karafka?</li> <li>How can I ensure that my Karafka consumers process data in parallel?</li> <li>How should I handle the migration to different consumer groups for parallel processing?</li> <li>What are the best practices for setting up consumer groups in Karafka for optimal parallel processing?</li> <li>How can I set up custom, per-message tracing in Karafka?</li> <li>When Karafka reaches <code>max.poll.interval.ms</code> time and the consumer is removed from the group, does this mean my code stops executing?</li> <li>Which component is responsible for committing the offset after consuming? Is it the listener or the worker?</li> <li>Can the <code>on_idle</code> and <code>handle_idle</code> methods be changed for a specific consumer?</li> <li>Is Multiplexing an alternative to running multiple Karafka processes but using Threads?</li> <li>Is it possible to get watermark offsets from inside a consumer class without using Admin?</li> <li>Why are message and batch numbers increasing even though I haven't sent any messages?</li> <li>What does <code>config.ui.sessions.secret</code> do for the Karafka Web UI? Do we need it if we are using our authentication layer?</li> <li>Is there middleware for consuming messages similar to the middleware for producing messages?</li> <li>Can we change the name of Karafka's internal topic for the Web UI?</li> <li>Is there a way to control which pages we show in the Karafka Web UI Explorer to prevent exposing PII data?</li> <li>What does the <code>strict_topics_namespacing</code> configuration setting control?</li> <li>Does librdkafka queue messages when using Waterdrop's <code>#produce_sync</code> method?</li> <li>How reliable is the Waterdrop async produce? Will messages be recovered if the Karafka process dies before producing the message?</li> <li>Will WaterDrop start dropping messages upon librdkafka buffer overflow?</li> <li>How can I handle <code>dispatch_to_dlq</code> method errors when using the same consumer for a topic and its DLQ?</li> <li>What should I do if I encounter the <code>Broker: Not enough in-sync replicas</code> error?</li> <li>Is there any way to measure message sizes post-compression in Waterdrop?</li> <li>What happens to a topic partition when a message fails, and the exponential backoff strategy is applied? Is the partition paused during the retry period?</li> <li>How can Virtual Partitions help with handling increased consumer lag in Karafka?</li> <li>Is scaling more processes a viable alternative to using Virtual Partitions?</li> <li>What is the optimal strategy for scaling in Karafka to handle high consumer lag?</li> <li>How does Karafka behave under heavy lag, and what should be considered in configuration?</li> <li>Is there an undo of Quiet for a consumer to get it consuming again?</li> <li>Can two Karafka server processes with the same group_id consume messages from the same partition in parallel?</li> <li>What are some good default settings for sending large \"trace\" batches of messages for load testing?</li> <li>Is it worth pursuing transactions for a low throughput but high-importance topic?</li> <li>Does the Waterdrop producer retry to deliver messages after errors such as <code>librdkafka.error</code> and <code>librdkafka.dispatch_error</code>, or are the messages lost?</li> <li>In a Rails request, can I publish a message asynchronously, continue the request, and block at the end to wait for the publish to finish?</li> <li>Why am I getting the error: \"No provider for SASL mechanism GSSAPI: recompile librdkafka with libsasl2 or openssl support\"?</li> <li>How can I check if <code>librdkafka</code> was compiled with SSL and SASL support in Karafka?</li> <li>Why does <code>librdkafka</code> lose SSL and SASL support in my multi-stage Docker build?</li> <li>Why do I see WaterDrop error events but no raised exceptions in sync producer?</li> <li>When does EOF (End of File) handling occur in Karafka, and how does it work?</li> <li>How can I determine if a message is a retry or a new message?</li> <li>Why does Karafka Web UI stop working after upgrading the Ruby slim/alpine Docker images?</li> <li>Why does installing <code>karafka-web</code> take exceptionally long?</li> <li>Why does Karafka routing accept consumer classes rather than instances?</li> <li>Why does Karafka define routing separate from consumer classes, unlike Sidekiq or Racecar?</li> <li>What's the difference between <code>key</code> and <code>partition_key</code> in WaterDrop?</li> <li>Can I disable logging for Karafka Web UI consumer operations while keeping it for my application consumers?</li> <li>How can I distinguish between sync and async producer errors in the <code>error.occurred</code> notification?</li> </ol>"}, {"location": "FAQ/#does-karafka-require-ruby-on-rails", "title": "Does Karafka require Ruby on Rails?", "text": "<p>No. Karafka is a fully independent framework that can operate in a standalone mode. It can be easily integrated with any Ruby-based application, including those written with Ruby on Rails. Please follow the Integrating with Ruby on Rails and other frameworks Wiki section.</p>"}, {"location": "FAQ/#why-there-used-to-be-an-applicationcontroller-mentioned-in-the-wiki-and-some-articles", "title": "Why there used to be an ApplicationController mentioned in the Wiki and some articles?", "text": "<p>You can name the main application consumer with any name. You can even call it <code>ApplicationController</code> or anything else you want. Karafka will sort that out, as long as your root application consumer inherits from the <code>Karafka::BaseConsumer</code>. It's not related to Ruby on Rails controllers. Karafka framework used to use the <code>*Controller</code> naming convention up until Karafka 1.2 where it was changed because many people had problems with name collisions.</p>"}, {"location": "FAQ/#does-karafka-require-redis-andor-sidekiq-to-work", "title": "Does Karafka require Redis and/or Sidekiq to work?", "text": "<p>No. Karafka is a standalone framework, with an additional process that will be used to consume Kafka messages.</p>"}, {"location": "FAQ/#could-an-http-controller-also-consume-a-fetched-message-through-the-karafka-router", "title": "Could an HTTP controller also consume a fetched message through the Karafka router?", "text": "<p>No. Kafka messages can be consumed only using Karafka consumers. You cannot use your Ruby on Rails HTTP consumers to consume Kafka messages, as Karafka is not an HTTP Kafka proxy. Karafka uses Kafka API for messages consumption.</p>"}, {"location": "FAQ/#does-karafka-require-a-separate-process-running", "title": "Does Karafka require a separate process running?", "text": "<p>No, however, it is recommended. By default, Karafka requires a separate process (Karafka server) to consume and process messages. You can read about it in the Consuming messages section of the Wiki.</p> <p>Karafka can also be embedded within another process so you do not need to run a separate process. You can read about it here.</p>"}, {"location": "FAQ/#can-i-start-karafka-process-with-only-particular-consumer-groups-running-for-given-topics", "title": "Can I start Karafka process with only particular consumer groups running for given topics?", "text": "<p>Yes. Karafka allows you to listen with a single consumer group on multiple topics, which means that you can tune up the number of threads that Karafka server runs, accordingly to your needs. You can also run multiple Karafka instances, specifying consumer groups that should be running per each process using the <code>--include-consumer-groups</code> server flag as follows:</p> <pre><code>bundle exec karafka server --include-consumer-groups group_name1 group_name3\n</code></pre> <p>You can also exclude particular groups the same way:</p> <pre><code>bundle exec karafka server --exclude-consumer-groups group_name1 group_name3\n</code></pre> <p>Visit the CLI section of our docs to learn more about how to limit the scope of things to which the server subscribes.</p>"}, {"location": "FAQ/#can-i-use-seek-to-start-processing-topics-partition-from-a-certain-point", "title": "Can I use <code>#seek</code> to start processing topics partition from a certain point?", "text": "<p>Karafka has a <code>#seek</code> consumer method that can be used to do that.</p>"}, {"location": "FAQ/#why-karafka-does-not-pre-initializes-consumers-prior-to-first-message-from-a-given-topic-being-received", "title": "Why Karafka does not pre-initializes consumers prior to first message from a given topic being received?", "text": "<p>Because Karafka does not have knowledge about the whole topology of a given Kafka cluster. We work on what we receive dynamically building consumers when it is required.</p>"}, {"location": "FAQ/#does-karafka-restart-dead-pg-connections", "title": "Does Karafka restart dead PG connections?", "text": "<p>Karafka, starting from <code>2.0.16</code> will automatically release no longer used ActiveRecord connections. They should be handled and reconnected by the Rails connection reaper. You can implement custom logic to reconnect them yourself if needed beyond the reaping frequency. More details on that can be found here(https://karafka.io/docs/Active-Record-Connections-Management/#dealing-with-dead-database-connections).</p>"}, {"location": "FAQ/#does-karafka-require-gems-to-be-thread-safe", "title": "Does Karafka require gems to be thread-safe?", "text": "<p>Yes. Karafka uses multiple threads to process data, similar to how Puma or Sidekiq does it. The same rules apply.</p>"}, {"location": "FAQ/#when-karafka-is-loaded-via-a-railtie-in-test-env-simplecov-does-not-track-code-changes", "title": "When Karafka is loaded via a railtie in test env, SimpleCov does not track code changes", "text": "<p>Karafka hooks with railtie to load <code>karafka.rb</code>. Simplecov needs to be required before any code is loaded.</p>"}, {"location": "FAQ/#can-i-use-threadcurrent-to-store-data-between-batches", "title": "Can I use Thread.current to store data between batches?", "text": "<p>No. The first available thread will pick up work from the queue to better distribute work. This means that you should not use <code>Thread.current</code> for any type of data storage.</p>"}, {"location": "FAQ/#why-karafka-process-does-not-pick-up-newly-created-topics-until-restarted", "title": "Why Karafka process does not pick up newly created topics until restarted?", "text": "<ul> <li>Karafka in the <code>development</code> mode will refresh cluster metadata every 5 seconds. It means that it will detect topic changes fairly fast.</li> <li>Karafka in <code>production</code> will refresh cluster metadata every 5 minutes. It is recommended to create production topics before running consumers.</li> </ul> <p>The frequency of cluster metadata refreshes can be changed via <code>topic.metadata.refresh.interval.ms</code> in the <code>kafka</code> config section.</p>"}, {"location": "FAQ/#why-is-karafka-not-doing-work-in-parallel-when-i-started-two-processes", "title": "Why is Karafka not doing work in parallel when I started two processes?", "text": "<p>Please make sure your topic contains more than one partition. Only then Karafka can distribute the work to more processes. Keep in mind, that all the topics create automatically with the first message sent will always contain only one partition. Use the Admin API to create topics with more partitions.</p>"}, {"location": "FAQ/#can-i-remove-a-topic-while-the-karafka-server-is-running", "title": "Can I remove a topic while the Karafka server is running?", "text": "<p>Not recommended. You may encounter the following errors if you decide to do so:</p> <pre><code>ERROR -- : librdkafka internal error occurred: Local: Unknown partition (unknown_partition)\nERROR -- : \nINFO -- : rdkafka: [thrd:main]: Topic extractor partition count changed from 1 to 0\nERROR -- : librdkafka internal error occurred: Broker: Unknown topic or partition (unknown_topic_or_part)\n</code></pre> <p>It is recommended to stop Karafka server instances and then remove and recreate the topic.</p>"}, {"location": "FAQ/#what-is-a-forceful-karafka-stop", "title": "What is a forceful Karafka stop?", "text": "<p>When you attempt to stop Karafka, you may notice the following information in your logs:</p> <pre><code>Received SIGINT system signal\nStopping Karafka server\nForceful Karafka server stop\n</code></pre> <p>When you ask Karafka to stop, it will wait for all the currently running jobs to finish. The <code>shutdown_timeout</code> configuration setting limits the time it waits. After this time passes and any work in listeners or workers are still being performed, Karafka will attempt to forcefully close itself, stopping all the work in the middle. If you see it happen, it means you need to either:</p> <ul> <li>extend the <code>shutdown_timeout</code> value to match your processing patterns</li> <li>debug your code to check what is causing the extensive processing beyond the <code>shutdown_timeout</code></li> </ul> <p>In any case, it is not recommended to ignore this if it happens frequently.</p>"}, {"location": "FAQ/#can-i-use-aws-msk-serverless-with-iam-authentication", "title": "Can I use AWS MSK Serverless with IAM authentication?", "text": "<p>No. IAM is a custom authentication engine that is not a part of the Kafka protocol and is not supported by <code>librdkafka</code>.</p> <p>Karafka supports following methods that work with AWS MSK:</p> <ul> <li>Standard SASL + SSL mechanisms.</li> <li>Custom OAuth Token Providers flow.</li> </ul>"}, {"location": "FAQ/#why-cant-i-connect-to-kafka-from-another-docker-container", "title": "Why can't I connect to Kafka from another Docker container?", "text": "<p>You need to modify the <code>docker-compose.yml</code> <code>KAFKA_ADVERTISED_HOST_NAME</code> value. You can read more about it here.</p>"}, {"location": "FAQ/#how-can-i-configure-multiple-bootstrap-servers", "title": "How can I configure multiple bootstrap servers?", "text": "<p>You need to define them comma-separated under <code>kafka</code> <code>bootstrap.servers</code> configuration key:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n\n    # This value needs to be a string string with comma separated servers\n    config.kafka = {\n      'bootstrap.servers': 'server1.address:9092, server2.address:9092'\n    }\n  end\nend\n</code></pre>"}, {"location": "FAQ/#why-when-using-cooperative-sticky-rebalance-strategy-all-topics-get-revoked-on-rebalance", "title": "Why, when using <code>cooperative-sticky</code> rebalance strategy, all topics get revoked on rebalance?", "text": "<p>This behavior can occur if you are using blocking <code>mark_as_consumed!</code> method and the offsets commit happens during rebalance. When using <code>cooperative-sticky</code> we recommend using <code>mark_as_consumed</code> instead.</p>"}, {"location": "FAQ/#what-will-happen-with-uncommitted-offsets-during-a-rebalance", "title": "What will happen with uncommitted offsets during a rebalance?", "text": "<p>When using <code>mark_as_consumed</code>, offsets are stored locally and periodically flushed to Kafka asynchronously.</p> <p>Upon rebalance, all uncommitted offsets will be committed before a given partition is re-assigned.</p>"}, {"location": "FAQ/#can-i-use-karafka-with-ruby-on-rails-as-a-part-of-an-internal-gem", "title": "Can I use Karafka with Ruby on Rails as a part of an internal gem?", "text": "<p>Karafka 2.x has Rails auto-detection, and it is loaded early, so some components may be available later, e.g., when ApplicationConsumer inherits from BaseConsumer that is provided by the separate gem that needs an initializer.</p> <p>Moreover, despite the same code base, some processes (<code>rails s</code>, <code>rails db:migrate</code>, <code>sidekiq s</code>) may not need to know about karafka, and there is no need to load it.</p> <p>The problem is presented in this example app PR.</p> <p>To mitigate this, you can create an empty karafka bootfile. With a file structure like this:</p> <pre><code>+-- karafka_root_dir\n|   +-- karafka.rb     # default bootfile (empty file)\n|   +-- karafka_app.rb # real bootfile with Karafka::App definition and other stuff\n|   +-- ...\n</code></pre> <p>It is possible to postpone the definition of the Karafka app and do it manually whenever &amp; wherever the user wants (<code>karafka_app.rb</code> could be loaded for example, in some initializer).</p> <pre><code># karafka_app.rb\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n    ...\n  end\nend\n\n# config/initializers/karafka_init.rb\n\nrequire 'karafka_root_dir/karafka_app'\n</code></pre> <p>Still not a perfect solution because karafka gem is still loaded.</p> <p>This description was prepared by AleksanderSzyszka.</p>"}, {"location": "FAQ/#can-i-skip-messages-on-errors", "title": "Can I skip messages on errors?", "text": "<p>Karafka Pro can skip messages non-recoverable upon errors as a part of the Enhanced Dead Letter Queue feature. You can read about this ability here.</p>"}, {"location": "FAQ/#what-does-static-consumer-fenced-by-other-consumer-with-same-groupinstanceid-mean", "title": "What does static consumer fenced by other consumer with same group.instance.id mean?", "text": "<p>If you see such messages in your logs:</p> <pre><code>Fatal error: Broker: Static consumer fenced by other consumer with same group.instance.id\n</code></pre> <p>It can mean two things:</p> <ol> <li>You are using the Karafka version before <code>2.0.20</code>. If that is the case, please upgrade.</li> <li>Your <code>group.instance.id</code> is not unique within your consumer group. You must always ensure that the value you assign to <code>group.instance.id</code> is unique within the whole consumer group, not unique per process or machine.</li> </ol>"}, {"location": "FAQ/#why-in-the-long-running-jobs-case-revoked-is-executed-even-if-consume-did-not-run-because-of-revocation", "title": "Why, in the Long-Running Jobs case, <code>#revoked</code> is executed even if <code>#consume</code> did not run because of revocation?", "text": "<p>The <code>#revoked</code> will be executed even though the <code>#consume</code> did not run upon revocation because <code>#revoked</code> can be used to teardown resources initialized prop to <code>#consume</code>. For example, for things initialized in a custom <code>initialize</code> method.</p>"}, {"location": "FAQ/#why-am-i-seeing-rdkafkardkafkaerror-local-timed-out-timed_out-error-when-producing-larger-quantities-of-messages", "title": "Why am I seeing <code>Rdkafka::RdkafkaError (Local: Timed out (timed_out)</code> error when producing larger quantities of messages?", "text": "<p>If you are seeing following error:</p> <pre><code>Rdkafka::RdkafkaError (Local: Timed out (timed_out)\n</code></pre> <p>It may mean one of four things:</p> <ol> <li>High probability: Broker can't keep up with the produce rate.</li> <li>High probability if you use <code>partition_key</code>: Broker is temporarily overloaded and cannot return info about the topic structure. A retry mechanism has been implemented in WaterDrop <code>2.4.4</code> to mitigate this.</li> <li>Low probability: Slow network connection.</li> <li>Low probability: SSL configuration issue. In this case, no messages would reach the broker.</li> </ol> <p>WaterDrop dispatches messages to <code>librdkafka</code> and <code>librdkafka</code> constructs message sets out of it. By default, it does it every five milliseconds. If you are producing messages fast, it may become inefficient for Kafka because it has to deal with separate incoming message sets and needs to keep up. Please consider increasing the <code>queue.buffering.max.ms</code>, so the batches are constructed less often and are bigger.</p> <p>Additionally, you may also:</p> <ul> <li>Dispatch smaller batches using <code>#produce_many_sync</code>.Effectively it will throttle the process that way.</li> <li>Establish a limit on how many messages you want to dispatch at once. This will prevent you from scenarios where you accidentally flush too much. If you dispatch based on an array of samples, you can do it that way:</li> </ul> <pre><code>data_to_dispatch.each_slice(2_00) do |data_slice|\n  Karafka.producer.produce_many_sync(data_slice)\nend\n</code></pre>"}, {"location": "FAQ/#do-i-need-to-use-revoked-when-not-using-long-running-jobs", "title": "Do I need to use <code>#revoked?</code> when not using Long-Running jobs?", "text": "<p>In a stable system, no. The Karafka default offset management strategy should be more than enough. It ensures that after batch processing as well as upon rebalances, before partition reassignment, all the offsets are committed.</p> <p>You can read about Karafka's revocation/rebalance behaviors here and here.</p>"}, {"location": "FAQ/#can-i-consume-from-more-than-one-kafka-cluster-simultaneously", "title": "Can I consume from more than one Kafka cluster simultaneously?", "text": "<p>Yes. Karafka allows you to redefine <code>kafka</code> settings on a per-topic basis. You can create separate consumer groups to consume from separate clusters:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    consumer_group :group_name do\n      topic :example do\n        kafka('bootstrap.servers': 'cluster1:9092')\n        consumer ExampleConsumer\n      end\n\n      topic :example2 do\n        kafka('bootstrap.servers': 'cluster1:9092')\n        consumer ExampleConsumer2\n      end\n    end\n\n    consumer_group :group_name2 do\n      topic :example3 do\n        kafka('bootstrap.servers': 'cluster2:9092')\n        consumer Example2Consumer3\n      end\n    end\n  end\nend\n</code></pre> <p>Please note that if your cluster configuration is complex, you may want to use set it up in the root scope and then alter it on a per-topic basis:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'enable.ssl.certificate.verification': kafka_config.ssl_verify_hostname,\n      'security.protocol': kafka_config.security_protocol,\n      'statistics.interval.ms': 1_000,\n      'ssl.key.password': kafka_config.auth[:cert_key_password],\n      'ssl.key.pem': Base64.decode64(kafka_config.auth[:base64_cert_key_pem]),\n      'ssl.certificate.pem': Base64.decode64(kafka_config.auth[:base64_client_cert_pem]),\n      'ssl.ca.pem': Base64.decode64(kafka_config.auth[:base64_ca_cert_pem])\n    }\n    end\n  end\n\n  routes.draw do\n    consumer_group :related_reviews do\n      topic :reviews do\n        target.kafka[:'bootstrap.servers'] = CLUSTERS[:related_reviews][:brokers]&amp;.join(',')\n        consumer ReviewsConsumer\n      end\n    end\n\n    consumer_group :related_products do\n      topic :products do\n        target.kafka[:'bootstrap.servers'] = CLUSTERS[:related_products][:brokers]&amp;.join(',')\n        consumer RelatedProductsConsumer\n      end\n    end\n  end\nend\n</code></pre> <p>Also, please remember that those settings apply to consumers only. <code>Karafka#producer</code> will always produce to the default cluster using the default settings. This may be confusing when working with things like Dead Letter Queue as the producer will produce the default cluster DLQ topic despite the origin cluster. You can read more about that behavior here.</p>"}, {"location": "FAQ/#why-karafka-uses-karafka-rdkafka-instead-of-rdkafka-directly", "title": "Why Karafka uses <code>karafka-rdkafka</code> instead of <code>rdkafka</code> directly?", "text": "<p>We release our version of the <code>rdkafka</code> gem to ensure it meets our quality and stability standards. That way, we ensure that unexpected <code>rdkafka</code> releases will not break the Karafka ecosystem.</p>"}, {"location": "FAQ/#why-am-i-seeing-an-implement-this-in-a-subclass-error", "title": "Why am I seeing an <code>Implement this in a subclass</code> error?", "text": "<pre><code>[bc01b9e1535f] Consume job for ExampleConsumer on my_topic started\nWorker processing failed due to an error: Implement this in a subclass\n</code></pre> <p>This error occurs when you have defined your consumer but without a <code>#consume</code> method:</p> <p>BAD:</p> <pre><code>class ExampleConsumer &lt; Karafka::BaseConsumer\n  # No consumption method\nend\n</code></pre> <p>GOOD:</p> <pre><code>class ExampleConsumer &lt; Karafka::BaseConsumer\n  def consume\n    messages.each do |message|\n      puts message.payload\n    end\n  end\nend\n</code></pre>"}, {"location": "FAQ/#what-is-karafka-client_id-used-for", "title": "What is Karafka <code>client_id</code> used for?", "text": "<p>Karafka <code>client_id</code> is, by default, used for populating kafka <code>client.id</code> value.</p> <p>kafka <code>client.id</code> is a string passed to the server when making requests. This is to track the source of requests beyond just IP/port by allowing a logical application name to be included in server-side request logging.</p>"}, {"location": "FAQ/#how-can-i-increase-kafka-and-karafka-max-message-size", "title": "How can I increase Kafka and Karafka max message size?", "text": "<p>To make Kafka accept messages bigger than 1MB, you must change both Kafka and Karafka configurations.</p> <p>To increase the maximum accepted payload size in Kafka, you can adjust the <code>message.max.bytes</code> and <code>replica.fetch.max.bytes</code> configuration parameters in the server.properties file. These parameters controls the maximum size of a message the Kafka broker will accept.</p> <p>To allow WaterDrop (Karafka producer) to send bigger messages, you need to:</p> <ul> <li>set the <code>max_payload_size</code> config option to value in bytes matching your maximum expected payload.</li> <li>set <code>kafka</code> scoped <code>message.max.bytes</code> to the same value.</li> </ul> <p>You can do this by reconfiguring WaterDrop during Karafka setup:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.producer = ::WaterDrop::Producer.new do |producer_config|\n      # Use all the settings already defined for consumer by default\n      producer_config.kafka = ::Karafka::Setup::AttributesMap.producer(config.kafka.dup)\n      producer_config.logger = config.logger\n\n      # Alter things you want to alter\n      producer_config.max_payload_size = 1_000_000_000\n      producer_config.kafka[:'message.max.bytes'] = 1_000_000_000\n    end\n  end\nend\n</code></pre> <p>It is essential to keep in mind that increasing the maximum payload size may impact the performance of your Kafka cluster, so you should carefully consider the trade-offs before making any changes.</p> <p>If you do not allow bigger payloads and try to send them, you will end up with one of the following errors:</p> <pre><code>WaterDrop::Errors::MessageInvalidError {:payload=&gt;\"is more than `max_payload_size` config value\"}\n</code></pre> <p>or</p> <pre><code>Rdkafka::RdkafkaError (Broker: Message size too large (msg_size_too_large)):\n</code></pre>"}, {"location": "FAQ/#why-do-dlq-messages-in-my-system-keep-disappearing", "title": "Why do DLQ messages in my system keep disappearing?", "text": "<p>DLQ messages may disappear due to many reasons. Some possible causes include the following:</p> <ul> <li>The DLQ topic has a retention policy that causes them to expire and be deleted.</li> <li>The DLQ topic is a compacted topic, which only retains the last message with a given key.</li> <li>The messages are being produced to a DLQ topic with a replication factor of 1, which means that if the broker storing the messages goes down, the messages will be lost.</li> </ul> <p>For more details, please look at the Compacting limitations section of the DLQ documentation.</p>"}, {"location": "FAQ/#what-is-the-optimal-number-of-threads-to-use", "title": "What is the optimal number of threads to use?", "text": "<p>The optimal number of threads for a specific application depends on various factors, including the number of processors and cores available, the amount of memory available, and the particular tasks the application performs and their type. In general, increasing number of threads brings the most significant benefits for IO-bound operations.</p> <p>It's recommended to use the number of available cores to determine the optimal number of threads for an application.</p> <p>When working with Karafka, you also need to take into consideration things that may reduce the number of threads being in use, that is:</p> <ul> <li>Your topics count.</li> <li>Your partitions count.</li> <li>Number of processes within a given consumer group.</li> <li>To how many topics and partitions a particular process is subscribed to.</li> </ul> <p>Karafka can parallelize work in a couple of scenarios, but unless you are a Karafka Pro user and you use Virtual Partitions, in a scenario where your process is assigned to a single topic partition, the work will always happen only in a single thread.</p> <p>You can read more about Karafka and Karafka Pro concurrency model here.</p> <p>It's also essential to monitor the performance of the application and the system as a whole while experimenting with different thread counts. This can help you identify bottlenecks and determine the optimal number of threads for the specific use case.</p> <p>Remember that the optimal number of threads may change as the workload and system resources change over time.</p>"}, {"location": "FAQ/#can-i-use-several-producers-with-different-configurations-with-karafka", "title": "Can I use several producers with different configurations with Karafka?", "text": "<p>Yes. You can create as many producers as you want using WaterDrop API directly:</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.deliver = true\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'request.required.acks': 1\n  }\nend\n</code></pre> <p>and you can use them.</p> <p>There are a few things to keep in mind, though:</p> <ol> <li>Producers should be long-lived.</li> <li>Producers should be closed before the process shutdown to ensure proper resource finalization.</li> <li>You need to instrument each producer using the WaterDrop instrumentation API.</li> <li>Karafka itself uses the <code>Karafka#producer</code> internal reasons such as error tracking, DLQ dispatches, and more. This means that the default producer instance should be configured to operate within the scope of Karafka's internal functionalities.</li> </ol>"}, {"location": "FAQ/#what-is-the-unsupported-value-ssl-for-configuration-property-securityprotocol-openssl-not-available-at-build-time", "title": "What is the Unsupported value \"SSL\" for configuration property \"security.protocol\": OpenSSL not available at build time?", "text": "<p>If you are seeing the following error:</p> <pre><code>`validate!':\n{:kafka=&gt;\"Unsupported value \"SSL\" for configuration property \"security.protocol\":\n OpenSSL not available at build time\"} (Karafka::Errors::InvalidConfigurationError)\n</code></pre> <p>It means you want to use SSL, but <code>librdkafka</code> was built without it. You have to:</p> <ol> <li>Uninstal it by running <code>gem remove karafka-rdkafka</code></li> <li>Install <code>openssl</code> (OS dependant but for macos, that would be <code>brew install openssl</code>)</li> <li>Run <code>bundle install</code> again, so <code>librdkafka</code> is recompiled with SSL support.</li> </ol>"}, {"location": "FAQ/#can-karafka-ask-kafka-to-list-available-topics", "title": "Can Karafka ask Kafka to list available topics?", "text": "<p>Yes. You can use admin API to do this:</p> <pre><code># Get cluster info and list all the topics\ninfo = Karafka::Admin.cluster_info\n\nputs info.topics.map { |topic| topic[:topic_name] }.join(', ')\n</code></pre>"}, {"location": "FAQ/#why-karafka-prints-some-of-the-logs-with-a-time-delay", "title": "Why Karafka prints some of the logs with a time delay?", "text": "<p>Karafka <code>LoggerListener</code> dispatches messages to the logger immediately. You may be encountering buffering in the stdout itself. This is done because IO operations are slow, and usually it makes more sense to avoid writing every single character immediately to the console.</p> <p>To avoid this behavior and instead write immediately to stdout, you can set it to a sync mode:</p> <pre><code>$stdout.sync = true\n</code></pre> <p>You can read more about sync here.</p>"}, {"location": "FAQ/#why-is-increasing-concurrency-not-helping-upon-a-sudden-burst-of-messages", "title": "Why is increasing <code>concurrency</code> not helping upon a sudden burst of messages?", "text": "<p>Karafka uses multiple threads to process messages from multiple partitions or topics in parallel. If your consumer process has a single topic partition assigned, increasing <code>concurrency</code> will not help because there is no work that could be parallelized.</p> <p>To handle such cases, you can:</p> <ul> <li>Increase the number of partitions beyond the number of active consumer processes to achieve multiple assignments in a single consumer process. In a case like this, the given process will be able to work in parallel.</li> <li>Use Virtual Partitions to parallelize the work of a single topic partition.</li> </ul> <p>You can read more about the Karafka concurrency model here.</p>"}, {"location": "FAQ/#why-am-i-seeing-a-needs-to-be-consistent-namespacing-style-error", "title": "Why am I seeing a \"needs to be consistent namespacing style\" error?", "text": "<p>Due to limitations in metric names, topics with a period (<code>.</code>) or underscore (<code>_</code>) could collide. To avoid issues, it is best to use either but not both.</p> <p>Karafka validates that your topics' names are consistent to minimize the collision risk. If you work with pre-existing topics, you can disable this check by setting <code>config.strict_topics_namespacing</code> value to <code>false</code>:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Do not validate topics naming consistency\n    config.strict_topics_namespacing = false\n  end\nend\n</code></pre>"}, {"location": "FAQ/#why-despite-setting-initial_offset-to-earliest-karafka-is-not-picking-up-messages-from-the-beginning", "title": "Why, despite setting <code>initial_offset</code> to <code>earliest</code>, Karafka is not picking up messages from the beginning?", "text": "<p>There are a few reasons why Karafka may not be picking up messages from the beginning, even if you set <code>initial_offset</code> to <code>earliest</code>:</p> <ol> <li>Consumer group already exists: If the consumer group you are using to consume messages already exists, Karafka will not start consuming from the beginning by default. Instead, it will start consuming from the last committed offset for that group. To start from the beginning, you need to reset the offsets for the consumer group using the Kafka CLI or using the Karafka consumer <code>#seek</code> method.</li> <li>Topic retention period: If the messages you are trying to consume are older than the retention period of the topic, they may have already been deleted from Kafka. In this case, setting <code>initial_offset</code> to <code>earliest</code> will not allow you to consume those messages.</li> <li>Message timestamps: If the messages you are trying to consume have timestamps that are older than the retention period of the topic, they may have already been deleted from Kafka. In this case, even setting <code>initial_offset</code> to <code>earliest</code> will not allow you to consume those messages.</li> <li>Kafka configuration: There may be a misconfiguration in your Kafka setup that is preventing Karafka from consuming messages from the beginning. For example, the <code>log.retention.ms</code> or <code>log.retention.bytes</code> settings may be set too low, causing messages to be deleted before you can consume them.</li> </ol> <p>To troubleshoot the issue, you can try:</p> <ul> <li>changing the Karafka <code>client_id</code> temporarily,</li> <li>renaming the consumer group,</li> <li>resetting the offsets for the consumer group using <code>#seek</code>,</li> <li>checking the retention period for the topic,</li> <li>verifying the messages timestamps,</li> <li>reviewing your Kafka configuration to ensure it is correctly set up for your use case.</li> </ul>"}, {"location": "FAQ/#should-i-tstp-wait-a-while-then-send-term-or-set-a-longer-shutdown_timeout-and-only-send-a-term-signal", "title": "Should I TSTP, wait a while, then send TERM or set a longer <code>shutdown_timeout</code> and only send a TERM signal?", "text": "<p>This depends on many factors:</p> <ul> <li>do you use <code>cooperative.sticky</code> rebalance strategy?</li> <li>do you use static group memberships?</li> <li>do you do rolling deploys or all at once?</li> <li>are your jobs long-running?</li> <li>are you ok with intermediate rebalances?</li> </ul> <p>The general rule is that if you want to ensure all of your current work finishes before you stop Karafka or that there won't be any short-lived rebalances, it is recommended to use <code>TSTP</code> and wait. When Karafka receives <code>TSTP</code> signal, it moves into a <code>quiet</code> mode. It won't accept any new work, but all the currently running and locally enqueued jobs will be finished. It will also not close any connections to Kafka, which means that rebalance will not be triggered.</p> <p>If you want to ensure that the shutdown always finishes in a given time, you should set the <code>shutdown_timeout</code> accordingly and use <code>TERM</code>, keeping in mind it may cause a forceful shutdown which kills the currently running jobs.</p> <p>If you decide to do a full deployment, you can send <code>TSTP</code> to all the processes, wait for all the work to be done (you can monitor if using the Web UI), and then stop the processes using <code>TERM</code>.</p>"}, {"location": "FAQ/#why-am-i-getting-error0a000086ssl-routinescertificate-verify-failed-after-upgrading-karafka", "title": "Why am I getting <code>error:0A000086:SSL routines::certificate verify failed</code> after upgrading Karafka?", "text": "<p>If you are getting following error after upgrading <code>karafka</code> and <code>karafka-core</code>:</p> <pre><code>SSL handshake failed: error:0A000086:SSL routines::certificate verify failed:  \nbroker certificate could not be verified, verify that ssl.ca.location is correctly configured or  \nroot CA certificates are installed (brew install openssl) (after 170ms in state SSL_HANDSHAKE)\n</code></pre> <p>Please disable the SSL verification in your configuration:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      # Other settings...\n      'enable.ssl.certificate.verification': false,\n      'ssl.endpoint.identification.algorithm': 'none'\n    }\n  end\nend\n</code></pre>"}, {"location": "FAQ/#why-am-i-seeing-a-karafka_admin-consumer-group-with-a-constant-lag-present", "title": "Why am I seeing a <code>karafka_admin</code> consumer group with a constant lag present?", "text": "<p>The <code>karafka_admin</code> consumer group was created when using certain admin API operations. After upgrading to karafka <code>2.0.37</code> or higher, this consumer group is no longer needed and can be safely removed.</p>"}, {"location": "FAQ/#can-i-consume-the-same-topic-independently-using-two-consumers-within-the-same-application", "title": "Can I consume the same topic independently using two consumers within the same application?", "text": "<p>Yes. You can define independent consumer groups operating within the same application. Let's say you want to consume messages from a topic called <code>event</code> using two consumers. You can do this as follows:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    consumer_group :db_storage do\n      topic :events do\n        consumer DbFlusherConsumer\n      end\n    end\n\n    consumer_group :s3_storage do\n      topic :events do\n        consumer S3StoringConsumer\n      end\n    end\n  end\nend\n</code></pre> <p>Such a setup will ensure that both of them can be processed independently in parallel. Error handling, dead letter queue, and all the other per-topic behaviors will remain independent despite consuming the same topic.</p>"}, {"location": "FAQ/#why-am-i-seeing-broker-failed-to-validate-record-invalid_record-error", "title": "Why am I seeing Broker failed to validate record (invalid_record) error?", "text": "<p>The error <code>Broker failed to validate record (invalid_record)</code> in Kafka means that the broker received a record that it could not accept. This error can occur if the record is malformed or does not conform to the schema expected by the broker.</p> <p>There are several reasons why a Kafka broker might reject some messages:</p> <ul> <li>Invalid message format: If the message format does not match the expected format of the topic, the broker may reject the message.</li> <li>Missing message key. If you use log compaction as your <code>cleanup.policy</code> Kafka will require you to provide the key. Log compaction ensures that Kafka will always retain at least the last known value for each message key within the log of data for a single topic partition. If you enable compaction for a topic, messages without a key may be rejected.</li> <li>Schema validation failure: If the message contains data that does not conform to the schema, the broker may reject the message. This can happen if the schema has changed or the data was not properly validated before being sent to Kafka.</li> <li>Authorization failure: If the client does not have the required permissions to write to the topic, the broker may reject the message.</li> <li>Broker capacity limitations: If the broker has limited resources and cannot handle the incoming message traffic, it may reject some messages.</li> </ul> <p>To resolve this error, it is essential to identify the root cause of the issue. Checking the message format and schema, ensuring proper authorization and permission, checking broker capacity, and addressing network issues can help resolve the issue. Additionally, monitoring Karafka logs to identify and resolve problems as quickly as possible is crucial.</p>"}, {"location": "FAQ/#how-can-i-make-polling-faster", "title": "How can I make polling faster?", "text": "<p>You can decrease the <code>max_wait_time</code> Karafka configuration or lower the <code>max_messages</code> setting.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other settings...\n\n    # Wait for messages at most 100ms\n    config.max_wait_time = 100\n    # If you got 10 messages faster than in 100ms also don't wait any longer\n    config.max_messages = 10\n  end\nend\n</code></pre>"}, {"location": "FAQ/#can-i-dynamically-add-consumer-groups-and-topics-to-a-running-karafka-process", "title": "Can I dynamically add consumer groups and topics to a running Karafka process?", "text": "<p>No. It is not possible. Changes like this require <code>karafka server</code> restart.</p>"}, {"location": "FAQ/#can-a-consumer-instance-be-called-multiple-times-from-multiple-threads", "title": "Can a consumer instance be called multiple times from multiple threads?", "text": "<p>No. Given consumer object instance will never be called/used from multiple threads simultaneously. Karafka ensures that a single consumer instance is always used from a single thread. Other threads may call the consumer object for coordination, but this is unrelated to your code.</p>"}, {"location": "FAQ/#can-multiple-threads-reuse-a-single-consumer-instance", "title": "Can multiple threads reuse a single consumer instance?", "text": "<p>A single consumer instance can perform work in many threads but only in one simultaneously. Karafka does not guarantee that consecutive batches of messages will be processed in the same thread, but it does ensure that the same consumer instance will process successive batches. A single consumer instance will never process any work in parallel.</p>"}, {"location": "FAQ/#what-does-broker-unknown-topic-or-partition-error-mean", "title": "What does <code>Broker: Unknown topic or partition</code> error mean?", "text": "<p><code>The Broker: Unknown topic or partition</code> error typically indicates that the Kafka broker cannot find the specified topic or partition that the client is trying to access.</p> <p>There are several possible reasons why this error might occur:</p> <ul> <li>The topic or partition may not exist on the broker. Double-check that the topic and partition you are trying to access exists on the Kafka cluster you are connecting to.</li> <li>The topic or partition may still need to be created. If you are trying to access a topic or partition that has not been created yet, you will need to create it before you can use it.</li> <li>The client may not have permission to access the topic or partition. Ensure that the client has the necessary permissions to read from or write to the topic or partition you are trying to access.</li> <li>The client may be using an incorrect topic or partition name. Ensure you use the correct topic or partition name in your client code.</li> </ul> <p>You can use Karafka Web UI or Karafka Admin API to inspect your cluster topics and ensure that the requested topic and partition exist.</p>"}, {"location": "FAQ/#why-some-of-consumer-subscriptions-are-not-visible-in-the-web-ui", "title": "Why some of consumer subscriptions are not visible in the Web UI?", "text": "<p>If some of your Karafka consumer subscriptions are not visible in the Karafka Web UI, there could be a few reasons for this:</p> <ul> <li>You are using Karafka Web older than the <code>0.4.1</code> version. Older Karafka Web UI versions used to only shows subscriptions that have at least one message processed.</li> <li>The consumer group that the subscription belongs to is not active. Karafka only displays active consumer groups in the Web UI. Make - sure that your consumer group is up and running. The subscription is not properly configured. Ensure that your subscription is appropriately defined, has the correct topic, and is active.</li> <li>There is a delay in the Karafka Web UI updating its data. Karafka Web UI may take a few seconds to update its data, especially if many subscriptions or messages are being processed.</li> </ul> <p>If none of these reasons explain why your subscriptions are not visible in the Karafka Web UI, you may need to investigate further and check your Karafka logs for any errors or warnings.</p>"}, {"location": "FAQ/#is-there-a-way-to-run-karafka-in-a-producer-only-mode", "title": "Is there a way to run Karafka in a producer-only mode?", "text": "<p>Yes, it is possible to run Karafka in producer-only mode. Karafka will not consume any messages from Kafka in this mode but only produce messages to Kafka.</p> <p>To run Karafka in producer-only mode, do not define any topics for consumption or set all of them as inactive:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # Leave this empty or set `active false` for all the topics\n  end\nend\n</code></pre> <p>With this configuration, Karafka will not create any consumer groups and will only initialize the <code>Karafka.producer</code>.</p> <p>Keep in mind that with this configuration, you will not be able to start <code>karafka server</code> but you will be able to access <code>Karafka.producer</code> from other processes like Puma or Sidekiq.</p>"}, {"location": "FAQ/#why-am-i-getting-the-cant-alloc-thread-threaderror-error-from-the-producer", "title": "Why am I getting the <code>can't alloc thread (ThreadError)</code> error from the producer?", "text": "<p>If you see this error from your Ruby process that is not a running Karafka process, you did not close the producer before finishing the process.</p> <p>It is recommended to always run <code>Karafka.producer.close</code> before finishing processes like rake tasks, Puma server, or Sidekiq, so Karafka producer has a chance to dispatch all pending messages and gracefully close.</p> <p>You can read more about producer shutdown here.</p>"}, {"location": "FAQ/#can-i-create-all-the-topics-needed-by-the-web-ui-manually", "title": "Can I create all the topics needed by the Web UI manually?", "text": "<p>While it is possible to create the necessary topics manually using the Kafka command-line tools, it is generally recommended to use the <code>bundle exec karafka-web install</code> command instead.</p> <p>This is because the <code>karafka-web install</code> command ensures that the topics are created with the correct configuration settings, including the appropriate number of partitions, retention policies, and other critical parameters for efficient and reliable message processing. If you create the topics manually, there is a risk that you may miss some configuration settings or make mistakes that can cause performance or stability issues.</p> <p>Overall, while it is technically possible to create the necessary topics for the Karafka Web UI manually, it is generally recommended to use the <code>karafka-web install</code> command instead.</p> <p>If you need to create them manually, please include the settings listed here.</p>"}, {"location": "FAQ/#can-i-consume-messages-from-a-rake-task", "title": "Can I consume messages from a Rake task?", "text": "<p>Yes. Karafka Pro provides the Iterator API that allows you to run one-off consumptions inline from within Rake tasks and any other Ruby processes.</p>"}, {"location": "FAQ/#do-you-provide-an-upgrade-support-when-upgrading-from-eol-versions", "title": "Do you provide an upgrade support when upgrading from EOL versions?", "text": "<p>While we always try to help anyone from the Karafka community with their problems, extensive upgrade support requiring involvement is part of our Pro Support offering.</p>"}, {"location": "FAQ/#why-there-are-so-many-karafka-strategies-in-the-codebase", "title": "Why there are so many Karafka strategies in the codebase?", "text": "<p>Karafka provides several different strategies for consuming messages from Kafka, each with its own trade-offs and use cases. The reason for this is to give developers the flexibility to choose the strategy that best fits their specific requirements, and another reason is code simplification. Particular strategies often differ with one or two lines of code, but those changes significantly impact how Karafka operates. With separate strategies, each case is handled independently and can be debugged and understood in isolation.</p> <p>But why would Karafka need multiple strategies in the codebase? The answer lies in the diverse range of use cases that Karafka is designed to support.</p> <p>By supporting multiple strategies in the codebase, Karafka can cater to a wide range of use cases and provide developers with the flexibility they need to build the applications they want.</p>"}, {"location": "FAQ/#why-am-i-having-problems-running-karafka-and-karafka-web-with-remote-kafka", "title": "Why am I having problems running Karafka and Karafka Web with remote Kafka?", "text": "<p>Karafka and librdkafka are not designed to work over unstable and slow network connections, and these libraries contain internal timeouts on network operations that slow networks may impact. As a result, it is recommended to use a local Docker-based Kafka instance for local development. We are aware of this issue and are actively working to make these timeouts configurable in the future. Using a local Kafka instance for local development can help you avoid network-related problems and ensure a smoother development experience.</p>"}, {"location": "FAQ/#why-after-moving-from-racecar-to-karafka-my-confluent-datadog-integration-stopped-working", "title": "Why after moving from Racecar to Karafka, my Confluent Datadog integration stopped working?", "text": "<p>When a new consumer group is introduced, Confluent reports things with a delay to Datadog. This is because the new consumer group needs to be registered with Confluent before it can start reporting metrics to Datadog.</p> <p>To ensure a smoother monitoring experience, we recommend enabling Karafka Datadog integration. It will allow you to easily monitor your Karafka operations and ensure everything is running smoothly. An out-of-the-box dashboard can be imported to Datadog for overseeing Karafka operations. This dashboard provides detailed metrics and insights into your Karafka operations, making identifying and resolving issues easier.</p>"}, {"location": "FAQ/#why-am-i-getting-env-cant-execute-bash-when-installing-karafka-in-an-alpine-docker", "title": "Why am I getting <code>env: can't execute 'bash'</code> when installing Karafka in an Alpine Docker?", "text": "<p>If you encounter the following error:</p> <pre><code>========================================================================\nenv: can't execute 'bash': No such file or directory\n========================================================================\nrake aborted!\nFailed to complete configure task\n/app/vendor/bundle/ruby/2.7.0/gems/mini_portile2-2.8.0/lib/mini_portile2/mini_portile.rb:460:in\n`block in execute'\n</code></pre> <p>you need to make sure that your Alpine-based image includes bash. Alpine Linux Docker image by default does not include it. To add it, please make sure to add this line before you run the <code>bundle install</code> process:</p> <pre><code>RUN apk update &amp;&amp; apk add bash\n</code></pre>"}, {"location": "FAQ/#can-i-intercept-waterdrop-messages-in-tests", "title": "Can I intercept WaterDrop messages in tests?", "text": "<p>Yes. You need to configure WaterDrop producer to use the <code>karafka-testing</code> spec dummy client:</p> <pre><code>require 'karafka/testing/errors'\nrequire 'karafka/testing/spec_consumer_client'\n\nRSpec.describe MyTestedLib do\n  subject(:my_lib) { described_class.new }\n\n  let(:karafka_producer_client) { Karafka::Testing::SpecProducerClient.new(self) }\n\n  before do\n    allow(MY_KARAFKA_PRODUCER).to receive(:client).and_return(karafka_producer_client)\n  end\n\n  it 'expect to dispatch one message' do\n    my_lib.do_something\n\n    expect(karafka_producer_client.messages.count).to eq(1)\n  end\nend\n</code></pre> <p>You can find the <code>SpecProducerClient</code> API here.</p>"}, {"location": "FAQ/#does-karafka-expiring-messages-remove-messages-from-kafka", "title": "Does Karafka Expiring Messages remove messages from Kafka?", "text": "<p>When a message is produced to a Kafka topic, it is stored in Kafka until it expires based on the retention policy of the topic. The retention policy determines how long messages are kept in Kafka before they are deleted.</p> <p>Karafka's Expiring Messages functionality removes messages from Karafka's internal processing queue after a specified amount of time has passed since the message was produced. This functionality is useful when processing messages with a limited lifetime, such as messages with time-sensitive data or messages that should not be processed after a certain amount of time has passed.</p> <p>However, it's important to note that Karafka's Expiring Messages functionality does not remove messages from Kafka itself, and it only removes messages from Karafka's internal processing queue. Therefore, the retention policy of the Kafka topic will still apply, and the message will remain in Kafka until it expires based on the topic's retention policy.</p> <p>To set the retention policy of a Kafka topic, you can use Kafka's built-in retention policies or configure custom retention policies using the declarative topics functionality. By configuring the retention policy, you can control how long messages are kept in Kafka before they are deleted, regardless of whether Karafka has processed them or not.</p>"}, {"location": "FAQ/#can-you-actively-ping-the-cluster-from-karafka-to-check-the-cluster-availability", "title": "Can you actively ping the cluster from Karafka to check the cluster availability?", "text": "<p>Yes, you can use Karafka's Admin API to retrieve cluster information and check the reachability of the Kafka cluster. The <code>Karafka::Admin.cluster_info</code> method can be used to retrieve metadata about the Kafka cluster, including details about brokers, topics, and partitions.</p> <p>If the method call is successful, it indicates that the Karafka application was able to connect to the Kafka cluster and retrieve metadata about the brokers and topics. However, it's important to note that this does not necessarily mean everything with the cluster is okay.</p> <p>\"Kafka being up\" is a rather complex matter. Many factors can affect the overall health and performance of a Kafka cluster, including network issues, broker failures, and misconfigured settings. Therefore, it's essential to use additional monitoring and alerting mechanisms to ensure the reliability and availability of your Kafka cluster.</p> <p>You can read more about this topic here.</p>"}, {"location": "FAQ/#how-do-i-specify-karafkas-environment", "title": "How do I specify Karafka's environment?", "text": "<p>Karafka uses the <code>KARAFKA_ENV</code> variable for that; if missing, it will try to detect it. You can read more about this topic here.</p>"}, {"location": "FAQ/#how-can-i-configure-waterdrop-with-scram", "title": "How can I configure WaterDrop with SCRAM?", "text": "<p>You can use the same setup as the one used by Karafka, described here.</p>"}, {"location": "FAQ/#why-am-i-getting-a-local-broker-transport-failure-transport-error-with-the-disconnected-info", "title": "Why am I getting a <code>Local: Broker transport failure (transport)</code> error with the <code>Disconnected</code> info?", "text": "<p>If you are seeing following or similar error:</p> <pre><code>rdkafka: [thrd:node_url]: node_url: Disconnected (after 660461ms in state UP)\nlibrdkafka internal error occurred: Local: Broker transport failure (transport)\n</code></pre> <p>The error message you mentioned may be related to the connection reaper in Kafka disconnecting because the TCP socket has been idle for a long time. The connection reaper is a mechanism in Kafka that monitors the idle TCP connections and disconnects them if they exceed a specific time limit. This is done to free up resources on the broker side and to prevent the accumulation of inactive connections.</p> <p>If the client application is not sending or receiving data over the TCP connection for a long time, the connection reaper may kick in and disconnect the client from the broker.</p> <p>However, this disconnection does not mean that any produced data will be lost. When the client application reconnects to the broker, it can resume sending or receiving messages from where it left off. </p> <p>Suppose your data production patterns are not stable, and there are times when your client application is not producing any data to Kafka for over 10 minutes. In that case, you may want to consider setting the <code>log.connection.close</code> value to <code>false</code> in your configuration. This configuration parameter controls whether the client logs a message when a connection is closed by the broker. By default, the client will log a message indicating that the connection was closed, which can generate false alarms if the connection was closed due to inactivity by the connection reaper.</p> <p>Setting <code>log.connection.close</code> to false will suppress these log messages and prevent the error from being raised. It's important to note that even if you set <code>log.connection.close</code> to <code>false,</code> critical non-recoverable errors that occur in Karafka and WaterDrop will still be reported via the instrumentation pipeline.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n\n    config.kafka = {\n      # other settings...\n      'log.connection.close': false\n    }\n  end\nend\n</code></pre> <p>Please note that you can control the <code>connections.max.idle.ms</code> on both Kafka and Karafka consumer / WaterDrop producer basis.</p> <p>You can read more about this issue here.</p>"}, {"location": "FAQ/#why-am-i-getting-a-all-broker-connections-are-down-all_brokers_down-error-together-with-the-disconnected-info", "title": "Why am I getting a <code>All broker connections are down (all_brokers_down)</code> error together with the <code>Disconnected</code> info?", "text": "<p>When you see both the <code>Disconnected</code> error and the <code>all_brokers_down</code> error, it means that the TCP connection to the cluster was closed and that you no longer have any active connections.</p> <p>Please read the explanation of the previous question to understand the reasons and get tips on mitigating this issue.</p> <pre><code>rdkafka: [thrd:node_url]: node_url: Disconnected (after 660461ms in state UP)\nlibrdkafka internal error occurred: Local: Broker transport failure (transport)\nError occurred: Local: All broker connections are down (all_brokers_down) - librdkafka.error\n</code></pre>"}, {"location": "FAQ/#what-is-the-difference-between-partition_key-and-key-in-the-waterdrop-gem", "title": "What is the difference between <code>partition_key</code> and <code>key</code> in the WaterDrop gem?", "text": "<p>In the WaterDrop gem, <code>partition_key</code> and <code>key</code> are two distinct options that can be used to set message keys, but they have different purposes and work slightly differently.</p> <ul> <li><code>partition_key</code> is used to determine the partition to which a message is sent and computes the destination partition in the Ruby process using the configured <code>partitioner</code> algorithm. The partitioner calculates a hash value based on the partition_key value and uses this hash value to select a partition for the message.</li> <li><code>key</code> is an optional property that can be set for a message. The Kafka broker uses the message key for log compaction, which ensures that only the latest message for a specific key is retained in the topic. Unless partition is explicitly provided via <code>partition</code> or <code>partition_key</code>, the <code>key</code> value will also be used for partition assignment. </li> </ul>"}, {"location": "FAQ/#how-can-i-set-up-waterdrop-with-scram", "title": "How can I set up WaterDrop with SCRAM?", "text": "<p>You can configure it the same way as Karafka support for SCRAM described here.</p>"}, {"location": "FAQ/#is-there-a-way-to-mark-messages-as-consumed-in-bulk", "title": "Is there a way to mark messages as consumed in bulk?", "text": "<p>In Kafka, there is no explicit need to mark messages as \"consumed\" in bulk because Kafka's offset mechanism takes care of this automatically.</p> <p>The moment you consume a message from a specific topic partition at a particular offset, Kafka considers all previous messages up to that offset as consumed.</p> <p>Kafka maintains a commit log that records the offset of each message within a topic partition. When a consumer reads messages from a partition, it keeps track of the offset of the last consumed message. This offset is then used to resume consumption from the same point if the consumer restarts or fails.</p> <p>When you mark a message as consumed with a higher offset, it implies that all previous messages with lower offsets have been successfully processed and considered consumed. Kafka's offset mechanism ensures that the consumer's offset is moved accordingly, indicating that those messages have been processed.</p> <p>While Kafka's offset mechanism automatically tracks the progress of message consumption and allows you to resume from the last consumed offset, there can be scenarios where explicitly marking each message as consumed becomes beneficial. This is particularly relevant when messages are processed sequentially, with a significant time gap between consuming each message.</p> <p>In such cases, marking each message as consumed provides finer-grained control over the consuming progress. By explicitly acknowledging the consumption of each message, you ensure that even if a crash or failure occurs during processing, the consumer can resume from the last successfully processed message.</p> <p>Here's an explanation of the benefits of marking each message as consumed:</p> <ul> <li> <p>Granular Progress Tracking: Marking each message as consumed allows you to have a more detailed view of the processing progress. You can precisely identify the last processed message and easily determine the remaining messages that need to be processed.</p> </li> <li> <p>Enhanced Fault Tolerance: In the event of a crash or failure, explicitly marking each message as consumed ensures that the consumer can restart from the last processed message rather than starting from the beginning or relying solely on the offset mechanism. This reduces duplicated processing and improves fault tolerance.</p> </li> <li> <p>Handling Long-running Processing: If the processing time for each message is significant, explicitly marking them as consumed provides better visibility into the progress. It allows you to identify any potential bottlenecks or delays in processing and take appropriate actions if needed.</p> </li> </ul> <p>When using Karafka Virtual Partitions, it is recommended to mark each message as consumed due to how Virtual Offset Management works.</p>"}, {"location": "FAQ/#how-can-i-consume-all-the-messages-from-a-kafka-topic-without-a-consumer-process", "title": "How can I consume all the messages from a Kafka topic without a consumer process?", "text": "<p>Karafka has an Iterator API for that. You can read about it here.</p>"}, {"location": "FAQ/#what-does-broker-invalid-message-invalid_msg-error-mean", "title": "What does <code>Broker: Invalid message (invalid_msg)</code> error mean?", "text": "<p>If you see the following error in your error tracking system:</p> <pre><code>ERROR -- : Listener fetch loop error: Broker: Invalid message (invalid_msg)\nERROR -- : gems/karafka-rdkafka-0.12.1/lib/rdkafka/consumer.rb:432:in `poll'\nERROR -- : gems/karafka-2.0.41/lib/karafka/connection/client.rb:368:in `poll'\n</code></pre> <p>It indicates that the broker contains a message that it cannot parse or understand. This error usually occurs when there is a mismatch or inconsistency in the format or structure of the message or when the message is corrupted.</p> <p>It is advised to check the Kafka logs around the polling time, as it may be a Kafka issue. You may encounter the following or similar errors:</p> <pre><code>org.apache.kafka.common.errors.CorruptRecordException:\n  Found record size 0 smaller than minimum record overhead (14)\n  in file /var/lib/my_topic-0/00000000000019077350.log.\n</code></pre> <p>This exception indicates a record has failed its internal CRC check; this generally indicates network or disk corruption.</p>"}, {"location": "FAQ/#is-there-an-option-in-karafka-to-re-consume-all-the-messages-from-a-topic-even-though-all-were-already-consumed", "title": "Is there an option in Karafka to re-consume all the messages from a topic even though all were already consumed?", "text": "<p>Yes.</p> <p>There are a few ways to do that:</p> <ol> <li>Use the Iterator API to run a one-time job alongside your regular Karafka consumption.</li> <li>Use the <code>#seek</code> consumer method in combination with Admin watermark API to move to the first offset and re-consume all the data.</li> <li>Create a new consumer group that will start from the beginning.</li> </ol>"}, {"location": "FAQ/#how-can-i-make-sure-that-karafkaproducer-does-not-blockdelay-my-processing", "title": "How can I make sure, that <code>Karafka.producer</code> does not block/delay my processing?", "text": "<p>To ensure that Karafka.producer does not block or delay your processing, you can utilize the <code>produce_async</code> and <code>produce_many_async</code>. These methods only block the execution flow if the underlying <code>librdkafka</code> queue is full.</p> <p>By default, if the queue is full, Karafka will enter a backoff state and wait for a specified time before retrying. The <code>wait_backoff_on_queue_full</code> and <code>wait_timeout_on_queue_full</code> settings in your Karafka configuration file control this behavior. If you want to disable the waiting behavior altogether, you can set the <code>wait_on_queue_full</code> option to <code>false</code>.</p> <p>Additionally, you can adjust the <code>message.timeout.ms</code> setting in <code>librdkafka</code> settings to potentially ignore the delivery handles of dispatched messages. By appropriately setting this value, you can reduce the time spent waiting for delivery confirmation, thus avoiding potential delays in your processing pipeline.</p> <p>When <code>wait_on_queue_full</code> is disabled and the queue becomes full, the producer will raise an exception. It's important to catch and handle this exception appropriately. You can ignore the exception if you don't want it to disrupt the execution flow of your program.</p> <p>Here's an example of how you can use <code>produce_async</code> and handle the exception:</p> <pre><code>begin\n  Karafka.producer.produce_async(topic: topic, payload: payload)\nrescue Rdkafka::RdkafkaError do |e|\n  raise unless e.code == :queue_full\nend\n</code></pre> <p>If you aim for maximum performance in your Karafka application, you can disable metrics collection by setting the <code>statistics.interval.ms</code> configuration to <code>0</code>. Doing so effectively disables the collection and emission of statistics data. This can be beneficial in scenarios where every bit of performance matters and you want to minimize any overhead caused by metric aggregation. However, it's important to note that disabling metrics collection will also prevent the Karafka Web UI from collecting important information, such as producer errors, including those in background threads. Therefore, consider the trade-off between performance optimization and the loss of detailed error tracking when deciding whether to disable metrics collection.</p>"}, {"location": "FAQ/#can-at_exit-be-used-to-close-the-waterdrop-producer", "title": "Can <code>at_exit</code> be used to close the WaterDrop producer?", "text": "<p><code>at_exit</code> is a Ruby method that allows you to register a block of code to be executed when the program is about to exit. It can be used for performing cleanup tasks or finalizing resources. However, using <code>at_exit</code> to close the WaterDrop producer in Karafka is not recommended.</p> <p>Instead of relying on <code>at_exit</code>, it is generally better to handle the cleanup and proper closing of the WaterDrop producer explicitly in your code. For example, you can use signal handlers from other Ruby libraries like Sidekiq or Puma.</p> <p>You can read more about this here.</p>"}, {"location": "FAQ/#why-when-dlq-is-used-with-max_retries-set-to-0-karafka-also-applies-a-back-off", "title": "Why, when DLQ is used with <code>max_retries</code> set to <code>0</code>, Karafka also applies a back-off?", "text": "<p>Even when no retries are requested, applying a back-off strategy is crucial in maintaining system stability and preventing system overload.</p> <p>When Karafka encounters an error processing a message, it might be due to a temporary or intermittent issue. Even if retries are not set, the system needs a moment to recover and stabilize after an error before moving on to the next message.</p> <p>By applying a back-off strategy, Karafka ensures that a pause is introduced between the occurrence of the error and the dispatch of the message to the Dead Letter Queue (DLQ) or the processing of the next message. This brief pause allows the system's resources to recover. For instance, if the error were due to a sudden spike in CPU usage, the back-off time would give the CPU a chance to cool down. If the error was due to a momentary network issue, the pause allows time for the network to stabilize.</p> <p>Without the back-off mechanism, even if retries are not requested, Karafka would move on to the next message immediately after an error. If errors are frequent, this could lead to the system getting into a state where it is constantly encountering errors and never getting a chance to recover. This, in turn, could lead to an overload of the system, causing degraded performance or even a complete system crash.</p>"}, {"location": "FAQ/#can-i-use-rdkafka-and-karafka-rdkafka-together-in-the-same-project", "title": "Can I use <code>rdkafka</code> and <code>karafka-rdkafka</code> together in the same project?", "text": "<p>No. <code>karafka-rdkafka</code> is a fork of <code>rdkafka</code> that includes many stability and performance enhancements while having a compatible API. If you try to use both, they will conflict with each other.</p>"}, {"location": "FAQ/#does-using-consumer-seek-resets-the-committed-offset", "title": "Does using consumer <code>#seek</code> resets the committed offset?", "text": "<p>No, using the <code>#seek</code> method in a Karafka consumer does not reset the committed offset.</p> <p>In Karafka, the <code>#seek</code> method is used to manually set the position of the next record that should be fetched, i.e., it changes the current position of the consumer. However, it does not affect the committed offset stored in Kafka.</p> <p>The committed offset is the position of the last record that Kafka will not read again in the event of recovery or failover. </p> <p>So, you can think of the position set by <code>#seek</code> as a volatile, in-memory value, while the committed offset is a more durable, stored value.</p> <p>If you would like to <code>#seek</code> back and be able to commit offsets from the seek location, please use the <code>reset_offset</code> flag when seeking:</p> <pre><code>def consume\n  # Some operations...\n  # ...\n  seek(100, reset_offset: true)\nend\n</code></pre>"}, {"location": "FAQ/#is-it-recommended-to-use-public-consumer-methods-from-outside-the-consumer", "title": "Is it recommended to use public consumer methods from outside the consumer?", "text": "<p>In general, it is not recommended to use public consumer methods from outside the consumer in Karafka.</p> <p>Karafka is designed to handle the concurrent processing of messages. Directly calling consumer methods from outside the consumer could result in race conditions or other concurrency issues if not done carefully.</p> <p>The only exception is when you are using Karafka instrumentation API. However, it is still not recommended to invoke any methods or operations that would result in consumer state changes.</p>"}, {"location": "FAQ/#why-do-i-see-sasl-authentication-error-after-aws-msk-finished-the-heal-cluster-operation", "title": "Why do I see <code>SASL authentication error</code> after AWS MSK finished the <code>Heal cluster</code> operation?", "text": "<p>Healing means that Amazon MSK is running an internal operation, like replacing an unhealthy broker. For example, the broker might be unresponsive. During this stage, under certain circumstances, the MSK permissions may be not restored correctly. We recommend that you reassign them back if the problem persists.</p>"}, {"location": "FAQ/#why-karafka-and-waterdrop-are-behaving-differently-than-rdkafka", "title": "Why Karafka and WaterDrop are behaving differently than <code>rdkafka</code>?", "text": "<ol> <li> <p><code>rdkafka-ruby</code> lack of instrumentation callbacks hooks by default: By default, <code>rdkafka</code> does not include instrumentation callback hooks. This means that it does not publish asynchronous errors unless explicitly configured to do so. On the other hand, Karafka and WaterDrop, provide a unified instrumentation framework that reports errors, even those happening asynchronously, by default.</p> </li> <li> <p>WaterDrop and Karafka use <code>karafka-rdkafka</code>, which is patched and provides specific improvements: Both WaterDrop and Karafka use a variant of <code>rdkafka-ruby</code>, known as <code>karafka-rdkafka</code>. This version is patched, meaning it includes improvements and modifications that the standard <code>rdkafka-ruby</code> client does not. These patches may offer enhanced performance, additional features, and/or bug fixes that can impact how the two systems behaves.</p> </li> <li> <p>Different setup conditions: Comparing different Kafka clients or frameworks can be like comparing apples to oranges if they aren't set up under the same conditions. Factors such as client configuration, Kafka cluster configuration, network latency, message sizes, targeted topics, and batching settings can significantly influence the behavior and performance of Kafka clients. Therefore, when you notice a discrepancy between the behavior of <code>rdkafka-ruby</code> and Karafka or WaterDrop, it might be because the conditions they are running under are not identical. To make a fair comparison, ensure that they are configured similarly and are running under the same conditions.</p> </li> </ol> <p>In summary, while <code>rdkafka-ruby</code>, Karafka, and WaterDrop all provide ways to interact with Kafka from a Ruby environment, differences in their design, their handling of errors, and the conditions under which they are run can result in different behavior. Always consider these factors when evaluating or troubleshooting these systems.</p>"}, {"location": "FAQ/#why-am-i-seeing-inconsistent-group-protocol-in-karafka-logs", "title": "Why am I seeing <code>Inconsistent group protocol</code> in Karafka logs?", "text": "<p>Seeing an <code>Inconsistent group protocol</code> message in your Karafka logs indicates a mismatch in the protocol type or version among the members of a Kafka consumer group.</p> <p>In Kafka, a consumer group consists of one or more consumers that jointly consume data from a topic. These consumers communicate with the Kafka broker and coordinate with each other to consume different partitions of the data. This coordination process is managed using group protocols.</p> <p>An <code>Inconsistent group protocol</code> error typically arises in the following scenarios:</p> <ul> <li> <p>Different consumers within the same group are using different protocol types or versions: Kafka supports several group protocols for consumer coordination, such as range or round-robin. If different consumers within the same group are configured with varying protocols or versions, this inconsistency will cause an error. Make sure all consumers in the group are using the same protocol.</p> </li> <li> <p>A mix of consumers with different session timeouts: When consumers in a group have different session timeout settings, they may not always be in sync, leading to this error. Ensure all consumers have the same session timeout setting. Misconfiguration during consumer setup: If you have recently made changes to your consumer setup, you might have inadvertently introduced a configuration that causes this error. Review your configuration changes to ensure consistency.</p> </li> </ul> <p>Consistency in consumer configuration within a group is vital to prevent this error. Review your consumers' settings and configurations to ensure they use the same group protocol, and adjust if necessary.</p>"}, {"location": "FAQ/#what-is-the-difference-between-waterdrops-max_payload_size-and-librdkafkas-messagemaxbytes", "title": "What is the difference between WaterDrop's <code>max_payload_size</code> and librdkafka's <code>message.max.bytes</code>?", "text": "<p>WaterDrop's <code>max_payload_size</code> and librdkafka's <code>message.max.bytes</code> are both settings related to message size in Kafka, but they play distinct roles and operate at different stages.</p> <p>WaterDrop's <code>max_payload_size</code> is a configuration parameter employed for internal validation within the WaterDrop producer library. This setting is used to limit the size of the messages before they're dispatched. If a message exceeds the <code>max_payload_size</code>, an error is raised, preventing the dispatch attempt. This setting helps ensure that you don't send messages larger than intended.</p> <p>On the other hand, librdkafka's <code>message.max.bytes</code> configuration is concerned with the Kafka protocol's message size. It represents the maximum permissible size of a message in line with the Kafka protocol, and the librdkafka library validates it. Essentially, it determines the maximum size of a ProduceRequest in Kafka.</p> <p>It's advisable to align these two settings to maintain consistency between the maximum payload size defined by WaterDrop and the Kafka protocol. To ensure that larger-than-expected messages are not accepted, it's beneficial to set the <code>max_payload_size</code> in WaterDrop. And for <code>message.max.bytes</code> in librdkafka, you might want to set it to the same value or even higher, bearing in mind its role in the Kafka protocol.</p> <p>There are a few nuances to be aware of, which are often seen as \"edge cases.\" One notable aspect is that the producer checks the uncompressed size of a message against the <code>message.max.bytes</code> setting while the broker validates the compressed size.</p> <p>Another noteworthy point is that if you set <code>message.max.bytes</code> to a low yet acceptable value, it could affect the batching process of librdkafka. Specifically, librdkafka might not be able to build larger message batches, leading to data being sent in much smaller batches, sometimes even as small as a single message. This could consequently limit the throughput.</p> <p>A detailed discussion on this topic can be found on this GitHub thread: https://github.com/confluentinc/librdkafka/issues/3246. Please note that this discussion remains open, indicating this topic's complexity and continuous exploration.</p> <p>Lastly, while the term <code>message.max.bytes</code> may not be intuitively understandable, its role in managing message size within the Kafka ecosystem is crucial.</p>"}, {"location": "FAQ/#what-are-consumer-groups-used-for", "title": "What are consumer groups used for?", "text": "<p>Consumer groups in Kafka are used to achieve parallel processing, high throughput, fault tolerance, and scalability in consuming messages from Kafka topics. They enable distributing of the workload among multiple consumers within a group, ensuring efficient processing and uninterrupted operation even in the presence of failures. In general, for 90% of cases, one consumer group is used per Karafka application. Using multiple consumer groups in a single app can be beneficial if you want to consume the same topic multiple times, structure your app for future division into microservices or introduce parallel consumption.</p>"}, {"location": "FAQ/#why-am-i-getting-the-all-topic-names-within-a-single-consumer-group-must-be-unique-error", "title": "Why am I getting the <code>all topic names within a single consumer group must be unique</code> error?", "text": "<p>If you are seeing the following error when starting Karafka:</p> <pre><code>{:topics=&gt;\"all topic names within a single consumer group must be unique\"}\n(Karafka::Errors::InvalidConfigurationError)\n</code></pre> <p>it indicates that you have duplicate topic names in your configuration of the same consumer group.</p> <p>In Karafka, each topic within a consumer group should have a unique name. This requirement is in place because each consumer within a consumer group reads from a unique partition of a specific topic. If there are duplicate topic names, then the consumers will not be able to distinguish between these topics.</p> <p>To solve this issue, you need to ensure that all topic names within a single consumer group in your Karafka configuration are unique.</p>"}, {"location": "FAQ/#why-am-i-getting-waterdroperrorsproduceerror-and-how-can-i-know-the-underlying-cause", "title": "Why am I getting <code>WaterDrop::Errors::ProduceError</code>, and how can I know the underlying cause?", "text": "<p>The specifics of why you're encountering this error will depend on the context of your use of WaterDrop and Kafka. Here are some possible causes:</p> <ul> <li> <p>Kafka is not running or unreachable: Ensure that Kafka is running and accessible from your application. If your application is running in a different environment (e.g., Docker, a different server, etc.), ensure there are no networking issues preventing communication.</p> </li> <li> <p>Invalid configuration: Your WaterDrop and/or Kafka configuration may be incorrect. This could involve things like incorrect broker addresses, authentication details, etc.</p> </li> <li> <p>Kafka topic does not exist: If you're trying to produce to a topic that doesn't exist, and if topic auto-creation is not enabled in your Kafka settings, the message production will fail.</p> </li> <li> <p>Kafka cluster is overloaded or has insufficient resources: If Kafka is not able to handle the volume of messages being produced, this error may occur.</p> </li> <li> <p>Kafka cluster is in a remote location with significant latency: Apache Kafka is designed to handle high-volume real-time data streams with low latency. If your Kafka cluster is located in a geographically distant location from your application or the network connectivity between your application and the Kafka cluster could be better, you may experience high latency. This can cause a variety of issues, including <code>WaterDrop::Errors::ProduceError</code>.</p> </li> <li> <p>Access Control Lists (ACLs) misconfiguration: ACLs control the permissions for Kafka resources; incorrect configurations might prevent messages from being produced or consumed. To diagnose, verify your Kafka ACLs settings to ensure your producer has the correct permissions for the operations it's trying to perform.</p> </li> </ul> <p>When you receive the <code>WaterDrop::Errors::ProduceError</code> error, you can check the underlying cause by invoking the <code>#cause</code> method on the received error:</p> <pre><code>error = nil\n\nbegin\n  Karafka.producer.produce_sync(topic: 'topic', payload: 'payload')\nrescue WaterDrop::Errors::ProduceError =&gt; e\n  error = e\nend\n\nputs error.cause\n\n#&lt;Rdkafka::AbstractHandle::WaitTimeoutError: Waiting for delivery timed out after 5 seconds&gt;\n</code></pre> <p>Please note that in the case of the <code>WaitTimeoutError</code>, the message may actually be delivered but in a more extended time because of the network or other issues. Always instrument your producers to ensure that you are notified about errors occurring in Karafka and WaterDrop internal background threads as well.</p> <p>The exact cause can often be determined by examining the error message and stack trace accompanying the <code>WaterDrop::Errors::ProduceError</code>. Also, check the Kafka logs for more information. If the error message or logs aren't clear, you should debug your code or configuration to identify the problem.</p> <p>If you're having trouble sending messages, a good debugging step is to set up a new producer with a shorter <code>message.timeout.ms</code> kafka setting. This means <code>librdkafka</code> won't keep retrying for long, and you'll see the main issue faster.</p> <pre><code># Create a producer configuration based on the Karafka one\nproducer_kafka_cfg = ::Karafka::Setup::AttributesMap.producer(\n  Karafka::App.config.kafka.dup\n)\n\n# Set the message timeout to five seconds to get the underlying error fast\nproducer_kafka_cfg[:'message.timeout.ms'] = 5_000\n\n# Build a producer\nproducer = ::WaterDrop::Producer.new do |p_config|\n  p_config.kafka = producer_kafka_cfg\n  p_config.logger = Karafka::App.config.logger\nend\n\n# Print all async errors details\nproducer.monitor.subscribe('error.occurred') do |event|\n  p event\n  p event[:error]\nend\n\n# Try to dispatch message to the topic with which you have problem\n#\n# Please note, that we use `#produce_async` here and wait.\n# That's because we do not want to crash the execution but instead wait on\n# the async error to appear.\nproducer.produce_async(\n  topic: 'problematic_topic',\n  payload: 'test'\n)\n\n# Wait for the async error (if any)\n# librdkafka will give up after 5 seconds, so this should be more than enough\nsleep(30)\n</code></pre>"}, {"location": "FAQ/#can-extra-information-be-added-to-the-messages-dispatched-to-the-dlq", "title": "Can extra information be added to the messages dispatched to the DLQ?", "text": "<p>Yes. Karafka Enhanced DLQ provides the ability to add custom details to any message dispatched to the DLQ. You can read about this feature here.</p>"}, {"location": "FAQ/#why-does-waterdrop-hang-when-i-attempt-to-close-it", "title": "Why does WaterDrop hang when I attempt to close it?", "text": "<p>WaterDrop works so that when the producer is requested to be closed, it triggers a process to flush out all the remaining messages in its buffers. The process is synchronous, meaning that it will hold the termination of the application until all the messages in the buffer are either delivered successfully or evicted from the queue.</p> <p>If Kafka is down, WaterDrop will still attempt to wait before closing for as long as there is even a single message in the queue. This waiting time is governed by the <code>message.timeout.ms</code> setting in the Kafka configuration. This setting determines how long the <code>librdkafka</code> library should keep the message in the queue and how long it should retry to deliver it. By default, this is set to 5 minutes.</p> <p>Effectively, this means that if the Kafka cluster is down, WaterDrop will not terminate or give up on delivering the messages until after this default timeout period of 5 minutes. This ensures maximum efforts are made to deliver the messages even under difficult circumstances.</p> <p>If a message is eventually evicted from the queue due to unsuccessful delivery, an error is emitted via the <code>error.occurred</code> channel in the WaterDrop's instrumentation bus. This allows developers to catch, handle, and log these events, giving them insight into any issues that might be causing message delivery failures.</p> <p>In summary, the hanging issue you are experiencing when attempting to close WaterDrop is a designed behavior intended to ensure all buffered messages are delivered to Kafka before the client stops, even if the Kafka cluster is temporarily unavailable.</p>"}, {"location": "FAQ/#why-karafka-commits-offsets-on-rebalances-and-librdkafka-does-not", "title": "Why Karafka commits offsets on rebalances and <code>librdkafka</code> does not?", "text": "<p>While Karafka uses <code>librdkafa</code> under the hood, they serve slightly purposes and follow different design principles.</p> <p>Karafka is designed with certain assumptions, such as auto-committing offsets, to simplify its usage for Ruby developers. One of the key decisions is to commit offsets on rebalances and assume that the offset management is done using Kafka itself with optional additional offset storage when needed. The reason behind this is to ensure that messages are processed only once in the case of a group rebalance. By committing offsets on rebalances, Karafka tries to ensure at-least-once delivery. That is, every message will be processed at least once, and no message will be lost, which is a typical requirement in many data processing tasks.</p> <p>On the other hand, <code>librdkafka</code> is a C library that implements the Apache Kafka protocol. It's designed to be more flexible and to offer more control to the user. It doesn't commit offsets on rebalances by default because it gives power to the application developer to decide when and how to commit offsets and where to store them. Depending on the specific requirements of your application, you may want to handle offsets differently.</p> <p>So the difference between the two libraries is mainly due to their different design principles and target audiences: Karafka is more opinionated and tries to simplify usage for Ruby developers, while <code>librdkafka</code> is more flexible and provides more control to the user but at the same time requires much more knowledge and effort.</p>"}, {"location": "FAQ/#what-is-karafkas-assignment-strategy-for-topics-and-partitions", "title": "What is Karafka's assignment strategy for topics and partitions?", "text": "<p>As of Karafka <code>2.0</code>, the default assignment strategy is <code>range</code>, which means that it attempts to assign partitions contiguously. For instance, if you have ten partitions and two consumers, then the first consumer might be assigned partitions 0-4, and the second consumer would be given partitions 5-9.</p> <p>The <code>range</code> strategy has some advantages over the <code>round-robin</code> strategy, where partitions are distributed evenly but not contiguously among consumers.</p> <p>Since data is often related within the same partition, <code>range</code> can keep related data processing within the same consumer, which could lead to benefits like better caching or business logic efficiencies. This can be useful, for example, to join records from two topics with the same number of partitions and the same key-partitioning logic.</p> <p>The assignment strategy is not a one-size-fits-all solution and can be changed based on the specific use case. If you want to change the assignment strategy in Karafka, you can set the <code>partition.assignment.strategy</code> configuration value to either <code>range</code>, <code>roundrobin</code> or <code>cooperative-sticky</code>. It's important to consider your particular use case, the number of consumers, and the nature of your data when choosing your assignment strategy.</p>"}, {"location": "FAQ/#why-cant-i-see-the-assignment-strategyprotocol-for-some-karafka-consumer-groups", "title": "Why can't I see the assignment strategy/protocol for some Karafka consumer groups?", "text": "<p>The assignment strategy or protocol for a Karafka consumer group might not be visible if a topic is empty, no data has been consumed, and no offsets were stored. These conditions indicate that no data has been produced to the topic and no consumer group has read any data, leaving no record of consumed data.</p> <p>In such cases, Kafka doesn't have any information to establish an assignment strategy. Hence, it remains invisible until data is produced, consumed, and offsets are committed.</p>"}, {"location": "FAQ/#what-can-be-done-to-log-why-the-produce_sync-has-failed", "title": "What can be done to log why the <code>produce_sync</code> has failed?", "text": "<p>WaterDrop allows you to listen to all errors that occur while producing messages and in its internal background threads. Things like reconnecting to Kafka upon network errors and others unrelated to publishing messages are all available under error.occurred notification key. You can subscribe to this event to ensure your setup is healthy and without any problems that would otherwise go unnoticed as long as messages are delivered:</p> <pre><code>Karafka.producer.monitor.subscribe('error.occurred') do |event|\n  error = event[:error]\n\n  p \"WaterDrop error occurred: #{error}\"\nend\n\n# Run this code without Kafka cluster\nloop do\n  Karafka.producer.produce_async(topic: 'events', payload: 'data')\n\n  sleep(1)\nend\n\n# After you stop your Kafka cluster, you will see a lot of those:\n#\n# WaterDrop error occurred: Local: Broker transport failure (transport)\n#\n# WaterDrop error occurred: Local: Broker transport failure (transport)\n</code></pre> <p>It is also recommended to check if the standard <code>LoggerListener</code> is enabled for the producer in your <code>karafka.rb</code>:</p> <pre><code>Karafka.producer.monitor.subscribe(\n  WaterDrop::Instrumentation::LoggerListener.new(Karafka.logger)\n)\n</code></pre> <p><code>error.occurred</code> will also include any errors originating from <code>librdkafka</code> for synchronous operations, including those that are raised back to the end user.</p>"}, {"location": "FAQ/#can-i-password-protect-karafka-web-ui", "title": "Can I password-protect Karafka Web UI?", "text": "<p>Yes, you can password-protect the Karafka Web UI, and it is highly recommended. Adding a layer of password protection adds a level of security to the interface, reducing the risk of unauthorized access to your data, configurations, and system settings.</p> <p>Karafka provides ways to implement password protection, and you can find detailed steps and guidelines here.</p>"}, {"location": "FAQ/#can-i-use-a-karafka-producer-without-setting-up-a-consumer", "title": "Can I use a Karafka producer without setting up a consumer?", "text": "<p>Yes, it's possible to use a Karafka producer without a consumer in two ways:</p> <ol> <li> <p>You can use WaterDrop, a standalone Karafka component for producing Kafka messages. WaterDrop was explicitly designed for use cases where only message production is required, with no need for consumption.</p> </li> <li> <p>Alternatively, if you have Karafka already in your application, avoid running the <code>karafka server</code> command, as it won't make sense without any topics to consume. You can run other processes and produce messages from them. In scenarios like that, there is no need to define any routes. <code>Karafka#producer</code> should operate without any problems.</p> </li> </ol> <p>Remember, if you're using Karafka without a consumer and encounter errors, ensure your consumer is set to inactive (active false), and refrain from running commands that necessitate a consumer, such as karafka server.</p>"}, {"location": "FAQ/#what-will-happen-when-a-message-is-dispatched-to-a-dead-letter-queue-topic-that-does-not-exist", "title": "What will happen when a message is dispatched to a dead letter queue topic that does not exist?", "text": "<p>When a message is dispatched to a dead letter queue (DLQ) topic that does not exist in Apache Kafka, the behavior largely depends on the <code>auto.create.topics.enable</code> Kafka configuration setting and the permissions of the Kafka broker. If <code>auto.create.topics.enable</code> is <code>true</code>, Kafka will automatically create the non-existent DLQ topic with one partition using the broker's default configurations, and the message will then be stored in the new topic.</p> <p>On the other hand, if <code>auto.create.topics.enable</code> is set to <code>false</code>, Kafka will not auto-create the topic, and instead, an error will be raised when trying to produce to the non-existent DLQ topic. This error could be a topic authorization exception if the client doesn't have permission to create topics or <code>unknown_topic_or_part</code> if the topic doesn't exist and auto-creation is disabled. </p> <p>In production environments, <code>auto.create.topics.enable</code> is often set to <code>false</code> to prevent unintended topic creation.</p> <p>For effective management of DLQs in Kafka, we recommend using Karafka's Declarative Topics, where you declare your topics in your code. This gives you more control over the specifics of each topic, such as the number of partitions and replication factors, and helps you avoid unintended topic creation. It also aids in efficiently managing and monitoring DLQs in your Kafka ecosystem.</p> <p>Below you can find an example routing that includes a DLQ declaration as well as a declarative definition of the target DLQ topic:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic 'events' do\n      config(\n        partitions: 6,\n        replication_factor: 3,\n        'retention.ms': 31 * 86_400_000 # 31 days in ms,\n        'cleanup.policy': 'delete'\n      )\n\n      consumer EventsConsumer\n\n      dead_letter_queue(\n        topic: 'dlq',\n        max_retries: 2\n      )\n    end\n\n    topic 'dlq' do\n      config(\n        partitions: 2,\n        replication_factor: 2,\n        'retention.ms': 31 * 86_400_000 # 31 days in ms,\n      )\n\n      # Set to false because of no automatic DLQ handling\n      active false\n    end\n  end\nend\n</code></pre>"}, {"location": "FAQ/#why-do-karafka-reports-lag-when-processes-are-not-overloaded-and-consume-data-in-real-time", "title": "Why do Karafka reports lag when processes are not overloaded and consume data in real-time?", "text": "<p>Kafka's consumer lag, which is the delay between a message being written into a Kafka topic and being consumed, is dictated not only by the performance of your consumers but also by how messages are marked as consumed in Kafka. This process of marking messages as consumed is done by committing offsets.</p> <p>After processing each message or batch, consumers can commit the offset of messages that have been processed, to Kafka, to mark them as consumed. So, Kafka considers the highest offset that a consumer group has committed for a partition as the current position of the consumer group in that partition.</p> <p>Now, if we look at Karafka, it follows a similar mechanism. In Karafka, by default, offsets are committed automatically in batches after a batch of messages is processed. That means if a batch is still being processed, the messages from that batch are not marked as consumed, even if some of them have already been processed, and hence those messages will still be considered as part of the consumer lag.</p> <p>This lag will grow with incoming messages, which is why it's not uncommon to see a lag of the size of one or two batches, especially in topics with high data traffic.</p> <p>To mitigate this situation, you can configure Karafka to prioritize latency over throughput. That means making Karafka commit offsets more frequently, even after each message, to decrease the lag and to fetch data more frequently in smaller batches. But keep in mind that committing offsets more frequently comes with the cost of reduced throughput, as each offset commit is a network call and can slow down the rate at which messages are consumed.</p> <p>You can adjust this balance between latency and throughput according to your specific use case and the performance characteristics of your Kafka cluster. You could increase the frequency of committing offsets during peak load times and decrease it during off-peak times if it suits your workload pattern.</p>"}, {"location": "FAQ/#does-kafka-guarantee-message-processing-orders-within-a-single-partition-for-single-or-multiple-topics-and-does-this-mean-kafka-topics-consumption-run-on-a-single-thread", "title": "Does Kafka guarantee message processing orders within a single partition for single or multiple topics? And does this mean Kafka topics consumption run on a single thread?", "text": "<p>Yes, within Kafka, the order of message processing is guaranteed for messages within a single partition, irrespective of whether you're dealing with one or multiple topics. However, this doesn't imply that all Kafka topics run on a single thread. In contrast, Karafka allows for multithreaded processing of topics, making it possible to process multiple topics or partitions concurrently. In some cases, you can even process data from one topic partition concurrently.</p>"}, {"location": "FAQ/#why-can-i-produce-messages-to-my-local-kafka-docker-instance-but-cannot-consume", "title": "Why can I produce messages to my local Kafka docker instance but cannot consume?", "text": "<p>There are several potential reasons why you can produce messages to your local Kafka Docker instance but cannot consume them. Here are some of the common issues and how to resolve them:</p> <ol> <li> <p>Network Issues: Docker's networking can cause problems if not configured correctly. Make sure your Kafka and Zookeeper instances can communicate with each other. Use the docker inspect command to examine the network settings of your containers.</p> </li> <li> <p>Configuration Errors: Incorrect settings in Kafka configuration files, such as the <code>server.properties</code> file, could lead to this issue. Make sure that you've configured things like the <code>advertised.listeners</code> property correctly.</p> </li> <li> <p>Incorrect Consumer Group: If you're consuming messages from a topic that a different consumer has already consumed in the same group, you won't see any messages. Kafka uses consumer groups to manage which messages have been consumed. You should use a new group or reset the offset of the existing group.</p> </li> <li> <p>Security Protocols: If you've set up your Kafka instance with security protocols like SSL/TLS or SASL, you'll need to ensure that your consumer is correctly configured to use these protocols.</p> </li> <li> <p>Offset Issue: The consumer might be reading from an offset where no messages exist. This often happens if the offset is set to the latest, but the messages were produced before the consumer started. Try consuming from the earliest offset to see if this resolves the issue.</p> </li> <li> <p>Zookeeper Connection Issue: Sometimes, the issue could be a faulty connection between Kafka and Zookeeper. Ensure that your Zookeeper instance is running without issues.</p> </li> </ol> <p>Remember, these issues are common, so don't worry if you face them. Persistence and careful debugging are key in these situations.</p> <p>If you're looking for a Kafka setup that doesn't require Zookeeper, consider using KRaft (KRaft is short for Kafka Raft mode), a mode in which Kafka operates in a self-managed way without Zookeeper. This new mode introduced in Kafka <code>2.8.0</code> allows you to reduce the operational complexity by eliminating the Zookeeper dependency.</p>"}, {"location": "FAQ/#what-is-the-release-schedule-for-karafka-and-its-components", "title": "What is the release schedule for Karafka and its components?", "text": "<p>Karafka and Karafka Pro do not follow a fixed official release schedule. Instead:</p> <ul> <li> <p>Releases containing breaking changes are rolled out once they are fully documented, and migration guides are prepared.</p> </li> <li> <p>New features are released as soon as they are ready and thoroughly documented.</p> </li> <li> <p>Bug fixes that don't involve API changes are released immediately.</p> </li> </ul> <p>We prioritize bugs and critical performance improvements to ensure optimal user experience and software performance. It's worth noting that most bugs are identified, reproduced, and fixed within seven days from the initial report acknowledgment.</p>"}, {"location": "FAQ/#can-i-pass-custom-parameters-during-consumer-initialization", "title": "Can I pass custom parameters during consumer initialization?", "text": "<p>No. In Karafka, consumers are typically created within the standard lifecycle of Kafka operations, after messages are polled but before they are consumed. This creation happens in the listener loop before work is delegated to the workers' queue.</p> <ul> <li> <p>You have the flexibility to modify the <code>#initialize</code> method. However, there are some nuances to note:</p> </li> <li> <p>You can redefine the <code>#initialized</code> but not define it with arguments, i.e., <code>def initialized(args)</code> is not allowed.</p> </li> <li> <p>If you redefine <code>#initialize</code>, you need to call <code>super</code>.</p> </li> <li> <p>While you can perform actions during initialization, be cautious not to overload this phase with heavy tasks or large resource loads like extensive caches. This is because the initialization happens in the listener loop thread, and any extensive process here could block message consumption.</p> </li> <li> <p>If there's a minor delay (a few seconds) during initialization, it's acceptable.</p> </li> </ul> <p>Furthermore, with no arguments in the initialize method, this API structure is designed for your customization, and there are no plans to change this in the foreseeable future.</p>"}, {"location": "FAQ/#where-can-i-find-producer-idempotence-settings", "title": "Where can I find producer idempotence settings?", "text": "<p>They are located in the WaterDrop wiki idempotence section.</p>"}, {"location": "FAQ/#how-can-i-control-or-limit-the-number-of-postgresql-database-connections-when-using-karafka", "title": "How can I control or limit the number of PostgreSQL database connections when using Karafka?", "text": "<p>Karafka, by itself, does not manage PostgreSQL or any other database connections directly. More details about that are available here.</p>"}, {"location": "FAQ/#why-is-my-karafka-application-consuming-more-memory-than-expected", "title": "Why is my Karafka application consuming more memory than expected?", "text": "<p>Several factors may lead to increased memory consumption:</p> <ul> <li> <p>Large Payloads: Handling large message payloads can inherently consume more memory. Remember that Karafka will keep the raw payload alongside newly deserialized information after the message is deserialized.</p> </li> <li> <p>Batch Processing: This can accumulate memory usage if you're processing large batches containing bigger messages.</p> </li> <li> <p>Memory Leaks: There might be memory leaks in your application or the libraries you use.</p> </li> </ul> <p>If your problems originate from batch and message sizes, we recommend looking into our Pro Cleaner API.</p>"}, {"location": "FAQ/#how-can-i-optimize-memory-usage-in-karafka", "title": "How can I optimize memory usage in Karafka?", "text": "<ul> <li> <p>Use Cleaner API: The Cleaner API, a part of Karafka Pro, provides a powerful mechanism to release memory used by message payloads once processed. This becomes particularly beneficial for 10KB or larger payloads, yielding considerable memory savings and ensuring a steadier memory usage pattern.</p> </li> <li> <p>Adjust <code>librdkafka</code> Memory Settings: The underlying library, <code>librdkafka</code>, has configurations related to memory usage. Tuning these settings according to your application's needs can optimize the memory footprint. Amongst others you may be interested in looking into the following settings: <code>fetch.message.max.bytes</code>, <code>queued.min.messages</code>, <code>queued.max.messages.kbytes</code> and <code>receive.message.max.bytes</code></p> </li> <li> <p>Modify the <code>max_messages</code> Value: By adjusting the <code>max_messages</code> setting to a lower value, you can control the number of messages deserialized in a batch. Smaller batches mean less memory consumption at a given time, although it might mean more frequent fetch operations. Ensure that you balance memory usage with processing efficiency while adjusting this value.</p> </li> </ul> <p>While tuning these settings can help optimize memory usage, it's essential to remember that it may also influence performance, latency, and other operational aspects of your Karafka applications. Balancing the memory and performance trade-offs based on specific application needs is crucial. Always monitor the impacts of changes and adjust accordingly.</p>"}, {"location": "FAQ/#why-am-i-getting-no-such-file-or-directory-ps-errnoenoent-from-the-web-ui", "title": "Why am I getting <code>No such file or directory - ps (Errno::ENOENT)</code> from the Web UI?", "text": "<p>If you are seeing the following error:</p> <pre><code>INFO pid=1 tid=gl9 Running Karafka 2.3.0 server\n#&lt;Thread:0x0000aaab008cc9d0 karafka-2.3.0/lib/karafka/helpers/async.rb:25 run&gt;\n# terminated with exception (report_on_exception is true):\nTraceback (most recent call last):\n17: lib/karafka/helpers/async.rb:28:in `block in async_call'\n16: lib/karafka/connection/listener.rb:48:in `call'\n15: lib/karafka/core/monitoring/monitor.rb:34:in `instrument'\n14: lib/karafka/core/monitoring/notifications.rb:101:in `instrument'\n13: lib/karafka/core/monitoring/notifications.rb:101:in `each'\n12: lib/karafka/core/monitoring/notifications.rb:105:in `block in instrument'\n11: lib/karafka/web/tracking/consumers/listeners/status.rb:18:in \\\n  `on_connection_listener_before_fetch_loop'\n10: lib/forwardable.rb:238:in `report'\n 9: lib/karafka/web/tracking/consumers/reporter.rb:35:in `report'\n 8: lib/karafka/web/tracking/consumers/reporter.rb:35:in `synchronize'\n 7: lib/karafka/web/tracking/consumers/reporter.rb:45:in `block in report'\n 6: lib/karafka/web/tracking/consumers/sampler.rb:68:in `to_report'\n 5: lib/karafka/web/tracking/consumers/sampler.rb:163:in `memory_total_usage'\n 4: lib/karafka/web/tracking/memoized_shell.rb:32:in `call'\n 3: open3.rb:342:in `capture2'\n 2: open3.rb:159:in `popen2'\n 1: open3.rb:213:in `popen_run'\n/usr/lib/ruby/2.7.0/open3.rb:213:in `spawn': No such file or directory - ps (Errno::ENOENT)\n</code></pre> <p>it typically indicates that the Karafka Web UI is trying to execute the <code>ps</code> command but the system cannot locate it. This can occur for a few reasons:</p> <ul> <li> <p>The Command is Not Installed: The required command (<code>ps</code> in this instance) may not be installed on your system. This is less likely if you are on a standard Linux or macOS setup because <code>ps</code> is usually a default command. However, minimal Docker images or other restricted environments might not have these common utilities by default.</p> </li> <li> <p>PATH Environment Variable: The environment in which your Web UI runs might need to set its PATH variable up correctly to include the directory where the <code>ps</code> command resides.</p> </li> <li> <p>Restricted Permissions: It could be a permission issue. The process/user running the Web UI may not have the necessary permissions to execute the <code>ps</code> command.</p> </li> </ul> <p>Please ensure you have all the Karafka Web UI required OS commands installed and executable. A complete list of the OS dependencies can be found here.</p>"}, {"location": "FAQ/#can-i-retrieve-all-records-produced-in-a-single-topic-using-karafka", "title": "Can I retrieve all records produced in a single topic using Karafka?", "text": "<p>Yes, you can consume all records from a specific topic in Karafka by setting up a new consumer for that topic or using the Iterator API. </p> <p>If your primary aim is to get the count of messages, you might have to maintain a counter as you consume the messages.</p> <p>If you are performing a one-time operation of that nature, Iterator API will be much better:</p> <pre><code>iterator = Karafka::Pro::Iterator.new('my_topic_name')\n\ni = 0\niterator.each do\n  puts i+= 1\nend\n</code></pre>"}, {"location": "FAQ/#how-can-i-get-the-total-number-of-messages-in-a-topic", "title": "How can I get the total number of messages in a topic?", "text": "<p>Getting the exact number of messages in a Kafka topic is more complicated due to the nature of Kafka's distributed log system and features such as log compaction. However, there are a few methods you can use:</p> <ol> <li>Using the <code>Karafa::Admin#read_watermark_offsets</code> to get offsets for each partition and summing them:</li> </ol> <pre><code>Karafka::Admin\n  .cluster_info\n  .topics\n  .find { |top| top[:topic_name] == 'my_topic_name' }\n  .then { |topic| topic.fetch(:partitions) }\n  .size\n  .times\n  .sum do |partition_id|\n    offsets = Karafka::Admin.read_watermark_offsets('my_topic_name', partition_id)\n    offsets.last - offsets.first\n  end\n</code></pre> <ol> <li>Using the Iterator API and counting all the messages:</li> </ol> <pre><code>iterator = Karafka::Pro::Iterator.new('my_topic_name')\n\ni = 0\niterator.each do\n  puts i+= 1\nend\n</code></pre> <p>The first approach offers rapid results, especially for topics with substantial messages. However, its accuracy may be compromised by factors such as log compaction. Conversely, the second method promises greater precision, but it's important to note that it could necessitate extensive data transfer and potentially operate at a reduced speed.</p>"}, {"location": "FAQ/#why-am-i-getting-broker-group-authorization-failed-group_authorization_failed-when-using-admin-api-or-the-web-ui", "title": "Why am I getting <code>Broker: Group authorization failed (group_authorization_failed)</code> when using Admin API or the Web UI?", "text": "<p>If you are seeing the following error</p> <pre><code>Broker: Group authorization failed (group_authorization_failed)\n</code></pre> <p>it most likely arises when there's an authorization issue related to the consumer group in your Kafka setup. This error indicates the lack of the necessary permissions for the consumer group to perform certain operations.</p> <p>When using the Admin API or the Web UI in the context of Karafka, you are operating under the consumer groups named <code>karafka_admin</code> and <code>karafka_web</code>.</p> <p>Please review and update your Kafka ACLs or broker configurations to ensure these groups have all the permissions they need.</p>"}, {"location": "FAQ/#why-am-i-getting-an-argumenterror-undefined-classmodule-yamlsyck-when-trying-to-install-karafka-license", "title": "Why am I getting an <code>ArgumentError: undefined class/module YAML::Syck</code> when trying to install <code>karafka-license</code>?", "text": "<p>The error <code>ArgumentError: undefined class/module YAML::Syck</code> you're seeing when trying to install <code>karafka-license</code> is not directly related to the <code>karafka-license</code> gem. It's important to note that <code>karafka-license</code> does not serialize data using <code>YAML::Syck</code>.</p> <p>Instead, this error is a manifestation of a known bug within the Bundler and the Ruby gems ecosystem. During the installation of <code>karafka-license</code>, other gems may also be installed or rebuilt, triggering this issue.</p> <p>To address and potentially resolve this problem, you can update your system gems to the most recent version, which doesn't have this bug. You can do this by running:</p> <pre><code>gem update --system\n</code></pre> <p>Once you've done this, attempt to install the <code>karafka-license</code> gem again. If the problem persists, please get in touch with us.</p>"}, {"location": "FAQ/#are-virtual-partitions-effective-in-case-of-not-having-io-or-not-having-a-lot-of-data", "title": "Are Virtual Partitions effective in case of not having IO or not having a lot of data?", "text": "<p>Karafka's Virtual Partitions are designed to parallelize data processing from a single partition, which can significantly enhance throughput when IO operations are involved. However, if there's minimal IO and not many messages to process, Virtual Partitions may not bring much advantage, as their primary benefit is realized in the presence of IO bottlenecks or large volumes of data. That said, even if your topics have a low average throughput, Virtual Partitions can still be a game-changer when catching up on lags. Virtual Partitions can speed up the catch-up process by processing the backlog of messages concurrently when there's a data buildup due to processing delays.</p>"}, {"location": "FAQ/#is-the-one-process-per-one-topic-partition-recommendation-in-kafka-also-applicable-to-karafka", "title": "Is the \"one process per one topic partition\" recommendation in Kafka also applicable to Karafka?", "text": "<p>Having one process per one topic partition in Kafka is a solid recommendation, especially for CPU-bound work. Here's why: When processing is CPU-intensive, having a single process per partition ensures that each partition gets dedicated computational resources. This prevents any undue contention or resource sharing, maximizing the efficiency of CPU utilization.</p> <p>However, Karafka's design philosophy and strengths come into play in a slightly different context. Most real-world applications involve IO operations \u2013 database reads/writes, network calls, or file system interactions. These operations inherently introduce waiting times, where Karafka stands out. Being multi-threaded, Karafka allows for concurrent processing. So, even when one thread waits for an IO operation, another can actively process data. This means that for many IO-bound applications, consuming a single Karafka process from multiple partitions can be more efficient, maximizing resource utilization during IO waits.</p> <p>Furthermore, Karafka introduces an additional layer of flexibility with its Virtual Partitions. Even if you're consuming data from a single topic partition, you can still leverage the power of parallelism using Virtual Partitions. They enable concurrently processing data from a singular topic partition, thus giving you the benefits of multi-threading even in scenarios with fewer actual topic partitions than processing threads.</p> <p>In summary, while the \"one process per partition\" recommendation is sound for CPU-intensive tasks when IO operations are the predominant factor, Karafka's multi-threaded design combined with the capability of Virtual Partitions can offer a more efficient processing strategy.</p>"}, {"location": "FAQ/#does-running-mark_as_consumed-increase-the-processing-time", "title": "Does running #mark_as_consumed increase the processing time?", "text": "<p>When working with Karafka, the <code>#mark_as_consumed</code> method is designed to be asynchronous, meaning it doesn't immediately commit the offset but schedules it to be committed later. In contrast, the <code>#mark_as_consumed!</code> (with the exclamation mark) is synchronous and commits the offset immediately, thus having a more noticeable impact on processing time.</p> <p>Given the asynchronous nature of <code>#mark_as_consumed</code>, its impact on the overall processing time should be marginal, less than 1%. It's optimized for performance and efficiency to ensure that offset management doesn't significantly slow down your primary processing logic.</p> <p>We recommend using <code>#mark_as_consumed</code> for most cases because of its non-blocking nature. By default, Karafka flushes the offsets every five seconds and during each rebalances. This approach strikes a good balance between ensuring offset accuracy and maintaining high throughput in message processing.</p>"}, {"location": "FAQ/#does-it-make-sense-to-have-multiple-worker-threads-when-operating-on-one-partition-in-karafka", "title": "Does it make sense to have multiple worker threads when operating on one partition in Karafka?", "text": "<p>Yes, but only when you employ Virtual Partitions.</p> <p>Without utilizing Virtual Partitions, Karafka's behavior is such that it will use, at most, as many worker threads concurrently as there are assigned partitions. This means that if you're operating on a single partition without virtualization, only one worker thread will be actively processing messages at a given time, even if multiple worker threads are available.</p> <p>However, with Virtual Partitions, you can parallelize data processing even from a single partition. Virtual Partitions allow the data from one Kafka partition to be virtually \"split\", enabling multiple worker threads to process that data concurrently. This mechanism can be especially beneficial when dealing with IO operations or other tasks that introduce latencies, as other threads can continue processing while one is waiting.</p> <p>Karafka provides a Web UI where you can monitor several metrics, including threads utilization. This gives you a clear view of how efficiently your threads are being used and can be a helpful tool in determining your setup's effectiveness.</p> <p>In summary, while operating on a single partition typically uses just one worker thread, integrating Virtual Partitions in Karafka allows you to effectively utilize multiple worker threads, potentially boosting performance and throughput.</p>"}, {"location": "FAQ/#why-dont-virtual-partitions-provide-me-with-any-performance-benefits", "title": "Why don't Virtual Partitions provide me with any performance benefits?", "text": "<p>Virtual Partitions in Karafka are primarily designed to increase parallelism when processing messages, which can significantly improve throughput in the right circumstances. However, there are several scenarios where the benefits of Virtual Partitions might not be evident:</p> <ol> <li> <p>Not Enough Messages in Batches: If there aren't many messages within the batches you're processing, splitting these already-small batches among multiple virtual partitions won't yield noticeable performance gains. There needs to be more work to be shared among the virtual partitions, leading to underutilization.</p> </li> <li> <p>No IO Involved: Virtual Partitions shine in scenarios where IO operations (e.g., database reads/writes, network calls) are predominant. These operations often introduce latencies, and with virtual partitions, while one thread waits on an IO operation, another can process data. If your processing doesn't involve IO, the parallelism introduced by virtual partitions might not offer substantial benefits.</p> </li> <li> <p>Heavy CPU Computations: If the primary task of your consumer is CPU-intensive computations, then the overhead introduced by managing multiple threads might offset the benefits. CPU-bound tasks usually require dedicated computational resources, and adding more threads (even with virtual partitions) might introduce contention without increasing throughput.</p> </li> <li> <p>Virtual Partitioner Assigns Data to a Single Virtual Partition: The purpose of virtual partitions is to distribute messages across multiple virtual sub-partitions for concurrent processing. If your virtual partitioner, for whatever reason, is consistently assigning messages to only one virtual partition, you effectively negate the benefits. This scenario is akin to not using virtual partitions, as all messages would be processed serially in a single \"stream\".</p> </li> </ol> <p>In conclusion, while Virtual Partitions can be a potent tool for improving throughput in certain scenarios, their utility is context-dependent. It's essential to understand the nature of the work being done, the volume of messages, and the behavior of the virtual partitioner to ascertain the effectiveness of virtual partitions in your setup.</p>"}, {"location": "FAQ/#what-are-long-running-jobs-in-kafka-and-karafka-and-when-should-i-consider-using-them", "title": "What are Long Running Jobs in Kafka and Karafka, and when should I consider using them?", "text": "<p>Despite its name, \"Long Running Jobs\" doesn't refer to the longevity of the underlying Ruby process (like a typical long-running Linux process). Instead, it denotes the duration of message processing. The term \"Long Running Jobs\" was chosen due to its popularity, even though a more accurate name might have been \"Long Running Consumers\".</p> <p>The Long Running Jobs feature adheres to the strategy recommended by Confluent. It involves \"pausing\" a given partition during message processing and then \"resuming\" the processing of that partition once the task is completed. This ensures that as long as no rebalances occur (which would result in the partition being revoked), the <code>poll()</code> command can happen within the confines of the <code>max.poll.interval.ms</code>, preventing unwanted rebalances and errors.</p> <p>However, it's essential to use this feature properly. If your regular workloads don't push the limits of <code>max.poll.interval.ms</code>, enabling Long Running Jobs might degrade performance. This is because pausing the partition prevents data polling, which can lead to inefficiencies in situations where message processing is typically fast.</p> <p>In conclusion, while Long Running Jobs provide a powerful tool to maintain stability during extended message processing tasks, it's crucial to understand your data and processing patterns to utilize them effectively. Otherwise, you might inadvertently introduce performance bottlenecks.</p>"}, {"location": "FAQ/#what-can-i-do-to-optimize-the-latency-in-karafka", "title": "What can I do to optimize the latency in Karafka?", "text": "<p>Optimizing latency in Karafka involves tweaking various configurations and making specific architectural decisions. Here are some strategies to reduce latency:</p> <ul> <li> <p>Max Wait Time Adjustments: The <code>max_wait_time</code> parameter determines the maximum time the consumer will block, waiting for sufficient data to come in during a poll before it returns control. By adjusting this value, you can balance between latency and throughput. If end-to-end latency is a primary concern and your consumers want to react quickly to smaller batches of incoming messages, consider reducing the max_wait_time. A shorter wait time means the process will fetch messages more frequently, leading to quicker processing of smaller data batches.</p> </li> <li> <p>Batch Size Adjustments: Use the <code>max_messages</code> parameter to control the number of messages fetched in a single poll. Decreasing the batch size can reduce the time taken to process each batch, potentially reducing end-to-end latency. However, note that smaller batches can also decrease throughput, so balance is key.</p> </li> <li> <p>Increase Consumer Instances: Scale out by adding more consumer instances to your application. This allows you to process more messages concurrently. However, ensure you have an appropriate number of topic partitions to distribute among the consumers and monitor the utilization of Karafka processes.</p> </li> <li> <p>Leverage Virtual Partitions: Virtual Partitions can be beneficial if your workload is IO-bound. You can better utilize available resources and potentially reduce processing latency by enabling further parallelization within a single partition.</p> </li> <li> <p>Optimize Message Processing: Review the actual processing logic in your consumers. Consider optimizing database queries, reducing external service calls, or employing caching mechanisms to speed up processing.</p> </li> </ul> <p>Remember, the best practices for optimizing latency in Karafka will largely depend on the specifics of your use case, workload, and infrastructure. Regularly monitoring, testing, and adjusting based on real-world data will yield the best results.</p>"}, {"location": "FAQ/#what-is-the-maximum-recommended-concurrency-value-for-karafka", "title": "What is the maximum recommended concurrency value for Karafka?", "text": "<p>For a system with a single topic and a process assigned per partition, there's generally no need for multiple workers. <code>50</code> workers are a lot, and you might not fully utilize them because of the overhead from context switching. While Sidekiq and Karafka differ in their internals, not setting concurrency too high is a valid point for both. The same applies to the connection pool.</p>"}, {"location": "FAQ/#are-there-concerns-about-having-unused-worker-threads-for-a-karafka-consumer-process", "title": "Are there concerns about having unused worker threads for a Karafka consumer process?", "text": "<p>The overhead of unused worker threads is minimal because they are blocked on pop, which is efficient, so they are considered sleeping. However, maintaining an unused pool of workers can distort the utilization metric, as a scenario where all workers are always busy is regarded as 100% utilized.</p>"}, {"location": "FAQ/#how-can-you-effectively-scale-karafka-during-busy-periods", "title": "How can you effectively scale Karafka during busy periods?", "text": "<p>If you plan to auto-scale for busy periods, be aware that increasing scale might lead to reduced concurrency, especially if you are IO-intensive. With Karafka, how data is polled from Kafka can be a critical factor. For instance, thread utilization might be at most 50% even if you process two partitions in parallel because the batches you are getting consist primarily of data from a single partition. For handling of such cases, we recommend using Virtual Partitions.</p>"}, {"location": "FAQ/#what-are-the-benefits-of-using-virtual-partitions-vps-in-karafka", "title": "What are the benefits of using Virtual Partitions (VPs) in Karafka?", "text": "<p>Virtual Partitions allow for more concurrent consumption, which could also be achieved by increasing the partition count. However, with Karafka, VPs operate regardless of whether you poll a batch from one or many partitions. This effectively fills up workers with tasks. If you are not well-tuned for polling the right amount of data, lags might reduce concurrency. Utilizing VPs can improve performance because they can parallelize data processing for a single topic partition based on a virtual partitioner key.</p>"}, {"location": "FAQ/#whats-the-difference-between-increasing-topic-partition-count-and-using-vps-in-terms-of-concurrency", "title": "What's the difference between increasing topic partition count and using VPs in terms of concurrency?", "text": "<p>Increasing topic partition count and Karafka concurrency (so that total worker threads match the total partitions) can parallelize work in Karafka. This strategy works as long as the assignment has consistent polling characteristics. With VPs, uneven distribution impact is negligible. This is because Karafka VPs can parallelize data processing of a single topic partition based on a virtual partitioner key, compensating for any uneven distribution from polling.</p>"}, {"location": "FAQ/#how-do-virtual-partitions-compare-to-multiple-subscription-groups-regarding-performance", "title": "How do Virtual Partitions compare to multiple subscription groups regarding performance?", "text": "<p>Using Virtual Partitions is not the same as increasing the number of partitions. You'd need to align the number of processes and match them with partitions to achieve similar results with multiple subscription groups. However, this might increase the number of Kafka connections, potentially leading to misassignments and sub-optimal resource allocation.</p>"}, {"location": "FAQ/#what-is-the-principle-of-strong-ordering-in-kafka-and-its-implications", "title": "What is the principle of strong ordering in Kafka and its implications?", "text": "<p>Strong ordering in Kafka means that records are strictly ordered in the partition log. Karafka consumers are bound by this design, which acts as a limiter. As a result, the way data is polled and distributed within Karafka is influenced by this principle, which can impact concurrency and performance.</p>"}, {"location": "FAQ/#why-do-i-see-rdkafkaconfigclientcreationerror-when-changing-the-partitionassignmentstrategy", "title": "Why do I see <code>Rdkafka::Config::ClientCreationError</code> when changing the <code>partition.assignment.strategy</code>?", "text": "<p>If you are seeing the following error:</p> <pre><code>All partition.assignment.strategy (cooperative-sticky,range)\nassignors must have the same protocol type,\nonline migration between assignors with different protocol types\nis not supported (Rdkafka::Config::ClientCreationError)\n</code></pre> <p>It indicates that you're attempting an online/rolling migration between two different <code>partition.assignment.strategy</code> assignors with different protocol types. Specifically, you might try switching between \"cooperative-sticky\" and \"range\" strategies without first shutting down all consumers.</p> <p>In Kafka, all consumers within a consumer group must utilize the same partition assignment strategy. Changing this strategy requires a careful offline migration process to prevent inconsistencies and errors like the one you've encountered.</p> <p>You can read more about this process here.</p>"}, {"location": "FAQ/#is-it-recommended-to-add-the-waterdrop-gem-to-the-gemfile-or-just-karafka-and-karafka-testing", "title": "Is it recommended to add the <code>waterdrop</code> gem to the Gemfile, or just <code>karafka</code> and <code>karafka-testing</code>?", "text": "<p>Adding the <code>waterdrop</code> gem to the Gemfile is unnecessary since <code>karafka</code> already depends on <code>waterdrop</code>. Karafka will ensure it selects the most compatible version of <code>waterdrop</code> on its own.</p>"}, {"location": "FAQ/#can-i-use-karafkaproducer-to-produce-messages-that-will-then-be-consumed-by-activejob-jobs", "title": "Can I use <code>Karafka.producer</code> to produce messages that will then be consumed by ActiveJob jobs?", "text": "<p>You cannot use <code>Karafka#producer</code> to produce messages that will then be consumed by ActiveJob jobs. The reason is that when integrating ActiveJob with Karafka, you should use ActiveJob's scheduling API, specifically <code>Job.perform_later</code>, and not the Karafka producer methods.</p> <p>Attempting to use the Karafka producer to send messages for ActiveJob consumption results in mismatches and errors. Karafka's ActiveJob integration has its way of handling messages internally, and how those messages look and what is being sent is abstracted away from the developer. The developer's responsibility is to stick with the ActiveJob APIs.</p> <p>When you want to consume a message produced by an external source, it is not the domain of ActiveJob anymore. That would be regular Karafka consuming, which is different from job scheduling and execution with ActiveJob.</p>"}, {"location": "FAQ/#why-am-i-getting-the-broker-policy-violation-policy_violation-error", "title": "Why am I getting the <code>Broker: Policy violation (policy_violation)</code> error?", "text": "<p>The <code>Broker: Policy violation (policy_violation)</code> error in Karafka is typically related to violating some broker-set policies or configurations.</p> <p>In Karafka, this error might surface during two scenarios:</p> <ul> <li>When upgrading the Web UI using the command <code>karafka-web migrate</code>.</li> <li>When employing the Declarative Topics with the <code>karafka topics migrate</code> command, especially if trying to establish a topic that doesn't align with the broker's policies.</li> </ul> <p>Should you encounter this error during a Web UI migration, we recommend manually creating the necessary topics and fine-tuning the settings to match your policies. You can review the settings Karafka relies on for these topics here.</p> <p>On the other hand, if this error appears while using Declarative Topics, kindly review your current configuration. Ensure that it's in harmony with the broker's policies and limitations.</p>"}, {"location": "FAQ/#why-am-i-getting-a-error-querying-watermark-offsets-for-partition-0-of-karafka_consumers_states-error", "title": "Why am I getting a <code>Error querying watermark offsets for partition 0 of karafka_consumers_states</code> error?", "text": "<pre><code>Error querying watermark offsets for partition 0 of karafka_consumers_states\nLocal: All broker connections are down (all_brokers_down)\n</code></pre> <p>It is indicative of a connectivity issue. Let's break down the meaning and implications of this error:</p> <ol> <li> <p>Main Message: The primary message is about querying watermark offsets. Watermark offsets are pointers indicating the highest and lowest offsets (positions) in a Kafka topic partition the consumer has read. The error suggests that the client is facing difficulties in querying these offsets for a particular partition (in this case, partition 0) of the karafka_consumers_states topic.</p> </li> <li> <p>Local: All broker connections are down (all_brokers_down): Despite the starkness of the phrasing, this doesn't necessarily mean that all the brokers in the Kafka cluster are offline. Instead, it suggests that the Karafka client cannot establish a connection to any of the brokers responsible for the mentioned partition. The reasons could be manifold:</p> <ul> <li> <p>Connectivity Issues: Network interruptions between your Karafka client and the Kafka brokers might occur. This can be due to firewalls, routing issues, or other network-related blocks.</p> </li> <li> <p>Broker Problems: There's a possibility that the specific broker or brokers responsible for the mentioned partition are down or facing internal issues.</p> </li> <li> <p>Misconfiguration: Incorrect configurations, such as too short connection or request timeouts, could lead to premature termination of requests, yielding such errors.</p> </li> </ul> </li> <li> <p>Implications for Karafka Web UI:</p> <ul> <li> <p>If you're experiencing this issue with topics related to Karafka Web UI, it's essential to note that Karafka improved its error handling in version 2.2.2. If you're using an older version, upgrading to the latest Karafka and Karafka Web UI versions might alleviate the issue.</p> </li> <li> <p>Another scenario where this error might pop up is during rolling upgrades of the Kafka cluster. If the Karafka Web UI topics have a replication factor 1, there's no redundancy for the partition data. During a rolling upgrade, as brokers are taken down sequentially for upgrades, there might be brief windows where the partition's data isn't available due to its residing broker being offline.</p> </li> </ul> </li> </ol> <p>Below, you can find a few recommendations in case you encounter this error:</p> <ol> <li> <p>Upgrade Karafka: If you're running a version older than <code>2.2.2</code>, consider upgrading both Karafka and Karafka Web UI. This might resolve the issue if it's related to previous error-handling mechanisms.</p> </li> <li> <p>Review Configurations: Examine your Karafka client configurations, especially timeouts and broker addresses, to ensure they're set appropriately.</p> </li> <li> <p>Replication Factor: For critical topics, especially if you're using Karafka Web UI, consider setting a replication factor greater than 1. This ensures data redundancy and availability even if a broker goes down.</p> </li> </ol> <p>In summary, while the error message might seem daunting, understanding its nuances can guide targeted troubleshooting, and being on the latest software versions can often preemptively avoid such challenges.</p>"}, {"location": "FAQ/#why-karafka-is-consuming-the-same-message-multiple-times", "title": "Why Karafka is consuming the same message multiple times?", "text": "<p>When you use Karafka and notice that the same message is being consumed multiple times, several reasons might be causing this. Here are the common reasons why you may experience the same message being processed numerous times:</p> <ul> <li> <p>At-Least-Once Delivery: Kafka guarantees at-least-once delivery, which means a message can be delivered more than once in specific scenarios. This is a trade-off to ensure that no messages are lost during transport. As a result, it's up to the consumer to handle duplicate messages appropriately.</p> </li> <li> <p>Consumer Failures: If a consumer crashes after processing a message but before it has had a chance to commit its offset, the consumer might process the same message again upon retry.</p> </li> <li> <p>Commit Interval: The interval at which the consumer commits its offset can also lead to messages being consumed multiple times. If the commit interval is too long, and there's a crash before an offset is committed, messages received since the last commit will be re-consumed.</p> </li> <li> <p>Similar-Looking Messages: It's possible that the messages aren't actually duplicates, but they look alike. This can be particularly common in systems where certain events occur regularly or when there's a glitch in the producing service. It's essential to check the message key, timestamp, or other unique identifiers to ascertain if two messages are identical or have similar payloads.</p> </li> <li> <p>Dead Letter Queue (DLQ) Misconfiguration with Manual Offset Management: If you're using a Dead Letter Queue in combination with manual offset management, it's possible to get into a situation where messages are consumed multiple times. If a message cannot be processed and is forwarded to the DLQ, but its offset isn't correctly committed, or the message isn't marked as consumed, the consumer may pick up the same message again upon its next iteration or restart. This behavior can especially become evident when a message consistently fails to be processed correctly, leading to it being consumed multiple times and continually ending up in the DLQ. Ensuring a proper synchronization between message processing, DLQ forwarding, and offset management is essential to avoid such scenarios.</p> </li> </ul> <p>Remember that distributed systems, Kafka included, are complex and can exhibit unexpected behaviors due to various factors. The key is to have comprehensive logging, monitoring, and alerting in place, which can provide insights into anomalies and help in their early detection and resolution.</p>"}, {"location": "FAQ/#why-do-karafka-web-ui-topics-contain-binaryunicode-data-instead-of-text", "title": "Why do Karafka Web UI topics contain binary/Unicode data instead of text?", "text": "<p>If you've checked Karafka Web UI topics in an alternative Kafka UI, you may notice that topics seem to contain binary/unicode data rather than plain text. It's not an oversight or an error. This design choice is rooted in our data management and transmission efficiency approach.</p> <ul> <li> <p>Compression for Efficient Data Transfer: Karafka Web UI compresses all data that it sends to Kafka. The primary objective behind this is to optimize data transmission by reducing the size of the messages. Smaller message sizes can lead to faster transmission rates and lower storage requirements. This is especially crucial when dealing with vast amounts of data, ensuring that Kafka remains efficient and responsive.</p> </li> <li> <p>Independent Compression without External Dependencies: We understand the significance of maintaining a lightweight, hassle-free setup for our users. We chose Zlib for data compression - it comes bundled with every Ruby version. This means there's no need to rely on third-party libraries or go through configuration changes to your Kafka cluster to use Karafka Web UI.</p> </li> </ul> <p>By choosing Zlib, we've simplified it for the end user. You won't have to grapple with additional compression settings or worry about compatibility issues. Zlib's ubiquity in Ruby ensures that Karafka remains user-friendly without compromising data transmission efficiency.</p> <p>While the binary/Unicode representation in the Karafka Web UI topics might seem unconventional at first glance, it's a strategic choice to streamline data transfers and keep the setup process straightforward. Karafka Web UI Explorer recognizes this format and will decompress it if you need to inspect this data.</p>"}, {"location": "FAQ/#can-i-use-same-karafka-web-ui-topics-for-multiple-environments-like-production-and-staging", "title": "Can I use same Karafka Web UI topics for multiple environments like production and staging?", "text": "<p>No. More details about that can be found here.</p>"}, {"location": "FAQ/#does-karafka-plan-to-submit-metrics-via-a-supported-datadog-integration-ensuring-the-metrics-arent-considered-custom-metrics", "title": "Does Karafka plan to submit metrics via a supported Datadog integration, ensuring the metrics aren't considered custom metrics?", "text": "<p>No, Karafka does not have plans to submit metrics through a dedicated Datadog integration that ensures these metrics are classified as non-custom. While Karafka has an integration with Datadog, the metrics from this integration will be visible as custom metrics.</p> <p>The reason for this approach is grounded in practicality and long-term maintainability. As with any software, weighing the benefits against the maintenance cost and the commitment involved is essential. While it might seem feasible to align certain features or integrations with the current framework changes, it could introduce challenges if the release cycle or external dependencies were to change.</p> <p>To put it in perspective:</p> <ul> <li> <p>Maintenance Cost &amp; Commitment: Introducing such a feature would mean an ongoing commitment to ensuring it works seamlessly with every subsequent update or change to Karafka or Datadog. It's imperative to consider the long-term cost of this commitment.</p> </li> <li> <p>External Dependencies: If Datadog's release cycle or features were to evolve unexpectedly, it could lead to complexities in ensuring smooth integration. This introduces an external dependency that's out of Karafka's direct control.</p> </li> <li> <p>Ecosystem Benefits: While such integrations can offer added value, assessing if their benefits are substantial enough to justify the effort and potential challenges is vital. In this case, the perceived benefit to the ecosystem seems insignificant.</p> </li> </ul> <p>In conclusion, while Karafka recognizes the value of integrations and continually seeks to enhance its capabilities, it's essential to strike a balance that ensures the software remains efficient, maintainable, and free from unnecessary complexities.</p>"}, {"location": "FAQ/#how-can-i-make-karafka-not-retry-processing-and-what-are-the-implications", "title": "How can I make Karafka not retry processing, and what are the implications?", "text": "<p>If you make Karafka not retry, the system will not attempt retries on errors but will continue processing forward. You can achieve this in two methods:</p> <ol> <li>Manual Exception Handling: This involves catching all exceptions arising from your code and choosing to ignore them. This means the system doesn't wait or retry; it simply moves to the next task or message.</li> </ol> <pre><code>def consume\n  messages.each do |message|\n    begin\n      persist(message)\n    # Ignore any errors and just log them\n    rescue StandardError =&gt; e\n      ErrorTracker.notify(e)\n    end\n\n    mark_as_consumed(message)\n  end\nend\n</code></pre> <ol> <li>Using Enhanced DLQ Capabilities: With this method, messages will be moved to the Dead Letter Queue (DLQ) immediately, without retrying them, and an appropriate backoff policy will be invoked, preventing you from further overloading your system in case of external resources' temporary unavailability.</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      # This setup will move broken messages to the DLQ, backoff and continue\n      dead_letter_queue(\n        topic: 'dead_messages',\n        max_retries: 0\n      )\n    end\n  end\nend\n\nclass OrdersStatesConsumer &lt; ApplicationConsumer\n  def consume\n    # No need to handle errors manually, if `#persist` fails,\n    # Karafka will pause, backoff and retry automatically and\n    # will move the failed messages to `dead_messages` topic\n    messages.each do |message|\n      persist(message)\n\n      mark_as_consumed(message)\n    end\n  end\nend\n</code></pre> <p>However, it's essential to be aware of the potential risks associated with these approaches. In the first method, there's a possibility of overloading temporarily unavailable resources, such as databases or external APIs. Since there is no backoff between a failure and the processing of the subsequent messages, this can exacerbate the problem, further straining the unavailable resource. To mitigate this, using the <code>#pause</code> API is advisable, which allows you to pause the processing manually. This will give strained resources some breathing room, potentially preventing more significant system failures.</p>"}, {"location": "FAQ/#we-faced-downtime-due-to-a-failure-in-updating-the-ssl-certs-how-can-we-retrieve-messages-that-were-sent-during-this-downtime", "title": "We faced downtime due to a failure in updating the SSL certs. How can we retrieve messages that were sent during this downtime?", "text": "<p>If the SSL certificates failed on the producer side, the data might not have been produced to Kafka in the first place. If your data is still present and not compacted due to the retention policy, then you should be able to read it.</p> <p>You can read from the last known consumed offset by seeking this offset + 1 or use Karafka Iterator API or Web UI Explorer to view those messages.</p>"}, {"location": "FAQ/#how-can-the-retention-policy-of-kafka-affect-the-data-sent-during-the-downtime", "title": "How can the retention policy of Kafka affect the data sent during the downtime?", "text": "<p>If your data retention policy has compacted the data, then the data from the downtime period might no longer be available.</p>"}, {"location": "FAQ/#is-it-possible-to-fetch-messages-per-topic-based-on-a-specific-time-period-in-karafka", "title": "Is it possible to fetch messages per topic based on a specific time period in Karafka?", "text": "<p>Yes, in newer versions of Karafka, you can use the Iterator API or the Enhanced Web UI to perform time-based offset lookups.</p>"}, {"location": "FAQ/#where-can-i-find-details-on-troubleshooting-and-debugging-for-karafka", "title": "Where can I find details on troubleshooting and debugging for Karafka?", "text": "<p>You can refer to this documentation page.</p>"}, {"location": "FAQ/#does-the-open-source-oss-version-of-karafka-offer-time-based-offset-lookup-features", "title": "Does the open-source (OSS) version of Karafka offer time-based offset lookup features?", "text": "<p>Only partially. Karafka OSS allows you to use the consumer <code>#seek</code> method to navigate to a specific time in the subscribed topic partition. Still, you cannot do it outside the consumer subscription flow.</p>"}, {"location": "FAQ/#i-see-a-joingroup-error-broker-invalid-session-timeout-error-what-does-this-mean-and-how-can-i-resolve-it", "title": "I see a \"JoinGroup error: Broker: Invalid session timeout\" error. What does this mean, and how can I resolve it?", "text": "<p>This error occurs when a consumer tries to join a consumer group in Apache Kafka but provides an invalid session timeout value. The session timeout is when the broker waits after losing contact with a consumer before considering it gone and starting a rebalance of the consumer group. If this value is too low or too high (outside the broker's allowed range), the broker will reject the consumer's request to join the group.</p> <ol> <li> <p>Check the configuration of your consumer to ensure you're setting an appropriate session timeout value.</p> </li> <li> <p>Ensure that the value lies within the broker's allowable range, which you can find in the broker's configuration.</p> </li> <li> <p>Adjust the consumer's session timeout value to be within this range and try reconnecting.</p> </li> </ol> <p>Remember to make sure that the timeout value you set is suitable for your use case, as it can affect the responsiveness of your consumer group to failures.</p>"}, {"location": "FAQ/#the-producer-network-latency-metric-in-dd-seems-too-high-is-there-something-wrong-with-it", "title": "The \"Producer Network Latency\" metric in DD seems too high. Is there something wrong with it?", "text": "<p>In this case, the high number you see is in microseconds, not milliseconds. To put it into perspective, 1 millisecond is 1,000 microseconds. So, if you see a metric like 15k, it's just 0.015 of a second. Always ensure you're reading the metrics with the correct scale in mind. </p> <p> </p>"}, {"location": "FAQ/#what-is-the-purpose-of-the-karafka_consumers_reports-topic", "title": "What is the purpose of the <code>karafka_consumers_reports</code> topic?", "text": "<p>The <code>karafka_consumers_reports</code> topic is an integral component of the Karafka Web UI. Its primary purpose is to store information related to the processes and operations of the Karafka application. This, along with other Web UI topics, is designed to capture and provide data. By doing so, Karafka Web UI eliminates the need for an external third-party database, allowing it to leverage Kafka as its primary source of information.</p>"}, {"location": "FAQ/#can-i-use-karafkaproducer-from-within-activejob-jobs-running-in-the-karafka-server", "title": "Can I use <code>Karafka.producer</code> from within ActiveJob jobs running in the karafka server?", "text": "<p>Yes, any ActiveJob job running in the karafka server can access and use the <code>Karafka.producer</code>.</p>"}, {"location": "FAQ/#do-you-recommend-using-the-singleton-producer-in-karafka-for-all-appsconsumersjobs-in-a-system", "title": "Do you recommend using the singleton producer in Karafka for all apps/consumers/jobs in a system?", "text": "<p>Yes, unless you use transactions. In that case, you can use a connection pool. Using a long-living pool is fine.</p>"}, {"location": "FAQ/#is-it-acceptable-to-declare-short-living-producers-in-each-appjobs-as-needed", "title": "Is it acceptable to declare short-living producers in each app/jobs as needed?", "text": "<p>It's not recommended to have a short-lived producer or per job class (e.g., 20 job classes and 20 producers). Instead, create producers that vary with usage or settings, not per class.</p>"}, {"location": "FAQ/#what-are-the-consequences-if-you-call-a-produce_async-and-immediately-close-the-producer-afterward", "title": "What are the consequences if you call a <code>#produce_async</code> and immediately close the producer afterward?", "text": "<p>No consequences. WaterDrop will wait until it is delivered because it knows the internal queue state.</p>"}, {"location": "FAQ/#is-it-problematic-if-a-developer-creates-a-new-producer-calls-produce_async-and-then-closes-the-producer-whenever-they-need-to-send-a-message", "title": "Is it problematic if a developer creates a new producer, calls <code>#produce_async</code>, and then closes the producer whenever they need to send a message?", "text": "<p>Yes, this is problematic. WaterDrop producers are designed to be long-lived. Creating short-lived Kafka connections can be expensive.</p>"}, {"location": "FAQ/#could-the-async-process-remain-open-somewhere-even-after-the-producer-has-been-closed", "title": "Could the async process remain open somewhere, even after the producer has been closed?", "text": "<p>No.</p>"}, {"location": "FAQ/#could-a-single-producer-be-saturated-and-if-so-what-kind-of-max-rate-of-message-production-would-be-the-limit", "title": "Could a single producer be saturated, and if so, what kind of max rate of message production would be the limit?", "text": "<p>This depends on factors like your cluster, number of topics, number of partitions, and how and where you send the messages. However, you can get up to 100k messages per second from a single producer instance.</p>"}, {"location": "FAQ/#how-does-the-batching-process-in-waterdrop-works", "title": "How does the batching process in WaterDrop works?", "text": "<p>Waterdrop and librdkafka batch messages under the hood and dispatch in groups. There's an internal queue limit you can set. If exceeded, a backoff will occur.</p>"}, {"location": "FAQ/#can-you-control-the-batching-process-in-waterdrop", "title": "Can you control the batching process in WaterDrop?", "text": "<p>It auto-batches the requests. If the queue is full, a throttle will kick in. You can also configure WaterDrop to wait on queue full errors. The general approach is to dispatch in batches (or in transactions) and wait on batches or finalize a transaction.</p>"}, {"location": "FAQ/#is-it-possible-to-exclude-karafka-web-related-reporting-counts-from-the-web-ui-dashboard", "title": "Is it possible to exclude <code>karafka-web</code> related reporting counts from the web UI dashboard?", "text": "<p>No.</p>"}, {"location": "FAQ/#can-i-log-errors-in-karafka-with-topic-partition-and-other-consumer-details", "title": "Can I log errors in Karafka with topic, partition, and other consumer details?", "text": "<p>Yes, it is possible to log errors with associated topic, partitions, and other information in Karafka, but it depends on the context and type of the event. Karafka's <code>error.occurred</code> event is used for logging any errors, including errors not only within your consumption code but also in other application areas due to its asynchronous nature.</p> <p>Key Points to Remember:</p> <ul> <li> <p>Check Event Type: Always examine the <code>event[:type]</code> to understand the nature of the error. This helps determine whether the error is related to message consumption and will have the relevant topic and partition information.</p> </li> <li> <p>Errors Outside Consumption Flow: There can be instances where the caller is a consumer, but the error occurs outside the consumption flow. In such cases, the necessary details might not be available.</p> </li> </ul> <p>The rationale for this Approach:</p> <p>Karafka provides a single, simplified API for all error instrumentation. This design choice aims to streamline error handling across different parts of the application, even though some errors may only sometimes have the complete contextual information you are looking for.</p> <p>Checking the <code>event[:type]</code> and recognizing the role of the <code>event[:caller]</code> will guide you in determining whether the necessary details are available for a specific error. Remember, due to the asynchronous nature of Karafka, not all errors will have associated topic and partition details.</p>"}, {"location": "FAQ/#why-did-our-kafka-consumer-start-from-the-beginning-after-a-2-week-downtime-but-resumed-correctly-after-a-brief-stop-and-restart", "title": "Why did our Kafka consumer start from the beginning after a 2-week downtime, but resumed correctly after a brief stop and restart?", "text": "<p>This issue is likely due to the <code>offsets.retention.minutes</code> setting in Kafka. Kafka deletes the saved offsets if a consumer is stopped for longer than this set retention period (like your 2-week downtime). Without these offsets, the consumer restarts from the beginning. However, the offsets are still available for shorter downtimes (like your 15-minute test), allowing the consumer to resume from where it left off.</p> <p>You can read more about this behavior here.</p>"}, {"location": "FAQ/#why-am-i-experiencing-a-load-error-when-using-karafka-with-ruby-27-and-how-can-i-fix-it", "title": "Why am I experiencing a load error when using Karafka with Ruby 2.7, and how can I fix it?", "text": "<p>If you're experiencing a load error with Karafka on Ruby 2.7, it's due to a bug in Bundler. To fix this:</p> <ul> <li>Install Bundler v2.4.22: Run <code>gem install bundler -v 2.4.22 --no-document</code>.</li> <li>Update RubyGems to v3.4.22: Run <code>gem update --system 3.4.22 --no-document</code>.</li> </ul> <p>Note: Ruby 2.7 is EOL and no longer supported. For better security and functionality, upgrading to Ruby 3.0 or higher is highly recommended.</p>"}, {"location": "FAQ/#why-am-i-getting-nscharacterset-initialize-may-have-been-in-progress-in-another-thread-when-fork-error-when-forking-on-macos", "title": "Why am I getting <code>+[NSCharacterSet initialize] may have been in progress in another thread when fork()</code> error when forking on macOS?", "text": "<p>When running a Rails application with Karafka and Puma on macOS, hitting the Karafka dashboard or endpoints can cause crashes with an error related to fork() and Objective-C initialization. This is especially prevalent in Puma's clustered mode.</p> <p>The error message is usually along the lines of:</p> <pre><code>objc[&lt;pid&gt;]: +[NSCharacterSet initialize]\nmay have been in progress in another thread when fork() was called.\nWe cannot safely call it or ignore it in the fork() child process.\nCrashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n</code></pre> <p>The issue concerns initializing specific libraries or components in a macOS environment, particularly in a multi-process environment. It may involve Objective-C libraries being initialized unsafely after a fork, which macOS does not allow.</p> <p>There are a few potential workarounds:</p> <ol> <li> <p>Setting the environment variable <code>OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES</code> when running Puma. This is not ideal as it might introduce other issues.</p> </li> <li> <p>Creating an instance of <code>Rdkafka</code> that would force-load all the needed components before the fork as follows:</p> </li> </ol> <pre><code>require 'rdkafka'\n\nbefore_fork do\n  # Make sure to configure it according to your cluster location\n  config = {\n    'bootstrap.servers': 'localhost:9092'\n  }\n\n  # Create a new instance and close it\n  # This will load or dynamic components on macOS\n  # Not needed under other OSes, so not worth running\n  if RUBY_PLATFORM.include?('darwin')\n    ::Rdkafka::Config.new(config).admin.close\n  end\nend\n</code></pre> <p>It is worth pointing out that this is not a Karafka-specific issue. While the issue manifests when using Karafka with Puma, it's more related to how macOS handles forking with Objective-C libraries and specific initializations post-fork.</p>"}, {"location": "FAQ/#how-does-karafka-handle-messages-with-undefined-topics-and-can-they-be-routed-to-a-default-consumer", "title": "How does Karafka handle messages with undefined topics, and can they be routed to a default consumer?", "text": "<p>Karafka Pro's Routing Patterns feature allows for flexible routing using regular expressions, automatically adapting to dynamically created Kafka topics. This means Karafka can instantly recognize and consume messages from new topics without extra configuration, streamlining the management of topic routes. For optimal use and implementation details, consulting the their documentation is highly recommended.</p>"}, {"location": "FAQ/#what-happens-if-an-error-occurs-while-consuming-a-message-in-karafka-will-the-message-be-marked-as-not-consumed-and-automatically-retried", "title": "What happens if an error occurs while consuming a message in Karafka? Will the message be marked as not consumed and automatically retried?", "text": "<p>In Karafka's default flow, if an error occurs during message consumption, the processing will pause at the problematic message, and attempts to consume it will automatically retry with an exponential backoff strategy. This is typically effective for resolving transient issues (e.g., database disconnections). However, it may not be suitable for persistent message-specific problems, such as corrupted JSON. In such cases, Karafka's Dead Letter Queue feature can be utilized. This feature allows a message to be retried several times before it's moved to a Dead Letter Queue (DLQ), enabling the process to continue with subsequent messages. More information on this can be found in the Dead Letter Queue Documentation.</p>"}, {"location": "FAQ/#what-does-setting-the-initial_offset-to-earliest-mean-in-karafka-does-it-mean-the-consumer-starts-consuming-from-the-earliest-message-that-has-not-been-consumed-yet", "title": "What does setting the <code>initial_offset</code> to <code>earliest</code> mean in Karafka? Does it mean the consumer starts consuming from the earliest message that has not been consumed yet?", "text": "<p>The <code>initial_offset</code> setting in Karafka is relevant only during the initial start of a consumer in a consumer group. It dictates the starting point for message consumption when a consumer group first encounters a topic. Setting <code>initial_offset</code> to <code>earliest</code> causes the consumer to start processing from the earliest available message in the topic (usually the message with offset 0, but not necessarily). Conversely, setting it to <code>latest</code> instructs the consumer to begin processing the next message after the consumer has started. It's crucial to note that <code>initial_offset</code> does not influence the consumption behavior during ongoing operations. For topics the consumer group has previously consumed, Karafka will continue processing from the last acknowledged message, ensuring that no message is missed or processed twice.</p>"}, {"location": "FAQ/#why-is-the-dead-tab-in-web-ui-empty-in-my-multi-app-setup", "title": "Why is the \"Dead\" tab in Web UI empty in my Multi App setup?", "text": "<p>If the \"Dead\" tab in your Karafka Web UI is empty, especially within a multi-app setup, there are two primary reasons to consider based on the DLQ routing awareness section:</p> <ol> <li> <p>DLQ Topic References Not Configured: The most likely reason is that the Dead Letter Queue (DLQ) topics have yet to be explicitly referenced in the <code>karafka.rb</code> configuration of the application serving the Web UI. Without these references, the Web UI lacks the context to identify which topics are designated as DLQs. This means that even if messages are being routed to a DLQ, the Web UI will not display these topics under the \"Dead\" tab because it does not recognize them as such. Ensure that all DLQ topics are correctly defined in the routing configuration of the Karafka application hosting the Web UI to resolve this issue. You can read more about this issue here.</p> </li> <li> <p>Non-existent DLQ Topic: Another possibility is that the DLQ topic itself does not exist. In scenarios where messages fail processing and are supposed to be routed to a DLQ, the absence of the designated DLQ topic would result in no messages being stored or visible in the \"Dead\" tab. This could occur if the DLQ topic were never created in Kafka or if there needs to be a misconfiguration in the topic name within your application's settings, leading to a mismatch between where Karafka attempts to route failed messages and the actual topic structure in Kafka.</p> </li> </ol> <p>To troubleshoot and resolve this issue, you should:</p> <ul> <li> <p>Verify DLQ Topic Configuration: Double-check your <code>karafka.rb</code> file to ensure that DLQ topics are correctly referenced within the routing configuration. Ensure the topic names match the expected DLQ topics in your Kafka setup.</p> </li> <li> <p>Check Kafka for DLQ Topic Existence: Ensure that the DLQ topics are created and exist within your Kafka cluster. You can use Kafka command-line tools or a Kafka management UI to list topics and verify their existence.</p> </li> <li> <p>Review Topic Naming Consistency: Ensure consistency in topic naming across your Kafka configuration and Karafka setup. Any discrepancy could lead to failed message routing.</p> </li> </ul>"}, {"location": "FAQ/#what-causes-a-broker-policy-violation-policy_violation-error-when-using-karafka-and-how-can-i-resolve-it", "title": "What causes a \"Broker: Policy violation (policy_violation)\" error when using Karafka, and how can I resolve it?", "text": "<p>The <code>Broker: Policy violation (policy_violation)</code> error in Karafka typically occurs due to exceeding Kafka's quota limitations or issues with Access Control Lists (ACLs). This error indicates that an operation attempted by Karafka has violated the policies set on your Kafka cluster. To resolve this issue, follow these steps:</p> <ol> <li> <p>ACL Verification: Ensure that your Kafka cluster's ACLs are configured correctly. ACLs control the permissions for topic creation, access, and modification. If ACLs are not properly set up, Karafka might be blocked from performing necessary operations.</p> </li> <li> <p>Quota Checks: Kafka administrators can set quotas on various resources, such as data throughput rates and the number of client connections. If Karafka exceeds these quotas, it may trigger a <code>policy_violation</code> error. Review your Kafka cluster's quota settings to ensure they align with your usage patterns.</p> </li> <li> <p>Adjust Configurations: Make the necessary adjustments based on your findings from the ACL and quota checks. This might involve modifying ACL settings to grant appropriate permissions or altering quota limits to accommodate your application's needs.</p> </li> </ol>"}, {"location": "FAQ/#why-do-i-see-hundreds-of-repeat-exceptions-with-pause_with_exponential_backoff-enabled", "title": "Why do I see hundreds of repeat exceptions with <code>pause_with_exponential_backoff</code> enabled?", "text": "<p>When <code>pause_with_exponential_backoff</code> is enabled, the timeout period for retries doubles after each attempt, but this does not prevent repeated exceptions from occurring. With <code>pause_max_timeout</code> set to the default 30 seconds, an unaddressed exception can recur up to 120 times per hour. This frequent repetition happens because the system continues to retry processing until the underlying issue is resolved.</p>"}, {"location": "FAQ/#does-karafka-store-the-kafka-server-address-anywhere-and-are-any-extra-steps-required-to-make-it-work-after-changing-the-server-iphostname", "title": "Does Karafka store the Kafka server address anywhere, and are any extra steps required to make it work after changing the server IP/hostname?", "text": "<p>Karafka does not persistently store the Kafka server address or cache any information about the cluster's IP addresses or hostnames. The issue you're experiencing is likely due to your cluster setup, as Karafka performs discovery based on the initial host address provided in the <code>config.kafka</code> setup. Upon startup, Karafka uses this initial address to discover the rest of the cluster. Ensure your configurations are correctly updated across your Docker setup, and restart the process to clear any temporary caches. Karafka has no intrinsic knowledge of AWS hosts or any hardcoded cluster information; it relies entirely on the configuration provided at startup.</p>"}, {"location": "FAQ/#what-should-i-do-if-i-encounter-a-loading-issue-with-karafka-after-upgrading-bundler-to-version-2322", "title": "What should I do if I encounter a loading issue with Karafka after upgrading Bundler to version <code>2.3.22</code>?", "text": "<p>This issue is typically caused by a gem conflict related to the Thor gem version. It has been observed that Thor version <code>1.3</code> can lead to errors when loading Karafka. The problem is addressed in newer versions of Karafka, which no longer depend on Thor. To resolve the issue:</p> <ol> <li> <p>Ensure you're using a version of Thor earlier than <code>1.3</code>, as recommended by community members.</p> </li> <li> <p>Upgrade to a newer version of Karafka that does not use Thor. It's recommended to upgrade to at least version <code>2.2.8</code> for stability and to take advantage of improvements.</p> </li> </ol>"}, {"location": "FAQ/#is-there-a-good-way-to-quiet-down-bundle-exec-karafka-server-extensive-logging-in-development", "title": "Is there a good way to quiet down <code>bundle exec karafka server</code> extensive logging in development?", "text": "<p>Yes. You can set <code>log_polling</code> to <code>false</code> for the <code>LoggerListener</code> as follows:</p> <pre><code>Karafka.monitor.subscribe(\n  Karafka::Instrumentation::LoggerListener.new(\n    # When set to false, polling will not be logged\n    # This makes logging in development less extensive\n    log_polling: false\n  )\n)\n</code></pre>"}, {"location": "FAQ/#how-can-i-namespace-messages-for-producing-in-karafka", "title": "How can I namespace messages for producing in Karafka?", "text": "<p>You can namespace messages topics for producing automatically in Karafka by leveraging the middleware in WaterDrop to transform the destination topics.</p>"}, {"location": "FAQ/#why-am-i-getting-the-all-topic-names-within-a-single-consumer-group-must-be-unique-error-when-changing-the-location-of-the-boot-file-using-karafka_boot_file", "title": "Why am I getting the <code>all topic names within a single consumer group must be unique</code> error when changing the location of the boot file using <code>KARAFKA_BOOT_FILE</code>?", "text": "<p>You're seeing this error most likely because you have moved the <code>karafka.rb</code> file to a location that is automatically loaded, meaning that it is loaded and used by the Karafka framework and also by the framework of your choice. In the case of Ruby on Rails, it may be so if you've placed your <code>karafka.rb</code>, for example, inside the <code>config/initializers</code> directory.</p>"}, {"location": "FAQ/#why-is-kafka-using-only-7-out-of-12-partitions-despite-specific-settings", "title": "Why Is Kafka Using Only 7 Out of 12 Partitions Despite Specific Settings?", "text": "<p>The issue you're encountering typically arises due to how Kafka calculates partition assignments when a key is provided. Kafka uses a hashing function (CRC32 by default) to determine the partition for each key. This function might not evenly distribute keys, especially if the key space is not large or diverse enough.</p> <p>As discussed, since the partitioner was configured to use the first argument (carrier name) as the key, the diversity and number of unique carrier names directly influence the distribution across partitions. If some carrier names hash the same partition, you will see less than 12 partitions being used.</p>"}, {"location": "FAQ/#why-does-the-dead-letter-queue-dlq-use-the-default-deserializer-instead-of-the-one-specified-for-the-original-topic-in-karafka", "title": "Why does the Dead Letter Queue (DLQ) use the default deserializer instead of the one specified for the original topic in Karafka?", "text": "<p>When a message is piped to the DLQ, if you decide to consume data from the DLQ topic, it defaults to using the default deserializer unless explicitly specified otherwise for the DLQ  topic. This behavior occurs because the deserializer setting is tied to specific topics rather than the consumer. If you require a different deserializer for the DLQ, you must define it directly on the DLQ topic within your routing setup. This setup ensures that each topic, including the DLQ, can have unique processing logic tailored to its specific needs.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n      deserializer SpecificDeserializer.new\n      dead_letter_queue(\n        topic: :failed_orders_dlq,\n        max_retries: 2\n      )\n    end\n\n    topic :failed_orders_dlq do\n      consumer FailedOrdersRecoveryConsumer\n      deserializer SuperSpecificDeserializer.new\n    end\n  end\nend\n</code></pre>"}, {"location": "FAQ/#what-should-i-consider-when-manually-dispatching-messages-to-the-dlq-in-karafka", "title": "What should I consider when manually dispatching messages to the DLQ in Karafka?", "text": "<p>When manually dispatching a message to the DLQ in Karafka, it's essential to understand that the dispatch action itself only moves the message to the DLQ and does not mark it as consumed. If your intention is to prevent further processing of the original message and to avoid halting the offset commitment, you need to explicitly mark the message as consumed. This can be crucial in maintaining the flow of message processing and ensuring that message consumption offsets are correctly committed.</p>"}, {"location": "FAQ/#how-can-i-ensure-that-my-karafka-consumers-process-data-in-parallel", "title": "How can I ensure that my Karafka consumers process data in parallel?", "text": "<p>Karafka utilizes multiple threads to consume and process data, allowing operations across multiple partitions or topics to occur in parallel. However, the perception of sequential processing might occur due to several factors, such as configuration, scale, and system design. To enhance parallel processing:</p> <ul> <li> <p>Consumer Groups: If you require completely independent processing streams, utilize multiple consumer groups. Each consumer group manages its connection, polling, and data handling.</p> </li> <li> <p>Subscription Groups: You can set up multiple subscription groups within a single consumer group. Each subscription group can subscribe to different topics, enabling parallel data fetching within the same consumer group.</p> </li> <li> <p>Configuration: Ensure your settings like <code>max.partition.fetch.bytes</code> and `max.poll.records are optimized based on your message size and throughput requirements. This helps in fetching data efficiently from multiple partitions.</p> </li> </ul> <p>By properly configuring consumer and subscription groups and optimizing Kafka connection settings, you can achieve effective parallel data processing in Karafka.</p>"}, {"location": "FAQ/#how-should-i-handle-the-migration-to-different-consumer-groups-for-parallel-processing", "title": "How should I handle the migration to different consumer groups for parallel processing?", "text": "<p>Migrating to different consumer groups to facilitate parallel processing involves a few considerations:</p> <ul> <li> <p>Offset Management: When introducing new consumer groups, they typically consume from the latest or earliest offset by default. This behavior can be managed using the <code>Karafka::Admin#seek_consumer_group</code> feature in newer Karafka releases, allowing you to specify the starting offset for each new consumer group.</p> </li> <li> <p>Consumer Group Configuration: Implementing multiple consumer groups or adjusting subscription groups within a consumer group can help distribute the workload more evenly. This setup minimizes the risk of any one consumer group becoming a bottleneck.</p> </li> <li> <p>Testing in Staging: Before rolling out changes in production, thoroughly test the new consumer group configurations in a staging environment. This helps identify any potential issues with offset handling or data processing delays.</p> </li> </ul>"}, {"location": "FAQ/#what-are-the-best-practices-for-setting-up-consumer-groups-in-karafka-for-optimal-parallel-processing", "title": "What are the best practices for setting up consumer groups in Karafka for optimal parallel processing?", "text": "<p>Best practices for setting up consumer groups in Karafka to optimize parallel processing include:</p> <p>Best practices for setting up consumer groups in Karafka to optimize parallel processing include:</p> <ul> <li> <p>Dedicated Consumer Groups: Allocate a consumer group for each logically separate function within your application. This isolation helps in managing the processing load and minimizes the impact of rebalances.</p> </li> <li> <p>Subscription Group Utilization: Within a consumer group, use subscription groups to handle different topics or partitions. This setup provides flexibility in managing which part of your application handles specific data streams.</p> </li> <li> <p>Resource Allocation: Ensure that each consumer group and subscription group is allocated adequate resources such as CPU and memory to handle the expected workload. This allocation prevents performance bottlenecks due to resource contention.</p> </li> <li> <p>Monitoring and Scaling: Regularly monitor the performance of your consumer groups and adjust their configurations as necessary. Utilize Karafka\u2019s monitoring tools to track processing times, throughput, and lag to make informed scaling decisions.</p> </li> </ul> <p>Implementing these best practices will help you fully leverage Karafka\u2019s capabilities for parallel processing, enhancing the throughput and efficiency of your Kafka data pipelines.</p>"}, {"location": "FAQ/#how-can-i-set-up-custom-per-message-tracing-in-karafka", "title": "How can I set up custom, per-message tracing in Karafka?", "text": "<p>Implementing detailed, per-message tracing in Karafka involves modifying the monitoring and tracing setup to handle individual messages. This setup enhances visibility into each message's\u00a0processing and\u00a0integrates seamlessly with many tracing products like DataDog.</p> <p>Here's how you can set up this  detailed tracing step-by-step:</p> <ol> <li>Register Custom Event</li> </ol> <p>Begin by registering a custom event in Karafka for each message processed. This is essential to create a unique event for the monitoring system to trigger on each message consumption.</p> <pre><code># This will trigger before consumption to start trace\nKarafka.monitor.notifications_bus.register_event('consumer.consume.message')\n# This will trigger after consumption to finish trace\nKarafka.monitor.notifications_bus.register_event('consumer.consumed.message')\n</code></pre> <p>Registering a custom event allows you to define specific behavior and tracking that aligns with your application's needs, distinct from the batch processing default.</p> <ol> <li>Instrument with Karafka Monitor</li> </ol> <p>Once the event is registered, use Karafka\u2019s monitor to instrument it. This step does not involve actual data processing but sets up the framework for tracing.</p> <pre><code>class OrdersStatesConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      Karafka.monitor.instrument('consumer.consume.message', message: message)\n\n      consume_one(message)\n\n      Karafka.monitor.instrument('consumer.consumed.message', message: message)\n\n      # Mark as consumed after each successfully processed message\n      mark_as_consumed(message)\n    end\n  end\n\n  def consume_one(message)\n    # Your logic goes here\n  end\nend\n</code></pre> <ol> <li>Build a Custom Tracing Listener</li> </ol> <p>Modify or build a new tracing listener that specifically handles the per-message tracing.</p> <pre><code>class MyTracingListener\n  def on_consumer_consume_message(event)\n    # Start tracing here...\n  end\n\n  def on_consumer_consumed_message(event)\n    # Finalize trace when message is processed\n  end\n\n  def on_error_occurred(event)\n    # Do not forget to finalize also on errors if trace available\n  end\nend\n\nKarafka.monitor.subscribe(MyTracingListener.new)\n</code></pre>"}, {"location": "FAQ/#when-karafka-reaches-maxpollintervalms-time-and-the-consumer-is-removed-from-the-group-does-this-mean-my-code-stops-executing", "title": "When Karafka reaches <code>max.poll.interval.ms</code> time and the consumer is removed from the group, does this mean my code stops executing?", "text": "<p>No, your code does not stop executing when Karafka reaches the <code>max.poll.interval.ms</code> time, and the consumer is removed from the group. Karafka does not interrupt the execution of your code. Instead, it reports an error  indicating that the maximum poll interval has been exceeded, like this:</p> <pre><code>Data polling error occurred: Application maximum poll interval (300000ms) exceeded by 348ms\n</code></pre> <p>Your code will continue to execute until it is complete. However, marking messages as consumed after this error will not be allowed.</p>"}, {"location": "FAQ/#which-component-is-responsible-for-committing-the-offset-after-consuming-is-it-the-listener-or-the-worker", "title": "Which component is responsible for committing the offset after consuming? Is it the listener or the worker?", "text": "<p>In the Karafka framework, the worker contains a consumer that handles the offset committing. The consumer within the worker sends a commit request to the underlying C client instance. This process involves the worker's consumer storing the offset to be saved, which then goes through a C thread for the actual commit operation. It's important to note that Karafka commits offsets asynchronously by default.</p>"}, {"location": "FAQ/#can-the-on_idle-and-handle_idle-methods-be-changed-for-a-specific-consumer", "title": "Can the <code>on_idle</code> and <code>handle_idle</code> methods be changed for a specific consumer?", "text": "<p>No. The <code>on_idle</code> and <code>handle_idle</code> methods are part of Karafka's internal API and are not editable. Internal components use these methods for periodic jobs within the Karafka framework. They are not intended for user modification or are not part of the official public API. If you need to execute a specific method when the consumer is idle or when the last message from the topic has been consumed, you should use Karafka's periodic jobs feature. This feature is designed to handle such use cases effectively.</p>"}, {"location": "FAQ/#is-multiplexing-an-alternative-to-running-multiple-karafka-processes-but-using-threads", "title": "Is Multiplexing an alternative to running multiple Karafka processes but using Threads?", "text": "<p>No, multiplexing serves a different use case. It's primarily for handling IO-bound operations, dealing with connections, and polling rather than work distribution and execution. Multiplexing is specifically for connection multiplexing within the same topic. Tuning Karafka processing is complex due to its flexibility. It can be influenced by the nature of your processing, deployment type, and data patterns, and there is no one best solution.</p>"}, {"location": "FAQ/#is-it-possible-to-get-watermark-offsets-from-inside-a-consumer-class-without-using-admin", "title": "Is it possible to get watermark offsets from inside a consumer class without using Admin?", "text": "<p>You can get watermark offsets and other metrics directly from within a consumer class using Karafka's Inline Insights. This feature provides a range of metrics, including watermark offsets, without using the Admin API. For more details, refer to the Inline Insights documentation.</p>"}, {"location": "FAQ/#why-are-message-and-batch-numbers-increasing-even-though-i-havent-sent-any-messages", "title": "Why are message and batch numbers increasing even though I haven't sent any messages?", "text": "<p>Karafka Web-UI uses Kafka to report the status of Karafka processes, sending status messages every 5 seconds by default. This is why you see the message and batch numbers increasing. The web UI uses these Kafka messages to show the status of the processes.</p> <p>Karafka processes messages in batches, and the value you see indicates how many batches have been processed, even if a batch contains only one message.</p> <p>To view the actual payload of messages sent from producer to consumer, you can use the Karafka Explorer.</p>"}, {"location": "FAQ/#what-does-configuisessionssecret-do-for-the-karafka-web-ui-do-we-need-it-if-we-are-using-our-authentication-layer", "title": "What does <code>config.ui.sessions.secret</code> do for the Karafka Web UI? Do we need it if we are using our authentication layer?", "text": "<p>The <code>config.ui.sessions.secret</code> configuration is used for CSRF (Cross-Site Request Forgery) protection in the Karafka Web UI. Even if you use your own authentication layer, you must set this configuration. It's not critical, but it needs to be set.</p> <p>Since you have your own authentication, this configuration becomes secondary, though it still provides an additional layer of protection. Ensure that the secret is consistent across all deployment instances, with one value per environment.</p>"}, {"location": "FAQ/#is-there-middleware-for-consuming-messages-similar-to-the-middleware-for-producing-messages", "title": "Is there middleware for consuming messages similar to the middleware for producing messages?", "text": "<p>Due to the complexity of the data flow, there are only a few middleware layers for consuming messages in Karafka, but several layers can function similarly. These are referred to as \"strategies\" in Karafka, and there are around 80 different combinations available.</p> <p>Karafka provides official APIs to alter the consumption and processing flow at various key points. The most notable among these is the Filtering API, which, despite its name, offers both flow control and filtering capabilities. This API spans from post-polling to post-batch execution stages.</p> <p>One of the key strengths of Karafka is its support for pluggable components. These components can be tailored to meet your specific requirements, offering a high degree of customization. Detailed information about these components and their configurations can be found in the Karafka documentation.</p>"}, {"location": "FAQ/#can-we-change-the-name-of-karafkas-internal-topic-for-the-web-ui", "title": "Can we change the name of Karafka's internal topic for the Web UI?", "text": "<p>Yes, you can change the name of Karafka's internal topic for the Web UI. </p> <p>For instance, if you need to prepend a unique combination before the topic's name, such as <code>12303-karafka_consumers_states</code>, this is feasible.</p> <p>Detailed instructions on how to configure this can be found in the Karafka documentation under this section.</p>"}, {"location": "FAQ/#is-there-a-way-to-control-which-pages-we-show-in-the-karafka-web-ui-explorer-to-prevent-exposing-pii-data", "title": "Is there a way to control which pages we show in the Karafka Web UI Explorer to prevent exposing PII data?", "text": "<p>Yes. Karafka provides an API for visibility filtering, which allows you to decide what to display, and whether options to download payloads and JSON versions should be usable. Additionally, you can sanitize certain fields from being presented.</p> <p>For detailed information, refer to the Pro Enhanced Web UI Policies documentation.</p>"}, {"location": "FAQ/#what-does-the-strict_topics_namespacing-configuration-setting-control", "title": "What does the <code>strict_topics_namespacing</code> configuration setting control?", "text": "<p>The <code>strict_topics_namespacing</code> configuration in Karafka enforces consistent naming for topics by ensuring they use either dots (<code>.</code>) or underscores (<code>_</code>) but not a mix of both in a topic name. This validation helps prevent inconsistencies in topic names, which is crucial because inconsistent namespacing can lead to issues like Kafka metrics reporting name collisions. Such collisions occur because Kafka uses these characters to structure metric names, and mixing them can cause metrics to overlap or be misinterpreted, leading to inaccurate monitoring and difficulties in managing Kafka topics. By enabling <code>strict_topics_namespacing</code>, you ensure that all topic names follow a uniform pattern, avoiding these potential problems. This validation can be turned off by setting <code>config.strict_topics_namespacing</code> to false if your environment does not require uniform naming.</p>"}, {"location": "FAQ/#does-librdkafka-queue-messages-when-using-waterdrops-produce_sync-method", "title": "Does librdkafka queue messages when using Waterdrop's <code>#produce_sync</code> method?", "text": "<p>Yes, librdkafka does queue messages internally. Even when WaterDrop does not use additional queues to accumulate messages before passing them to librdkafka, librdkafka maintains an internal queue. This queue is used to build message batches that are dispatched to the appropriate brokers.</p> <p>By default, librdkafka flushes this internal queue every <code>5</code> milliseconds. This means that when you call <code>#produce_sync</code>, the message is moved to librdkafka's internal queue, flushed within this 5ms window. The synchronous produce call waits for the result of this flush.</p> <p>Waterdrop also manages buffer overflows for this internal queue in synchronous and asynchronous modes. Depending on the Waterdrop configuration, it will handle retries appropriately in case of overflows or raise an error.</p>"}, {"location": "FAQ/#how-reliable-is-the-waterdrop-async-produce-will-messages-be-recovered-if-the-karafka-process-dies-before-producing-the-message", "title": "How reliable is the Waterdrop async produce? Will messages be recovered if the Karafka process dies before producing the message?", "text": "<p>The Waterdrop async produce is not reliable in terms of message recovery if the Karafka process dies before producing the message. If the process is killed while a message is being sent to Kafka, the message will be lost. This applies to both asynchronous and synchronous message production. This, however, is not specific to Kafka. SQL database transactions in the middle of being sent will also be interrupted, as will any other type of communication that did not finish.</p> <p>For improved performance and reliability, you might want to consider using the Karafka transactional producer. This feature can enhance the efficiency and robustness of your message production workflow.</p>"}, {"location": "FAQ/#will-waterdrop-start-dropping-messages-upon-librdkafka-buffer-overflow", "title": "Will WaterDrop start dropping messages upon librdkafka buffer overflow?", "text": "<p>By default, WaterDrop will not drop messages when the librdkafka buffer overflows. Instead, it has a built-in mechanism to handle such situations by backing off and retrying the message production.</p> <p>When WaterDrop detects that the librdkafka queue is full, an exception will not be immediately raised. Instead, it waits for a specified amount of time before attempting to resend the message. This backoff period allows librdkafka to dispatch previously buffered messages, freeing up space in the queue. During this waiting period, an error is logged in the <code>error.occurred</code> notification pipeline. While this error is recoverable, frequent occurrences might indicate underlying issues that need to be addressed.</p> <p>If the queue remains full even after the backoff period, WaterDrop will continue to retry sending the message until there is enough space. This retry mechanism ensures that messages are not lost.</p> <p>This behavior can be aligned by changing appropriate configuration settings.</p>"}, {"location": "FAQ/#how-can-i-handle-dispatch_to_dlq-method-errors-when-using-the-same-consumer-for-a-topic-and-its-dlq", "title": "How can I handle <code>dispatch_to_dlq</code> method errors when using the same consumer for a topic and its DLQ?", "text": "<p>If you use the same consumer for a particular topic and its Dead Letter Queue (DLQ), you might encounter an issue where the <code>dispatch_to_dlq</code> method is unavailable in the DLQ context. This can lead to errors if the method is called again during DLQ reprocessing.</p> <p>In Karafka, different consumer instances may operate in different contexts. Specifically, the DLQ context does not have access to DLQ-specific methods because these methods are injected only for the original topic consumer context. This ensures that specific methods are not used outside their intended context, maintaining a clean and safe API.</p> <p>To handle this, you can use a guard to check whether the <code>#dispatch_to_dlq</code> method is available before calling it. Here are a couple of approaches:</p> <ol> <li>Check for Method Availability:</li> </ol> <p>You can use the <code>#respond_to?</code> method to check if dispatch_to_dlq is available before calling it.</p> <pre><code>if respond_to?(:dispatch_to_dlq)\n  dispatch_to_dlq\nelse\n  # Handle the error or reprocess logic here\nend\n</code></pre> <ol> <li>Differentiate Using Topic Reference:</li> </ol> <p>Alternatively, you can check if the consumer is processing a DLQ topic by using the <code>topic.dead_letter_queue?</code> method. This method returns true if the current topic has DLQ enabled but will be false when processing the DLQ itself.</p> <pre><code>if topic.dead_letter_queue?\n  # This is the original topic, so `dispatch_to_dlq` is safe to use\n  dispatch_to_dlq\nelse\n  # This is the DLQ topic, handle accordingly\n  # Handle the error or reprocess logic here\nend\n</code></pre> <p>When using the same consumer for both a topic and its DLQ in Karafka, ensure that you handle method availability appropriately to avoid errors. Using guards like checking the topic context with <code>topic.dead_letter_queue?</code> can help maintain robustness and prevent unexpected exceptions during reprocessing.</p>"}, {"location": "FAQ/#what-should-i-do-if-i-encounter-the-broker-not-enough-in-sync-replicas-error", "title": "What should I do if I encounter the <code>Broker: Not enough in-sync replicas</code> error?", "text": "<p>This error indicates that there are not enough in-sync replicas to handle the message persistence. Here's how to address the issue:</p> <ol> <li> <p>Check Cluster Size and Configuration: Ensure that your Kafka cluster has enough brokers to meet the required replication factor for the topics. If your replication factor is set to <code>3</code>, you need at least <code>3</code> brokers.</p> </li> <li> <p>Increase Broker Storage Size: If your brokers are running out of storage space, they will not be able to stay in sync. Increasing the storage size, if insufficient, can help maintain enough in-sync replicas.</p> </li> <li> <p>Check the Cluster's <code>min.insync.replicas</code> Setting: Ensure that the <code>min.insync.replicas</code> setting in your Kafka cluster is not higher than the replication factor of your topics. If <code>min.insync.replicas</code> is set to a value higher than the replication factor of a topic, this error will persist. In such cases, manually adjust the affected topics' replication factor to match the required <code>min.insync.replicas</code> or recreating the topics with the correct replication factor.</p> </li> </ol> <p>By following these steps, you should be able to resolve the \"Broker: Not enough in-sync replicas\" error and ensure your Kafka cluster is correctly configured to handle the required replication.</p>"}, {"location": "FAQ/#is-there-any-way-to-measure-message-sizes-post-compression-in-waterdrop", "title": "Is there any way to measure message sizes post-compression in Waterdrop?", "text": "<p>Waterdrop metrics do not provide direct measurements for post-compression message sizes.</p> <p>To estimate message sizes post-client compression, you can use the <code>txmsgs</code> and <code>txbytes</code> metrics in Waterdrop instrumentation. These metrics provide information per topic partition and can give you a reasonable estimate of the message sizes after compression if the compression occurs on the client side.</p>"}, {"location": "FAQ/#what-happens-to-a-topic-partition-when-a-message-fails-and-the-exponential-backoff-strategy-is-applied-is-the-partition-paused-during-the-retry-period", "title": "What happens to a topic partition when a message fails, and the exponential backoff strategy is applied? Is the partition paused during the retry period?", "text": "<p>Yes, when a message fails on a specific topic partition and the exponential backoff strategy is applied, that partition is effectively paused during the retry period. This ensures strong ordering, which is a key guarantee of Karafka. If you want to bypass this behavior, you can configure a DLQ with delayed processing, allowing you to manage retries without pausing the partition.</p>"}, {"location": "FAQ/#how-can-virtual-partitions-help-with-handling-increased-consumer-lag-in-karafka", "title": "How can Virtual Partitions help with handling increased consumer lag in Karafka?", "text": "<p>Virtual Partitions (VPs) can significantly improve the parallelism of message consumption in Karafka. Suppose you have a topic with 6 partitions and are running 6 processes (each with 1 partition assigned). In that case, the parallelism is limited to one thread per partition. Using VPs, you can further split the data from each partition into multiple virtual partitions, allowing more threads to process the data concurrently. For example, with 5 virtual partitions per physical partition, you can achieve 30 virtual partitions, increasing the throughput and reducing the lag more efficiently.</p>"}, {"location": "FAQ/#is-scaling-more-processes-a-viable-alternative-to-using-virtual-partitions", "title": "Is scaling more processes a viable alternative to using Virtual Partitions?", "text": "<p>Scaling processes can help up to the number of partitions available. For instance, with 6 partitions, you can scale up to 6 processes. Beyond that, additional processes will not contribute to processing since Kafka consumer groups assign one partition per process. Each process will run a single thread per partition, which can become IO-constrained, especially with tasks like bulk inserts into Timescale. In contrast, using VPs allows better utilization of threads within the same partitions, providing a cost-effective performance boost without needing to scale processes proportionally.</p>"}, {"location": "FAQ/#what-is-the-optimal-strategy-for-scaling-in-karafka-to-handle-high-consumer-lag", "title": "What is the optimal strategy for scaling in Karafka to handle high consumer lag?", "text": "<p>The optimal strategy depends on your specific processing patterns and data distribution. A balanced approach involves using a combination of more partitions and virtual partitions. For example, with 6 partitions, you could configure 2 processes with VPs that utilize half the concurrency each and a multiplexing factor of 2. This setup can balance cost and performance well, ensuring you have enough headroom to handle lag spikes efficiently.</p>"}, {"location": "FAQ/#how-does-karafka-behave-under-heavy-lag-and-what-should-be-considered-in-configuration", "title": "How does Karafka behave under heavy lag, and what should be considered in configuration?", "text": "<p>Under heavy lag, incorrect settings can reduce Karafka's ability to utilize multiple threads effectively. While Karafka may perform well with multiple threads under normal conditions, heavy lag can force it to operate with reduced concurrency. Configuring Karafka with appropriate settings is crucial to maintaining optimal performance during lag periods. Understanding how to balance threads, processes, and virtual partitions is key to effectively managing high-lag situations.</p>"}, {"location": "FAQ/#is-there-an-undo-of-quiet-for-a-consumer-to-get-it-consuming-again", "title": "Is there an undo of Quiet for a consumer to get it consuming again?", "text": "<p>A quietened consumer needs to be replaced. When a consumer is quieted, it holds the connections and technically still \"moves\" forward, but it does so without processing messages. Therefore, to resume consuming, the consumer should be stopped, and a new one should be started.</p>"}, {"location": "FAQ/#can-two-karafka-server-processes-with-the-same-group_id-consume-messages-from-the-same-partition-in-parallel", "title": "Can two Karafka server processes with the same group_id consume messages from the same partition in parallel?", "text": "<p>No, two Karafka server processes with the same <code>group_id</code> cannot consume messages from the same partition in parallel because this would violate Kafka's strong ordering guarantees. However, you can use virtual partitions to parallelize the work within a single process. Additionally, you can use direct assignments to assign specific partitions to specific processes, but managing offsets would still require a separate consumer group.</p>"}, {"location": "FAQ/#what-are-some-good-default-settings-for-sending-large-trace-batches-of-messages-for-load-testing", "title": "What are some good default settings for sending large \"trace\" batches of messages for load testing?", "text": "<p>You can use the <code>produce_many_sync</code> method to send large batches of messages, as it tends to avoid buffer overflows and performs well even with default settings. You might also want to increase the <code>queue.buffering.max.ms</code> setting. Consider dispatching multi-partition messages to delegate faster if you have a larger cluster.</p>"}, {"location": "FAQ/#is-it-worth-pursuing-transactions-for-a-low-throughput-but-high-importance-topic", "title": "Is it worth pursuing transactions for a low throughput but high-importance topic?", "text": "<p>Yes, for low throughput but high-importance topics, it is advisable to use transactions. Transactions ensure that operations are tied together and can be managed atomically, reducing the risk of data inconsistency. Even though the finalization of a transaction is synchronous, it provides an additional layer of reliability for critical data flows.</p>"}, {"location": "FAQ/#does-the-waterdrop-producer-retry-to-deliver-messages-after-errors-such-as-librdkafkaerror-and-librdkafkadispatch_error-or-are-the-messages-lost", "title": "Does the Waterdrop producer retry to deliver messages after errors such as <code>librdkafka.error</code> and <code>librdkafka.dispatch_error</code>, or are the messages lost?", "text": "<p>It depends on the type of error. Waterdrop will retry the delivery for intermediate errors, such as network issues. However, for final errors like an unknown partition, the message will not be retried. If a message is purged, you should receive a final error notification indicating the purge.</p>"}, {"location": "FAQ/#in-a-rails-request-can-i-publish-a-message-asynchronously-continue-the-request-and-block-at-the-end-to-wait-for-the-publish-to-finish", "title": "In a Rails request, can I publish a message asynchronously, continue the request, and block at the end to wait for the publish to finish?", "text": "<p>Yes, you can publish a message asynchronously using Waterdrop. You can get the handler for the async publish and wait on it before the response is returned. This approach ensures that the request continues while the publish happens in parallel. It blocks only at the end to ensure the publish is complete.</p>"}, {"location": "FAQ/#why-am-i-getting-the-error-no-provider-for-sasl-mechanism-gssapi-recompile-librdkafka-with-libsasl2-or-openssl-support", "title": "Why am I getting the error: \"No provider for SASL mechanism GSSAPI: recompile librdkafka with libsasl2 or openssl support\"?", "text": "<p>This error:</p> <pre><code>No provider for SASL mechanism GSSAPI:\n  recompile librdkafka with libsasl2 or openssl support.\n  Current build options: PLAIN SASL_SCRAM OAUTHBEARER (Rdkafka::Config::ClientCreationError)\n</code></pre> <p>indicates that <code>librdkafka</code> was not built with support for GSSAPI (Kerberos). This often occurs when the necessary development packages (<code>libsasl2-dev</code>, <code>libssl-dev</code> and <code>libkrb5-dev</code>) were not available during the <code>librdkafka</code> build process. To fix this, make sure to install these packages before compiling librdkafka. On a Debian-based system, use:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y libsasl2-dev libssl-dev libkrb5-dev\n</code></pre>"}, {"location": "FAQ/#how-can-i-check-if-librdkafka-was-compiled-with-ssl-and-sasl-support-in-karafka", "title": "How can I check if <code>librdkafka</code> was compiled with SSL and SASL support in Karafka?", "text": "<p>You can check if <code>librdkafka</code> has SSL and SASL support by running the following code in an interactive Ruby session (<code>irb</code>):</p> <pre><code>require 'rdkafka'\n\nconfig = {\n  :\"bootstrap.servers\" =&gt; \"localhost:9092\",\n  :\"group.id\" =&gt; \"ruby-test\",\n  debug: 'all'\n}\n\nconsumer = Rdkafka::Config.new(config).consumer\nconsumer.subscribe(\"ruby-test-topic\")\nconsumer.close\n</code></pre> <p>This will generate logs. Look for lines containing <code>builtin.features</code> that list <code>ssl</code> and <code>sasl_scram</code>:</p> <pre><code>rdkafka#consumer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4, \\\n  sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer,http,oidc, GCC \\\n  GXX PKGCONFIG INSTALL GNULD LDS C11THREADS LIBDL PLUGINS ZLIB SSL SASL_CYRUS \\\n  ZSTD CURL HDRHISTOGRAM LZ4_EXT SYSLOG SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER \\\n  OAUTHBEARER_OIDC CRC32C_HW, debug 0xfffff)\n</code></pre>"}, {"location": "FAQ/#why-does-librdkafka-lose-ssl-and-sasl-support-in-my-multi-stage-docker-build", "title": "Why does <code>librdkafka</code> lose SSL and SASL support in my multi-stage Docker build?", "text": "<p>There are a couple of reasons <code>librdkafka</code> might lose SSL and SASL support in a multi-stage Docker build:</p> <ol> <li>Removing Essential Build Dependencies Too Early: In multi-stage builds, it's common to install libraries and development tools in an earlier stage and then copy the built software into a slimmer final stage to reduce the image size. However, if the necessary packages (like <code>libssl-dev</code> and <code>libsasl2-dev</code>) are removed or not available during the initial build stage, librdkafka will compile without SSL and SASL support.</li> </ol> <p>Solution: Ensure that <code>libssl-dev</code>, <code>libsasl2-dev</code>, and other required libraries are installed in the stage where you build librdkafka. Only clean up or remove these libraries after the build is complete.</p> <ol> <li>Build Layers Removal During Docker Image Creation:</li> </ol> <p>During the process of building Docker images, each command in the Dockerfile creates a new layer. When a layer is removed, all the changes made in that layer (including installed libraries) are also discarded. If the layers containing the installation of <code>libssl-dev</code>, <code>libsasl2-dev</code>, or other dependencies are removed before librdkafka is fully built and linked, then the resulting image will lack SSL and SASL support.</p> <p>Solution: To avoid this issue, ensure that any cleanup commands (like <code>apt-get</code> remove or <code>rm</code>) are executed after the software is compiled and only if you do not need those libraries anymore for runtime.</p>"}, {"location": "FAQ/#why-do-i-see-waterdrop-error-events-but-no-raised-exceptions-in-sync-producer", "title": "Why do I see WaterDrop error events but no raised exceptions in sync producer?", "text": "<p>This behavior is by design and relates to WaterDrop's sophisticated error handling model. Here's why this happens:</p> <ol> <li>Retryable vs. Fatal Errors</li> </ol> <ul> <li>WaterDrop distinguishes between intermediate retryable errors and fatal errors</li> <li>Many errors (like network glitches) are considered retryable</li> <li>These errors are logged but don't necessarily cause the operation to fail</li> </ul> <ol> <li>Recovery Process</li> </ol> <ul> <li>As long as a message isn't purged from dispatch, WaterDrop will attempt to deliver it</li> <li>If WaterDrop can recover before the message purge time, the produce_sync operation will still succeed</li> <li>Background errors are emitted to inform you about these recovery attempts</li> </ul> <ol> <li>Why This Matters</li> </ol> <ul> <li>You want to know about intermediate issues (like socket disconnects) as they might indicate underlying cluster problems</li> <li>However, if WaterDrop successfully recovers and delivers the message, there's no need to raise an exception</li> <li>The operation ultimately succeeded from the user's perspective</li> </ul> <p>For example, if there's a temporary network disconnection:</p> <ol> <li>The error event is emitted and logged</li> <li>WaterDrop reestablishes the connection</li> <li>The message is successfully delivered</li> <li>No exception is raised because the operation ultimately succeeded</li> </ol> <p>For more detailed information about WaterDrop's error handling model, refer to this documentation.</p>"}, {"location": "FAQ/#when-does-eof-end-of-file-handling-occur-in-karafka-and-how-does-it-work", "title": "When does EOF (End of File) handling occur in Karafka, and how does it work?", "text": "<p>EOF handling in Karafka only occurs when it is explicitly enabled. However, it's important to understand that EOF execution may not always trigger when the end of a partition is reached during message processing.</p> <p>When messages are present in the final batch that reaches the end of a partition, Karafka will execute a regular consumption run with the <code>#eofed?</code> flag set to <code>true</code>, rather than triggering the EOF handling logic.</p> <p>This is because the primary purpose of EOF handling is to deal with scenarios where there are no more messages to process, rather than handling the last message batch. The EOF handling can be useful for executing cleanup or maintenance tasks when a partition has been fully consumed, but it should not be relied upon as a guaranteed trigger for end-of-partition processing logic. If you need guaranteed processing for the last message or batch in a partition, you should implement that logic within your regular message consumption flow using the <code>#eofed?</code> check.</p>"}, {"location": "FAQ/#how-can-i-determine-if-a-message-is-a-retry-or-a-new-message", "title": "How can I determine if a message is a retry or a new message?", "text": "<p>You can use:</p> <ul> <li><code>#attempt</code> - shows retry attempt number  </li> <li><code>#retrying?</code> - boolean indicating if message is being retried</li> </ul> <p>Note that:</p> <ol> <li>This works per offset location, not per individual message, unless you mark each message as consumed</li> <li>The error causing the retry may differ between retry attempts</li> </ol> <p>You can find more details about this here.</p>"}, {"location": "FAQ/#why-does-karafka-web-ui-stop-working-after-upgrading-the-ruby-slimalpine-docker-images", "title": "Why does Karafka Web UI stop working after upgrading the Ruby slim/alpine Docker images?", "text": "<p>Recent changes in the official Ruby slim and alpine Docker images removed several system dependencies, including the <code>procps</code> package that provides the <code>ps</code> command. The <code>ps</code> command is required by Karafka Web UI for process management and monitoring.</p> <p>To resolve this issue, you need to explicitly add the <code>procps</code> package to your Dockerfile. For Debian-based images (slim), add:</p> <pre><code>RUN apt-get update &amp;&amp; apt-get install -y procps\n</code></pre> <p>For Alpine-based images, add:</p> <pre><code>RUN apk add --no-cache procps\n</code></pre> <p>You can find the complete list of required system commands in our Getting Started documentation.</p>"}, {"location": "FAQ/#why-does-installing-karafka-web-take-exceptionally-long", "title": "Why does installing <code>karafka-web</code> take exceptionally long?", "text": "<p>When installing <code>karafka</code> and <code>karafka-web</code>, especially in Docker environments like <code>ruby:3.4.2-slim</code>, you might notice installation appears exceptionally slow. However, this delay is typically not caused by <code>karafka</code> itself.</p> <p>Instead, the slowdown usually results from compiling native extensions for other gems - most commonly <code>grpc</code>. Bundler's parallel installation can mislead you into thinking that <code>karafka-web</code> is slow, as the log messages for other gems (like <code>grpc</code>) may appear after the message indicating <code>karafka-web</code> installation, giving a false impression of delay.</p> <p>Karafka itself includes native extensions via the <code>karafka-rdkafka</code> gem, but its compilation typically completes quickly (usually within 1-2 minutes).</p> <p>On the other hand, compiling <code>grpc</code> from source can take significantly longer (up to 20 minutes or more), particularly in resource-constrained CI environments.</p> <p>You can verify the bottleneck by running:</p> <pre><code>bundle install --jobs=1\n</code></pre> <p>This command disables parallel installation, clearly showing compilation times per gem.</p>"}, {"location": "FAQ/#why-does-karafka-routing-accept-consumer-classes-rather-than-instances", "title": "Why does Karafka routing accept consumer classes rather than instances?", "text": "<p>Karafka routing requires that you provide a consumer class reference rather than a consumer instance:</p> <pre><code># Correct approach\ntopic :topic_name do\n  consumer ConsumerClass\nend\n\n# Incorrect approach\ntopic :topic_name do\n  consumer ConsumerClass.new  # This will not work\nend\n</code></pre> <p>This design decision offers several important benefits:</p> <ol> <li> <p>Instance lifecycle management: Karafka needs to control when and how consumer instances are created to properly manage the message processing lifecycle.</p> </li> <li> <p>Resource management: By controlling instantiation, Karafka ensures proper resource cleanup after message processing is complete.</p> </li> <li> <p>Concurrency considerations: When running with multiple threads or processes, Karafka creates separate consumer instances for each concurrent execution unit to maintain thread safety.</p> </li> <li> <p>Configuration integration: Class-based routing allows Karafka to apply configuration and middleware to the class before instantiation.</p> </li> </ol> <p>This pattern follows the principle of Inversion of Control (IoC), where the framework controls object creation rather than the application code. It's similar to how other Ruby frameworks (like Rails) reference controllers by class in routes, not by instances.</p>"}, {"location": "FAQ/#why-does-karafka-define-routing-separate-from-consumer-classes-unlike-sidekiq-or-racecar", "title": "Why does Karafka define routing separate from consumer classes, unlike Sidekiq or Racecar?", "text": "<p>Unlike frameworks such as Sidekiq or Racecar, where message-processing destinations are defined directly within classes, Karafka uses a separate routing layer that maps topics to consumer classes:</p> <pre><code># Karafka approach - separate routing definition\nApp.routes.draw do\n  topic :orders do\n    consumer OrdersConsumer\n  end\n\n  topic :notifications do\n    consumer NotificationsConsumer\n  end\nend\n\n# vs. embedded approach (not used in Karafka)\nclass OrdersConsumer\n  subscribes_to :orders\n  # ...\nend\n</code></pre> <p>This deliberate architectural decision provides several significant benefits:</p> <ol> <li> <p>Consumer reusability: The same consumer class can be used with multiple topics without modification. This enables powerful patterns where a single processing implementation can handle data from various sources.</p> </li> <li> <p>Multi-level configuration: Karafka's routing system allows configuration at different levels of abstraction:    - Consumer group level    - Subscription group level    - Topic level</p> </li> <li> <p>Separation of concerns: Routing (what to consume) is separated from consumption logic (how to process). This creates cleaner, more maintainable code.</p> </li> <li> <p>Dynamic routing capabilities: The routing layer can be extended with logic that determines routes based on runtime conditions.</p> </li> <li> <p>Enhanced testing: Consumers can be tested independently from their routing configuration, improving unit test isolation.</p> </li> <li> <p>Flexibility for complex setups: The separate routing layer provides much better organization and clarity for advanced Kafka deployments with many topics and consumers.</p> </li> </ol> <p>This approach follows established software architecture principles and provides significantly more flexibility when working with complex Kafka-based systems, especially as your application grows.</p>"}, {"location": "FAQ/#whats-the-difference-between-key-and-partition_key-in-waterdrop", "title": "What's the difference between <code>key</code> and <code>partition_key</code> in WaterDrop?", "text": "<p>When producing messages with WaterDrop, you have the option to specify both a <code>key</code> and a <code>partition_key</code>:</p> <pre><code># Using both key and partition_key\nproducer.produce_async(\n  topic: 'orders',\n  payload: order.to_json,\n  key: order.id.to_s,\n  partition_key: order.customer_id.to_s\n)\n</code></pre> <p>These two parameters serve different purposes:</p>"}, {"location": "FAQ/#key", "title": "Key", "text": "<p>The <code>key</code> parameter sets the actual Kafka message key stored with the message. This key:</p> <ul> <li>Is used by Kafka for log compaction (if enabled on the topic)</li> <li>Can be accessed by consumers when processing the message</li> <li>Is included in the message payload that gets stored in Kafka</li> </ul>"}, {"location": "FAQ/#partition-key", "title": "Partition Key", "text": "<p>The <code>partition_key</code> parameter is only used to determine which partition the message should be sent to. It:</p> <ul> <li>Is used solely for the partitioning algorithm</li> <li>Is not stored with the message in Kafka</li> <li>Allows you to control message distribution across partitions without affecting the message key</li> </ul> <p>When only <code>key</code> is provided, WaterDrop uses it for both purposes - as the stored message key and for partition determination. When both are specified, <code>partition_key</code> precedes partition selection, while <code>key</code> is still stored with the message.</p> <p>This separation is particularly useful when:</p> <ul> <li>You need messages with different keys to end up in the same partition (for ordering)</li> <li>You want to use a different attribute for partition selection than what makes sense as a logical message key</li> <li>You need to change how messages are partitioned without affecting downstream consumers that rely on the message key</li> </ul> <p>Remember that messages with the same <code>partition_key</code> (or <code>key</code> if no <code>partition_key</code> is specified) will always be routed to the same partition, ensuring ordered processing within that data subset.</p>"}, {"location": "FAQ/#can-i-disable-logging-for-karafka-web-ui-consumer-operations-while-keeping-it-for-my-application-consumers", "title": "Can I disable logging for Karafka Web UI consumer operations while keeping it for my application consumers?", "text": "<p>Yes, you can selectively disable logging for Karafka Web UI consumer operations by subclassing the <code>LoggerListener</code> and filtering based on the consumer type.</p> <p>By default, Karafka logs all consumer operations, including those from the Web UI, because this information can be valuable for debugging overloaded processes or understanding system behavior. However, if you want to reduce log noise, especially in development, you can create a custom logger listener:</p> <pre><code>class MyLogger &lt; Karafka::Instrumentation::LoggerListener\n  def on_worker_process(event)\n    job = event[:job]\n    consumer = job.executor.topic.consumer\n\n    return if consumer == Karafka::Web::Processing::Consumer\n\n    super\n  end\n\n  def on_worker_processed(event)\n    job = event[:job]\n    consumer = job.executor.topic.consumer\n\n    return if consumer == Karafka::Web::Processing::Consumer\n\n    super\n  end\nend\n</code></pre> <p>Then replace the default logger listener with your custom one in your <code>karafka.rb</code>:</p> <pre><code># Remove the default logger listener and add your custom one\nKarafka.monitor.subscribe(MyLogger.new)\n</code></pre> <p>This approach allows you to maintain detailed logging for your application consumers while filtering out the Web UI consumer logs that may flood your development logs. The same pattern can be extended to filter other types of operations as needed.</p>"}, {"location": "FAQ/#how-can-i-distinguish-between-sync-and-async-producer-errors-in-the-erroroccurred-notification", "title": "How can I distinguish between sync and async producer errors in the <code>error.occurred</code> notification?", "text": "<p>Both <code>produce_sync</code> and <code>produce_async</code> trigger the same <code>error.occurred</code> notification, making it difficult to distinguish between them. Since sync errors are typically already handled with backtraces, you can use WaterDrop's labeling feature to differentiate async errors that need special logging. Label your async messages and check for those labels in the error handler to process only async errors. See the detailed guide on distinguishing between sync and async producer errors for implementation examples.</p> <p>Last modified: 2025-06-30 16:48:05</p>"}, {"location": "Forking/", "title": "Forking", "text": "<p>Karafka under the hood relies on <code>librdkafka</code> to manage Kafka connections. It is crucial to understand that <code>librdkafka</code> is not fork-safe, which means special care must be taken when managing Ruby processes interacting with Kafka. This document provides guidelines for handling forking in Karafka, especially under macOS and in environments using Rails' Spring loader.</p> <p>Ecosystem-Wide Recommendations</p> <p>This guidance applies to all components of the Karafka ecosystem interacting with Kafka, including <code>rdkafka</code>, <code>karafka-rdkafka</code>, <code>WaterDrop</code>, and <code>Karafka</code>. Ensure these recommendations are followed to maintain system stability and prevent resource leaks.</p>"}, {"location": "Forking/#fork-safety-with-librdkafka", "title": "Fork Safety with <code>librdkafka</code>", "text": "<p>When forking Ruby processes, ensuring there are no active connections to Kafka is required. Active connections include consumer, producer, and admin connections. Failing to close these connections before forking can leak file descriptors and other resources, potentially destabilizing your application.</p>"}, {"location": "Forking/#karafkas-swarm-forking-strategy", "title": "Karafka's Swarm Forking Strategy", "text": "<p>Karafka uses forking in its Swarm Mode. This process is carefully designed to ensure that forks occur only when no Kafka connections are active. After forking, new connections are established in the child processes, thus maintaining clean and safe operations.</p>"}, {"location": "Forking/#forking-issues-on-macos", "title": "Forking Issues on macOS", "text": "<p>Forking on macOS, particularly from macOS High Sierra (10.13) onwards, introduces additional challenges due to changes in how macOS handles system calls in forked processes. These issues can manifest as errors like:</p> <ul> <li><code>[NSCharacterSet initialize] may have been in progress in another thread when fork()</code></li> <li>Segmentation faults such as <code>/Users/dev_machine/.rvm/gems/ruby-3.3.0/gems/rdkafka-0.15.0/lib/rdkafka/config.rb:291: [BUG] Segmentation fault at 0x0000000000000110</code></li> </ul> <p>These errors indicate processes in the middle of certain operations during a fork, which macOS now handles differently.</p>"}, {"location": "Forking/#solutions-for-macos-forking-issues", "title": "Solutions for macOS Forking Issues", "text": "<ol> <li>Pre-load <code>rdkafka</code> before forking: Ensure <code>rdkafka</code> is loaded in the parent process before any fork occurs. For Puma web server users, add this line to your <code>puma.rb</code> configuration file:</li> </ol> <pre><code>require 'rdkafka'\n</code></pre> <p>This ensures that the necessary libraries and Objective-C dynamic libraries (DLLs) are properly loaded before forking, preventing segmentation faults.</p> <ol> <li> <p>Environment Variable: You can set the environment variable <code>OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES</code> to help manage initialization issues related to forking in macOS environments.</p> </li> <li> <p>Rails Spring Strategy: For developers using Rails' Spring loader, managing forking can be particularly complex. This complexity arises because parts of <code>librdkafka</code> may not load correctly when Spring forks the Ruby process. Consider one of these approaches:</p> <ul> <li>Establish a short-lived connection to a local development Kafka instance when Spring boots using <code>Karafka::Admin.cluster_info</code></li> <li>Disable Spring in development if you're encountering persistent issues</li> </ul> </li> </ol> <p>Note that forking issues typically occur when the required dependencies aren't loaded in the parent process prior to forking. The underlying cause is related to how Objective-C DLLs handle forking on macOS.</p> <p>For more detailed information on macOS forking issues and solutions, see Phusion's blog on Ruby app servers and macOS High Sierra.</p>"}, {"location": "Forking/#conclusion", "title": "Conclusion", "text": "<p>Forking in Ruby applications that use Karafka and <code>librdkafka</code> requires careful planning and implementation to prevent resource leakage and ensure stable operation. This is especially true on macOS, where changes to the system's handling of forks can lead to critical issues. By following the outlined best practices, developers can effectively manage these challenges in a multi-process environment.</p> <p>Last modified: 2025-04-01 16:54:08</p>"}, {"location": "Getting-Started/", "title": "Getting Started", "text": ""}, {"location": "Getting-Started/#prerequisites", "title": "Prerequisites", "text": "<ol> <li>Make sure Apache Kafka is running. You can start it by following instructions from here.</li> </ol>"}, {"location": "Getting-Started/#for-existing-applications", "title": "For Existing Applications", "text": "<ol> <li>Add Karafka to your Gemfile:</li> </ol> <pre><code># Make sure to install Karafka 2.5 as Karafka 1.4 is no longer maintained\nbundle add karafka --version \"&gt;= 2.5.0\"\n</code></pre> <ol> <li>Install Karafka (works for both Rails and standalone applications) by running:</li> </ol> <pre><code>bundle exec karafka install\n</code></pre> <p>the above command will create all the necessary files and directories to get you started:</p> <ul> <li><code>karafka.rb</code> - main file where you configure Karafka and where you define which consumers should consume what topics.</li> <li><code>app/consumers/example_consumer.rb</code> - example consumer.</li> <li><code>app/consumers/application_consumer.rb</code> - base consumer from which all consumers should inherit.</li> </ul> <ol> <li>After that, you can run a development console to produce messages to this example topic:</li> </ol> <pre><code># Works from any place in your code and is thread-safe\n# You usually want to produce async but here it may raise exception if Kafka is not available, etc\nKarafka.producer.produce_sync(topic: 'example', payload: { 'ping' =&gt; 'pong' }.to_json)\n</code></pre> <ol> <li>Run the karafka server to start consuming messages:</li> </ol> <pre><code>bundle exec karafka server\n\n# example outcome\n[7616dc24-505a-417f-b87b-6bf8fc2d98c5] Polled 2 messages in 1000ms\n[dcf3a8d8-0bd9-433a-8f63-b70a0cdb0732] Consume job for ExampleConsumer on example started\n{\"ping\"=&gt;\"pong\"}\n{\"ping\"=&gt;\"pong\"}\n[dcf3a8d8-0bd9-433a-8f63-b70a0cdb0732] Consume job for ExampleConsumer on example finished in 0ms\n</code></pre> <p>Here's the demo of the installation process:</p>      Note: Asciinema videos are not visible when viewing this wiki on GitHub. Please use our     online     documentation instead.    <ol> <li>(Optionally) Install and configure the Web UI by following this documentation section.</li> </ol>"}, {"location": "Getting-Started/#for-new-applications-starting-from-scratch", "title": "For New Applications (Starting From Scratch)", "text": "<p>If you're starting with an empty directory:</p> <ol> <li>First create a <code>Gemfile</code>:</li> </ol> <pre><code># Gemfile\nsource \"https://rubygems.org\"\n\ngem \"karafka\", \"&gt;= 2.5.0\"\n</code></pre> <ol> <li> <p>Run: <code>bundle install</code></p> </li> <li> <p>Install Karafka (works for both Rails and standalone applications) by running:</p> </li> </ol> <pre><code>bundle exec karafka install\n</code></pre> <p>the above command will create all the necessary files and directories to get you started:</p> <ul> <li><code>karafka.rb</code> - main file where you configure Karafka and where you define which consumers should consume what topics.</li> <li><code>app/consumers/example_consumer.rb</code> - example consumer.</li> <li><code>app/consumers/application_consumer.rb</code> - base consumer from which all consumers should inherit.</li> </ul> <ol> <li>After that, you can run a development console to produce messages to this example topic:</li> </ol> <pre><code># Works from any place in your code and is thread-safe\n# You usually want to produce async but here it may raise exception if Kafka is not available, etc\nKarafka.producer.produce_sync(topic: 'example', payload: { 'ping' =&gt; 'pong' }.to_json)\n</code></pre> <ol> <li>Run the karafka server to start consuming messages:</li> </ol> <pre><code>bundle exec karafka server\n\n# example outcome\n[7616dc24-505a-417f-b87b-6bf8fc2d98c5] Polled 2 messages in 1000ms\n[dcf3a8d8-0bd9-433a-8f63-b70a0cdb0732] Consume job for ExampleConsumer on example started\n{\"ping\"=&gt;\"pong\"}\n{\"ping\"=&gt;\"pong\"}\n[dcf3a8d8-0bd9-433a-8f63-b70a0cdb0732] Consume job for ExampleConsumer on example finished in 0ms\n</code></pre>"}, {"location": "Getting-Started/#example-applications", "title": "Example applications", "text": "<p>If you have any problems setting things up or want a ready application to play around with, then the best idea is just to clone our examples repository:</p> <pre><code>git clone https://github.com/karafka/example-apps ./example_apps\n</code></pre> <p>and follow the instructions from the example apps Wiki.</p>"}, {"location": "Getting-Started/#use-cases-edge-cases-and-usage-examples", "title": "Use-cases, edge-cases, and usage examples", "text": "<p>Karafka ships with a full integration test suite that illustrates various use-cases and edge-cases of working with Karafka and Kafka. Please visit this directory of the Karafka repository to see them.</p> <p>Last modified: 2025-06-16 12:29:46</p>"}, {"location": "Inline-Insights/", "title": "Inline Insights", "text": "<p>Staying informed and adjusting to real-time changes is critical in the dynamic data processing world. Karafka's Inline Insights provides a way to enhance your data processing capabilities by allowing your consumers to adjust their actions based on real-time metrics. Whether dealing with lag or responding to specific partition data changes, Inline Insights empowers developers to have a more hands-on and responsive approach. By tapping into this feature, you can ensure your system runs efficiently and adapts swiftly to any incoming data fluctuations.</p>"}, {"location": "Inline-Insights/#using-inline-insights", "title": "Using Inline Insights", "text": "<p>To get started, incorporate the <code>inline_insights</code> definition for the desired topics in your setup:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      inline_insights(true)\n    end\n  end\nend\n</code></pre> <p>After activating Inline Insights, your consumer gets supercharged with two additional methods:</p> <ul> <li><code>#insights?</code> - A straightforward boolean method indicating whether Karafka possesses metrics for the current topic partition.</li> <li><code>#insights</code> - A handy method that fetches a hash filled with Karafka statistics about the current topic partition in the context of its consumer group.</li> </ul> <p>Below, you can find an example consumer that is lag aware to optimize its operations accordingly:</p> <pre><code># This consumer adjusts buffering based on lag: quick flushes for minimal lags\n# and batched flushes during heavy lags to optimize IO.\nclass LogEventsConsumer &lt; ApplicationConsumer\n  # Defines the threshold for significant lag.\n  LAG_THRESHOLD = 100_000\n\n  # Sets the minimum buffer size before flush. under lag\n  FLUSH_THRESHOLD = 20_000\n\n  def initialized\n    @buffer = InMemoryDbFlushingBuffer.new\n  end\n\n  def consume\n    @buffer &lt;&lt; messages.payloads  # Data is always buffered.\n\n    # Remember that insights might not always be available immediately after consumer initiation.\n    current_lag = insights? ? insights.fetch('consumer_lag') : 0\n\n    # Buffering decisions are made based on current lag and buffer size.\n    return if current_lag &gt; LAG_THRESHOLD &amp;&amp; @buffer.size &lt; FLUSH_SIZE\n\n    @buffer.flush \n    mark_as_consumed(messages.last)\n  end\n\n  def shutdown\n    @buffer.flush\n    mark_as_consumed messages.last\n  end\nend\n</code></pre>"}, {"location": "Inline-Insights/#insights-runtime-availability", "title": "Insights Runtime Availability", "text": "<p>Upon establishing a connection with Kafka, it is essential to understand that metrics related to Inline Insights might not be immediately accessible. This delay is attributed to the way Karafka fetches and computes these metrics. By default, the metrics are calculated at intervals of 5 seconds.</p> <p>Furthermore, depending on the version of Kafka you are using and other underlying factors, the availability of insights, especially during the first batch processing, may vary. This means that for a brief period after connecting, your consumers might not be insight-aware and insights will be empty.</p> <pre><code>class LogEventsConsumer &lt; ApplicationConsumer\n  def process\n    if insights?\n      process_with_insights_awareness(messages)\n    else\n      process_with_default_flow(messages)\n    end\n  end\nend\n</code></pre>"}, {"location": "Inline-Insights/#crucial-insights-presence-with-karafka-pro", "title": "Crucial Insights Presence with Karafka Pro", "text": "<p>For scenarios where the presence of insights is vital for the functioning of your system, we recommend upgrading to Karafka Pro. With its extended capabilities regarding Inline Insights, you can ensure that insights are always available when you need them.</p> <p>In Karafka Pro, there's an option to define in the routing that insights are mandatory for a specific topic. When this setting is activated, Karafka will hold back and not consume any data from the specified topic partition until the necessary metrics are present. This guarantees that your consumers are always operating with the insights they require, ensuring optimized data consumption and processing. You can read more about this capability here.</p>"}, {"location": "Inline-Insights/#insights-freshness", "title": "Insights Freshness", "text": "<p>When metrics related to Inline Insights are requested for the first time during a given batch processing, they become \"materialized\". This means that the metrics become tangible and ready for the system to leverage. One significant aspect to note is that once these metrics are materialized, they remain consistent throughout the data consumption process until new metrics become available. This means that metrics may be updated while you are processing the data. Metrics, however, will never disappear, meaning you can rely on their presence if you have a metrics state. They may be updated, but they will not disappear.</p> <pre><code># Simulate slow processing on a regular (non LRJ) consumer\nclass LogEventsConsumer &lt; ApplicationConsumer\n  def process\n    before = insights\n\n    messages.each do |message|\n      puts message.payload\n      sleep(10)\n    end\n\n    after = insights\n\n    # This will never raise - metrics will always be the same during processing\n    raise if before != after\n  end\nend\n</code></pre>"}, {"location": "Inline-Insights/#behaviour-on-revocation", "title": "Behaviour on Revocation", "text": "<p>Insights are only collected for partitions a given consumer owns. This means that if a consumer does not have ownership of a particular partition, no insights will be collected. This design ensures that the metrics are always pertinent and directly related to the partitions being consumed, maintaining the relevance and efficiency of the insights provided.</p> <p>There can be scenarios where a partition might be involuntarily revoked from a consumer by Kafka. In such events, rather than leaving the consumer without any insights, Karafka takes a proactive approach. Karafka will provide the last known metrics before the partition was forcefully revoked. This is paramount as it ensures that metrics are consistently available to the consumer, even during unforeseen Kafka operations.</p> <p>Karafka ensures that consumers can continue processing with insights throughout their entire operation by ensuring the continuous availability of metrics, even post-revocation. This approach minimizes disruptions and ensures consumers won't be left operating without metrics even if Kafka alters assignments.</p>"}, {"location": "Inline-Insights/#memory-footprint", "title": "Memory Footprint", "text": "<p>One of the primary considerations for any system is the memory footprint of its features, and Karafka's Inline Insights is no exception. The insights occupy 4KB of space for a single topic partition. Thus, even if you have a multitude of partitions, the memory footprint remains low, enabling you to leverage insights without significantly impacting your system's performance.</p>"}, {"location": "Inline-Insights/#insights-tracker-cache-management", "title": "Insights Tracker Cache Management", "text": "<p>The Karafka Inline Insights Tracker is designed with optimal memory usage in mind. It employs a caching system that retains metrics data for up to 5 minutes. This means that any metric data related to a partition that is no longer relevant or needed - like when a partition is lost or revoked - will be automatically cleared from the cache after a maximum of five minutes. This cache management approach ensures that memory is used efficiently, keeping only relevant and recent metrics in memory and preventing accumulating stale or unnecessary data.</p>"}, {"location": "Inline-Insights/#example-use-cases", "title": "Example Use Cases", "text": "<ol> <li> <p>E-Commerce Inventory Management: In an e-commerce platform, inventory levels for hot-selling products are crucial. If there is a sudden surge in orders for a product, inventory levels need to be updated in real time to prevent overselling. Insights-aware consumers can monitor the lag in the inventory update partition. If the lag increases beyond a threshold, indicating potential discrepancies between actual and recorded inventory, the consumer can notify the inventory management system to pause new orders temporarily.</p> </li> <li> <p>Financial Transactions Processing: In a digital banking platform, processing financial transactions in real time is crucial for maintaining customer trust. By monitoring partitions related to transaction processing, insights-aware consumers can detect lags or errors. If a particular partition starts showing a significant lag, the system can prioritize it or reroute the traffic to ensure transactions are processed in real time.</p> </li> <li> <p>Real-time Health Monitoring Systems: In health tech, wearable devices send continuous data about a user's health metrics. Any delay in processing this data can be critical. If the consumer detects a lag in a partition related to a particular user or set of users, it can prioritize this data. For instance, the system can't afford a delay if a heart monitor shows irregular patterns with insights. Such critical data can be processed on priority.</p> </li> <li> <p>Financial Trading Platforms: In stock trading applications where milliseconds matter, lag awareness can trigger rapid response mechanisms to adjust trading algorithms or halt specific activities when real-time data metrics indicate potential issues.</p> </li> </ol> <p>In each of these business scenarios, the ability of Karafka's insights-aware consumers to adapt in real-time based on partition metrics can play a critical role in maintaining efficient operations, ensuring customer satisfaction, and even averting crises.</p>"}, {"location": "Inline-Insights/#summary", "title": "Summary", "text": "<p>Karafka's Inline Insights is a powerful feature that enhances data processing capabilities by allowing consumers to adapt based on real-time metrics. By integrating Inline Insights, developers can address issues such as lag or respond promptly to specific partition data changes, ensuring an efficient and swift adaptation to data fluctuations.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Integrating-with-Ruby-on-Rails-and-other-frameworks/", "title": "Integrating with Ruby on Rails and other frameworks", "text": "<p>Want to use Karafka with Ruby on Rails or any other framework? It can be done easily!</p>"}, {"location": "Integrating-with-Ruby-on-Rails-and-other-frameworks/#integrating-with-ruby-on-rails", "title": "Integrating with Ruby on Rails", "text": "<p>Karafka detects Ruby on Rails by itself, so no extra changes are required besides running the standard installation process.</p> <p>Add Karafka to your Gemfile:</p> <pre><code>bundle add karafka --version \"&gt;= 2.3.0\"\n</code></pre> <p>and run the installation command:</p> <pre><code>bundle exec karafka install\n</code></pre> <p>It will create all the needed directories and files and the <code>karafka.rb</code> configuration file. After that, you should be good to go.</p> <p>We also have an example Ruby on Rails application that illustrates integration with this framework.</p>"}, {"location": "Integrating-with-Ruby-on-Rails-and-other-frameworks/#integrating-with-sinatra-and-other-frameworks", "title": "Integrating with Sinatra and other frameworks", "text": "<p>Non-Rails applications differ from one another. There are single-file applications and apps similar to the Rails structure. That's why we cannot provide a simple single tutorial. Here are some guidelines that you should follow to integrate it with Sinatra based application:</p> <p>Add Karafka to your application Gemfile:</p> <pre><code>gem 'karafka'\n</code></pre> <p>run the installation process:</p> <pre><code>bundle exec karafka install\n</code></pre> <p>After that, ensure that your application is loaded before setting up and booting Karafka.</p> <p>Last modified: 2024-01-26 15:03:27</p>"}, {"location": "Kafka-Cluster-Configuration/", "title": "Kafka Cluster Configuration", "text": "<p>Kafka Configuration Variability</p> <p>The defaults and exact list of cluster configuration options may differ between various Kafka versions. For the most accurate information, please refer to the documentation for the specific Kafka version.</p> Names Default Value Read-only Sensitive Description <code>advertised.listeners</code> <code>PLAINTEXT://127.0.0.1:9092</code> \u274c \u274c Listeners to publish to ZooKeeper for clients to use, if different than the listeners config property. <code>allow.everyone.if.no.acl.found</code> <code></code> \u2705 \u2705 Indicates whether a request should be allowed if no ACL is found for the resource. <code>alter.config.policy.class.name</code> <code></code> \u2705 \u274c A class name that implements the AlterConfigPolicy interface, used for custom policies when altering configurations. <code>alter.log.dirs.replication.quota.window.num</code> <code>11</code> \u2705 \u274c Number of samples to use for tracking alter log dirs replication quotas. <code>alter.log.dirs.replication.quota.window.size.seconds</code> <code>1</code> \u2705 \u274c Time span for each sample used for tracking alter log dirs replication quotas. <code>authorizer.class.name</code> <code>org.apache.kafka.metadata.authorizer.StandardAuthorizer</code> \u2705 \u274c Class name of the authorizer used for ACL-based authorization. <code>auto.create.topics.enable</code> <code>true</code> \u2705 \u274c Enable auto creation of topics. <code>auto.include.jmx.reporter</code> <code>true</code> \u2705 \u274c Enable automatic inclusion of JMX reporter. <code>auto.leader.rebalance.enable</code> <code>true</code> \u2705 \u274c Enable auto leader balancing. <code>background.threads</code> <code>10</code> \u274c \u274c Number of threads to use for various background processing tasks. <code>broker.heartbeat.interval.ms</code> <code>2000</code> \u2705 \u274c The interval at which the broker sends heartbeats to ZooKeeper. <code>broker.id</code> <code>1</code> \u2705 \u274c Unique ID of the broker. <code>broker.id.generation.enable</code> <code>true</code> \u2705 \u274c Enable automatic generation of broker IDs. <code>broker.rack</code> <code></code> \u2705 \u274c Rack ID of the broker. <code>broker.session.timeout.ms</code> <code>9000</code> \u2705 \u274c The timeout used to detect broker failures when using ZooKeeper. <code>client.quota.callback.class</code> <code></code> \u2705 \u274c A class name that implements the ClientQuotaCallback interface. <code>compression.type</code> <code>producer</code> \u274c \u274c Specify the compression type for data generated by the broker. <code>connection.failed.authentication.delay.ms</code> <code>100</code> \u2705 \u274c Time in milliseconds to delay an authentication failure. <code>connections.max.idle.ms</code> <code>600000</code> \u2705 \u274c Idle time after which connections are closed. <code>connections.max.reauth.ms</code> <code>0</code> \u2705 \u274c Maximum time to delay re-authentication of connections. <code>control.plane.listener.name</code> <code></code> \u2705 \u274c Listener name for the control plane. <code>controlled.shutdown.enable</code> <code>true</code> \u2705 \u274c Enable controlled shutdown of the broker. <code>controlled.shutdown.max.retries</code> <code>3</code> \u2705 \u274c Maximum number of retries for controlled shutdown. <code>controlled.shutdown.retry.backoff.ms</code> <code>5000</code> \u2705 \u274c Backoff time between retries during controlled shutdown. <code>controller.listener.names</code> <code>CONTROLLER</code> \u2705 \u274c Listener names for the controller. <code>controller.quorum.append.linger.ms</code> <code>25</code> \u2705 \u274c Time in milliseconds to delay appending to the controller quorum log. <code>controller.quorum.election.backoff.max.ms</code> <code>1000</code> \u2705 \u274c Maximum backoff time in milliseconds for controller quorum election. <code>controller.quorum.election.timeout.ms</code> <code>1000</code> \u2705 \u274c Timeout in milliseconds for controller quorum election. <code>controller.quorum.fetch.timeout.ms</code> <code>2000</code> \u2705 \u274c Timeout in milliseconds for fetching from the controller quorum. <code>controller.quorum.request.timeout.ms</code> <code>2000</code> \u2705 \u274c Timeout in milliseconds for controller quorum requests. <code>controller.quorum.retry.backoff.ms</code> <code>20</code> \u2705 \u274c Backoff time in milliseconds between retries for controller quorum operations. <code>controller.quorum.voters</code> <code>1@127.0.0.1:9093</code> \u2705 \u274c Voters in the controller quorum. <code>controller.quota.window.num</code> <code>11</code> \u2705 \u274c Number of samples to use for tracking controller quotas. <code>controller.quota.window.size.seconds</code> <code>1</code> \u2705 \u274c Time span for each sample used for tracking controller quotas. <code>controller.socket.timeout.ms</code> <code>30000</code> \u2705 \u274c Socket timeout for controller connections. <code>create.topic.policy.class.name</code> <code></code> \u2705 \u274c A class name that implements the CreateTopicPolicy interface. <code>default.replication.factor</code> <code>1</code> \u2705 \u274c Default replication factor for automatically created topics. <code>delegation.token.expiry.check.interval.ms</code> <code>3600000</code> \u2705 \u274c Interval in milliseconds for delegation token expiry checks. <code>delegation.token.expiry.time.ms</code> <code>86400000</code> \u2705 \u274c Time in milliseconds for delegation token expiry. <code>delegation.token.master.key</code> <code></code> \u2705 \u2705 Master key for signing delegation tokens. <code>delegation.token.max.lifetime.ms</code> <code>604800000</code> \u2705 \u274c Maximum lifetime for delegation tokens. <code>delegation.token.secret.key</code> <code></code> \u2705 \u2705 Secret key for signing delegation tokens. <code>delete.records.purgatory.purge.interval.requests</code> <code>1</code> \u2705 \u274c Interval in requests to purge the delete records purgatory. <code>delete.topic.enable</code> <code>true</code> \u2705 \u274c Enable topic deletion. <code>early.start.listeners</code> <code></code> \u2705 \u274c Listeners to start early during broker startup. <code>fetch.max.bytes</code> <code>57671680</code> \u2705 \u274c Maximum bytes fetched per request. <code>fetch.purgatory.purge.interval.requests</code> <code>1000</code> \u2705 \u274c Interval in requests to purge the fetch purgatory. <code>group.initial.rebalance.delay.ms</code> <code>3000</code> \u2705 \u274c Initial rebalance delay for consumer groups. <code>group.max.session.timeout.ms</code> <code>1800000</code> \u2705 \u274c Maximum session timeout for consumer groups. <code>group.max.size</code> <code>2147483647</code> \u2705 \u274c Maximum size of consumer groups. <code>group.min.session.timeout.ms</code> <code>6000</code> \u2705 \u274c Minimum session timeout for consumer groups. <code>initial.broker.registration.timeout.ms</code> <code>60000</code> \u2705 \u274c Timeout for initial broker registration. <code>inter.broker.listener.name</code> <code>PLAINTEXT</code> \u2705 \u274c Listener name for inter-broker communication. <code>inter.broker.protocol.version</code> <code>3.6-IV2</code> \u2705 \u274c Protocol version for inter-broker communication. <code>kafka.metrics.polling.interval.secs</code> <code>10</code> \u2705 \u274c Polling interval for Kafka metrics. <code>kafka.metrics.reporters</code> <code></code> \u2705 \u274c List of classes implementing the MetricsReporter interface. <code>leader.imbalance.check.interval.seconds</code> <code>300</code> \u2705 \u274c Interval in seconds to check for leader imbalance. <code>leader.imbalance.per.broker.percentage</code> <code>10</code> \u2705 \u274c Percentage of imbalance tolerated per broker. <code>listener.security.protocol.map</code> <code>CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT</code> \u274c \u274c Mapping of listener names to security protocols. <code>listeners</code> <code>PLAINTEXT://:9092,CONTROLLER://:9093</code> \u274c \u274c Listeners for broker connections. <code>log.cleaner.backoff.ms</code> <code>15000</code> \u274c \u274c Backoff time between log cleaner operations. <code>log.cleaner.dedupe.buffer.size</code> <code>134217728</code> \u274c \u274c Size of the buffer used for log cleaning. <code>log.cleaner.delete.retention.ms</code> <code>86400000</code> \u274c \u274c Retention time for deleted records. <code>log.cleaner.enable</code> <code>true</code> \u2705 \u274c Enable log cleaner. <code>log.cleaner.io.buffer.load.factor</code> <code>0.9</code> \u274c \u274c Load factor for the log cleaner IO buffer. <code>log.cleaner.io.buffer.size</code> <code>524288</code> \u274c \u274c Size of the log cleaner IO buffer. <code>log.cleaner.io.max.bytes.per.second</code> <code>1.7976931348623157E308</code> \u274c \u274c Maximum bytes per second for log cleaner IO. <code>log.cleaner.max.compaction.lag.ms</code> <code>9223372036854775807</code> \u274c \u274c Maximum compaction lag for log cleaner. <code>log.cleaner.min.cleanable.ratio</code> <code>0.5</code> \u274c \u274c Minimum cleanable ratio for log cleaner. <code>log.cleaner.min.compaction.lag.ms</code> <code>0</code> \u274c \u274c Minimum compaction lag for log cleaner. <code>log.cleaner.threads</code> <code>1</code> \u274c \u274c Number of threads for log cleaner. <code>log.cleanup.policy</code> <code>delete</code> \u274c \u274c Policy for log cleanup. <code>log.dir</code> <code>/tmp/kafka-logs</code> \u2705 \u274c Directory for log files. <code>log.dirs</code> <code>/var/lib/kafka/data</code> \u2705 \u274c Directories for log files. <code>log.flush.interval.messages</code> <code>9223372036854775807</code> \u274c \u274c Interval in messages for log flush. <code>log.flush.interval.ms</code> <code></code> \u274c \u274c Interval in milliseconds for log flush. <code>log.flush.offset.checkpoint.interval.ms</code> <code>60000</code> \u2705 \u274c Interval in milliseconds for log offset checkpoint. <code>log.flush.scheduler.interval.ms</code> <code>9223372036854775807</code> \u2705 \u274c Interval in milliseconds for log flush scheduler. <code>log.flush.start.offset.checkpoint.interval.ms</code> <code>60000</code> \u2705 \u274c Interval in milliseconds for log flush start offset checkpoint. <code>log.index.interval.bytes</code> <code>4096</code> \u274c \u274c Interval in bytes for log index. <code>log.index.size.max.bytes</code> <code>10485760</code> \u274c \u274c Maximum size in bytes for log index. <code>log.local.retention.bytes</code> <code>-2</code> \u274c \u274c Local retention size in bytes for log. <code>log.local.retention.ms</code> <code>-2</code> \u274c \u274c Local retention time in milliseconds for log. <code>log.message.downconversion.enable</code> <code>true</code> \u274c \u274c Enable downconversion of log messages. <code>log.message.format.version</code> <code>3.0-IV1</code> \u2705 \u274c Format version for log messages. <code>log.message.timestamp.after.max.ms</code> <code>9223372036854775807</code> \u274c \u274c Maximum timestamp difference for log messages. <code>log.message.timestamp.before.max.ms</code> <code>9223372036854775807</code> \u274c \u274c Maximum timestamp before log messages. <code>log.message.timestamp.difference.max.ms</code> <code>9223372036854775807</code> \u274c \u274c Maximum timestamp difference for log messages. <code>log.message.timestamp.type</code> <code>CreateTime</code> \u274c \u274c Type of timestamp for log messages. <code>log.preallocate</code> <code>false</code> \u274c \u274c Preallocate log segments. <code>log.retention.bytes</code> <code>-1</code> \u274c \u274c Retention size in bytes for log. <code>log.retention.check.interval.ms</code> <code>300000</code> \u2705 \u274c Interval in milliseconds for log retention check. <code>log.retention.hours</code> <code>168</code> \u2705 \u274c Retention time in hours for log. <code>log.retention.minutes</code> <code></code> \u2705 \u274c Retention time in minutes for log. <code>log.retention.ms</code> <code></code> \u274c \u274c Retention time in milliseconds for log. <code>log.roll.hours</code> <code>168</code> \u2705 \u274c Roll interval in hours for log segments. <code>log.roll.jitter.hours</code> <code>0</code> \u2705 \u274c Jitter for log roll interval in hours. <code>log.roll.jitter.ms</code> <code></code> \u274c \u274c Jitter for log roll interval in milliseconds. <code>log.roll.ms</code> <code></code> \u274c \u274c Roll interval in milliseconds for log segments. <code>log.segment.bytes</code> <code>1073741824</code> \u274c \u274c Size of log segments in bytes. <code>log.segment.delete.delay.ms</code> <code>60000</code> \u274c \u274c Delay in milliseconds before deleting log segments. <code>max.connection.creation.rate</code> <code>2147483647</code> \u274c \u274c Maximum rate of connection creation. <code>max.connections</code> <code>2147483647</code> \u274c \u274c Maximum number of connections. <code>max.connections.per.ip</code> <code>2147483647</code> \u274c \u274c Maximum number of connections per IP. <code>max.connections.per.ip.overrides</code> <code></code> \u274c \u274c Overrides for maximum connections per IP. <code>max.incremental.fetch.session.cache.slots</code> <code>1000</code> \u2705 \u274c Maximum cache slots for incremental fetch sessions. <code>message.max.bytes</code> <code>1048588</code> \u274c \u274c Maximum size of messages. <code>metadata.log.dir</code> <code></code> \u2705 \u274c Directory for metadata log. <code>metadata.log.max.record.bytes.between.snapshots</code> <code>20971520</code> \u2705 \u274c Maximum bytes between metadata log snapshots. <code>metadata.log.max.snapshot.interval.ms</code> <code>3600000</code> \u2705 \u274c Maximum interval between metadata log snapshots. <code>metadata.log.segment.bytes</code> <code>1073741824</code> \u2705 \u274c Size of metadata log segments in bytes. <code>metadata.log.segment.ms</code> <code>604800000</code> \u2705 \u274c Interval in milliseconds for metadata log segments. <code>metadata.max.idle.interval.ms</code> <code>500</code> \u2705 \u274c Maximum idle interval for metadata. <code>metadata.max.retention.bytes</code> <code>104857600</code> \u2705 \u274c Maximum retention size for metadata. <code>metadata.max.retention.ms</code> <code>604800000</code> \u2705 \u274c Maximum retention time for metadata. <code>metric.reporters</code> <code></code> \u274c \u274c List of metric reporter classes. <code>metrics.num.samples</code> <code>2</code> \u2705 \u274c Number of metric samples. <code>metrics.recording.level</code> <code>INFO</code> \u2705 \u274c Recording level for metrics. <code>metrics.sample.window.ms</code> <code>30000</code> \u2705 \u274c Sample window for metrics. <code>min.insync.replicas</code> <code>1</code> \u274c \u274c Minimum in-sync replicas. <code>node.id</code> <code>1</code> \u2705 \u274c ID of the node. <code>num.io.threads</code> <code>8</code> \u274c \u274c Number of IO threads. <code>num.network.threads</code> <code>3</code> \u274c \u274c Number of network threads. <code>num.partitions</code> <code>1</code> \u2705 \u274c Number of partitions. <code>num.recovery.threads.per.data.dir</code> <code>1</code> \u274c \u274c Number of recovery threads per data directory. <code>num.replica.alter.log.dirs.threads</code> <code></code> \u2705 \u274c Number of threads for altering replica log dirs. <code>num.replica.fetchers</code> <code>1</code> \u274c \u274c Number of replica fetchers. <code>offset.metadata.max.bytes</code> <code>4096</code> \u2705 \u274c Maximum bytes for offset metadata. <code>offsets.commit.required.acks</code> <code>-1</code> \u2705 \u274c Required acknowledgments for offset commits. <code>offsets.commit.timeout.ms</code> <code>5000</code> \u2705 \u274c Timeout for offset commits. <code>offsets.load.buffer.size</code> <code>5242880</code> \u2705 \u274c Buffer size for loading offsets. <code>offsets.retention.check.interval.ms</code> <code>600000</code> \u2705 \u274c Interval for checking offset retention. <code>offsets.retention.minutes</code> <code>10080</code> \u2705 \u274c Retention time in minutes for offsets. <code>offsets.topic.compression.codec</code> <code>0</code> \u2705 \u274c Compression codec for offsets topic. <code>offsets.topic.num.partitions</code> <code>50</code> \u2705 \u274c Number of partitions for offsets topic. <code>offsets.topic.replication.factor</code> <code>1</code> \u2705 \u274c Replication factor for offsets topic. <code>offsets.topic.segment.bytes</code> <code>104857600</code> \u2705 \u274c Segment size for offsets topic. <code>password.encoder.cipher.algorithm</code> <code>AES/CBC/PKCS5Padding</code> \u2705 \u274c Cipher algorithm for password encoder. <code>password.encoder.iterations</code> <code>4096</code> \u2705 \u274c Iterations for password encoder. <code>password.encoder.key.length</code> <code>128</code> \u2705 \u274c Key length for password encoder. <code>password.encoder.keyfactory.algorithm</code> <code></code> \u2705 \u274c Key factory algorithm for password encoder. <code>password.encoder.old.secret</code> <code></code> \u2705 \u2705 Old secret for password encoder. <code>password.encoder.secret</code> <code></code> \u2705 \u2705 Secret for password encoder. <code>principal.builder.class</code> <code>org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder</code> \u274c \u274c Class name of the principal builder. <code>process.roles</code> <code>broker,controller</code> \u2705 \u274c Roles of the process. <code>producer.id.expiration.ms</code> <code>86400000</code> \u274c \u274c Expiration time for producer IDs. <code>producer.purgatory.purge.interval.requests</code> <code>1000</code> \u2705 \u274c Purge interval for producer purgatory. <code>queued.max.request.bytes</code> <code>-1</code> \u2705 \u274c Maximum request bytes in the queue. <code>queued.max.requests</code> <code>500</code> \u2705 \u274c Maximum number of requests in the queue. <code>quota.window.num</code> <code>11</code> \u2705 \u274c Number of samples for quota tracking. <code>quota.window.size.seconds</code> <code>1</code> \u2705 \u274c Time span for each quota sample. <code>remote.log.manager.task.interval.ms</code> <code>30000</code> \u2705 \u274c Interval for remote log manager tasks. <code>remote.log.manager.thread.pool.size</code> <code>10</code> \u2705 \u274c Thread pool size for remote log manager. <code>remote.log.metadata.custom.metadata.max.bytes</code> <code>128</code> \u2705 \u274c Maximum custom metadata bytes for remote log metadata. <code>remote.log.metadata.manager.class.name</code> <code>org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager</code> \u2705 \u274c Class name of the remote log metadata manager. <code>remote.log.metadata.manager.class.path</code> <code></code> \u2705 \u274c Class path for the remote log metadata manager. <code>remote.log.metadata.manager.impl.prefix</code> <code>rlmm.config.</code> \u2705 \u274c Prefix for remote log metadata manager implementation. <code>remote.log.metadata.manager.listener.name</code> <code></code> \u2705 \u274c Listener name for the remote log metadata manager. <code>remote.log.reader.max.pending.tasks</code> <code>100</code> \u2705 \u274c Maximum pending tasks for remote log reader. <code>remote.log.reader.threads</code> <code>10</code> \u2705 \u274c Number of threads for remote log reader. <code>remote.log.storage.manager.class.name</code> <code></code> \u2705 \u274c Class name of the remote log storage manager. <code>remote.log.storage.manager.class.path</code> <code></code> \u2705 \u274c Class path for the remote log storage manager. <code>remote.log.storage.manager.impl.prefix</code> <code>rsm.config.</code> \u2705 \u274c Prefix for remote log storage manager implementation. <code>remote.log.storage.system.enable</code> <code>false</code> \u2705 \u274c Enable remote log storage system. <code>replica.fetch.backoff.ms</code> <code>1000</code> \u2705 \u274c Backoff time for replica fetch. <code>replica.fetch.max.bytes</code> <code>1048576</code> \u2705 \u274c Maximum bytes for replica fetch. <code>replica.fetch.min.bytes</code> <code>1</code> \u2705 \u274c Minimum bytes for replica fetch. <code>replica.fetch.response.max.bytes</code> <code>10485760</code> \u2705 \u274c Maximum response bytes for replica fetch. <code>replica.fetch.wait.max.ms</code> <code>500</code> \u2705 \u274c Maximum wait time for replica fetch. <code>replica.high.watermark.checkpoint.interval.ms</code> <code>5000</code> \u2705 \u274c Interval for high watermark checkpointing. <code>replica.lag.time.max.ms</code> <code>30000</code> \u2705 \u274c Maximum lag time for replica. <code>replica.selector.class</code> <code></code> \u2705 \u274c Class name of the replica selector. <code>replica.socket.receive.buffer.bytes</code> <code>65536</code> \u2705 \u274c Receive buffer size for replica socket. <code>replica.socket.timeout.ms</code> <code>30000</code> \u2705 \u274c Timeout for replica socket. <code>replication.quota.window.num</code> <code>11</code> \u2705 \u274c Number of samples for replication quota. <code>replication.quota.window.size.seconds</code> <code>1</code> \u2705 \u274c Time span for each replication quota sample. <code>request.timeout.ms</code> <code>30000</code> \u2705 \u274c Timeout for requests. <code>reserved.broker.max.id</code> <code>1000</code> \u2705 \u274c Maximum reserved broker ID. <code>sasl.client.callback.handler.class</code> <code></code> \u2705 \u274c Class name of the SASL client callback handler. <code>sasl.enabled.mechanisms</code> <code>GSSAPI</code> \u274c \u274c Enabled SASL mechanisms. <code>sasl.jaas.config</code> <code></code> \u274c \u2705 JAAS configuration for SASL. <code>sasl.kerberos.kinit.cmd</code> <code>/usr/bin/kinit</code> \u274c \u274c Kerberos kinit command. <code>sasl.kerberos.min.time.before.relogin</code> <code>60000</code> \u274c \u274c Minimum time before Kerberos relogin. <code>sasl.kerberos.principal.to.local.rules</code> <code>DEFAULT</code> \u274c \u274c Rules for Kerberos principal to local mapping. <code>sasl.kerberos.service.name</code> <code></code> \u274c \u274c Kerberos service name. <code>sasl.kerberos.ticket.renew.jitter</code> <code>0.05</code> \u274c \u274c Jitter for Kerberos ticket renewal. <code>sasl.kerberos.ticket.renew.window.factor</code> <code>0.8</code> \u274c \u274c Window factor for Kerberos ticket renewal. <code>sasl.login.callback.handler.class</code> <code></code> \u2705 \u274c Class name of the SASL login callback handler. <code>sasl.login.class</code> <code></code> \u2705 \u274c Class name of the SASL login module. <code>sasl.login.connect.timeout.ms</code> <code></code> \u2705 \u274c Connection timeout for SASL login. <code>sasl.login.read.timeout.ms</code> <code></code> \u2705 \u274c Read timeout for SASL login. <code>sasl.login.refresh.buffer.seconds</code> <code>300</code> \u274c \u274c Buffer time for SASL login refresh. <code>sasl.login.refresh.min.period.seconds</code> <code>60</code> \u274c \u274c Minimum period for SASL login refresh. <code>sasl.login.refresh.window.factor</code> <code>0.8</code> \u274c \u274c Window factor for SASL login refresh. <code>sasl.login.refresh.window.jitter</code> <code>0.05</code> \u274c \u274c Jitter for SASL login refresh window. <code>sasl.login.retry.backoff.max.ms</code> <code>10000</code> \u2705 \u274c Maximum backoff for SASL login retry. <code>sasl.login.retry.backoff.ms</code> <code>100</code> \u2705 \u274c Backoff time for SASL login retry. <code>sasl.mechanism.controller.protocol</code> <code>GSSAPI</code> \u2705 \u274c SASL mechanism for controller protocol. <code>sasl.mechanism.inter.broker.protocol</code> <code>GSSAPI</code> \u274c \u274c SASL mechanism for inter-broker protocol. <code>sasl.oauthbearer.clock.skew.seconds</code> <code>30</code> \u2705 \u274c Clock skew for OAuthBearer. <code>sasl.oauthbearer.expected.audience</code> <code></code> \u2705 \u274c Expected audience for OAuthBearer. <code>sasl.oauthbearer.expected.issuer</code> <code></code> \u2705 \u274c Expected issuer for OAuthBearer. <code>sasl.oauthbearer.jwks.endpoint.refresh.ms</code> <code>3600000</code> \u2705 \u274c JWKS endpoint refresh interval for OAuthBearer. <code>sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms</code> <code>10000</code> \u2705 \u274c Maximum backoff for JWKS endpoint retry. <code>sasl.oauthbearer.jwks.endpoint.retry.backoff.ms</code> <code>100</code> \u2705 \u274c Backoff time for JWKS endpoint retry. <code>sasl.oauthbearer.jwks.endpoint.url</code> <code></code> \u2705 \u274c JWKS endpoint URL for OAuthBearer. <code>sasl.oauthbearer.scope.claim.name</code> <code>scope</code> \u2705 \u274c Scope claim name for OAuthBearer. <code>sasl.oauthbearer.sub.claim.name</code> <code>sub</code> \u2705 \u274c Sub claim name for OAuthBearer. <code>sasl.oauthbearer.token.endpoint.url</code> <code></code> \u2705 \u274c Token endpoint URL for OAuthBearer. <code>sasl.server.callback.handler.class</code> <code></code> \u2705 \u274c Class name of the SASL server callback handler. <code>sasl.server.max.receive.size</code> <code>524288</code> \u2705 \u274c Maximum receive size for SASL server. <code>security.inter.broker.protocol</code> <code>PLAINTEXT</code> \u2705 \u274c Security protocol for inter-broker communication. <code>security.providers</code> <code></code> \u2705 \u274c List of security providers. <code>socket.connection.setup.timeout.max.ms</code> <code>30000</code> \u2705 \u274c Maximum setup timeout for socket connection. <code>socket.connection.setup.timeout.ms</code> <code>10000</code> \u2705 \u274c Setup timeout for socket connection. <code>socket.listen.backlog.size</code> <code>50</code> \u2705 \u274c Listen backlog size for socket. <code>socket.receive.buffer.bytes</code> <code>102400</code> \u2705 \u274c Receive buffer size for socket. <code>socket.request.max.bytes</code> <code>104857600</code> \u2705 \u274c Maximum request size for socket. <code>socket.send.buffer.bytes</code> <code>102400</code> \u2705 \u274c Send buffer size for socket. <code>ssl.cipher.suites</code> <code></code> \u274c \u274c Cipher suites for SSL. <code>ssl.client.auth</code> <code>none</code> \u274c \u274c Client authentication setting for SSL. <code>ssl.enabled.protocols</code> <code>TLSv1.2,TLSv1.3</code> \u274c \u274c Enabled protocols for SSL. <code>ssl.endpoint.identification.algorithm</code> <code>https</code> \u274c \u274c Endpoint identification algorithm for SSL. <code>ssl.engine.factory.class</code> <code></code> \u274c \u274c Class name of the SSL engine factory. <code>ssl.key.password</code> <code></code> \u274c \u2705 Password for SSL key. <code>ssl.keymanager.algorithm</code> <code>SunX509</code> \u274c \u274c Key manager algorithm for SSL. <code>ssl.keystore.certificate.chain</code> <code></code> \u274c \u2705 Certificate chain for SSL keystore. <code>ssl.keystore.key</code> <code></code> \u274c \u2705 Key for SSL keystore. <code>ssl.keystore.location</code> <code></code> \u274c \u274c Location of the SSL keystore. <code>ssl.keystore.password</code> <code></code> \u274c \u2705 Password for SSL keystore. <code>ssl.keystore.type</code> <code>JKS</code> \u274c \u274c Type of the SSL keystore. <code>ssl.principal.mapping.rules</code> <code>DEFAULT</code> \u2705 \u274c Principal mapping rules for SSL. <code>ssl.protocol</code> <code>TLSv1.3</code> \u274c \u274c Protocol for SSL. <code>ssl.provider</code> <code></code> \u274c \u274c Provider for SSL. <code>ssl.secure.random.implementation</code> <code></code> \u274c \u274c Implementation of secure random for SSL. <code>ssl.trustmanager.algorithm</code> <code>PKIX</code> \u274c \u274c Trust manager algorithm for SSL. <code>ssl.truststore.certificates</code> <code></code> \u274c \u2705 Certificates for SSL truststore. <code>ssl.truststore.location</code> <code></code> \u274c \u274c Location of the SSL truststore. <code>ssl.truststore.password</code> <code></code> \u274c \u2705 Password for SSL truststore. <code>ssl.truststore.type</code> <code>JKS</code> \u274c \u274c Type of the SSL truststore. <code>transaction.abort.timed.out.transaction.cleanup.interval.ms</code> <code>10000</code> \u2705 \u274c Interval for cleaning up timed out transactions. <code>transaction.max.timeout.ms</code> <code>900000</code> \u2705 \u274c Maximum timeout for transactions. <code>transaction.partition.verification.enable</code> <code>true</code> \u274c \u274c Enable partition verification for transactions. <code>transaction.remove.expired.transaction.cleanup.interval.ms</code> <code>3600000</code> \u2705 \u274c Interval for cleaning up expired transactions. <code>transaction.state.log.load.buffer.size</code> <code>5242880</code> \u2705 \u274c Buffer size for loading transaction state logs. <code>transaction.state.log.min.isr</code> <code>1</code> \u2705 \u274c Minimum in-sync replicas for transaction state logs. <code>transaction.state.log.num.partitions</code> <code>50</code> \u2705 \u274c Number of partitions for transaction state logs. <code>transaction.state.log.replication.factor</code> <code>1</code> \u2705 \u274c Replication factor for transaction state logs. <code>transaction.state.log.segment.bytes</code> <code>104857600</code> \u2705 \u274c Segment size for transaction state logs. <code>transactional.id.expiration.ms</code> <code>604800000</code> \u2705 \u274c Expiration time for transactional IDs. <code>unclean.leader.election.enable</code> <code>false</code> \u274c \u274c Enable unclean leader election. <code>zookeeper.clientCnxnSocket</code> <code></code> \u2705 \u274c Client connection socket for ZooKeeper. <code>zookeeper.connect</code> <code></code> \u2705 \u274c ZooKeeper connection string. <code>zookeeper.connection.timeout.ms</code> <code></code> \u2705 \u274c Connection timeout for ZooKeeper. <code>zookeeper.max.in.flight.requests</code> <code>10</code> \u2705 \u274c Maximum in-flight requests for ZooKeeper. <code>zookeeper.metadata.migration.enable</code> <code>false</code> \u2705 \u274c Enable metadata migration to ZooKeeper. <code>zookeeper.session.timeout.ms</code> <code>18000</code> \u2705 \u274c Session timeout for ZooKeeper. <code>zookeeper.set.acl</code> <code>false</code> \u2705 \u274c Set ACL for ZooKeeper. <code>zookeeper.ssl.cipher.suites</code> <code></code> \u2705 \u274c Cipher suites for ZooKeeper SSL. <code>zookeeper.ssl.client.enable</code> <code>false</code> \u2705 \u274c Enable ZooKeeper SSL for clients. <code>zookeeper.ssl.crl.enable</code> <code>false</code> \u2705 \u274c Enable CRL for ZooKeeper SSL. <code>zookeeper.ssl.enabled.protocols</code> <code></code> \u2705 \u274c Enabled protocols for ZooKeeper SSL. <code>zookeeper.ssl.endpoint.identification.algorithm</code> <code>HTTPS</code> \u2705 \u274c Endpoint identification algorithm for ZooKeeper SSL. <code>zookeeper.ssl.keystore.location</code> <code></code> \u2705 \u274c Location of the keystore for ZooKeeper SSL. <code>zookeeper.ssl.keystore.password</code> <code></code> \u2705 \u2705 Password for the keystore for ZooKeeper SSL. <code>zookeeper.ssl.keystore.type</code> <code></code> \u2705 \u274c Type of the keystore for ZooKeeper SSL. <code>zookeeper.ssl.ocsp.enable</code> <code>false</code> \u2705 \u274c Enable OCSP for ZooKeeper SSL. <code>zookeeper.ssl.protocol</code> <code>TLSv1.2</code> \u2705 \u274c Protocol for ZooKeeper SSL. <code>zookeeper.ssl.truststore.location</code> <code></code> \u2705 \u274c Location of the truststore for ZooKeeper SSL. <code>zookeeper.ssl.truststore.password</code> <code></code> \u2705 \u2705 Password for the truststore for ZooKeeper SSL. <code>zookeeper.ssl.truststore.type</code> <code></code> \u2705 \u274c Type of the truststore for ZooKeeper SSL."}, {"location": "Kafka-Cluster-Configuration/#legend", "title": "Legend", "text": "<ul> <li>Names: The name of the parameter or setting with its synonyms.</li> <li>Default Value: The initial value assigned to the parameter if not explicitly set.</li> <li>Read-Only: Indicates if the parameter is immutable and cannot be modified.</li> <li>Sensitive: Specifies if the parameter contains sensitive information that will not be accessible or visible using Karafka.</li> <li>Description: A detailed explanation of the parameter's purpose and usage.</li> </ul> <p>Last modified: 2024-05-20 10:59:04</p>"}, {"location": "Kafka-Setting-Up/", "title": "Setting Up Kafka", "text": "<p>Before we combine Kafka with Ruby, it would be good to have a workable local Kafka process. The easiest way to do that is by using our <code>docker-compose.yml</code> present in Karafka:</p> <pre><code>git clone git@github.com:karafka/karafka.git\ncd karafka\ndocker-compose up\n</code></pre> <p>To check that it works, you can just telnet to it:</p> <pre><code>telnet 127.0.0.1 9092\nTrying 127.0.0.1...\nConnected to 127.0.0.1.\nEscape character is '^]'.\n</code></pre>"}, {"location": "Kafka-Setting-Up/#connecting-to-kafka-from-other-docker-containers", "title": "Connecting to Kafka from other Docker containers", "text": "<p>The <code>docker-compose.yml</code> we provide with Karafka has a setting called <code>KAFKA_ADVERTISED_HOST_NAME</code>, and it is by default set to <code>localhost</code>.</p> <p>Modify the <code>KAFKA_ADVERTISED_HOST_NAME</code> to match your docker host IP if you want to connect to Kafka from other docker containers:</p> <pre><code># KAFKA_ADVERTISED_HOST_NAME: localhost\nKAFKA_ADVERTISED_HOST_NAME: 192.168.0.5\n</code></pre> <p>Once you've changed that, you should be able to connect from other docker containers to your Kafka by using the host IP address:</p> <pre><code># Run an example docker container to check it via telnet\ndocker run -it --rm --entrypoint=bash python:3.8-slim-buster\n\n# And then from within\napt update &amp;&amp; apt install telnet\n\ntelnet 192.168.0.5 9092\nTrying 192.168.0.5...\nConnected to 192.168.0.5.\nEscape character is '^]'.\n</code></pre> <p>Last modified: 2024-05-19 21:57:23</p>"}, {"location": "Kafka-Topic-Configuration/", "title": "Kafka Topic Configuration", "text": "<p>Kafka Configuration Variability</p> <p>The defaults and exact list of per-topic configuration options may differ between various Kafka versions. For the most accurate information, please refer to the documentation for the specific Kafka version.</p> Names Default Value Read-Only Sensitive Description <code>cleanup.policy</code> <code>log.cleanup.policy</code> <code>delete</code> \u274c \u274c The policy to use for log cleanup. The default is 'delete' which removes old log segments. 'compact' is for log compaction. <code>compression.type</code> <code>producer</code> \u274c \u274c Specify the final compression type for a given topic. Valid values are 'uncompressed', 'zstd', 'lz4', 'gzip', and 'snappy'. <code>delete.retention.ms</code> <code>log.cleaner.delete.retention.ms</code> <code>86400000</code> \u274c \u274c The amount of time to retain delete markers in log compacted topics. This is also the time to retain tombstone messages in topics without log compaction. <code>file.delete.delay.ms</code> <code>log.segment.delete.delay.ms</code> <code>60000</code> \u274c \u274c The time to wait before deleting a file from the filesystem. <code>flush.messages</code> <code>log.flush.interval.messages</code> <code>9223372036854775807</code> \u274c \u274c The number of messages to accept before forcing a flush of data to disk. <code>flush.ms</code> <code>9223372036854775807</code> \u274c \u274c The maximum time in milliseconds to wait before forcing a flush of data to disk. <code>follower.replication.throttled.replicas</code> <code></code> \u274c \u274c A list of replica IDs that will be throttled on replication. <code>index.interval.bytes</code> <code>log.index.interval.bytes</code> <code>4096</code> \u274c \u274c The interval with which Kafka adds an entry to the offset index. <code>leader.replication.throttled.replicas</code> <code></code> \u274c \u274c A list of replica IDs that will be throttled on replication. <code>local.retention.bytes</code> <code>log.local.retention.bytes</code> <code>-2</code> \u274c \u274c The maximum size of the log before deleting it. <code>local.retention.ms</code> <code>log.local.retention.ms</code> <code>-2</code> \u274c \u274c The maximum time to retain the log before deleting it. <code>log.cleaner.max.compaction.lag.ms</code> <code>max.compaction.lag.ms</code> <code>9223372036854775807</code> \u274c \u274c The maximum time a message will remain uncompacted in the log. <code>max.message.bytes</code> <code>message.max.bytes</code> <code>1048588</code> \u274c \u274c The largest record batch size allowed by Kafka. Records larger than this will be rejected. <code>log.message.downconversion.enable</code> <code>message.downconversion.enable</code> <code>true</code> \u274c \u274c Enables or disables the automatic down-conversion of messages to older message formats for consumers with older clients. <code>log.message.format.version</code> <code>message.format.version</code> <code>3.0-IV1</code> \u274c \u274c The message format version used for the topic. <code>log.message.timestamp.after.max.ms</code> <code>message.timestamp.after.max.ms</code> <code>9223372036854775807</code> \u274c \u274c The maximum difference allowed between the timestamp of a message and the log append time. <code>log.message.timestamp.before.max.ms</code> <code>message.timestamp.before.max.ms</code> <code>9223372036854775807</code> \u274c \u274c The maximum difference allowed between the timestamp of a message and the current time before the message is rejected. <code>log.message.timestamp.difference.max.ms</code> <code>message.timestamp.difference.max.ms</code> <code>9223372036854775807</code> \u274c \u274c The maximum difference allowed between the timestamp of a message as set by the producer and the log append time. <code>log.message.timestamp.type</code> <code>message.timestamp.type</code> <code>CreateTime</code> \u274c \u274c Define whether the timestamp in the message is set by the create time or the log append time. <code>log.cleaner.min.cleanable.ratio</code> <code>min.cleanable.dirty.ratio</code> <code>0.5</code> \u274c \u274c The minimum ratio of dirty log to total log for log compaction to start. <code>log.cleaner.min.compaction.lag.ms</code> <code>min.compaction.lag.ms</code> <code>0</code> \u274c \u274c The minimum time a message will remain uncompacted in the log. <code>min.insync.replicas</code> <code>1</code> \u274c \u274c When a producer sets acks to 'all', the minimum number of replicas that must acknowledge a write for it to be considered successful. <code>log.preallocate</code> <code>preallocate</code> <code>false</code> \u274c \u274c Should preallocate file segments for this topic. <code>remote.storage.enable</code> <code>false</code> \u274c \u274c Enable the usage of remote storage for this topic. <code>log.retention.bytes</code> <code>retention.bytes</code> <code>-1</code> \u274c \u274c The maximum size of the log before deleting it. <code>retention.ms</code> <code>604800000</code> \u274c \u274c The maximum time to retain the log before deleting it. <code>log.segment.bytes</code> <code>segment.bytes</code> <code>1073741824</code> \u274c \u274c The maximum size of a single log segment file before a new log segment is rolled. <code>log.index.size.max.bytes</code> <code>segment.index.bytes</code> <code>10485760</code> \u274c \u274c The maximum size of the offset index that Kafka will allow before a new log segment is rolled. <code>segment.jitter.ms</code> <code>0</code> \u274c \u274c The maximum jitter to add to log segment roll time. <code>segment.ms</code> <code>604800000</code> \u274c \u274c The maximum time to retain a log segment before rolling it. <code>unclean.leader.election.enable</code> <code>false</code> \u274c \u274c Indicates whether unclean leader election is enabled for the topic."}, {"location": "Kafka-Topic-Configuration/#legend", "title": "Legend", "text": "<ul> <li>Names: The name of the parameter or setting with its synonyms.</li> <li>Default Value: The initial value assigned to the parameter if not explicitly set.</li> <li>Read-Only: Indicates if the parameter is immutable and cannot be modified.</li> <li>Sensitive: Specifies if the parameter contains sensitive information that will not be accessible or visible using Karafka.</li> <li>Description: A detailed explanation of the parameter's purpose and usage.</li> </ul> <p>Last modified: 2024-05-19 21:57:23</p>"}, {"location": "Latency-and-Throughput/", "title": "Latency and Throughput", "text": ""}, {"location": "Latency-and-Throughput/#introduction", "title": "Introduction", "text": "<p>Latency management is crucial for optimizing the performance of any Kafka-based system. The Karafka framework's default settings aim to balance average latency and decent throughput. However, these default settings might not be suitable for all use cases, particularly those requiring low latency or high throughput. This document is intended to guide you through the necessary configurations and best practices to manage latency effectively in your Karafka setup.</p> <p>The producer and consumer sides of a Kafka system have distinct roles and,  consequently, require different strategies for latency management. For producers, the focus is often on optimizing the message batching and sending mechanisms to minimize delays. On the consumer side, the goal is to reduce the time it takes to fetch and process messages. This document will detail specific configurations and practices for producers and consumers to help you achieve the desired latency levels.</p> <p>Understanding the Complexity of Latency and Throughput Management</p> <p>Latency management in Karafka is a complex and multifaceted topic. While this document provides a comprehensive overview of latency management techniques and best practices, it only covers some possible aspects. Many factors influence latency, including framework configuration, message types, scale, and the nature of your processing workload. These factors often extend beyond the scope of framework documentation.</p> <p>If you are seeking in-depth assistance or facing persistent latency issues, we recommend exploring our commercial offerings, which include, among other things, high-performance features, support, and consultancy services.</p>"}, {"location": "Latency-and-Throughput/#producer-management", "title": "Producer Management", "text": "<p>Managing latency and throughput on the producer side is critical for ensuring that messages are sent efficiently and quickly. This section covers the key aspects of producer latency management, including configuration tuning,  batching and compression, asynchronous sending, and best practices.</p>"}, {"location": "Latency-and-Throughput/#configuration-tuning", "title": "Configuration Tuning", "text": "<p>WaterDrop (Karafka's message producer) is fundamentally asynchronous, with a synchronous API around it. Any message you wait for will ultimately be dispatched from a background thread. Several settings impact how WaterDrop dispatches these messages.</p>"}, {"location": "Latency-and-Throughput/#queuebufferingmaxms", "title": "<code>queue.buffering.max.ms</code>", "text": "<p>The <code>queue.buffering.max.ms</code> parameter defines the maximum waiting time for additional messages before sending the current batch to the broker. Setting this to <code>0</code> or a very low value (like <code>1</code> ms) can help lower latency by minimizing wait times. However, this might increase overhead due to smaller batch sizes, impacting throughput and resource usage. When set to <code>0</code>, messages are dispatched immediately, allowing for sub-millisecond synchronous dispatches at the cost of increased CPU and network usage and lower throughput.</p> <p>Below are logs from dispatches with <code>queue.buffering.max.ms</code> set to <code>5</code> ms compared to <code>queue.buffering.max.ms</code> set to 0 in a local Kafka setup:</p> <pre><code># buffer for 5ms and produce sync\n\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.458219051361084 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.43287992477417 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.429641962051392 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.377591848373413 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.311408996582031 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.407937049865723 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.422562122344971 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.36993408203125 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.342764139175415 ms\n[waterdrop-80ad817ceb09] Sync producing (...) took 5.406744956970215 ms\n</code></pre> <p>vs.</p> <pre><code># dispatch immediately\n\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.2399919033050537 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.28732800483703613 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.2857530117034912 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.28434205055236816 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.26418089866638184 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.279433012008667 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.2934098243713379 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.2789499759674072 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.3075120449066162 ms\n[waterdrop-e2c291b6b0f3] Sync producing (...) took 0.24221014976501465 ms\n</code></pre>"}, {"location": "Latency-and-Throughput/#batchsize", "title": "<code>batch.size</code>", "text": "<p>The <code>batch.size</code> parameter determines the maximum size of a batch of messages. Larger batch sizes can improve throughput but might increase latency. Finding the right balance is key to optimal performance.</p>"}, {"location": "Latency-and-Throughput/#compressioncodec", "title": "<code>compression.codec</code>", "text": "<p>Compression reduces the size of the messages sent to the broker, potentially decreasing network latency and increasing throughput. However, while compression can reduce network latency, it introduces CPU overhead. Test different compression settings to find the best balance for your use case.</p>"}, {"location": "Latency-and-Throughput/#requestrequiredacks", "title": "<code>request.required.acks</code>", "text": "<p>The <code>request.required.acks</code> parameter determines the number of acknowledgments the leader broker must receive from an in-sync replica (ISR) brokers before responding to the producer. This setting is crucial for balancing message durability and producer latency.</p> <ul> <li> <p>0 (No Acknowledgement): The broker does not respond to the producer, and the message is considered delivered the moment it is dispatched. This setting provides the lowest latency since the producer doesn't wait for acknowledgments. It's highly performant, allowing for rapid message dispatch, but it risks data loss because there's no confirmation that the broker received the message. Suitable for non-critical data where speed is crucial.</p> </li> <li> <p>1 (Leader Acknowledgement): The leader broker responds once it has written the message to its log. This setting balances latency and durability, providing a quick acknowledgment while ensuring the leader broker logs the message. It offers a good trade-off for most use cases, ensuring reasonable reliability with moderate latency. </p> </li> <li> <p>-1 or all (All ISR Acknowledgements): The leader broker waits until all in-sync replicas have acknowledged the message. This setting ensures maximum durability, minimizing the risk of data loss as the message is replicated across multiple brokers. However, it results in higher latency and reduced throughput. It is best for critical applications where data integrity is essential.</p> </li> </ul> <p>Choosing the right acks setting depends on your application's requirements:</p> <ul> <li> <p>Low Latency Needs: Use acks=0 for the fastest message dispatch at the expense of reliability.</p> </li> <li> <p>Balanced Approach: Use acks=1 to compromise performance and reliability, suitable for most applications.</p> </li> <li> <p>High Durability Needs: Use acks=-1 or acks=all for critical applications where data loss is unacceptable despite the higher latency and reduced throughput.</p> </li> </ul> <p>Fine-Tuning <code>acks</code> with WaterDrop Variants</p> <p>Using variants, you can customize <code>request.required.acks</code> within the same producer instance. This feature allows different configuration settings per topic while sharing TCP connections, optimizing producer efficiency.</p>"}, {"location": "Latency-and-Throughput/#socketnagledisable", "title": "<code>socket.nagle.disable</code>", "text": "<p>The <code>socket.nagle.disable</code> parameter in librdkafka controls the use of the Nagle algorithm for Kafka broker connections. The Nagle algorithm is a TCP optimization that reduces the number of small packets sent over the network by combining them into larger packets. While this can improve network efficiency, it also introduces latency as small messages wait to be sent together.</p> <ul> <li> <p><code>true</code>: Disables the Nagle algorithm, ensuring messages are sent immediately without waiting to combine them into larger packets. This setting can significantly reduce latency, particularly in scenarios with many small messages, but may increase network overhead due to more frequent transmissions.</p> </li> <li> <p><code>false</code>: Enables the Nagle algorithm, which can reduce the number of packets sent by aggregating smaller messages. This setting may improve overall throughput by reducing network load but at the cost of slightly higher latency.</p> </li> </ul> <p>When configuring <code>socket.nagle.disable</code>, consider your application's priorities:</p> <ul> <li> <p>Low Latency Needs: Set <code>socket.nagle.disable</code> to <code>true</code> to minimize latency, ensuring that messages are sent as quickly as possible.</p> </li> <li> <p>Throughput Optimization: Set <code>socket.nagle.disable</code> to <code>false</code> to leverage the Nagle algorithm for reduced network traffic and improved throughput, if your application can tolerate the slight increase in latency.</p> </li> </ul> <p>Here's an example configuration:</p> <pre><code>class App &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': 'localhost:9092',\n      'socket.nagle.disable': true\n    }\n\n    config.shutdown_timeout = 60_000\n    config.max_wait_time = 5_000 # Default max_wait_time for all topics\n    config.max_messages = 50 # Default max_messages for all topics\n  end\nend\n</code></pre>"}, {"location": "Latency-and-Throughput/#asynchronous-producing", "title": "Asynchronous Producing", "text": "<p>Asynchronous dispatch reduces waiting time for acknowledgments, allowing your application to continue processing tasks immediately. This significantly lowers end-to-end latency within your application and increases WaterDrop throughput. </p> <p>Asynchronous producing is the recommended way for any non-critical messages.</p> <pre><code>producer.produce_many_async(\n  [\n    { topic: 'my-topic', payload: 'my message'},\n    { topic: 'my-topic', payload: 'my message'}\n  ]\n)\n</code></pre>"}, {"location": "Latency-and-Throughput/#consumer-management", "title": "Consumer Management", "text": "<p>Tuning producers may seem straightforward, but managing consumers is a different and more complex matter. Understanding and optimizing consumer latency and throughput requires a deep dive into various aspects spanning several book chapters. The sections below should be viewed as an introduction rather than exhaustive documentation.</p> <p>Consumer management is influenced by numerous external factors that go beyond the framework itself. These include:</p> <ul> <li> <p>Types of Data Consumed: Different data types may require different processing strategies and resources.</p> </li> <li> <p>Nature of Assignments and Infrastructure: How partitions are assigned to consumers and the underlying infrastructure significantly impact performance.</p> </li> <li> <p>Type of Processing: Whether CPU-intensive or IO-intensive affects how consumers should be tuned.</p> </li> <li> <p>Amount and Sizes of Data: The volume and size of consumed data are crucial in determining the optimal consumer configuration.</p> </li> <li> <p>Number of Worker Threads: The number of threads available for processing can influence how effectively data is consumed.</p> </li> <li> <p>Routing Setup: The complexity of the routing setup affect consumer performance.</p> </li> <li> <p>Data Production Patterns: Steady data streams versus sudden bursts (flushes) require different handling strategies to maintain performance.</p> </li> <li> <p>Topics Configuration and Number of Partitions: The configuration of topics and the number of partitions can significantly impact workload distribution and overall consumer efficiency.</p> </li> </ul> <p>This document is designed to provide you with a solid foundation in consumer latency management. It offers detailed explanations and descriptions that can serve as a basis for making informed decisions about consumer configurations. While it does not cover every possible scenario, it provides essential insights to help you navigate the complexities of consumer management in your Kafka setup, ensuring you are well-prepared for any situation.</p> <p>The strategy and methods selected for consumer management can vary significantly depending on whether the priority is throughput or latency.</p> <ul> <li> <p>Prioritizing Throughput: When the goal is to maximize throughput, the focus is on efficiently processing large volumes of data. Strategies include increasing batch sizes, optimizing worker thread counts, and ensuring consumers can handle high loads without frequent pauses. This approach might accept higher latency for processing more messages per unit of time.</p> </li> <li> <p>Prioritizing Latency: This strategy, when low latency is the priority, is a swift approach that minimizes the time taken for each message to be processed and acknowledged. It involves reducing batch sizes, using faster data processing methods, and ensuring that the consumer system is highly responsive. Here, throughput might be sacrificed, but the assurance of quick message processing is maintained.</p> </li> </ul> <p>No Silver Bullet for Latency and Throughput Tuning</p> <p>There is no one-size-fits-all solution when it comes to tuning latency and throughput. As mentioned above, achieving optimal performance requires deeply understanding your specific expectations and use cases.</p> <p>Scope of this Guide</p> <p>This document focuses on aspects related to the operational flow of a single subscription group within Karafka. It provides guidance on tuning configurations and managing latency and throughput for individual subscription groups. However, it's important to note that system dynamics can differ significantly when dealing with multi-subscription group operations. The interplay between multiple groups, their configurations, and the shared resources can introduce additional complexities and considerations not covered in this document.</p>"}, {"location": "Latency-and-Throughput/#prerequisites-and-initial-references", "title": "Prerequisites and Initial References", "text": "<p>To effectively understand Karafka consumer processes latency and the topics discussed in this document, it is essential to be familiar with several key concepts and operations within the framework. Here is a list of topics you should be accustomed to:</p> <ul> <li>Routing DSL including multi-consumer group and multi-subscription group operations.</li> <li>Concurrency and Multithreading design of the framework.</li> <li>Error Handling (Especially Backoff Policies)</li> <li>Offset Management Strategies</li> <li>Monitoring and Logging basics.</li> <li>Web UI Usage for monitoring.</li> <li>Swarm/Multi-Process Mode</li> <li>Connection Multiplexing</li> <li>Virtual Partitions</li> </ul>"}, {"location": "Latency-and-Throughput/#latency-types", "title": "Latency Types", "text": "<p>When working with Karafka, there are two primary types of latency: consumption latency and processing latency. Understanding these will help you optimize performance and ensure timely message processing.</p> <ul> <li> <p>Consumption Latency: Measures the time from when a message is created to when it is consumed.</p> <ul> <li>Importance: Indicates the real-time performance of your consumer setup. High consumption latency suggests bottlenecks in message processing.</li> <li>Impact of Processing Lag: Any increase in processing lag directly affects consumption latency. If the consumer process is overloaded, jobs will wait longer in the internal queue, increasing consumption lag.</li> </ul> </li> <li> <p>Processing Latency: Measures the time a batch of messages waits before being picked up by a worker.</p> <ul> <li>Importance: Helps identify internal delays within the consumer process. High processing latency can indicate an overloaded system.</li> <li>Relation to Consumption Latency: Increased processing latency will also increase consumption latency, as delays in processing lead to longer overall time in the system.</li> </ul> </li> </ul> <p>You will most often focus on consumption latency, as it reflects the overall performance and responsiveness of your consumer setup. Monitoring both consumption and processing latency helps identify and address performance issues, ensuring the efficient operation of your Karafka consumers.</p>"}, {"location": "Latency-and-Throughput/#configuration-tuning_1", "title": "Configuration Tuning", "text": "<p>Tuning consumer configurations in Karafka involves adjusting various settings that impact how messages are consumed and processed. These settings help balance latency, throughput, and resource utilization according to your requirements. Below are the primary settings within the Karafka configuration and important librdkafka-specific settings that you must consider.</p> <p>Optimizing Data Fetching</p> <p>Tuning the configurations below helps improve how fast or how much data Karafka can fetch from Kafka in a given time frame, as long as the consumer process is not blocked by processing or other factors. These are the primary ways to control latency and throughput when getting data from Kafka for the Ruby process.</p> <p>Further sections also contain information on how to deal with lags caused by the processing phase.</p> Setting Description Tuning Tips Default <code>max_wait_time</code> Maximum time a consumer will wait for messages before delegating them for processing. <ul> <li>Lower values reduce latency by ensuring they are processed faster.</li> <li>Higher values can improve throughput by allowing more messages to accumulate before processing.</li> <li>When lowering consider also lowering the <code>fetch.wait.max.ms</code>to at least match it</li> </ul> 1000 ms (Karafka) <code>max_messages</code> Maximum number of messages a consumer processes in a single batch. <ul> <li>Lower values reduce processing latency per batch.</li> <li>Higher values can improve throughput but may increase latency and resource usage.</li> </ul> 100 (Karafka) <code>fetch.wait.max.ms</code> Maximum time the consumer waits for the broker to fill the fetch request with <code>fetch.min.bytes</code> worth of messages. <ul> <li>Lower values reduce latency by minimizing wait time.</li> <li>Higher values can reduce fetch requests, improving throughput by fetching larger batches.</li> </ul> 500 ms (librdkafka) <code>fetch.min.bytes</code> Minimum amount of data the broker should return for a fetch request. <ul> <li>Higher values can improve throughput by reducing the number of fetch requests.</li> <li>Lower values ensure quicker fetching but might increase the number of requests.</li> </ul> 1 byte (librdkafka) <code>fetch.message.max.bytes</code> Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. <ul> <li>Lower values can help achieve a better mix of topic partitions data, especially in subscription groups subscribed to multiple topics and/or partitions.</li> <li>Higher values may be necessary for consuming larger messages but can reduce the mix of data and increase memory usage.</li> </ul> 1048576 bytes (1 MB) (librdkafka) <code>fetch.error.backoff.ms</code> Wait time before retrying a fetch request in case of an error. <ul> <li>Lower values reduce delay in message consumption after an error, helping maintain low latency.</li> <li>Avoid excessively low values to prevent frequent retries that could impact performance.</li> </ul> 500 ms (librdkafka) <code>enable.partition.eof</code> Enables the consumer to raise an event when it reaches the end of a partition. <ul> <li>When set to <code>true</code>, Karafka will bypass <code>max_wait_time</code> and <code>max_messages</code>, delegating all available messages for processing each time the end of the partition is reached.</li> </ul> false (librdkafka) <code>queued.max.messages.kbytes</code> Maximum number of kilobytes of queued pre-fetched messages in the local consumer queue. <ul> <li>Remember that this value applies to each subscription group connection independently.</li> <li>Set higher values to allow for more data to be pre-fetched and buffered, which can improve throughput.</li> <li>Lower values can help reduce memory usage and improve responsiveness in low-latency scenarios.</li> <li>Monitor memory consumption to ensure that increasing this value does not lead to excessive memory usage.</li> <li>Adjust this setting in conjunction with <code>fetch.min.bytes</code> and <code>fetch.message.max.bytes</code> to balance throughput and memory usage.</li> </ul> 65536 KB (65 MB) (librdkafka) <p>Below is an example latency impact of enabling <code>enable.partition.eof</code> and lowering <code>fetch.wait.max.ms</code> to <code>100</code>ms, compared to the default values, on a low-traffic topic (1 message per second). In both cases, <code>max_wait_time</code> was set to <code>2000</code>ms (less is better).</p> <p> </p> <p> *Latency in ms on low-volume topic with different configurations (less is better).    </p> <p>The presented example illustrates how big of an impact latency configuration can have. Proper configuration tuning can tremendously impact the Karafka data ingestion patterns.</p>"}, {"location": "Latency-and-Throughput/#per-topic-configuration", "title": "Per Topic Configuration", "text": "<p>In Karafka, you can configure settings per topic. This allows you to tailor the configuration to the specific needs of different topics, optimizing for various use cases and workloads. However, it's important to understand the implications of such configurations.</p> <p>When reconfiguring settings per topic, Karafka will create a distinct subscription group and an independent connection to Kafka for each topic with altered non-default settings. This isolation ensures that the specific configurations are applied correctly but also means that these topics will be managed independently, which can impact resource usage and system behavior.</p> <pre><code>class App &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': 'localhost:9092',\n      'max.poll.interval.ms': 300_000,\n    }\n\n    config.shutdown_timeout = 60_000\n    config.max_wait_time = 5_000 # Default max_wait_time for all topics\n    config.max_messages = 50 # Default max_messages for all topics\n  end\n\n  routes.draw do\n    # This topic will use only defaults\n    topic :default_topic do\n      consumer DefaultConsumer\n    end\n\n    # This topic will use defaults that were not overwritten and special kafka level settings\n    topic :custom_topic do\n      consumer CustomConsumer\n      # Custom max_messages for this topic\n      max_messages 200\n      # It is important to remember that defaults are **not** merged, so things like\n      # `bootstrap.servers` need to be provided again\n      kafka(\n        'bootstrap.servers': 'localhost:9092',\n        'fetch.wait.max.ms': 200,\n        'queued.max.messages.kbytes': 50_000\n      )\n    end\n  end\nend\n</code></pre>"}, {"location": "Latency-and-Throughput/#parallel-processing", "title": "Parallel Processing", "text": "<p>Aside from the fast polling of data from Kafka, Karafka optimizes the processing phase to reduce latency by processing more data in parallel. Even when data is in the in-process buffer, latency increases if it cannot be processed promptly. Karafka leverages native Ruby threads and supports multiple concurrency features to handle processing efficiently.</p> <p>Polling-Related Factors Affecting Parallel Processing</p> <p>Various polling-related factors can impact Karafka's ability to distribute and process obtained data in parallel. In some scenarios, the nature of the data or how it is polled from Kafka may prevent or reduce Karafka's ability to effectively distribute and process work in parallel. It's important to consider these factors when configuring and tuning your Karafka setup to ensure optimal performance.</p> <p>Below, you can find a table summarizing the key aspects of Karafka's parallel processing capabilities, along with detailed descriptions and tips for optimizing latency and throughput:</p> Aspect Details Tips Concurrent Processing of Messages        Karafka uses multiple threads to process messages concurrently.        <ul> <li>           From Different Topics and Partitions         </li> <li>           From the Same Topic but Different Partitions         </li> <li>           From a Single Partition using Virtual Partitions         </li> </ul> <ul> <li>Control the number of worker threads using the <code>concurrency</code> setting.</li> <li>Use Virtual Partitions to maximize throughput for IO-intensive tasks.</li> <li>Optimize thread count based on the workload characteristics and system capabilities.</li> <li>Make sure you do not over-saturate your worker threads.</li> </ul> Consumer Group Management Each consumer group is managed by a separate thread allowing for efficient data prefetching and buffering. <ul> <li>When working with multiple topics, consider using multiple consumer groups or multiple subscription groups.</li> </ul> Swarm Mode for Enhanced Concurrency        Forks independent processes to optimize CPU utilization, leveraging Ruby's Copy-On-Write (CoW) mechanism.       It may enhance throughput and scalability by distributing the workload across multiple CPU cores.      <ul> <li>Configure the number of processes based on available CPU cores and workload demands.</li> </ul> Subscription Groups for Kafka Connections        Karafka organizes topics into subscription groups to manage Kafka connections efficiently.       Each subscription group operates in a separate background thread, sharing the worker pool for processing.      <ul> <li>Adjust the number of subscription groups based on system performance and workload distribution.</li> <li>Consider using multiplexing to increase the number of parallel Kafka connections for given topic.</li> </ul> Virtual Partitions The Virtual Partitions feature allows for parallel processing of data from a single partition. <ul> <li>Use <code>partitioner</code> that will provide you a good distribution of data.</li> <li>Analyze the <code>reducer</code> virtual key operations to ensure that the reduction process does not limit the parallelization.</li> <li>Make sure your virtualization does not oversaturate the jobs queue.</li> </ul> <p>Below is an example latency impact of enabling Virtual Partitions compared to the default values on a partition with 10 messages per second and an IO cost of around 150ms per message. In both cases, the same settings are used.</p> <p> </p> <p> *Latency in ms on a partition with and without Virtual Partitions enabled (same traffic) with around 150ms of IO per message.    </p> <p>The presented example illustrates a growing latency due to the non-virtualized processing's inability to keep up with the steady pace of messages because of the IO cost. Work is distributed across multiple threads in the case of Virtual Partitions so the system can keep up.</p>"}, {"location": "Latency-and-Throughput/#prefetching-messages", "title": "Prefetching Messages", "text": "<p>Karafka is designed to prefetch data from Kafka while previously obtained messages are being processed. This prefetching occurs independently for each subscription group, ensuring continuous data flow as long as more data is available in Kafka. Prefetching behavior is governed by several settings mentioned in previous sections, such as <code>queued.max.messages.kbytes</code>, <code>fetch.wait.max.ms</code>, <code>fetch.min.bytes</code>, and <code>fetch.message.max.bytes</code>.</p>"}, {"location": "Latency-and-Throughput/#prefetching-behavior", "title": "Prefetching Behavior", "text": ""}, {"location": "Latency-and-Throughput/#normal-operations", "title": "Normal Operations", "text": "<p>Under normal operations, when there are no significant lags, Karafka prefetches some data from each assigned topic partition. This is because there is little data ahead to process. For example, if 200 messages are available for processing from 10 partitions, Karafka might prefetch these messages in small batches, resulting in 10 independent jobs with 20 messages each.</p> <p> </p> <p> *This example illustrates the work distribution of prefetched data coming from two partitions.    </p>"}, {"location": "Latency-and-Throughput/#data-spikes-and-lags", "title": "Data Spikes and Lags", "text": "<p>The situation changes when Karafka experiences data spikes or when a significant amount of data is ahead. In such cases, Karafka will prefetch data in larger batches, especially with default settings. This can reduce Karafka's ability to parallelize work, except when using virtual partitions. Karafka may prefetch large chunks of data from a single topic partition during these periods. For instance, it may prefetch 500 messages from one partition, resulting in a single job with 200 messages from that partition, thus limiting parallel processing.</p> <p>This behavior is driven by the need to process data quickly. Still, it can lead to reduced parallelism when large batches from a single partition dominate the internal buffer.</p> <p> </p> <p> *This example illustrates the lack of work distribution of prefetched data when batch comes from a single partition.    </p>"}, {"location": "Latency-and-Throughput/#optimizing-prefetching-for-parallelism", "title": "Optimizing Prefetching for Parallelism", "text": "<p>To mitigate the impact of large batch prefetching under lag conditions and to enhance system utilization, consider the following strategies:</p> <ol> <li> <p>Prefetching Configuration Tuning: Adjust prefetching-related settings such as <code>queued.max.messages.kbytes</code>, <code>fetch.wait.max.ms</code>, <code>fetch.min.bytes</code>, and <code>fetch.message.max.bytes</code> to fine-tune the balance between throughput and latency. Tailoring these settings to your specific workload can optimize prefetching behavior.</p> </li> <li> <p>Multiple Subscription Groups: Configure multiple subscription groups to ensure that data from independent partitions (across one or many topics) is prefetching and processing independently. This setup can enhance system utilization, reduce latency, and increase throughput by allowing Karafka to handle more partitions concurrently.</p> </li> <li> <p>Connection Multiplexing: Use connection multiplexing to create multiple connections for a single subscription group. This enables Karafka to independently prefetch data from different partitions, improving parallel processing capabilities.</p> </li> <li> <p>Virtual Partitions: Implement virtual partitions to parallelize processing within a single partition, maintaining high throughput even under lag conditions.</p> </li> </ol>"}, {"location": "Latency-and-Throughput/#prefetching-with-single-partition-assignments", "title": "Prefetching with Single Partition Assignments", "text": "<p>If a Karafka consumer process is assigned only one topic partition, the prefetching behavior is straightforward and consistently fetches data from that single partition. In such cases, there are no concerns about parallelism or the need to distribute the processing load across multiple partitions or subscription groups. Your processing can still greatly benefit by using Virtual Partitions.</p>"}, {"location": "Latency-and-Throughput/#conclusion", "title": "Conclusion", "text": "<p>Understanding and configuring the prefetching behavior in Karafka is crucial for optimizing performance, especially under varying data loads. By adjusting settings and utilizing strategies like multiple subscription groups and connection multiplexing, you can enhance Karafka's ability to parallelize work, reduce latency, and increase throughput. This ensures that your Karafka deployment remains efficient and responsive, even during data spikes and lags recovery.</p>"}, {"location": "Latency-and-Throughput/#subscription-group-blocking-polls", "title": "Subscription Group Blocking Polls", "text": "<p>Karafka is designed to prebuffer data to ensure efficient message processing. Still, it's important to understand that this prefetched data is not utilized until all jobs based on data from the previous batch poll are completed. This behavior is by design and is a common characteristic of Kafka processors, not just in Ruby.</p> <p>This approach prevents race conditions and ensures data consistency. The poll operation in Kafka acts as a heartbeat mechanism governed by the <code>max.poll.interval.ms</code> setting. This interval defines the maximum delay between poll invocations before the consumer is considered dead, triggering a rebalance. By ensuring that all jobs from the previous poll are finished before new data is used, Karafka maintains data integrity and avoids processing the same message multiple times.</p>"}, {"location": "Latency-and-Throughput/#impact-of-uneven-work-distribution", "title": "Impact of Uneven Work Distribution", "text": "<p>When a subscription group fetches data from multiple topics with uneven workloads (one topic with a lot of data to process and one with less), the smaller topic can experience increased latency. This happens because the consumer will only start processing new data after completing all jobs from the previous poll, including the large dataset. Consequently, messages from the less busy topic must wait until the more intensive processing is finished.</p> <p>Similarly, uneven work distribution with virtual partitions can lead to similar latency issues. If the partition key or reducer used for virtual partitions is not well balanced, certain virtual partitions may have significantly more data to process than others. This imbalance causes delays, as the consumer will wait for all data from the more loaded partitions to be processed before moving on to the next batch.</p>"}, {"location": "Latency-and-Throughput/#recommendations-for-mitigating-latency-issues", "title": "Recommendations for Mitigating Latency Issues", "text": "<p>To address these issues, consider the following strategies:</p> <ol> <li> <p>Multiplexing: Use connection multiplexing to create multiple connections for a single subscription group. This allows Karafka to fetch and process data from different partitions independently, improving parallel processing and reducing latency.</p> </li> <li> <p>Multiple Subscription Groups: Configure multiple subscription groups to distribute the workload more evenly. By isolating topics with significantly different workloads into separate subscription groups, you can ensure that heavy processing on one topic does not delay processing on another.</p> </li> <li> <p>Monitoring and Tuning Work Distribution: Regularly monitor the performance and work distribution of your virtual partitions. Ensure that your partition key and reducer are well-balanced to avoid uneven workloads. Fine-tuning these elements can help maintain efficient and timely processing across all partitions.</p> </li> </ol>"}, {"location": "Latency-and-Throughput/#summary", "title": "Summary", "text": "<p>Managing consumers in Karafka involves numerous internal and external factors. Each case presents unique challenges, requiring a tailored approach to optimization.</p> <p>Consumer management is influenced by data types, infrastructure, processing nature (CPU vs. IO-intensive), data volume, and worker threads. Key Karafka settings like <code>max_wait_time</code>, <code>max_messages</code>, and <code>fetch.wait.max.ms</code> play a crucial role in data fetching and processing efficiency.</p> <p>External factors, such as infrastructure setup, network conditions, and data production patterns, can significantly impact performance. To stay ahead of these potential issues, it's crucial to emphasize the need for regular monitoring of consumption and processing latency. This practice is key to identifying bottlenecks and ensuring the system's responsiveness.</p> <p>Optimization strategies, including multiple subscription groups, connection multiplexing, and virtual partitions, help balance workloads and enhance parallel processing. Each use case demands a unique configuration, underscoring the need for a thorough understanding of the framework and application requirements.</p> <p>In conclusion, effective consumer management in Karafka requires considering various factors and regular adjustments to maintain efficiency and responsiveness.</p> <p>Last modified: 2024-08-14 15:45:18</p>"}, {"location": "Librdkafka-Changelog/", "title": "Librdkafka Changelog", "text": "<p>This page is a copy of the releases of <code>librdkafka</code>.</p>"}, {"location": "Librdkafka-Changelog/#2110-2025-07-03", "title": "2.11.0 (2025-07-03)", "text": "<p>librdkafka v2.11.0 is a feature release:</p> <ul> <li>KIP-1102 Enable clients to rebootstrap based on timeout or error code (#4981).</li> <li>KIP-1139 Add support for OAuth jwt-bearer grant type (#4978).</li> <li>Fix for poll ratio calculation in case the queues are forwarded (#5017).</li> <li>Fix data race when buffer queues are being reset instead of being   initialized (#4718).</li> <li>Features BROKER_BALANCED_CONSUMER and SASL_GSSAPI don't depend on   JoinGroup v0 anymore, missing in AK 4.0 and CP 8.0 (#5131).</li> <li>Improve HTTPS CA certificates configuration by probing several paths   when OpenSSL is statically linked and providing a way to customize their location   or value (#5133).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes", "title": "General fixes", "text": "<ul> <li>Issues: #4522.   A data race happened when emptying buffers of a failing broker, in its thread,   with the statistics callback in main thread gathering the buffer counts.   Solved by resetting the atomic counters instead of initializing them.   Happening since 1.x (#4718).</li> <li>Issues: #4948   Features BROKER_BALANCED_CONSUMER and SASL_GSSAPI don't depend on   JoinGroup v0 anymore, missing in AK 4.0 and CP 8.0. This PR partially   fixes the linked issue, a complete fix for all features will follow.   Rest of fixes are necessary only for a subsequent Apache Kafka major   version (e.g. AK 5.x).   Happening since 1.x (#5131).</li> </ul>"}, {"location": "Librdkafka-Changelog/#telemetry-fixes", "title": "Telemetry fixes", "text": "<ul> <li>Issues: #5109   Fix for poll ratio calculation in case the queues are forwarded.   Poll ratio is now calculated per-queue instead of per-instance and   it allows to avoid calculation problems linked to using the same   field.   Happens since 2.6.0 (#5017).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.11.0.zip SHA256 <code>9e76a408f0ed346f21be5e2df58b672d07ff9c561a5027f16780d1b26ef24683</code>  * v2.11.0.tar.gz SHA256 <code>592a823dc7c09ad4ded1bc8f700da6d4e0c88ffaf267815c6f25e7450b9395ca</code></p>"}, {"location": "Librdkafka-Changelog/#2101-2025-06-11", "title": "2.10.1 (2025-06-11)", "text": "<p>librdkafka v2.10.1 is a maintenance release:</p> <ul> <li>Fix to add locks when updating the metadata cache for the consumer    after no broker connection is available (@marcin-krystianc, #5066).</li> <li>Fix to the re-bootstrap case when <code>bootstrap.servers</code> is <code>NULL</code> and   brokers were added manually through <code>rd_kafka_brokers_add</code> (#5067).</li> <li>Fix an issue where the first message to any topic produced via <code>producev</code> or   <code>produceva</code> was delivered late (by up to 1 second) (#5032).</li> <li>Fix for a loop of re-bootstrap sequences in case the client reaches the   <code>all brokers down</code> state (#5086).</li> <li>Fix for frequent disconnections on push telemetry requests   with particular metric configurations (#4912).</li> <li>Avoid copy outside boundaries when reading metric names in telemetry   subscription (#5105)</li> <li>Metrics aren't duplicated when multiple prefixes match them (#5104)</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_1", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_1", "title": "General fixes", "text": "<ul> <li>Issues: #5088.   Fix for a loop of re-bootstrap sequences in case the client reaches the   <code>all brokers down</code> state. The client continues to select the   bootstrap brokers given they have no connection attempt and doesn't   re-connect to the learned ones. In case it happens a broker restart   can break the loop for the clients using the affected version.   Fixed by giving a higher chance to connect to the learned brokers   even if there are new ones that never tried to connect.   Happens since 2.10.0 (#5086).</li> <li>Issues: #5057.   Fix to the re-bootstrap case when <code>bootstrap.servers</code> is <code>NULL</code> and   brokers were added manually through <code>rd_kafka_brokers_add</code>.   Avoids a segmentation fault in this case.   Happens since 2.10.0 (#5067).</li> </ul>"}, {"location": "Librdkafka-Changelog/#producer-fixes", "title": "Producer fixes", "text": "<ul> <li>In case of <code>producev</code> or <code>produceva</code>, the producer did not enqueue a leader   query metadata request immediately, and rather, waited for the 1 second   timer to kick in. This could cause delays in the sending of the first message   by up to 1 second.   Happens since 1.x (#5032).</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes", "title": "Consumer fixes", "text": "<ul> <li>Issues: #5051.   Fix to add locks when updating the metadata cache for the consumer.   It can cause memory corruption or use-after-free in case   there's no broker connection and the consumer   group metadata needs to be updated.   Happens since 2.10.0 (#5066).</li> </ul>"}, {"location": "Librdkafka-Changelog/#telemetry-fixes_1", "title": "Telemetry fixes", "text": "<ul> <li>Issues: #5106.   Fix for frequent disconnections on push telemetry requests   with particular metric configurations.   A <code>NULL</code> payload is sent in a push telemetry request when   an empty one is needed. This causes disconnections every time the   push is sent, only when metrics are requested and   some metrics are matching the producer but none the consumer   or the other way around.   Happens since 2.5.0 (#4912).</li> <li>Issues: #5102.   Avoid copy outside boundaries when reading metric names in telemetry   subscription. It can cause that some metrics aren't matched.   Happens since 2.5.0 (#5105).</li> <li>Issues: #5103.   Telemetry metrics aren't duplicated when multiple prefixes match them.   Fixed by keeping track of the metrics that already matched.   Happens since 2.5.0 (#5104).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_1", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.10.1.zip SHA256 <code>7cb72c4f3d162f50d30d81fd7f7ba0f3d9e8ecd09d9b4c5af7933314e24dd0ba</code>  * v2.10.1.tar.gz SHA256 <code>75f59a2d948276504afb25bcb5713a943785a413b84f9099d324d26b2021f758</code></p>"}, {"location": "Librdkafka-Changelog/#2100-2025-04-17", "title": "2.10.0 (2025-04-17)", "text": "<p>librdkafka v2.10.0 is a feature release:</p>"}, {"location": "Librdkafka-Changelog/#kip-848-now-in-preview", "title": "KIP-848 \u2013 Now in Preview", "text": "<ul> <li>KIP-848 has transitioned from Early Access to Preview.</li> <li>Added support for regex-based subscriptions.</li> <li>Implemented client-side member ID generation as per KIP-1082.</li> <li><code>rd_kafka_DescribeConsumerGroups()</code> now supports KIP-848-style <code>consumer</code> groups. Two new fields have been added:</li> <li>Group type \u2013 Indicates whether the group is <code>classic</code> or <code>consumer</code>.</li> <li>Target assignment \u2013 Applicable only to <code>consumer</code> protocol groups (defaults to <code>NULL</code>).</li> <li>Group configuration is now supported in <code>AlterConfigs</code>, <code>IncrementalAlterConfigs</code>, and <code>DescribeConfigs</code>. (#4939)</li> <li>Added Topic Authorization Error support in the <code>ConsumerGroupHeartbeat</code> response.</li> <li>Removed usage of the <code>partition.assignment.strategy</code> property for the <code>consumer</code> group protocol. An error will be raised if this is set with <code>group.protocol=consumer</code>.</li> <li>Deprecated and disallowed the following properties for the <code>consumer</code> group protocol:</li> <li><code>session.timeout.ms</code></li> <li><code>heartbeat.interval.ms</code></li> <li><code>group.protocol.type</code>   Attempting to set any of these will result in an error.</li> <li>Enhanced handling for <code>subscribe()</code> and <code>unsubscribe()</code> edge cases.</li> </ul> <p>[!Note] The KIP-848 consumer is currently in Preview and should not be used in production environments. Implementation is feature complete but contract could have minor changes before General Availability.</p> <p>## Enhancements and Fixes</p> <ul> <li>Identify brokers only by broker id (#4557, @mfleming)</li> <li>Remove unavailable brokers and their thread (#4557, @mfleming)</li> <li>Commits during a cooperative incremental rebalance aren't causing    an assignment lost if the generation id was bumped in between (#4908).</li> <li>Fix for librdkafka yielding before timeouts had been reached (#4970)</li> <li>Removed a 500ms latency when a consumer partition switches to a different    leader (#4970)</li> <li>The mock cluster implementation removes brokers from Metadata response    when they're not available, this simulates better the actual behavior of    a cluster that is using KRaft (#4970).</li> <li>Doesn't remove topics from cache on temporary Metadata errors but only    on metadata cache expiry (#4970).</li> <li>Doesn't mark the topic as unknown if it had been marked as existent earlier    and <code>topic.metadata.propagation.max.ms</code> hasn't passed still (@marcin-krystianc, #4970).</li> <li>Doesn't update partition leaders if the topic in metadata    response has errors (#4970).</li> <li>Only topic authorization errors in a metadata response are considered    permanent and are returned to the user (#4970).</li> <li>The function <code>rd_kafka_offsets_for_times</code> refreshes leader information    if the error requires it, allowing it to succeed on    subsequent manual retries (#4970).</li> <li>Deprecated <code>api.version.request</code>, <code>api.version.fallback.ms</code> and    <code>broker.version.fallback</code> configuration properties (#4970).</li> <li>When consumer is closed before destroying the client, the operations queue    isn't purged anymore as it contains operations    unrelated to the consumer group (#4970).</li> <li>When making multiple changes to the consumer subscription in a short time,    no unknown topic error is returned for topics that are in the new subscription but weren't in previous one (#4970).</li> <li>Prevent metadata cache corruption when topic id changes    (@kwdubuc, @marcin-krystianc, @GerKr, #4970).</li> <li>Fix for the case where a metadata refresh enqueued on an unreachable broker    prevents refreshing the controller or the coordinator until that broker    becomes reachable again (#4970).</li> <li>Remove a one second wait after a partition fetch is restarted following a    leader change and offset validation (#4970).</li> <li>Fix the Nagle algorithm (TCP_NODELAY) on broker sockets to not be enabled    by default (#4986).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_2", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_2", "title": "General fixes", "text": "<ul> <li>Issues: #4212    Identify brokers only by broker id, as happens in Java,    avoid to find the broker with same hostname and use the same thread    and connection.    Happens since 1.x (#4557, @mfleming).</li> <li>Issues: #4557    Remove brokers not reported in a metadata call, along with their thread.    Avoids that unavailable brokers are selected for a new connection when    there's no one available. We cannot tell if a broker was removed    temporarily or permanently so we always remove it and it'll be added back when    it becomes available again.    Happens since 1.x (#4557, @mfleming).</li> <li>Issues: #4970    librdkafka code using <code>cnd_timedwait</code> was yielding before a timeout occurred    without the condition being fulfilled because of spurious wake-ups.    Solved by verifying with a monotonic clock that the expected point in time    was reached and calling the function again if needed.    Happens since 1.x (#4970).</li> <li>Issues: #4970    Doesn't remove topics from cache on temporary Metadata errors but only    on metadata cache expiry. It allows the client to continue working    in case of temporary problems to the Kafka metadata plane.    Happens since 1.x (#4970).</li> <li>Issues: #4970    Doesn't mark the topic as unknown if it had been marked as existent earlier    and <code>topic.metadata.propagation.max.ms</code> hasn't passed still. It achieves    this property expected effect even if a different broker had    previously reported the topic as existent.    Happens since 1.x (@marcin-krystianc, #4970).</li> <li>Issues: #4907    Doesn't update partition leaders if the topic in metadata    response has errors. It's in line with what Java client does and allows    to avoid segmentation faults for unknown partitions.    Happens since 1.x (#4970).</li> <li>Issues: #4970    Only topic authorization errors in a metadata response are considered    permanent and are returned to the user. It's in line with what Java client    does and avoids returning to the user an error that wasn't meant to be    permanent.    Happens since 1.x (#4970).</li> <li>Issues: #4964, #4778    Prevent metadata cache corruption when topic id for the same topic name    changes. Solved by correctly removing the entry with the old topic id from metadata cache    to prevent subsequent use-after-free.    Happens since 2.4.0 (@kwdubuc, @marcin-krystianc, @GerKr, #4970).</li> <li>Issues: #4970    Fix for the case where a metadata refresh enqueued on an unreachable broker    prevents refreshing the controller or the coordinator until that broker    becomes reachable again. Given the request continues to be retried on that    broker, the counter for refreshing complete broker metadata doesn't reach    zero and prevents the client from obtaining the new controller or group or transactional coordinator.    It causes a series of debug messages like:    \"Skipping metadata request: ... full request already in-transit\", until    the broker the request is enqueued on is up again.    Solved by not retrying these kinds of metadata requests.    Happens since 1.x (#4970).</li> <li>The Nagle algorithm (TCP_NODELAY) is now disabled by default. It caused a    large increase in latency for some use cases, for example, when using an    SSL connection.    For efficient batching, the application should use <code>linger.ms</code>,    <code>batch.size</code> etc.    Happens since: 0.x (#4986).</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_1", "title": "Consumer fixes", "text": "<ul> <li>Issues: #4059    Commits during a cooperative incremental rebalance could cause an    assignment lost if the generation id was bumped by a second join    group request.    Solved by not rejoining the group in case an illegal generation error happens    during a rebalance.    Happening since v1.6.0 (#4908)</li> <li>Issues: #4970    When switching to a different leader a consumer could wait 500ms     (<code>fetch.error.backoff.ms</code>) before starting to fetch again. The fetch backoff wasn't reset when joining the new broker.    Solved by resetting it, given it's not needed to backoff    the first fetch on a different node. This way faster leader switches are    possible.    Happens since 1.x (#4970).</li> <li>Issues: #4970    The function <code>rd_kafka_offsets_for_times</code> refreshes leader information    if the error requires it, allowing it to succeed on    subsequent manual retries. Similar to the fix done in 2.3.0 in    <code>rd_kafka_query_watermark_offsets</code>. Additionally, the partition    current leader epoch is taken from metadata cache instead of    from passed partitions.    Happens since 1.x (#4970).</li> <li>Issues: #4970    When consumer is closed before destroying the client, the operations queue    isn't purged anymore as it contains operations    unrelated to the consumer group.    Happens since 1.x (#4970).</li> <li>Issues: #4970    When making multiple changes to the consumer subscription in a short time,    no unknown topic error is returned for topics that are in the new subscription    but weren't in previous one. This was due to the metadata request relative    to previous subscription.    Happens since 1.x (#4970).</li> <li>Issues: #4970    Remove a one second wait after a partition fetch is restarted following a    leader change and offset validation. This is done by resetting the fetch    error backoff and waking up the delegated broker if present.    Happens since 2.1.0 (#4970).</li> </ul> <p>Note: there was no v2.9.0 librdkafka release,  it was a dependent clients release only</p>"}, {"location": "Librdkafka-Changelog/#checksums_2", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.10.0.zip SHA256 <code>e30944f39b353ee06e70861348011abfc32d9ab6ac850225b0666e9d97b9090d</code>  * v2.10.0.tar.gz SHA256 <code>004b1cc2685d1d6d416b90b426a0a9d27327a214c6b807df6f9ea5887346ba3a</code></p>"}, {"location": "Librdkafka-Changelog/#280-2025-01-07", "title": "2.8.0 (2025-01-07)", "text": "<p>librdkafka v2.8.0 is a maintenance release:</p> <ul> <li>Socket options are now all set before connection (#4893).</li> <li>Client certificate chain is now sent when using <code>ssl.certificate.pem</code>   or <code>ssl_certificate</code> or <code>ssl.keystore.location</code> (#4894).</li> <li>Avoid sending client certificates whose chain doesn't match with broker   trusted root certificates (#4900).</li> <li>Fixes to allow to migrate partitions to leaders with same leader epoch,   or NULL leader epoch (#4901).</li> <li>Support versions of OpenSSL without the ENGINE component (Chris Novakovic, #3535   and @remicollet, #4911).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_3", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_3", "title": "General fixes", "text": "<ul> <li>Socket options are now all set before connection, as documentation   says it's needed for socket buffers to take effect, even if in some   cases they could have effect even after connection.   Happening since v0.9.0 (#4893).</li> <li>Issues: #3225.   Client certificate chain is now sent when using <code>ssl.certificate.pem</code>   or <code>ssl_certificate</code> or <code>ssl.keystore.location</code>.   Without that, broker must explicitly add any intermediate certification   authority certificate to its truststore to be able to accept client   certificate.   Happens since: 1.x (#4894).</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_2", "title": "Consumer fixes", "text": "<ul> <li>Issues: #4796.   Fix to allow to migrate partitions to leaders with NULL leader epoch.   NULL leader epoch can happen during a cluster roll with an upgrade to a   version supporting KIP-320.   Happening since v2.1.0 (#4901).</li> <li>Issues: #4804.   Fix to allow to migrate partitions to leaders with same leader epoch.   Same leader epoch can happen when partition is   temporarily migrated to the internal broker (#4804), or if broker implementation   never bumps it, as it's not needed to validate the offsets.   Happening since v2.4.0 (#4901).</li> </ul> <p>Note: there was no v2.7.0 librdkafka release</p>"}, {"location": "Librdkafka-Changelog/#checksums_3", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.8.0.zip SHA256 <code>5525efaad154e277e6ce30ab78bb00dbd882b5eeda6c69c9eeee69b7abee11a4</code>  * v2.8.0.tar.gz SHA256 <code>5bd1c46f63265f31c6bfcedcde78703f77d28238eadf23821c2b43fc30be3e25</code></p>"}, {"location": "Librdkafka-Changelog/#221-2025-01-13", "title": "2.2.1 (2025-01-13)", "text": "<p>Note: given this patch version contains only a single fix, it's suggested to upgrade to latest backward compatible release instead, as it contains all the issued fixes. Following semver 2.0, all our patch and minor releases are backward compatible and our minor releases may also contain fixes. Please note that 2.x versions of librdkafka are also backward compatible with 1.x as the major version release was only for the upgrade to OpenSSL 3.x.</p> <p>librdkafka v2.2.1 is a maintenance release backporting:</p> <ul> <li>Fix for idempotent producer fatal errors, triggered after a possibly persisted message state (#4438).</li> <li>Update bundled lz4 (used when <code>./configure --disable-lz4-ext</code>) to       v1.9.4, which contains       bugfixes and performance improvements (#4726).</li> <li>Upgrade OpenSSL to v3.0.13 (while building from source) with various security fixes,      check the release notes      (@janjwerner-confluent, #4690).</li> <li>Upgrade zstd to v1.5.6, zlib to v1.3.1, and curl to v8.8.0 (@janjwerner-confluent, #4690).</li> <li>Upgrade Linux dependencies: OpenSSL 3.0.15, CURL 8.10.1 (#4875).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_4", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.2.1.zip SHA256 <code>2d7fdb54b17be8442b61649916b94eda1744c21d2325795d92f9ad6dec4e5621</code>  * v2.2.1.tar.gz SHA256 <code>c6f0ccea730ce8f67333e75cc785cce28a8941d5abf041d7a9b8fef91d4778e8</code></p>"}, {"location": "Librdkafka-Changelog/#261-2024-11-18", "title": "2.6.1 (2024-11-18)", "text": "<p>librdkafka v2.6.1 is a maintenance release:</p> <ul> <li>Fix for a Fetch regression when connecting to Apache Kafka &lt; 2.7 (#4871).</li> <li>Fix for an infinite loop happening with cooperative-sticky assignor   under some particular conditions (#4800).</li> <li>Fix for retrieving offset commit metadata when it contains   zeros and configured with <code>strndup</code> (#4876)</li> <li>Fix for a loop of ListOffset requests, happening in a Fetch From Follower   scenario, if such request is made to the follower (#4616, #4754, @kphelps).</li> <li>Fix to remove fetch queue messages that blocked the destroy of rdkafka   instances (#4724)</li> <li>Upgrade Linux dependencies: OpenSSL 3.0.15, CURL 8.10.1 (#4875).</li> <li>Upgrade Windows dependencies: MSVC runtime to 14.40.338160.0,   zstd 1.5.6, zlib 1.3.1, OpenSSL 3.3.2, CURL 8.10.1 (#4872).</li> <li>SASL/SCRAM authentication fix: avoid concatenating   client side nonce once more, as it's already prepended in server sent nonce (#4895).</li> <li>Allow retrying for status code 429 ('Too Many Requests') in HTTP requests for   OAUTHBEARER OIDC (#4902).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_4", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_4", "title": "General fixes", "text": "<ul> <li>SASL/SCRAM authentication fix: avoid concatenating   client side nonce once more, as it's already prepended in    server sent nonce.   librdkafka was incorrectly concatenating the client side nonce again, leading to this fix being made on AK side, released with 3.8.1, with <code>endsWith</code> instead of <code>equals</code>.   Happening since v0.0.99 (#4895).</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_3", "title": "Consumer fixes", "text": "<ul> <li>Issues: #4870   Fix for a Fetch regression when connecting to Apache Kafka &lt; 2.7, causing   fetches to fail.   Happening since v2.6.0 (#4871)</li> <li>Issues: #4783.   A consumer configured with the <code>cooperative-sticky</code> partition assignment   strategy could get stuck in an infinite loop, with corresponding spike of   main thread CPU usage.   That happened with some particular orders of members and potential    assignable partitions.   Solved by removing the infinite loop cause.   Happening since: 1.6.0 (#4800).</li> <li>Issues: #4649.   When retrieving offset metadata, if the binary value contained zeros   and librdkafka was configured with <code>strndup</code>, part of   the buffer after first zero contained uninitialized data   instead of rest of metadata. Solved by avoiding to use   <code>strndup</code> for copying metadata.   Happening since: 0.9.0 (#4876).</li> <li>Issues: #4616   When an out of range on a follower caused an offset reset, the corresponding   ListOffsets request is made to the follower, causing a repeated   \"Not leader for partition\" error. Fixed by sending the request always   to the leader.   Happening since 1.5.0 (tested version) or previous ones (#4616, #4754, @kphelps).</li> <li>Issues:   Fix to remove fetch queue messages that blocked the destroy of rdkafka   instances. Circular dependencies from a partition fetch queue message to   the same partition blocked the destroy of an instance, that happened   in case the partition was removed from the cluster while it was being   consumed. Solved by purging internal partition queue, after being stopped   and removed, to allow reference count to reach zero and trigger a destroy.   Happening since 2.0.2 (#4724).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_5", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.6.1.zip SHA256 <code>b575811865d9c0439040ccb2972ae6af963bc58ca39d433243900dddfdda79cf</code>  * v2.6.1.tar.gz SHA256 <code>0ddf205ad8d36af0bc72a2fec20639ea02e1d583e353163bf7f4683d949e901b</code></p>"}, {"location": "Librdkafka-Changelog/#260-2024-10-10", "title": "2.6.0 (2024-10-10)", "text": "<p>librdkafka v2.6.0 is a feature release:</p> <ul> <li>KIP-460 Admin Leader Election RPC (#4845)</li> <li>[KIP-714] Complete consumer metrics support (#4808).</li> <li>[KIP-714] Produce latency average and maximum metrics support for parity with Java client (#4847).</li> <li>[KIP-848] ListConsumerGroups Admin API now has an optional filter to return only groups    of given types.</li> <li>Added Transactional id resource type for ACL operations (@JohnPreston, #4856).</li> <li>Fix for permanent fetch errors when using a newer Fetch RPC version with an older    inter broker protocol (#4806).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_5", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#consumer-fixes_4", "title": "Consumer fixes", "text": "<ul> <li>Issues: #4806    Fix for permanent fetch errors when brokers support a Fetch RPC version greater than 12     but cluster is configured to use an inter broker protocol that is less than 2.8.    In this case returned topic ids are zero valued and Fetch has to fall back    to version 12, using topic names.    Happening since v2.5.0 (#4806)</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_6", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.6.0.zip SHA256 <code>e9eb7faedb24da3a19d5f056e08630fc2dae112d958f9b714ec6e35cd87c032e</code>  * v2.6.0.tar.gz SHA256 <code>abe0212ecd3e7ed3c4818a4f2baf7bf916e845e902bb15ae48834ca2d36ac745</code></p>"}, {"location": "Librdkafka-Changelog/#253-2024-09-02", "title": "2.5.3 (2024-09-02)", "text": "<p>librdkafka v2.5.3 is a maintenance release.</p> <ul> <li>Fix an assert being triggered during push telemetry call when no metrics matched on the client side. (#4826)</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_6", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#telemetry-fixes_2", "title": "Telemetry fixes", "text": "<ul> <li>Issue: #4833 Fix a regression introduced with KIP-714 support in which an assert is triggered during PushTelemetry call. This happens when no metric is matched on the client side among those requested by broker subscription. Happening since 2.5.0 (#4826).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_7", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.5.3.zip SHA256 <code>5b058006fcd403bc23fc1fcc14fe985641203f342c5715794af51023bcd047f9</code>  * v2.5.3.tar.gz SHA256 <code>eaa1213fdddf9c43e28834d9a832d9dd732377d35121e42f875966305f52b8ff</code></p> <p>Note: there were no v2.5.1 and v2.5.2 librdkafka releases</p>"}, {"location": "Librdkafka-Changelog/#250-2024-07-10", "title": "2.5.0 (2024-07-10)", "text": "<p>[!WARNING] This version has introduced a regression in which an assert is triggered during PushTelemetry call. This happens when no metric is matched on the client side among those requested by broker subscription. </p> <p>You won't face any problem if: * Broker doesn't support KIP-714. * KIP-714 feature is disabled on the broker side. * KIP-714 feature is disabled on the client side. This is enabled by default. Set configuration <code>enable.metrics.push</code> to <code>false</code>. * If KIP-714 is enabled on the broker side and there is no subscription configured there. * If KIP-714 is enabled on the broker side with subscriptions that match the KIP-714 metrics defined on the client.</p> <p>Having said this, we strongly recommend using <code>v2.5.3</code> and above to not face this regression at all.</p> <p>librdkafka v2.5.0 is a feature release.</p> <ul> <li>KIP-951   Leader discovery optimisations for the client (#4756, #4767).</li> <li>Fix segfault when using long client id because of erased segment when using flexver. (#4689)</li> <li>Fix for an idempotent producer error, with a message batch not reconstructed   identically when retried (#4750)</li> <li>Removed support for CentOS 6 and CentOS 7 (#4775).</li> <li>KIP-714 Client    metrics and observability (#4721).</li> </ul>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations", "title": "Upgrade considerations", "text": "<ul> <li>CentOS 6 and CentOS 7 support was removed as they reached EOL    and security patches aren't publicly available anymore.    ABI compatibility from CentOS 8 on is maintained through pypa/manylinux,    AlmaLinux based.    See also Confluent supported OSs page (#4775).</li> </ul>"}, {"location": "Librdkafka-Changelog/#enhancements", "title": "Enhancements", "text": "<ul> <li>Update bundled lz4 (used when <code>./configure --disable-lz4-ext</code>) to     v1.9.4, which contains     bugfixes and performance improvements (#4726).</li> <li>KIP-951     With this KIP leader updates are received through Produce and Fetch responses     in case of errors corresponding to leader changes and a partition migration     happens before refreshing the metadata cache (#4756, #4767).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_7", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_5", "title": "General fixes", "text": "<ul> <li>Issues: confluentinc/confluent-kafka-dotnet#2084    Fix segfault when a segment is erased and more data is written to the buffer.    Happens since 1.x when a portion of the buffer (segment) is erased for flexver or compression.    More likely to happen since 2.1.0, because of the upgrades to flexver, with certain string sizes like a long client id (#4689).</li> </ul>"}, {"location": "Librdkafka-Changelog/#idempotent-producer-fixes", "title": "Idempotent producer fixes", "text": "<ul> <li>Issues: #4736    Fix for an idempotent producer error, with a message batch not reconstructed    identically when retried. Caused the error message \"Local: Inconsistent state: Unable to reconstruct MessageSet\".    Happening on large batches. Solved by using the same backoff baseline for all messages    in the batch.    Happens since 2.2.0 (#4750).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_8", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.5.0.zip SHA256 <code>644c1b7425e2241ee056cf8a469c84d69c7f6a88559491c0813a6cdeb5563206</code>  * v2.5.0.tar.gz SHA256 <code>3dc62de731fd516dfb1032861d9a580d4d0b5b0856beb0f185d06df8e6c26259</code></p>"}, {"location": "Librdkafka-Changelog/#240-2024-05-07", "title": "2.4.0 (2024-05-07)", "text": "<p>librdkafka v2.4.0 is a feature release:</p> <ul> <li>KIP-848: The Next Generation of the Consumer Rebalance Protocol.    Early Access: This should be used only for evaluation and must not be used in production. Features and contract of this KIP might change in future (#4610).</li> <li>KIP-467: Augment ProduceResponse error messaging for specific culprit records (#4583).</li> <li>KIP-516    Continue partial implementation by adding a metadata cache by topic id    and updating the topic id corresponding to the partition name (#4676)</li> <li>Upgrade OpenSSL to v3.0.12 (while building from source) with various security fixes,    check the release notes.</li> <li>Integration tests can be started in KRaft mode and run against any    GitHub Kafka branch other than the released versions.</li> <li>Fix pipeline inclusion of static binaries (#4666)</li> <li>Fix to main loop timeout calculation leading to a tight loop for a    max period of 1 ms (#4671).</li> <li>Fixed a bug causing duplicate message consumption from a stale    fetch start offset in some particular cases (#4636)</li> <li>Fix to metadata cache expiration on full metadata refresh (#4677).</li> <li>Fix for a wrong error returned on full metadata refresh before joining    a consumer group (#4678).</li> <li>Fix to metadata refresh interruption (#4679).</li> <li>Fix for an undesired partition migration with stale leader epoch (#4680).</li> <li>Fix hang in cooperative consumer mode if an assignment is processed    while closing the consumer (#4528).</li> </ul>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_1", "title": "Upgrade considerations", "text": "<ul> <li>With KIP 467, <code>INVALID_MSG</code> (Java: CorruptRecordExpection) will    be retried automatically. <code>INVALID_RECORD</code> (Java: InvalidRecordException) instead    is not retriable and will be set only to the records that caused the    error. Rest of records in the batch will fail with the new error code    <code>_INVALID_DIFFERENT_RECORD</code> (Java: KafkaException) and can be retried manually,    depending on the application logic (#4583).</li> </ul>"}, {"location": "Librdkafka-Changelog/#early-access", "title": "Early Access", "text": ""}, {"location": "Librdkafka-Changelog/#kip-848-the-next-generation-of-the-consumer-rebalance-protocol", "title": "KIP-848: The Next Generation of the Consumer Rebalance Protocol", "text": "<ul> <li>With this new protocol the role of the Group Leader (a member) is removed and    the assignment is calculated by the Group Coordinator (a broker) and sent    to each member through heartbeats.</li> </ul> <p>The feature is still not production-ready.    It's possible to try it in a non-production enviroment.</p> <p>A guide is available    with considerations and steps to follow to test it (#4610).</p>"}, {"location": "Librdkafka-Changelog/#fixes_8", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_6", "title": "General fixes", "text": "<ul> <li>Issues: confluentinc/confluent-kafka-go#981.    In librdkafka release pipeline a static build containing libsasl2    could be chosen instead of the alternative one without it.    That caused the libsasl2 dependency to be required in confluent-kafka-go    v2.1.0-linux-musl-arm64 and v2.3.0-linux-musl-arm64.    Solved by correctly excluding the binary configured with that library,    when targeting a static build.    Happening since v2.0.2, with specified platforms,    when using static binaries (#4666).</li> <li>Issues: #4684.    When the main thread loop was awakened less than 1 ms    before the expiration of a timeout, it was serving with a zero timeout,    leading to increased CPU usage until the timeout was reached.    Happening since 1.x.</li> <li>Issues: #4685.    Metadata cache was cleared on full metadata refresh, leading to unnecessary    refreshes and occasional <code>UNKNOWN_TOPIC_OR_PART</code> errors. Solved by updating    cache for existing or hinted entries instead of clearing them.    Happening since 2.1.0 (#4677).</li> <li>Issues: #4589.    A metadata call before member joins consumer group,    could lead to an <code>UNKNOWN_TOPIC_OR_PART</code> error. Solved by updating    the consumer group following a metadata refresh only in safe states.    Happening since 2.1.0 (#4678).</li> <li>Issues: #4577.    Metadata refreshes without partition leader change could lead to a loop of    metadata calls at fixed intervals. Solved by stopping metadata refresh when    all existing metadata is non-stale. Happening since 2.3.0 (#4679).</li> <li>Issues: #4687.    A partition migration could happen, using stale metadata, when the partition    was undergoing a validation and being retried because of an error.    Solved by doing a partition migration only with a non-stale leader epoch.    Happening since 2.1.0 (#4680).</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_5", "title": "Consumer fixes", "text": "<ul> <li>Issues: #4686.    In case of subscription change with a consumer using the cooperative assignor    it could resume fetching from a previous position.    That could also happen if resuming a partition that wasn't paused.    Fixed by ensuring that a resume operation is completely a no-op when    the partition isn't paused.    Happening since 1.x (#4636).</li> <li>Issues: #4527.    While using the cooperative assignor, given an assignment is received while closing the consumer    it's possible that it gets stuck in state <code>WAIT_ASSIGN_CALL</code>, while the method is converted to    a full unassign. Solved by changing state from <code>WAIT_ASSIGN_CALL</code> to <code>WAIT_UNASSIGN_CALL</code>    while doing this conversion.    Happening since 1.x (#4528).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_9", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.4.0.zip SHA256 <code>24b30d394fc6ce5535eaa3c356ed9ed9ae4a6c9b4fc9159c322a776786d5dd15</code>  * v2.4.0.tar.gz SHA256 <code>d645e47d961db47f1ead29652606a502bdd2a880c85c1e060e94eea040f1a19a</code></p>"}, {"location": "Librdkafka-Changelog/#230-2023-10-25", "title": "2.3.0 (2023-10-25)", "text": "<p>librdkafka v2.3.0 is a feature release:</p> <ul> <li>KIP-516    Partial support of topic identifiers. Topic identifiers in metadata response    available through the new <code>rd_kafka_DescribeTopics</code> function (#4300, #4451).</li> <li>KIP-117 Add support for AdminAPI <code>DescribeCluster()</code> and <code>DescribeTopics()</code>   (#4240, @jainruchir).</li> <li>KIP-430:    Return authorized operations in Describe Responses.    (#4240, @jainruchir).</li> <li>KIP-580: Added Exponential Backoff mechanism for    retriable requests with <code>retry.backoff.ms</code> as minimum backoff and <code>retry.backoff.max.ms</code> as the    maximum backoff, with 20% jitter (#4422).</li> <li>KIP-396: completed the implementation with    the addition of ListOffsets (#4225).</li> <li>Fixed ListConsumerGroupOffsets not fetching offsets for all the topics in a group with Apache Kafka version below 2.4.0.</li> <li>Add missing destroy that leads to leaking partition structure memory when there    are partition leader changes and a stale leader epoch is received (#4429).</li> <li>Fix a segmentation fault when closing a consumer using the    cooperative-sticky assignor before the first assignment (#4381).</li> <li>Fix for insufficient buffer allocation when allocating rack information (@wolfchimneyrock, #4449).</li> <li>Fix for infinite loop of OffsetForLeaderEpoch requests on quick leader changes. (#4433).</li> <li>Fix to add leader epoch to control messages, to make sure they're stored    for committing even without a subsequent fetch message (#4434).</li> <li>Fix for stored offsets not being committed if they lacked the leader epoch (#4442).</li> <li>Upgrade OpenSSL to v3.0.11 (while building from source) with various security fixes,    check the release notes    (#4454, started by @migarc1).</li> <li>Fix to ensure permanent errors during offset validation continue being retried and    don't cause an offset reset (#4447).</li> <li>Fix to ensure max.poll.interval.ms is reset when rd_kafka_poll is called with    consume_cb (#4431).</li> <li>Fix for idempotent producer fatal errors, triggered after a possibly persisted message state (#4438).</li> <li>Fix <code>rd_kafka_query_watermark_offsets</code> continuing beyond timeout expiry (#4460).</li> <li>Fix <code>rd_kafka_query_watermark_offsets</code> not refreshing the partition leader    after a leader change and subsequent <code>NOT_LEADER_OR_FOLLOWER</code> error (#4225).</li> </ul>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_2", "title": "Upgrade considerations", "text": "<ul> <li> <p><code>retry.backoff.ms</code>:    If it is set greater than <code>retry.backoff.max.ms</code> which has the default value of 1000 ms then it is assumes the value of <code>retry.backoff.max.ms</code>.    To change this behaviour make sure that <code>retry.backoff.ms</code> is always less than <code>retry.backoff.max.ms</code>.    If equal then the backoff will be linear instead of exponential.</p> </li> <li> <p><code>topic.metadata.refresh.fast.interval.ms</code>:    If it is set greater than <code>retry.backoff.max.ms</code> which has the default value of 1000 ms then it is assumes the value of <code>retry.backoff.max.ms</code>.    To change this behaviour make sure that <code>topic.metadata.refresh.fast.interval.ms</code> is always less than <code>retry.backoff.max.ms</code>.    If equal then the backoff will be linear instead of exponential.</p> </li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_9", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_7", "title": "General fixes", "text": "<ul> <li>An assertion failed with insufficient buffer size when allocating    rack information on 32bit architectures.    Solved by aligning all allocations to the maximum allowed word size (#4449).</li> <li>The timeout for <code>rd_kafka_query_watermark_offsets</code> was not enforced after    making the necessary ListOffsets requests, and thus, it never timed out in    case of broker/network issues. Fixed by setting an absolute timeout (#4460).</li> </ul>"}, {"location": "Librdkafka-Changelog/#idempotent-producer-fixes_1", "title": "Idempotent producer fixes", "text": "<ul> <li>After a possibly persisted error, such as a disconnection or a timeout, next expected sequence    used to increase, leading to a fatal error if the message wasn't persisted and    the second one in queue failed with an <code>OUT_OF_ORDER_SEQUENCE_NUMBER</code>.    The error could contain the message \"sequence desynchronization\" with    just one possibly persisted error or \"rewound sequence number\" in case of    multiple errored messages.    Solved by treating the possible persisted message as not persisted,    and expecting a <code>DUPLICATE_SEQUENCE_NUMBER</code> error in case it was or    <code>NO_ERROR</code> in case it wasn't, in both cases the message will be considered    delivered (#4438).</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_6", "title": "Consumer fixes", "text": "<ul> <li>Stored offsets were excluded from the commit if the leader epoch was     less than committed epoch, as it's possible if leader epoch is the default -1.     This didn't happen in Python, Go and .NET bindings when stored position was     taken from the message.     Solved by checking only that the stored offset is greater     than committed one, if either stored or committed leader epoch is -1 (#4442).</li> <li>If an OffsetForLeaderEpoch request was being retried, and the leader changed     while the retry was in-flight, an infinite loop of requests was triggered,     because we weren't updating the leader epoch correctly.     Fixed by updating the leader epoch before sending the request (#4433).</li> <li>During offset validation a permanent error like host resolution failure     would cause an offset reset.     This isn't what's expected or what the Java implementation does.     Solved by retrying even in case of permanent errors (#4447).</li> <li>If using <code>rd_kafka_poll_set_consumer</code>, along with a consume callback, and then     calling <code>rd_kafka_poll</code> to service the callbacks, would not reset     <code>max.poll.interval.ms.</code> This was because we were only checking <code>rk_rep</code> for     consumer messages, while the method to service the queue internally also     services the queue forwarded to from <code>rk_rep</code>, which is <code>rkcg_q</code>.     Solved by moving the <code>max.poll.interval.ms</code> check into <code>rd_kafka_q_serve</code> (#4431).</li> <li>After a leader change a <code>rd_kafka_query_watermark_offsets</code> call would continue     trying to call ListOffsets on the old leader, if the topic wasn't included in     the subscription set, so it started querying the new leader only after     <code>topic.metadata.refresh.interval.ms</code> (#4225).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_10", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.3.0.zip SHA256 <code>15e77455811b3e5d869d6f97ce765b634c7583da188792e2930a2098728e932b</code>  * v2.3.0.tar.gz SHA256 <code>2d49c35c77eeb3d42fa61c43757fcbb6a206daa560247154e60642bcdcc14d12</code></p>"}, {"location": "Librdkafka-Changelog/#220-2023-07-12", "title": "2.2.0 (2023-07-12)", "text": "<p>librdkafka v2.2.0 is a feature release:</p> <ul> <li>Fix a segmentation fault when subscribing to non-existent topics and    using the consume batch functions (#4273).</li> <li>Store offset commit metadata in <code>rd_kafka_offsets_store</code> (@mathispesch, #4084).</li> <li>Fix a bug that happens when skipping tags, causing buffer underflow in    MetadataResponse (#4278).</li> <li>Fix a bug where topic leader is not refreshed in the same metadata call even if the leader is    present.</li> <li>KIP-881:    Add support for rack-aware partition assignment for consumers    (#4184, #4291, #4252).</li> <li>Fix several bugs with sticky assignor in case of partition ownership    changing between members of the consumer group (#4252).</li> <li>KIP-368:    Allow SASL Connections to Periodically Re-Authenticate    (#4301, started by @vctoriawu).</li> <li>Avoid treating an OpenSSL error as a permanent error and treat unclean SSL    closes as normal ones (#4294).</li> <li>Added <code>fetch.queue.backoff.ms</code> to the consumer to control how long    the consumer backs off next fetch attempt. (@bitemyapp, @edenhill, #2879)</li> <li>KIP-235:    Add DNS alias support for secured connection (#4292).</li> <li>KIP-339:    IncrementalAlterConfigs API (started by @PrasanthV454, #4110).</li> <li>KIP-554: Add Broker-side SCRAM Config API (#4241).</li> </ul>"}, {"location": "Librdkafka-Changelog/#enhancements_1", "title": "Enhancements", "text": "<ul> <li>Added <code>fetch.queue.backoff.ms</code> to the consumer to control how long    the consumer backs off next fetch attempt. When the pre-fetch queue    has exceeded its queuing thresholds: <code>queued.min.messages</code> and    <code>queued.max.messages.kbytes</code> it backs off for 1 seconds.    If those parameters have to be set too high to hold 1 s of data,    this new parameter allows to back off the fetch earlier, reducing memory    requirements.</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_10", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_8", "title": "General fixes", "text": "<ul> <li>Fix a bug that happens when skipping tags, causing buffer underflow in    MetadataResponse. This is triggered since RPC version 9 (v2.1.0),    when using Confluent Platform, only when racks are set,    observers are activated and there is more than one partition.    Fixed by skipping the correct amount of bytes when tags are received.</li> <li>Avoid treating an OpenSSL error as a permanent error and treat unclean SSL    closes as normal ones. When SSL connections are closed without <code>close_notify</code>,    in OpenSSL 3.x a new type of error is set and it was interpreted as permanent    in librdkafka. It can cause a different issue depending on the RPC.    If received when waiting for OffsetForLeaderEpoch response, it triggers    an offset reset following the configured policy.    Solved by treating SSL errors as transport errors and    by setting an OpenSSL flag that allows to treat unclean SSL closes as normal    ones. These types of errors can happen it the other side doesn't support <code>close_notify</code> or if there's a TCP connection reset.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_7", "title": "Consumer fixes", "text": "<ul> <li>In case of multiple owners of a partition with different generations, the     sticky assignor would pick the earliest (lowest generation) member as the     current owner, which would lead to stickiness violations. Fixed by     choosing the latest (highest generation) member.</li> <li>In case where the same partition is owned by two members with the same     generation, it indicates an issue. The sticky assignor had some code to     handle this, but it was non-functional, and did not have parity with the     Java assignor. Fixed by invalidating any such partition from the current     assignment completely.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_11", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.2.0.zip SHA256 <code>e9a99476dd326089ce986afd3a5b069ef8b93dbb845bc5157b3d94894de53567</code>  * v2.2.0.tar.gz SHA256 <code>af9a820cbecbc64115629471df7c7cecd40403b6c34bfdbb9223152677a47226</code></p>"}, {"location": "Librdkafka-Changelog/#211-2023-05-02", "title": "2.1.1 (2023-05-02)", "text": "<p>librdkafka v2.1.1 is a maintenance release:</p> <ul> <li>Avoid duplicate messages when a fetch response is received    in the middle of an offset validation request (#4261).</li> <li>Fix segmentation fault when subscribing to a non-existent topic and    calling <code>rd_kafka_message_leader_epoch()</code> on the polled <code>rkmessage</code> (#4245).</li> <li>Fix a segmentation fault when fetching from follower and the partition lease    expires while waiting for the result of a list offsets operation (#4254).</li> <li>Fix documentation for the admin request timeout, incorrectly stating -1 for infinite    timeout. That timeout can't be infinite.</li> <li>Fix CMake pkg-config cURL require and use    pkg-config <code>Requires.private</code> field (@FantasqueX, @stertingen, #4180).</li> <li>Fixes certain cases where polling would not keep the consumer    in the group or make it rejoin it (#4256).</li> <li>Fix to the C++ set_leader_epoch method of TopicPartitionImpl,    that wasn't storing the passed value (@pavel-pimenov, #4267).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_11", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#consumer-fixes_8", "title": "Consumer fixes", "text": "<ul> <li>Duplicate messages can be emitted when a fetch response is received    in the middle of an offset validation request. Solved by avoiding    a restart from last application offset when offset validation succeeds.</li> <li>When fetching from follower, if the partition lease expires after 5 minutes,    and a list offsets operation was requested to retrieve the earliest    or latest offset, it resulted in segmentation fault. This was fixed by    allowing threads different from the main one to call    the <code>rd_kafka_toppar_set_fetch_state</code> function, given they hold    the lock on the <code>rktp</code>.</li> <li>In v2.1.0, a bug was fixed which caused polling any queue to reset the    <code>max.poll.interval.ms</code>. Only certain functions were made to reset the timer,    but it is possible for the user to obtain the queue with messages from    the broker, skipping these functions. This was fixed by encoding information    in a queue itself, that, whether polling, resets the timer.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_12", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.1.1.zip SHA256 <code>3b8a59f71e22a8070e0ae7a6b7ad7e90d39da8fddc41ce6c5d596ee7f5a4be4b</code>  * v2.1.1.tar.gz SHA256 <code>7be1fc37ab10ebdc037d5c5a9b35b48931edafffae054b488faaff99e60e0108</code></p>"}, {"location": "Librdkafka-Changelog/#210-2023-04-06", "title": "2.1.0 (2023-04-06)", "text": "<p>librdkafka v2.1.0 is a feature release:</p> <ul> <li>KIP-320   Allow fetchers to detect and handle log truncation (#4122).</li> <li>Fix a reference count issue blocking the consumer from closing (#4187).</li> <li>Fix a protocol issue with ListGroups API, where an extra   field was appended for API Versions greater than or equal to 3 (#4207).</li> <li>Fix an issue with <code>max.poll.interval.ms</code>, where polling any queue would cause   the timeout to be reset (#4176).</li> <li>Fix seek partition timeout, was one thousand times lower than the passed   value (#4230).</li> <li>Fix multiple inconsistent behaviour in batch APIs during pause or resume operations (#4208).   See Consumer fixes section below for more information.</li> <li>Update lz4.c from upstream. Fixes CVE-2021-3520   (by @filimonov, #4232).</li> <li>Upgrade OpenSSL to v3.0.8 with various security fixes,   check the release notes (#4215).</li> </ul>"}, {"location": "Librdkafka-Changelog/#enhancements_2", "title": "Enhancements", "text": "<ul> <li>Added <code>rd_kafka_topic_partition_get_leader_epoch()</code> (and <code>set..()</code>).</li> <li>Added partition leader epoch APIs:</li> <li><code>rd_kafka_topic_partition_get_leader_epoch()</code> (and <code>set..()</code>)</li> <li><code>rd_kafka_message_leader_epoch()</code></li> <li><code>rd_kafka_*assign()</code> and <code>rd_kafka_seek_partitions()</code> now supports      partitions with a leader epoch set.</li> <li><code>rd_kafka_offsets_for_times()</code> will return per-partition leader-epochs.</li> <li><code>leader_epoch</code>, <code>stored_leader_epoch</code>, and <code>committed_leader_epoch</code>      added to per-partition statistics.</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_12", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#openssl-fixes", "title": "OpenSSL fixes", "text": "<ul> <li>Fixed OpenSSL static build not able to use external modules like FIPS    provider module.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_9", "title": "Consumer fixes", "text": "<ul> <li>A reference count issue was blocking the consumer from closing.    The problem would happen when a partition is lost, because forcibly    unassigned from the consumer or if the corresponding topic is deleted.</li> <li>When using <code>rd_kafka_seek_partitions</code>, the remaining timeout was    converted from microseconds to milliseconds but the expected unit    for that parameter is microseconds.</li> <li>Fixed known issues related to Batch Consume APIs mentioned in v2.0.0    release notes.</li> <li>Fixed <code>rd_kafka_consume_batch()</code> and <code>rd_kafka_consume_batch_queue()</code>    intermittently updating <code>app_offset</code> and <code>store_offset</code> incorrectly when    pause or resume was being used for a partition.</li> <li>Fixed <code>rd_kafka_consume_batch()</code> and <code>rd_kafka_consume_batch_queue()</code>    intermittently skipping offsets when pause or resume was being    used for a partition.</li> </ul>"}, {"location": "Librdkafka-Changelog/#known-issues", "title": "Known Issues", "text": ""}, {"location": "Librdkafka-Changelog/#consume-batch-api", "title": "Consume Batch API", "text": "<ul> <li>When <code>rd_kafka_consume_batch()</code> and <code>rd_kafka_consume_batch_queue()</code> APIs are used with    any of the seek, pause, resume or rebalancing operation, <code>on_consume</code>    interceptors might be called incorrectly (maybe multiple times) for not consumed messages.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consume-api", "title": "Consume API", "text": "<ul> <li>Duplicate messages can be emitted when a fetch response is received    in the middle of an offset validation request.</li> <li>Segmentation fault when subscribing to a non-existent topic and    calling <code>rd_kafka_message_leader_epoch()</code> on the polled <code>rkmessage</code>.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_13", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.1.0.zip SHA256 <code>2fe898f9f5e2b287d26c5f929c600e2772403a594a691e0560a2a1f2706edf57</code>  * v2.1.0.tar.gz SHA256 <code>d8e76c4b1cde99e283a19868feaaff5778aa5c6f35790036c5ef44bc5b5187aa</code></p>"}, {"location": "Librdkafka-Changelog/#202-2023-01-20", "title": "2.0.2 (2023-01-20)", "text": "<p>librdkafka v2.0.2 is a bugfix release:</p> <ul> <li>Fix OpenSSL version in Win32 nuget package (#4152).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_14", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.0.2.zip SHA256 <code>87010c722111539dc3c258a6be0c03b2d6d4a607168b65992eb0076c647e4e9d</code>  * v2.0.2.tar.gz SHA256 <code>f321bcb1e015a34114c83cf1aa7b99ee260236aab096b85c003170c90a47ca9d</code></p>"}, {"location": "Librdkafka-Changelog/#201-2023-01-19", "title": "2.0.1 (2023-01-19)", "text": "<p>librdkafka v2.0.1 is a bugfix release:</p> <ul> <li>Fixed nuget package for Linux ARM64 release (#4150).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_15", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.0.1.zip SHA256 <code>7121df3fad1f72ea1c42dcc4e5367337207a75966216c63e58222c6433c528e0</code>  * v2.0.1.tar.gz SHA256 <code>3670f8d522e77f79f9d09a22387297ab58d1156b22de12ef96e58b7d57fca139</code></p>"}, {"location": "Librdkafka-Changelog/#200-2023-01-18", "title": "2.0.0 (2023-01-18)", "text": "<p>librdkafka v2.0.0 is a feature release:</p> <ul> <li>KIP-88    OffsetFetch Protocol Update (#3995).</li> <li>KIP-222    Add Consumer Group operations to Admin API (started by @lesterfan, #3995).</li> <li>KIP-518    Allow listing consumer groups per state (#3995).</li> <li>KIP-396    Partially implemented: support for AlterConsumerGroupOffsets    (started by @lesterfan, #3995).</li> <li>OpenSSL 3.0.x support - the maximum bundled OpenSSL version is now 3.0.7 (previously 1.1.1q).</li> <li>Fixes to the transactional and idempotent producer.</li> </ul>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_3", "title": "Upgrade considerations", "text": ""}, {"location": "Librdkafka-Changelog/#openssl-30x", "title": "OpenSSL 3.0.x", "text": ""}, {"location": "Librdkafka-Changelog/#openssl-default-ciphers", "title": "OpenSSL default ciphers", "text": "<p>The introduction of OpenSSL 3.0.x in the self-contained librdkafka bundles changes the default set of available ciphers, in particular all obsolete or insecure ciphers and algorithms as listed in the OpenSSL legacy manual page are now disabled by default.</p> <p>WARNING: These ciphers are disabled for security reasons and it is highly recommended NOT to use them.</p> <p>Should you need to use any of these old ciphers you'll need to explicitly enable the <code>legacy</code> provider by configuring <code>ssl.providers=default,legacy</code> on the librdkafka client.</p>"}, {"location": "Librdkafka-Changelog/#openssl-engines-and-providers", "title": "OpenSSL engines and providers", "text": "<p>OpenSSL 3.0.x deprecates the use of engines, which is being replaced by providers. As such librdkafka will emit a deprecation warning if <code>ssl.engine.location</code> is configured.</p> <p>OpenSSL providers may be configured with the new <code>ssl.providers</code> configuration property.</p>"}, {"location": "Librdkafka-Changelog/#broker-tls-certificate-hostname-verification", "title": "Broker TLS certificate hostname verification", "text": "<p>The default value for <code>ssl.endpoint.identification.algorithm</code> has been changed from <code>none</code> (no hostname verification) to <code>https</code>, which enables broker hostname verification (to counter man-in-the-middle impersonation attacks) by default.</p> <p>To restore the previous behaviour, set <code>ssl.endpoint.identification.algorithm</code> to <code>none</code>.</p>"}, {"location": "Librdkafka-Changelog/#known-issues_1", "title": "Known Issues", "text": ""}, {"location": "Librdkafka-Changelog/#poor-consumer-batch-api-messaging-guarantees", "title": "Poor Consumer batch API messaging guarantees", "text": "<p>The Consumer Batch APIs <code>rd_kafka_consume_batch()</code> and <code>rd_kafka_consume_batch_queue()</code> are not thread safe if <code>rkmessages_size</code> is greater than 1 and any of the seek, pause, resume or rebalancing operation is performed in parallel with any of the above APIs. Some of the messages might be lost, or erroneously returned to the  application, in the above scenario.</p> <p>It is strongly recommended to use the Consumer Batch APIs and the mentioned operations in sequential order in order to get consistent result.</p> <p>For rebalancing operation to work in sequencial manner, please set <code>rebalance_cb</code> configuration property (refer examples/rdkafka_complex_consumer_example.c for the help with the usage) for the consumer.</p>"}, {"location": "Librdkafka-Changelog/#enhancements_3", "title": "Enhancements", "text": "<ul> <li>Self-contained static libraries can now be built on Linux arm64 (#4005).</li> <li>Updated to zlib 1.2.13, zstd 1.5.2, and curl 7.86.0 in self-contained    librdkafka bundles.</li> <li>Added <code>on_broker_state_change()</code> interceptor</li> <li>The C++ API no longer returns strings by const value, which enables better move optimization in callers.</li> <li>Added <code>rd_kafka_sasl_set_credentials()</code> API to update SASL credentials.</li> <li>Setting <code>allow.auto.create.topics</code> will no longer give a warning if used by a producer, since that is an expected use case.   Improvement in documentation for this property.</li> <li>Added a <code>resolve_cb</code> configuration setting that permits using custom DNS resolution logic.</li> <li>Added <code>rd_kafka_mock_broker_error_stack_cnt()</code>.</li> <li>The librdkafka.redist NuGet package has been updated to have fewer external    dependencies for its bundled librdkafka builds, as everything but cyrus-sasl    is now built-in. There are bundled builds with and without linking to    cyrus-sasl for maximum compatibility.</li> <li>Admin API DescribeGroups() now provides the group instance id    for static members KIP-345 (#3995).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_13", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_9", "title": "General fixes", "text": "<ul> <li>Windows: couldn't read a PKCS#12 keystore correctly because binary mode    wasn't explicitly set and Windows defaults to text mode.</li> <li>Fixed memory leak when loading SSL certificates (@Mekk, #3930)</li> <li>Load all CA certificates from <code>ssl.ca.pem</code>, not just the first one.</li> <li>Each HTTP request made when using OAUTHBEARER OIDC would leak a small    amount of memory.</li> </ul>"}, {"location": "Librdkafka-Changelog/#transactional-producer-fixes", "title": "Transactional producer fixes", "text": "<ul> <li>When a PID epoch bump is requested and the producer is waiting    to reconnect to the transaction coordinator, a failure in a find coordinator    request could cause an assert to fail. This is fixed by retrying when the    coordinator is known (#4020).</li> <li>Transactional APIs (except <code>send_offsets_for_transaction()</code>) that    timeout due to low timeout_ms may now be resumed by calling the same API    again, as the operation continues in the background.</li> <li>For fatal idempotent producer errors that may be recovered by bumping the    epoch the current transaction must first be aborted prior to the epoch bump.    This is now handled correctly, which fixes issues seen with fenced    transactional producers on fatal idempotency errors.</li> <li>Timeouts for EndTxn requests (transaction commits and aborts) are now    automatically retried and the error raised to the application is also    a retriable error.</li> <li>TxnOffsetCommitRequests were retried immediately upon temporary errors in    <code>send_offsets_to_transactions()</code>, causing excessive network requests.    These retries are now delayed 500ms.</li> <li>If <code>init_transactions()</code> is called with an infinite timeout (-1),    the timeout will be limited to 2 * <code>transaction.timeout.ms</code>.    The application may retry and resume the call if a retriable error is    returned.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_10", "title": "Consumer fixes", "text": "<ul> <li>Back-off and retry JoinGroup request if coordinator load is in progress.</li> <li>Fix <code>rd_kafka_consume_batch()</code> and <code>rd_kafka_consume_batch_queue()</code> skipping    other partitions' offsets intermittently when seek, pause, resume    or rebalancing is used for a partition.</li> <li>Fix <code>rd_kafka_consume_batch()</code> and <code>rd_kafka_consume_batch_queue()</code>    intermittently returing incorrect partitions' messages if rebalancing     happens during these operations.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_16", "title": "Checksums", "text": "<p>Release asset checksums:  * v2.0.0.zip SHA256 <code>9d8a8be30ed09daf6c560f402e91db22fcaea11cac18a0d3c0afdbf884df1d4e</code>  * v2.0.0.tar.gz SHA256 <code>f75de3545b3c6cc027306e2df0371aefe1bb8f86d4ec612ed4ebf7bfb2f817cd</code></p>"}, {"location": "Librdkafka-Changelog/#192-2022-08-01", "title": "1.9.2 (2022-08-01)", "text": "<p>librdkafka v1.9.2 is a maintenance release:</p> <ul> <li>The SASL OAUTHBEAR OIDC POST field was sometimes truncated by one byte (#3192).</li> <li>The bundled version of OpenSSL has been upgraded to version 1.1.1q for non-Windows builds. Windows builds remain on OpenSSL 1.1.1n for the time being.</li> <li>The bundled version of Curl has been upgraded to version 7.84.0.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_17", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.9.2.zip SHA256 <code>4ecb0a3103022a7cab308e9fecd88237150901fa29980c99344218a84f497b86</code>  * v1.9.2.tar.gz SHA256 <code>3fba157a9f80a0889c982acdd44608be8a46142270a389008b22d921be1198ad</code></p>"}, {"location": "Librdkafka-Changelog/#191-2022-07-06", "title": "1.9.1 (2022-07-06)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v191", "title": "librdkafka v1.9.1", "text": "<p>librdkafka v1.9.1 is a maintenance release:</p> <ul> <li>The librdkafka.redist NuGet package now contains OSX M1/arm64 builds.</li> <li>Self-contained static libraries can now be built on OSX M1 too, thanks to    disabling curl's configure runtime check.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_18", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.9.1.zip SHA256 <code>d3fc2e0bc00c3df2c37c5389c206912842cca3f97dd91a7a97bc0f4fc69f94ce</code>  * v1.9.1.tar.gz SHA256 <code>3a54cf375218977b7af4716ed9738378e37fe400a6c5ddb9d622354ca31fdc79</code></p>"}, {"location": "Librdkafka-Changelog/#190-2022-06-16", "title": "1.9.0 (2022-06-16)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v190", "title": "librdkafka v1.9.0", "text": "<p>librdkafka v1.9.0 is a feature release:</p> <ul> <li>Added KIP-768 OUATHBEARER OIDC support (by @jliunyu, #3560)</li> <li>Added KIP-140 Admin API ACL support (by @emasab, #2676)</li> </ul>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_4", "title": "Upgrade considerations", "text": "<ul> <li>Consumer:    <code>rd_kafka_offsets_store()</code> (et.al) will now return an error for any    partition that is not currently assigned (through <code>rd_kafka_*assign()</code>).    This prevents a race condition where an application would store offsets    after the assigned partitions had been revoked (which resets the stored    offset), that could cause these old stored offsets to be committed later    when the same partitions were assigned to this consumer again - effectively    overwriting any committed offsets by any consumers that were assigned the    same partitions previously. This would typically result in the offsets    rewinding and messages to be reprocessed.    As an extra effort to avoid this situation the stored offset is now    also reset when partitions are assigned (through <code>rd_kafka_*assign()</code>).    Applications that explicitly call <code>..offset*_store()</code> will now need    to handle the case where <code>RD_KAFKA_RESP_ERR__STATE</code> is returned    in the per-partition <code>.err</code> field - meaning the partition is no longer    assigned to this consumer and the offset could not be stored for commit.</li> </ul>"}, {"location": "Librdkafka-Changelog/#enhancements_4", "title": "Enhancements", "text": "<ul> <li>Improved producer queue scheduling. Fixes the performance regression    introduced in v1.7.0 for some produce patterns. (#3538, #2912)</li> <li>Windows: Added native Win32 IO/Queue scheduling. This removes the    internal TCP loopback connections that were previously used for timely    queue wakeups.</li> <li>Added <code>socket.connection.setup.timeout.ms</code> (default 30s).    The maximum time allowed for broker connection setups (TCP connection as    well as SSL and SASL handshakes) is now limited to this value.    This fixes the issue with stalled broker connections in the case of network    or load balancer problems.    The Java clients has an exponential backoff to this timeout which is    limited by <code>socket.connection.setup.timeout.max.ms</code> - this was not    implemented in librdkafka due to differences in connection handling and    <code>ERR__ALL_BROKERS_DOWN</code> error reporting. Having a lower initial connection    setup timeout and then increase the timeout for the next attempt would    yield possibly false-positive <code>ERR__ALL_BROKERS_DOWN</code> too early.</li> <li>SASL OAUTHBEARER refresh callbacks can now be scheduled for execution    on librdkafka's background thread. This solves the problem where an    application has a custom SASL OAUTHBEARER refresh callback and thus needs to    call <code>rd_kafka_poll()</code> (et.al.) at least once to trigger the    refresh callback before being able to connect to brokers.    With the new <code>rd_kafka_conf_enable_sasl_queue()</code> configuration API and    <code>rd_kafka_sasl_background_callbacks_enable()</code> the refresh callbacks    can now be triggered automatically on the librdkafka background thread.</li> <li><code>rd_kafka_queue_get_background()</code> now creates the background thread    if not already created.</li> <li>Added <code>rd_kafka_consumer_close_queue()</code> and <code>rd_kafka_consumer_closed()</code>.    This allow applications and language bindings to implement asynchronous    consumer close.</li> <li>Bundled zlib upgraded to version 1.2.12.</li> <li>Bundled OpenSSL upgraded to 1.1.1n.</li> <li>Added <code>test.mock.broker.rtt</code> to simulate RTT/latency for mock brokers.</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_14", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_10", "title": "General fixes", "text": "<ul> <li>Fix various 1 second delays due to internal broker threads blocking on IO    even though there are events to handle.    These delays could be seen randomly in any of the non produce/consume    request APIs, such as <code>commit_transaction()</code>, <code>list_groups()</code>, etc.</li> <li>Windows: some applications would crash with an error message like    <code>no OPENSSL_Applink()</code> written to the console if <code>ssl.keystore.location</code>    was configured.    This regression was introduced in v1.8.0 due to use of vcpkgs and how    keystore file was read. #3554.</li> <li>Windows 32-bit only: 64-bit atomic reads were in fact not atomic and could    in rare circumstances yield incorrect values.    One manifestation of this issue was the <code>max.poll.interval.ms</code> consumer    timer expiring even though the application was polling according to profile.    Fixed by @WhiteWind (#3815).</li> <li><code>rd_kafka_clusterid()</code> would previously fail with timeout if    called on cluster with no visible topics (#3620).    The clusterid is now returned as soon as metadata has been retrieved.</li> <li>Fix hang in <code>rd_kafka_list_groups()</code> if there are no available brokers    to connect to (#3705).</li> <li>Millisecond timeouts (<code>timeout_ms</code>) in various APIs, such as <code>rd_kafka_poll()</code>,    was limited to roughly 36 hours before wrapping. (#3034)</li> <li>If a metadata request triggered by <code>rd_kafka_metadata()</code> or consumer group rebalancing    encountered a non-retriable error it would not be propagated to the caller and thus    cause a stall or timeout, this has now been fixed. (@aiquestion, #3625)</li> <li>AdminAPI <code>DeleteGroups()</code> and <code>DeleteConsumerGroupOffsets()</code>:    if the given coordinator connection was not up by the time these calls were    initiated and the first connection attempt failed then no further connection    attempts were performed, ulimately leading to the calls timing out.    This is now fixed by keep retrying to connect to the group coordinator    until the connection is successful or the call times out.    Additionally, the coordinator will be now re-queried once per second until    the coordinator comes up or the call times out, to detect change in    coordinators.</li> <li>Mock cluster <code>rd_kafka_mock_broker_set_down()</code> would previously    accept and then disconnect new connections, it now refuses new connections.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_11", "title": "Consumer fixes", "text": "<ul> <li><code>rd_kafka_offsets_store()</code> (et.al) will now return an error for any    partition that is not currently assigned (through <code>rd_kafka_*assign()</code>).    See Upgrade considerations above for more information.</li> <li><code>rd_kafka_*assign()</code> will now reset/clear the stored offset.    See Upgrade considerations above for more information.</li> <li><code>seek()</code> followed by <code>pause()</code> would overwrite the seeked offset when    later calling <code>resume()</code>. This is now fixed. (#3471).    Note: Avoid storing offsets (<code>offsets_store()</code>) after calling    <code>seek()</code> as this may later interfere with resuming a paused partition,    instead store offsets prior to calling seek.</li> <li>A <code>ERR_MSG_SIZE_TOO_LARGE</code> consumer error would previously be raised    if the consumer received a maximum sized FetchResponse only containing    (transaction) aborted messages with no control messages. The fetching did    not stop, but some applications would terminate upon receiving this error.    No error is now raised in this case. (#2993)    Thanks to @jacobmikesell for providing an application to reproduce the    issue.</li> <li>The consumer no longer backs off the next fetch request (default 500ms) when    the parsed fetch response is truncated (which is a valid case).    This should speed up the message fetch rate in case of maximum sized    fetch responses.</li> <li>Fix consumer crash (<code>assert: rkbuf-&gt;rkbuf_rkb</code>) when parsing    malformed JoinGroupResponse consumer group metadata state.</li> <li>Fix crash (<code>cant handle op type</code>) when using <code>consume_batch_queue()</code> (et.al)    and an OAUTHBEARER refresh callback was set.    The callback is now triggered by the consume call. (#3263)</li> <li>Fix <code>partition.assignment.strategy</code> ordering when multiple strategies are configured.    If there is more than one eligible strategy, preference is determined by the    configured order of strategies. The partitions are assigned to group members according    to the strategy order preference now. (#3818)</li> <li>Any form of unassign*() (absolute or incremental) is now allowed during    consumer close rebalancing and they're all treated as absolute unassigns.    (@kevinconaway)</li> </ul>"}, {"location": "Librdkafka-Changelog/#transactional-producer-fixes_1", "title": "Transactional producer fixes", "text": "<ul> <li>Fix message loss in idempotent/transactional producer.    A corner case has been identified that may cause idempotent/transactional    messages to be lost despite being reported as successfully delivered:    During cluster instability a restarting broker may report existing topics    as non-existent for some time before it is able to acquire up to date    cluster and topic metadata.    If an idempotent/transactional producer updates its topic metadata cache    from such a broker the producer will consider the topic to be removed from    the cluster and thus remove its local partition objects for the given topic.    This also removes the internal message sequence number counter for the given    partitions.    If the producer later receives proper topic metadata for the cluster the    previously \"removed\" topics will be rediscovered and new partition objects    will be created in the producer. These new partition objects, with no    knowledge of previous incarnations, would start counting partition messages    at zero again.    If new messages were produced for these partitions by the same producer    instance, the same message sequence numbers would be sent to the broker.    If the broker still maintains state for the producer's PID and Epoch it could    deem that these messages with reused sequence numbers had already been    written to the log and treat them as legit duplicates.    This would seem to the producer that these new messages were successfully    written to the partition log by the broker when they were in fact discarded    as duplicates, leading to silent message loss.    The fix included in this release is to save the per-partition idempotency    state when a partition is removed, and then recover and use that saved    state if the partition comes back at a later time.</li> <li>The transactional producer would retry (re)initializing its PID if a    <code>PRODUCER_FENCED</code> error was returned from the    broker (added in Apache Kafka 2.8), which could cause the producer to    seemingly hang.    This error code is now correctly handled by raising a fatal error.</li> <li>If the given group coordinator connection was not up by the time    <code>send_offsets_to_transactions()</code> was called, and the first connection    attempt failed then no further connection attempts were performed, ulimately    leading to <code>send_offsets_to_transactions()</code> timing out, and possibly    also the transaction timing out on the transaction coordinator.    This is now fixed by keep retrying to connect to the group coordinator    until the connection is successful or the call times out.    Additionally, the coordinator will be now re-queried once per second until    the coordinator comes up or the call times out, to detect change in    coordinators.</li> </ul>"}, {"location": "Librdkafka-Changelog/#producer-fixes_1", "title": "Producer fixes", "text": "<ul> <li>Improved producer queue wakeup scheduling. This should significantly    decrease the number of wakeups and thus syscalls for high message rate    producers. (#3538, #2912)</li> <li>The logic for enforcing that <code>message.timeout.ms</code> is greather than    an explicitly configured <code>linger.ms</code> was incorrect and instead of    erroring out early the lingering time was automatically adjusted to the    message timeout, ignoring the configured <code>linger.ms</code>.    This has now been fixed so that an error is returned when instantiating the    producer. Thanks to @larry-cdn77 for analysis and test-cases. (#3709)</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_19", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.9.0.zip SHA256 <code>a2d124cfb2937ec5efc8f85123dbcfeba177fb778762da506bfc5a9665ed9e57</code>  * v1.9.0.tar.gz SHA256 <code>59b6088b69ca6cf278c3f9de5cd6b7f3fd604212cd1c59870bc531c54147e889</code></p>"}, {"location": "Librdkafka-Changelog/#162-2021-11-25", "title": "1.6.2 (2021-11-25)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v162", "title": "librdkafka v1.6.2", "text": "<p>librdkafka v1.6.2 is a maintenance release with the following backported fixes:</p> <ul> <li>Upon quick repeated leader changes the transactional producer could receive    an <code>OUT_OF_ORDER_SEQUENCE</code> error from the broker, which triggered an    Epoch bump on the producer resulting in an InitProducerIdRequest being sent    to the transaction coordinator in the middle of a transaction.    This request would start a new transaction on the coordinator, but the    producer would still think (erroneously) it was in the current transaction.    Any messages produced in the current transaction prior to this event would    be silently lost when the application committed the transaction, leading    to message loss.    To avoid message loss a fatal error is now raised.    This fix is specific to v1.6.x. librdkafka v1.8.x implements a recoverable    error state instead. #3575.</li> <li>The transactional producer could stall during a transaction if the transaction    coordinator changed while adding offsets to the transaction (send_offsets_to_transaction()).    This stall lasted until the coordinator connection went down, the    transaction timed out, transaction was aborted, or messages were produced    to a new partition, whichever came first. #3571.</li> <li>librdkafka's internal timers would not start if the timeout was set to 0,    which would result in some timeout operations not being enforced correctly,    e.g., the transactional producer API timeouts.    These timers are now started with a timeout of 1 microsecond.</li> <li>Force address resolution if the broker epoch changes (#3238).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_20", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.6.2.zip SHA256 <code>1d389a98bda374483a7b08ff5ff39708f5a923e5add88b80b71b078cb2d0c92e</code>  * v1.6.2.tar.gz SHA256 <code>b9be26c632265a7db2fdd5ab439f2583d14be08ab44dc2e33138323af60c39db</code></p>"}, {"location": "Librdkafka-Changelog/#182-2021-10-18", "title": "1.8.2 (2021-10-18)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v182", "title": "librdkafka v1.8.2", "text": "<p>librdkafka v1.8.2 is a maintenance release.</p>"}, {"location": "Librdkafka-Changelog/#enhancements_5", "title": "Enhancements", "text": "<ul> <li>Added <code>ssl.ca.pem</code> to add CA certificate by PEM string. (#2380)</li> <li>Prebuilt binaries for Mac OSX now contain statically linked OpenSSL v1.1.1l.    Previously the OpenSSL version was either v1.1.1 or v1.0.2 depending on    build type.</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_15", "title": "Fixes", "text": "<ul> <li>The <code>librdkafka.redist</code> 1.8.0 package had two flaws:</li> <li>the linux-arm64 .so build was a linux-x64 build.</li> <li>the included Windows MSVC 140 runtimes for x64 were infact x86.    The release script has been updated to verify the architectures of    provided artifacts to avoid this happening in the future.</li> <li>Prebuilt binaries for Mac OSX Sierra (10.12) and older are no longer provided.    This affects confluent-kafka-go.</li> <li>Some of the prebuilt binaries for Linux were built on Ubuntu 14.04,    these builds are now performed on Ubuntu 16.04 instead.    This may affect users on ancient Linux distributions.</li> <li>It was not possible to configure <code>ssl.ca.location</code> on OSX, the property    would automatically revert back to <code>probe</code> (default value).    This regression was introduced in v1.8.0. (#3566)</li> <li>librdkafka's internal timers would not start if the timeout was set to 0,    which would result in some timeout operations not being enforced correctly,    e.g., the transactional producer API timeouts.    These timers are now started with a timeout of 1 microsecond.</li> </ul>"}, {"location": "Librdkafka-Changelog/#transactional-producer-fixes_2", "title": "Transactional producer fixes", "text": "<ul> <li>Upon quick repeated leader changes the transactional producer could receive    an <code>OUT_OF_ORDER_SEQUENCE</code> error from the broker, which triggered an    Epoch bump on the producer resulting in an InitProducerIdRequest being sent    to the transaction coordinator in the middle of a transaction.    This request would start a new transaction on the coordinator, but the    producer would still think (erroneously) it was in current transaction.    Any messages produced in the current transaction prior to this event would    be silently lost when the application committed the transaction, leading    to message loss.    This has been fixed by setting the Abortable transaction error state    in the producer. #3575.</li> <li>The transactional producer could stall during a transaction if the transaction    coordinator changed while adding offsets to the transaction (send_offsets_to_transaction()).    This stall lasted until the coordinator connection went down, the    transaction timed out, transaction was aborted, or messages were produced    to a new partition, whichever came first. #3571.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_21", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.8.2.zip SHA256 <code>8b03d8b650f102f3a6a6cff6eedc29b9e2f68df9ba7e3c0f3fb00838cce794b8</code>  * v1.8.2.tar.gz SHA256 <code>6a747d293a7a4613bd2897e28e8791476fbe1ae7361f2530a876e0fd483482a6</code></p> <p>Note: there was no v1.8.1 librdkafka release</p>"}, {"location": "Librdkafka-Changelog/#180-2021-09-16", "title": "1.8.0 (2021-09-16)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v180", "title": "librdkafka v1.8.0", "text": "<p>librdkafka v1.8.0 is a security release:</p> <ul> <li>Upgrade bundled zlib version from 1.2.8 to 1.2.11 in the <code>librdkafka.redist</code>    NuGet package. The updated zlib version fixes CVEs:    CVE-2016-9840, CVE-2016-9841, CVE-2016-9842, CVE-2016-9843    See https://github.com/edenhill/librdkafka/issues/2934 for more information.</li> <li>librdkafka now uses vcpkg for up-to-date Windows    dependencies in the <code>librdkafka.redist</code> NuGet package:    OpenSSL 1.1.1l, zlib 1.2.11, zstd 1.5.0.</li> <li>The upstream dependency (OpenSSL, zstd, zlib) source archive checksums are    now verified when building with <code>./configure --install-deps</code>.    These builds are used by the librdkafka builds bundled with    confluent-kafka-go, confluent-kafka-python and confluent-kafka-dotnet.</li> </ul>"}, {"location": "Librdkafka-Changelog/#enhancements_6", "title": "Enhancements", "text": "<ul> <li>Producer <code>flush()</code> now overrides the <code>linger.ms</code> setting for the duration    of the <code>flush()</code> call, effectively triggering immediate transmission of    queued messages. (#3489)</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_16", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_11", "title": "General fixes", "text": "<ul> <li>Correctly detect presence of zlib via compilation check. (Chris Novakovic)</li> <li><code>ERR__ALL_BROKERS_DOWN</code> is no longer emitted when the coordinator    connection goes down, only when all standard named brokers have been tried.    This fixes the issue with <code>ERR__ALL_BROKERS_DOWN</code> being triggered on    <code>consumer_close()</code>. It is also now only emitted if the connection was fully    up (past handshake), and not just connected.</li> <li><code>rd_kafka_query_watermark_offsets()</code>, <code>rd_kafka_offsets_for_times()</code>,    <code>consumer_lag</code> metric, and <code>auto.offset.reset</code> now honour    <code>isolation.level</code> and will return the Last Stable Offset (LSO)    when <code>isolation.level</code> is set to <code>read_committed</code> (default), rather than    the uncommitted high-watermark when it is set to <code>read_uncommitted</code>. (#3423)</li> <li>SASL GSSAPI is now usable when <code>sasl.kerberos.min.time.before.relogin</code>    is set to 0 - which disables ticket refreshes (by @mpekalski, #3431).</li> <li>Rename internal crc32c() symbol to rd_crc32c() to avoid conflict with    other static libraries (#3421).</li> <li><code>txidle</code> and <code>rxidle</code> in the statistics object was emitted as 18446744073709551615 when no idle was known. -1 is now emitted instead. (#3519)</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_12", "title": "Consumer fixes", "text": "<ul> <li>Automatically retry offset commits on <code>ERR_REQUEST_TIMED_OUT</code>,    <code>ERR_COORDINATOR_NOT_AVAILABLE</code>, and <code>ERR_NOT_COORDINATOR</code> (#3398).    Offset commits will be retried twice.</li> <li>Timed auto commits did not work when only using assign() and not subscribe().    This regression was introduced in v1.7.0.</li> <li>If the topics matching the current subscription changed (or the application    updated the subscription) while there was an outstanding JoinGroup or    SyncGroup request, an additional request would sometimes be sent before    handling the response of the first. This in turn lead to internal state    issues that could cause a crash or malbehaviour.    The consumer will now wait for any outstanding JoinGroup or SyncGroup    responses before re-joining the group.</li> <li><code>auto.offset.reset</code> could previously be triggered by temporary errors,    such as disconnects and timeouts (after the two retries are exhausted).    This is now fixed so that the auto offset reset policy is only triggered    for permanent errors.</li> <li>The error that triggers <code>auto.offset.reset</code> is now logged to help the    application owner identify the reason of the reset.</li> <li>If a rebalance takes longer than a consumer's <code>session.timeout.ms</code>, the    consumer will remain in the group as long as it receives heartbeat responses    from the broker.</li> </ul>"}, {"location": "Librdkafka-Changelog/#admin-fixes", "title": "Admin fixes", "text": "<ul> <li><code>DeleteRecords()</code> could crash if one of the underlying requests    (for a given partition leader) failed at the transport level (e.g., timeout).    (#3476).</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_22", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.8.0.zip SHA256 <code>4b173f759ea5fdbc849fdad00d3a836b973f76cbd3aa8333290f0398fd07a1c4</code>  * v1.8.0.tar.gz SHA256 <code>93b12f554fa1c8393ce49ab52812a5f63e264d9af6a50fd6e6c318c481838b7f</code></p>"}, {"location": "Librdkafka-Changelog/#170-2021-05-10", "title": "1.7.0 (2021-05-10)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v170", "title": "librdkafka v1.7.0", "text": "<p>librdkafka v1.7.0 is feature release:</p> <ul> <li>KIP-360 - Improve reliability of transactional producer.    Requires Apache Kafka 2.5 or later.</li> <li>OpenSSL Engine support (<code>ssl.engine.location</code>) by @adinigam and @ajbarb.</li> </ul>"}, {"location": "Librdkafka-Changelog/#enhancements_7", "title": "Enhancements", "text": "<ul> <li>Added <code>connections.max.idle.ms</code> to automatically close idle broker    connections.    This feature is disabled by default unless <code>bootstrap.servers</code> contains    the string <code>azure</code> in which case the default is set to &lt;4 minutes to improve    connection reliability and circumvent limitations with the Azure load    balancers (see #3109 for more information).</li> <li>Bumped to OpenSSL 1.1.1k in binary librdkafka artifacts.</li> <li>The binary librdkafka artifacts for Alpine are now using Alpine 3.12.</li> <li>Improved static librdkafka Windows builds using MinGW (@neptoess, #3130).</li> </ul>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_5", "title": "Upgrade considerations", "text": "<ul> <li>The C++ <code>oauthbearer_token_refresh_cb()</code> was missing a <code>Handle *</code>    argument that has now been added. This is a breaking change but the original    function signature is considered a bug.    This change only affects C++ OAuth developers.</li> <li>KIP-735 The consumer <code>session.timeout.ms</code>    default was changed from 10 to 45 seconds to make consumer groups more    robust and less sensitive to temporary network and cluster issues.</li> <li>Statistics: <code>consumer_lag</code> is now using the <code>committed_offset</code>,    while the new <code>consumer_lag_stored</code> is using <code>stored_offset</code>    (offset to be committed).    This is more correct than the previous <code>consumer_lag</code> which was using    either <code>committed_offset</code> or <code>app_offset</code> (last message passed    to application).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_17", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_12", "title": "General fixes", "text": "<ul> <li>Fix accesses to freed metadata cache mutexes on client termination (#3279)</li> <li>There was a race condition on receiving updated metadata where a broker id    update (such as bootstrap to proper broker transformation) could finish after    the topic metadata cache was updated, leading to existing brokers seemingly    being not available.    One occurrence of this issue was query_watermark_offsets() that could return    <code>ERR__UNKNOWN_PARTITION</code> for existing partitions shortly after the    client instance was created.</li> <li>The OpenSSL context is now initialized with <code>TLS_client_method()</code>    (on OpenSSL &gt;= 1.1.0) instead of the deprecated and outdated    <code>SSLv23_client_method()</code>.</li> <li>The initial cluster connection on client instance creation could sometimes    be delayed up to 1 second if a <code>group.id</code> or <code>transactional.id</code>    was configured (#3305).</li> <li>Speed up triggering of new broker connections in certain cases by exiting    the broker thread io/op poll loop when a wakeup op is received.</li> <li>SASL GSSAPI: The Kerberos kinit refresh command was triggered from    <code>rd_kafka_new()</code> which made this call blocking if the refresh command    was taking long. The refresh is now performed by the background rdkafka    main thread.</li> <li>Fix busy-loop (100% CPU on the broker threads) during the handshake phase    of an SSL connection.</li> <li>Disconnects during SSL handshake are now propagated as transport errors    rather than SSL errors, since these disconnects are at the transport level    (e.g., incorrect listener, flaky load balancer, etc) and not due to SSL    issues.</li> <li>Increment metadata fast refresh interval backoff exponentially (@ajbarb, #3237).</li> <li>Unthrottled requests are no longer counted in the <code>brokers[].throttle</code>    statistics object.</li> <li>Log CONFWARN warning when global topic configuration properties    are overwritten by explicitly setting a <code>default_topic_conf</code>.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_13", "title": "Consumer fixes", "text": "<ul> <li>If a rebalance happened during a <code>consume_batch..()</code> call the already    accumulated messages for revoked partitions were not purged, which would    pass messages to the application for partitions that were no longer owned    by the consumer. Fixed by @jliunyu. #3340.</li> <li>Fix balancing and reassignment issues with the cooperative-sticky assignor.    #3306.</li> <li>Fix incorrect detection of first rebalance in sticky assignor (@hallfox).</li> <li>Aborted transactions with no messages produced to a partition could    cause further successfully committed messages in the same Fetch response to    be ignored, resulting in consumer-side message loss.    A log message along the lines <code>Abort txn ctrl msg bad order at offset    7501: expected before or at 7702: messages in aborted transactions may be delivered to the application</code>    would be seen.    This is a rare occurrence where a transactional producer would register with    the partition but not produce any messages before aborting the transaction.</li> <li>The consumer group deemed cached metadata up to date by checking    <code>topic.metadata.refresh.interval.ms</code>: if this property was set too low    it would cause cached metadata to be unusable and new metadata to be fetched,    which could delay the time it took for a rebalance to settle.    It now correctly uses <code>metadata.max.age.ms</code> instead.</li> <li>The consumer group timed auto commit would attempt commits during rebalances,    which could result in \"Illegal generation\" errors. This is now fixed, the    timed auto committer is only employed in the steady state when no rebalances    are taking places. Offsets are still auto committed when partitions are    revoked.</li> <li>Retriable FindCoordinatorRequest errors are no longer propagated to    the application as they are retried automatically.</li> <li>Fix rare crash (assert <code>rktp_started</code>) on consumer termination    (introduced in v1.6.0).</li> <li>Fix unaligned access and possibly corrupted snappy decompression when    building with MSVC (@azat)</li> <li>A consumer configured with the <code>cooperative-sticky</code> assignor did    not actively Leave the group on unsubscribe(). This delayed the    rebalance for the remaining group members by up to <code>session.timeout.ms</code>.</li> <li>The current subscription list was sometimes leaked when unsubscribing.</li> </ul>"}, {"location": "Librdkafka-Changelog/#producer-fixes_2", "title": "Producer fixes", "text": "<ul> <li>The timeout value of <code>flush()</code> was not respected when delivery reports    were scheduled as events (such as for confluent-kafka-go) rather than    callbacks.</li> <li>There was a race conditition in <code>purge()</code> which could cause newly    created partition objects, or partitions that were changing leaders, to    not have their message queues purged. This could cause    <code>abort_transaction()</code> to time out. This issue is now fixed.</li> <li>In certain high-thruput produce rate patterns producing could stall for    1 second, regardless of <code>linger.ms</code>, due to rate-limiting of internal    queue wakeups. This is now fixed by not rate-limiting queue wakeups but    instead limiting them to one wakeup per queue reader poll. #2912.</li> </ul>"}, {"location": "Librdkafka-Changelog/#transactional-producer-fixes_3", "title": "Transactional Producer fixes", "text": "<ul> <li>KIP-360: Fatal Idempotent producer errors are now recoverable by the    transactional producer and will raise a <code>txn_requires_abort()</code> error.</li> <li>If the cluster went down between <code>produce()</code> and <code>commit_transaction()</code>    and before any partitions had been registered with the coordinator, the    messages would time out but the commit would succeed because nothing    had been sent to the coordinator. This is now fixed.</li> <li>If the current transaction failed while <code>commit_transaction()</code> was    checking the current transaction state an invalid state transaction could    occur which in turn would trigger a assertion crash.    This issue showed up as \"Invalid txn state transition: ..\" crashes, and is    now fixed by properly synchronizing both checking and transition of state.</li> </ul>"}, {"location": "Librdkafka-Changelog/#161-2021-02-24", "title": "1.6.1 (2021-02-24)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v161", "title": "librdkafka v1.6.1", "text": "<p>librdkafka v1.6.1 is a maintenance release.</p>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_6", "title": "Upgrade considerations", "text": "<ul> <li>Fatal idempotent producer errors are now also fatal to the transactional    producer. This is a necessary step to maintain data integrity prior to    librdkafka supporting KIP-360. Applications should check any transactional    API errors for the is_fatal flag and decommission the transactional producer    if the flag is set.</li> <li>The consumer error raised by <code>auto.offset.reset=error</code> now has error-code    set to <code>ERR__AUTO_OFFSET_RESET</code> to allow an application to differentiate    between auto offset resets and other consumer errors.</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_18", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_13", "title": "General fixes", "text": "<ul> <li>Admin API and transactional <code>send_offsets_to_transaction()</code> coordinator    requests, such as TxnOffsetCommitRequest, could in rare cases be sent    multiple times which could cause a crash.</li> <li><code>ssl.ca.location=probe</code> is now enabled by default on Mac OSX since the    librdkafka-bundled OpenSSL might not have the same default CA search paths    as the system or brew installed OpenSSL. Probing scans all known locations.</li> </ul>"}, {"location": "Librdkafka-Changelog/#transactional-producer-fixes_4", "title": "Transactional Producer fixes", "text": "<ul> <li>Fatal idempotent producer errors are now also fatal to the transactional    producer.</li> <li>The transactional producer could crash if the transaction failed while    <code>send_offsets_to_transaction()</code> was called.</li> <li>Group coordinator requests for transactional    <code>send_offsets_to_transaction()</code> calls would leak memory if the    underlying request was attempted to be sent after the transaction had    failed.</li> <li>When gradually producing to multiple partitions (resulting in multiple    underlying AddPartitionsToTxnRequests) sub-sequent partitions could get    stuck in pending state under certain conditions. These pending partitions    would not send queued messages to the broker and eventually trigger    message timeouts, failing the current transaction. This is now fixed.</li> <li>Committing an empty transaction (no messages were produced and no    offsets were sent) would previously raise a fatal error due to invalid state    on the transaction coordinator. We now allow empty/no-op transactions to    be committed.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_14", "title": "Consumer fixes", "text": "<ul> <li>The consumer will now retry indefinitely (or until the assignment is changed)    to retrieve committed offsets. This fixes the issue where only two retries    were attempted when outstanding transactions were blocking OffsetFetch    requests with <code>ERR_UNSTABLE_OFFSET_COMMIT</code>. #3265</li> </ul>"}, {"location": "Librdkafka-Changelog/#160-2021-01-26", "title": "1.6.0 (2021-01-26)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v160", "title": "librdkafka v1.6.0", "text": "<p>librdkafka v1.6.0 is feature release:</p> <ul> <li>KIP-429 Incremental rebalancing with sticky consumer group partition assignor (KIP-54) (by @mhowlett).</li> <li>KIP-480 Sticky producer partitioning (<code>sticky.partitioning.linger.ms</code>) - achieves higher throughput and lower latency through sticky selection of random partition (by @abbycriswell).</li> <li>AdminAPI: Add support for <code>DeleteRecords()</code>, <code>DeleteGroups()</code> and <code>DeleteConsumerGroupOffsets()</code> (by @gridaphobe)</li> <li>KIP-447 Producer scalability for exactly once semantics - allows a single transactional producer to be used for multiple input partitions. Requires Apache Kafka 2.5 or later.</li> <li>Transactional producer fixes and improvements, see Transactional Producer fixes below.</li> <li>The librdkafka.redist NuGet package now supports Linux ARM64/Aarch64.</li> </ul>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_7", "title": "Upgrade considerations", "text": "<ul> <li>Sticky producer partitioning (<code>sticky.partitioning.linger.ms</code>) is    enabled by default (10 milliseconds) which affects the distribution of    randomly partitioned messages, where previously these messages would be    evenly distributed over the available partitions they are now partitioned    to a single partition for the duration of the sticky time    (10 milliseconds by default) before a new random sticky partition    is selected.</li> <li>The new KIP-447 transactional producer scalability guarantees are only    supported on Apache Kafka 2.5 or later, on earlier releases you will    need to use one producer per input partition for EOS. This limitation    is not enforced by the producer or broker.</li> <li>Error handling for the transactional producer has been improved, see    the Transactional Producer fixes below for more information.</li> </ul>"}, {"location": "Librdkafka-Changelog/#known-issues_2", "title": "Known issues", "text": "<ul> <li>The Transactional Producer's API timeout handling is inconsistent with the    underlying protocol requests, it is therefore strongly recommended that    applications call <code>rd_kafka_commit_transaction()</code> and    <code>rd_kafka_abort_transaction()</code> with the <code>timeout_ms</code> parameter    set to <code>-1</code>, which will use the remaining transaction timeout.</li> </ul>"}, {"location": "Librdkafka-Changelog/#enhancements_8", "title": "Enhancements", "text": "<ul> <li>KIP-107, KIP-204: AdminAPI: Added <code>DeleteRecords()</code> (by @gridaphobe).</li> <li>KIP-229: AdminAPI: Added <code>DeleteGroups()</code> (by @gridaphobe).</li> <li>KIP-496: AdminAPI: Added <code>DeleteConsumerGroupOffsets()</code>.</li> <li>KIP-464: AdminAPI: Added support for broker-side default partition count    and replication factor for <code>CreateTopics()</code>.</li> <li>Windows: Added <code>ssl.ca.certificate.stores</code> to specify a list of    Windows Certificate Stores to read CA certificates from, e.g.,    <code>CA,Root</code>. <code>Root</code> remains the default store.</li> <li>Use reentrant <code>rand_r()</code> on supporting platforms which decreases lock    contention (@azat).</li> <li>Added <code>assignor</code> debug context for troubleshooting consumer partition    assignments.</li> <li>Updated to OpenSSL v1.1.1i when building dependencies.</li> <li>Update bundled lz4 (used when <code>./configure --disable-lz4-ext</code>) to v1.9.3    which has vast performance improvements.</li> <li>Added <code>rd_kafka_conf_get_default_topic_conf()</code> to retrieve the    default topic configuration object from a global configuration object.</li> <li>Added <code>conf</code> debugging context to <code>debug</code> - shows set configuration    properties on client and topic instantiation. Sensitive properties    are redacted.</li> <li>Added <code>rd_kafka_queue_yield()</code> to cancel a blocking queue call.</li> <li>Will now log a warning when multiple ClusterIds are seen, which is an    indication that the client might be erroneously configured to connect to    multiple clusters which is not supported.</li> <li>Added <code>rd_kafka_seek_partitions()</code> to seek multiple partitions to    per-partition specific offsets.</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_19", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_14", "title": "General fixes", "text": "<ul> <li>Fix a use-after-free crash when certain coordinator requests were retried.</li> <li>The C++ <code>oauthbearer_set_token()</code> function would call <code>free()</code> on    a <code>new</code>-created pointer, possibly leading to crashes or heap corruption (#3194)</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_15", "title": "Consumer fixes", "text": "<ul> <li>The consumer assignment and consumer group implementations have been    decoupled, simplified and made more strict and robust. This will sort out    a number of edge cases for the consumer where the behaviour was previously    undefined.</li> <li>Partition fetch state was not set to STOPPED if OffsetCommit failed.</li> <li>The session timeout is now enforced locally also when the coordinator    connection is down, which was not previously the case.</li> </ul>"}, {"location": "Librdkafka-Changelog/#transactional-producer-fixes_5", "title": "Transactional Producer fixes", "text": "<ul> <li>Transaction commit or abort failures on the broker, such as when the    producer was fenced by a newer instance, were not propagated to the    application resulting in failed commits seeming successful.    This was a critical race condition for applications that had a delay after    producing messages (or sendings offsets) before committing or    aborting the transaction. This issue has now been fixed and test coverage    improved.</li> <li>The transactional producer API would return <code>RD_KAFKA_RESP_ERR__STATE</code>    when API calls were attempted after the transaction had failed, we now    try to return the error that caused the transaction to fail in the first    place, such as <code>RD_KAFKA_RESP_ERR__FENCED</code> when the producer has    been fenced, or <code>RD_KAFKA_RESP_ERR__TIMED_OUT</code> when the transaction    has timed out.</li> <li>Transactional producer retry count for transactional control protocol    requests has been increased from 3 to infinite, retriable errors    are now automatically retried by the producer until success or the    transaction timeout is exceeded. This fixes the case where    <code>rd_kafka_send_offsets_to_transaction()</code> would fail the current    transaction into an abortable state when <code>CONCURRENT_TRANSACTIONS</code> was    returned by the broker (which is a transient error) and the 3 retries    were exhausted.</li> </ul>"}, {"location": "Librdkafka-Changelog/#producer-fixes_3", "title": "Producer fixes", "text": "<ul> <li>Calling <code>rd_kafka_topic_new()</code> with a topic config object with    <code>message.timeout.ms</code> set could sometimes adjust the global <code>linger.ms</code>    property (if not explicitly configured) which was not desired, this is now    fixed and the auto adjustment is only done based on the    <code>default_topic_conf</code> at producer creation.</li> <li><code>rd_kafka_flush()</code> could previously return <code>RD_KAFKA_RESP_ERR__TIMED_OUT</code>    just as the timeout was reached if the messages had been flushed but    there were now no more messages. This has been fixed.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_23", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.6.0.zip SHA256 <code>af6f301a1c35abb8ad2bb0bab0e8919957be26c03a9a10f833c8f97d6c405aa8</code>  * v1.6.0.tar.gz SHA256 <code>3130cbd391ef683dc9acf9f83fe82ff93b8730a1a34d0518e93c250929be9f6b</code></p>"}, {"location": "Librdkafka-Changelog/#153-2020-12-09", "title": "1.5.3 (2020-12-09)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v153", "title": "librdkafka v1.5.3", "text": "<p>librdkafka v1.5.3 is a maintenance release.</p>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_8", "title": "Upgrade considerations", "text": "<ul> <li>CentOS 6 is now EOL and is no longer included in binary librdkafka packages,    such as NuGet.</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_20", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_15", "title": "General fixes", "text": "<ul> <li>Fix a use-after-free crash when certain coordinator requests were retried.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_16", "title": "Consumer fixes", "text": "<ul> <li>Consumer would not filter out messages for aborted transactions    if the messages were compressed (#3020).</li> <li>Consumer destroy without prior <code>close()</code> could hang in certain    cgrp states (@gridaphobe, #3127).</li> <li>Fix possible null dereference in <code>Message::errstr()</code> (#3140).</li> <li>The <code>roundrobin</code> partition assignment strategy could get stuck in an    endless loop or generate uneven assignments in case the group members    had asymmetric subscriptions (e.g., c1 subscribes to t1,t2 while c2    subscribes to t2,t3).  (#3159)</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_24", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.5.3.zip SHA256 <code>3f24271232a42f2d5ac8aab3ab1a5ddbf305f9a1ae223c840d17c221d12fe4c1</code>  * v1.5.3.tar.gz SHA256 <code>2105ca01fef5beca10c9f010bc50342b15d5ce6b73b2489b012e6d09a008b7bf</code></p>"}, {"location": "Librdkafka-Changelog/#152-2020-10-20", "title": "1.5.2 (2020-10-20)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v152", "title": "librdkafka v1.5.2", "text": "<p>librdkafka v1.5.2 is a maintenance release.</p>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_9", "title": "Upgrade considerations", "text": "<ul> <li>The default value for the producer configuration property <code>retries</code> has    been increased from 2 to infinity, effectively limiting Produce retries to    only <code>message.timeout.ms</code>.    As the reasons for the automatic internal retries vary (various broker error    codes as well as transport layer issues), it doesn't make much sense to limit    the number of retries for retriable errors, but instead only limit the    retries based on the allowed time to produce a message.</li> <li>The default value for the producer configuration property    <code>request.timeout.ms</code> has been increased from 5 to 30 seconds to match    the Apache Kafka Java producer default.    This change yields increased robustness for broker-side congestion.</li> </ul>"}, {"location": "Librdkafka-Changelog/#enhancements_9", "title": "Enhancements", "text": "<ul> <li>The generated <code>CONFIGURATION.md</code> (through <code>rd_kafka_conf_properties_show())</code>)    now include all properties and values, regardless if they were included in    the build, and setting a disabled property or value through    <code>rd_kafka_conf_set()</code> now returns <code>RD_KAFKA_CONF_INVALID</code> and provides    a more useful error string saying why the property can't be set.</li> <li>Consumer configs on producers and vice versa will now be logged with    warning messages on client instantiation.</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_21", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#security-fixes", "title": "Security fixes", "text": "<ul> <li>There was an incorrect call to zlib's <code>inflateGetHeader()</code> with    unitialized memory pointers that could lead to the GZIP header of a fetched    message batch to be copied to arbitrary memory.    This function call has now been completely removed since the result was    not used.    Reported by Ilja van Sprundel.</li> </ul>"}, {"location": "Librdkafka-Changelog/#general-fixes_16", "title": "General fixes", "text": "<ul> <li><code>rd_kafka_topic_opaque()</code> (used by the C++ API) would cause object    refcounting issues when used on light-weight (error-only) topic objects    such as consumer errors (#2693).</li> <li>Handle name resolution failures when formatting IP addresses in error logs,    and increase printed hostname limit to ~256 bytes (was ~60).</li> <li>Broker sockets would be closed twice (thus leading to potential race    condition with fd-reuse in other threads) if a custom <code>socket_cb</code> would    return error.</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_17", "title": "Consumer fixes", "text": "<ul> <li>The <code>roundrobin</code> <code>partition.assignment.strategy</code> could crash (assert)    for certain combinations of members and partitions.    This is a regression in v1.5.0. (#3024)</li> <li>The C++ <code>KafkaConsumer</code> destructor did not destroy the underlying    C <code>rd_kafka_t</code> instance, causing a leak if <code>close()</code> was not used.</li> <li>Expose rich error strings for C++ Consumer <code>Message-&gt;errstr()</code>.</li> <li>The consumer could get stuck if an outstanding commit failed during    rebalancing (#2933).</li> <li>Topic authorization errors during fetching are now reported only once (#3072).</li> </ul>"}, {"location": "Librdkafka-Changelog/#producer-fixes_4", "title": "Producer fixes", "text": "<ul> <li>Topic authorization errors are now properly propagated for produced messages,    both through delivery reports and as <code>ERR_TOPIC_AUTHORIZATION_FAILED</code>    return value from <code>produce*()</code> (#2215)</li> <li>Treat cluster authentication failures as fatal in the transactional    producer (#2994).</li> <li>The transactional producer code did not properly reference-count partition    objects which could in very rare circumstances lead to a use-after-free bug    if a topic was deleted from the cluster when a transaction was using it.</li> <li><code>ERR_KAFKA_STORAGE_ERROR</code> is now correctly treated as a retriable    produce error (#3026).</li> <li>Messages that timed out locally would not fail the ongoing transaction.    If the application did not take action on failed messages in its delivery    report callback and went on to commit the transaction, the transaction would    be successfully committed, simply omitting the failed messages.</li> <li>EndTxnRequests (sent on commit/abort) are only retried in allowed    states (#3041).    Previously the transaction could hang on commit_transaction() if an abortable    error was hit and the EndTxnRequest was to be retried.</li> </ul> <p>Note: there was no v1.5.1 librdkafka release</p>"}, {"location": "Librdkafka-Changelog/#checksums_25", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.5.2.zip SHA256 <code>de70ebdb74c7ef8c913e9a555e6985bcd4b96eb0c8904572f3c578808e0992e1</code>  * v1.5.2.tar.gz SHA256 <code>ca3db90d04ef81ca791e55e9eed67e004b547b7adedf11df6c24ac377d4840c6</code></p>"}, {"location": "Librdkafka-Changelog/#150-2020-07-20", "title": "1.5.0 (2020-07-20)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v150", "title": "librdkafka v1.5.0", "text": "<p>The v1.5.0 release brings usability improvements, enhancements and fixes to librdkafka.</p>"}, {"location": "Librdkafka-Changelog/#enhancements_10", "title": "Enhancements", "text": "<ul> <li>Improved broker connection error reporting with more useful information and    hints on the cause of the problem.</li> <li>Consumer: Propagate errors when subscribing to unavailable topics (#1540)</li> <li>Producer: Add <code>batch.size</code> producer configuration property (#638)</li> <li>Add <code>topic.metadata.propagation.max.ms</code> to allow newly manually created    topics to be propagated throughout the cluster before reporting them    as non-existent. This fixes race issues where CreateTopics() is    quickly followed by produce().</li> <li>Prefer least idle connection for periodic metadata refreshes, et.al.,    to allow truly idle connections to time out and to avoid load-balancer-killed    idle connection errors (#2845)</li> <li>Added <code>rd_kafka_event_debug_contexts()</code> to get the debug contexts for    a debug log line (by @wolfchimneyrock).</li> <li>Added Test scenarios which define the cluster configuration.</li> <li>Added MinGW-w64 builds (@ed-alertedh, #2553)</li> <li><code>./configure --enable-XYZ</code> now requires the XYZ check to pass,    and <code>--disable-XYZ</code> disables the feature altogether (@benesch)</li> <li>Added <code>rd_kafka_produceva()</code> which takes an array of produce arguments    for situations where the existing <code>rd_kafka_producev()</code> va-arg approach    can't be used.</li> <li>Added <code>rd_kafka_message_broker_id()</code> to see the broker that a message    was produced or fetched from, or an error was associated with.</li> <li>Added RTT/delay simulation to mock brokers.</li> </ul>"}, {"location": "Librdkafka-Changelog/#upgrade-considerations_10", "title": "Upgrade considerations", "text": "<ul> <li>Subscribing to non-existent and unauthorized topics will now propagate    errors <code>RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART</code> and    <code>RD_KAFKA_RESP_ERR_TOPIC_AUTHORIZATION_FAILED</code> to the application through    the standard consumer error (the err field in the message object).</li> <li>Consumer will no longer trigger auto creation of topics,    <code>allow.auto.create.topics=true</code> may be used to re-enable the old deprecated    functionality.</li> <li>The default consumer pre-fetch queue threshold <code>queued.max.messages.kbytes</code>    has been decreased from 1GB to 64MB to avoid excessive network usage for low    and medium throughput consumer applications. High throughput consumer    applications may need to manually set this property to a higher value.</li> <li>The default consumer Fetch wait time has been increased from 100ms to 500ms    to avoid excessive network usage for low throughput topics.</li> <li>If OpenSSL is linked statically, or <code>ssl.ca.location=probe</code> is configured,    librdkafka will probe known CA certificate paths and automatically use the    first one found. This should alleviate the need to configure    <code>ssl.ca.location</code> when the statically linked OpenSSL's OPENSSLDIR differs    from the system's CA certificate path.</li> <li>The heuristics for handling Apache Kafka &lt; 0.10 brokers has been removed to    improve connection error handling for modern Kafka versions.    Users on Brokers 0.9.x or older should already be configuring    <code>api.version.request=false</code> and <code>broker.version.fallback=...</code> so there    should be no functional change.</li> <li>The default producer batch accumulation time, <code>linger.ms</code>, has been changed    from 0.5ms to 5ms to improve batch sizes and throughput while reducing    the per-message protocol overhead.    Applications that require lower produce latency than 5ms will need to    manually set <code>linger.ms</code> to a lower value.</li> <li>librdkafka's build tooling now requires Python 3.x (python3 interpreter).</li> </ul>"}, {"location": "Librdkafka-Changelog/#fixes_22", "title": "Fixes", "text": ""}, {"location": "Librdkafka-Changelog/#general-fixes_17", "title": "General fixes", "text": "<ul> <li>The client could crash in rare circumstances on ApiVersion or    SaslHandshake request timeouts (#2326)</li> <li><code>./configure --LDFLAGS='a=b, c=d'</code> with arguments containing = are now    supported (by @sky92zwq).</li> <li><code>./configure</code> arguments now take precedence over cached <code>configure</code> variables    from previous invocation.</li> <li>Fix theoretical crash on coord request failure.</li> <li>Unknown partition error could be triggered for existing partitions when    additional partitions were added to a topic (@benesch, #2915)</li> <li>Quickly refresh topic metadata for desired but non-existent partitions.    This will speed up the initial discovery delay when new partitions are added    to an existing topic (#2917).</li> </ul>"}, {"location": "Librdkafka-Changelog/#consumer-fixes_18", "title": "Consumer fixes", "text": "<ul> <li>The roundrobin partition assignor could crash if subscriptions    where asymmetrical (different sets from different members of the group).    Thanks to @ankon and @wilmai for identifying the root cause (#2121).</li> <li>The consumer assignors could ignore some topics if there were more subscribed    topics than consumers in taking part in the assignment.</li> <li>The consumer would connect to all partition leaders of a topic even    for partitions that were not being consumed (#2826).</li> <li>Initial consumer group joins should now be a couple of seconds quicker    thanks expedited query intervals (@benesch).</li> <li>Fix crash and/or inconsistent subscriptions when using multiple consumers    (in the same process) with wildcard topics on Windows.</li> <li>Don't propagate temporary offset lookup errors to application.</li> <li>Immediately refresh topic metadata when partitions are reassigned to other    brokers, avoiding a fetch stall of up to <code>topic.metadata.refresh.interval.ms</code>. (#2955)</li> <li>Memory for batches containing control messages would not be freed when    using the batch consume APIs (@pf-qiu, #2990).</li> </ul>"}, {"location": "Librdkafka-Changelog/#producer-fixes_5", "title": "Producer fixes", "text": "<ul> <li>Proper locking for transaction state in EndTxn handler.</li> </ul>"}, {"location": "Librdkafka-Changelog/#checksums_26", "title": "Checksums", "text": "<p>Release asset checksums:  * v1.5.0.zip SHA256 <code>76a1e83d643405dd1c0e3e62c7872b74e3a96c52be910233e8ec02d501fa33c8</code>  * v1.5.0.tar.gz SHA256 <code>f7fee59fdbf1286ec23ef0b35b2dfb41031c8727c90ced6435b8cf576f23a656</code></p>"}, {"location": "Librdkafka-Changelog/#144-2020-06-20", "title": "1.4.4 (2020-06-20)", "text": ""}, {"location": "Librdkafka-Changelog/#librdkafka-v144", "title": "librdkafka v1.4.4", "text": "<p>v1.4.4 is a maintenance release with the following fixes and enhancements:</p> <ul> <li>Transactional producer could crash on request timeout due to dereferencing    NULL pointer of non-existent response object.</li> <li>Mark <code>rd_kafka_send_offsets_to_transaction()</code> CONCURRENT_TRANSACTION (et.al)    errors as retriable.</li> <li>Fix crash on transactional coordinator FindCoordinator request failure.</li> <li>Minimize broker re-connect delay when broker's connection is needed to    send requests.</li> <li><code>socket.timeout.ms</code> was ignored when <code>transactional.id</code> was set.</li> <li>Added RTT/delay simulation to mock brokers.</li> </ul> <p>Note: there was no v1.4.3 librdkafka release</p> <p>Last modified: 2025-07-08 11:55:03</p>"}, {"location": "Librdkafka-Configuration/", "title": "Configuration", "text": ""}, {"location": "Librdkafka-Configuration/#librdkafka-configuration-properties", "title": "Librdkafka Configuration Properties", "text": "<p>This page is a copy of the CONFIGURATION.md of <code>librdkafka</code>. </p> <p>The configuration values below fall under the <code>kafka</code> scope within WaterDrop and Karafka. Aside from these, both frameworks have multiple root configuration options that dictate their overall behavior. Ensure you differentiate between kafka-specific settings and the broader root configurations for effective setup and adjustments.</p> <p>Notice on Configuration Defaults</p> <p>Please be aware that the settings listed below are the default configurations for <code>librdkafka</code> and not for Karafka or WaterDrop. It's important to note that, in some cases, Karafka and WaterDrop employ different default values for their configurations. We strongly recommend consulting the documentation and the configuration code to understand the exact values used within the Karafka ecosystem.</p> <p>Last modified: 2024-05-19 21:57:23</p>"}, {"location": "Librdkafka-Configuration/#global-configuration-properties", "title": "Global configuration properties", "text": "Property C/P Range Default Importance Description builtin.features * gzip, snappy, ssl, sasl, regex, lz4, sasl_gssapi, sasl_plain, sasl_scram, plugins, zstd, sasl_oauthbearer, http, oidc low Indicates the builtin features for this build of librdkafka. An application can either query this value or attempt to set it with its list of required features to check for library support. Type: CSV flags client.id * rdkafka low Client identifier. Type: string metadata.broker.list * high Initial list of brokers as a CSV list of broker host or host:port. The application may also use <code>rd_kafka_brokers_add()</code> to add brokers during runtime. Type: string bootstrap.servers * high Alias for <code>metadata.broker.list</code>: Initial list of brokers as a CSV list of broker host or host:port. The application may also use <code>rd_kafka_brokers_add()</code> to add brokers during runtime. Type: string message.max.bytes * 1000 .. 1000000000 1000000 medium Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's <code>max.message.bytes</code> limit (see Apache Kafka documentation). Type: integer message.copy.max.bytes * 0 .. 1000000000 65535 low Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs. Type: integer receive.message.max.bytes * 1000 .. 2147483647 100000000 medium Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least <code>fetch.max.bytes</code>  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set. Type: integer max.in.flight.requests.per.connection * 1 .. 1000000 1000000 low Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one. Type: integer max.in.flight * 1 .. 1000000 1000000 low Alias for <code>max.in.flight.requests.per.connection</code>: Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one. Type: integer metadata.recovery.strategy * none, rebootstrap rebootstrap low Controls how the client recovers when none of the brokers known to it is available. If set to <code>none</code>, the client doesn't re-bootstrap. If set to <code>rebootstrap</code>, the client repeats the bootstrap process using <code>bootstrap.servers</code> and brokers added through <code>rd_kafka_brokers_add()</code>. Rebootstrapping is useful when a client communicates with brokers so infrequently that the set of brokers may change entirely before the client refreshes metadata. Metadata recovery is triggered when all last-known brokers appear unavailable simultaneously or the client cannot refresh metadata within <code>metadata.recovery.rebootstrap.trigger.ms</code> or it's requested in a metadata response. Type: enum value metadata.recovery.rebootstrap.trigger.ms * 0 .. 2147483647 300000 low If a client configured to rebootstrap using <code>metadata.recovery.strategy=rebootstrap</code> is unable to obtain metadata from any of the brokers for this interval, client repeats the bootstrap process using <code>bootstrap.servers</code> configuration and brokers added through <code>rd_kafka_brokers_add()</code>. Type: integer topic.metadata.refresh.interval.ms * -1 .. 3600000 300000 low Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s. Type: integer metadata.max.age.ms * 1 .. 86400000 900000 low Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3 Type: integer topic.metadata.refresh.fast.interval.ms * 1 .. 60000 100 low When a topic loses its leader a new metadata request will be enqueued immediately and then with this initial interval, exponentially increasing upto <code>retry.backoff.max.ms</code>, until the topic metadata has been refreshed. If not set explicitly, it will be defaulted to <code>retry.backoff.ms</code>. This is used to recover quickly from transitioning leader brokers. Type: integer topic.metadata.refresh.fast.cnt * 0 .. 1000 10 low DEPRECATED No longer used. Type: integer topic.metadata.refresh.sparse * true, false true low Sparse metadata requests (consumes less network bandwidth) Type: boolean topic.metadata.propagation.max.ms * 0 .. 3600000 30000 low Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with <code>ERR__UNKNOWN_TOPIC</code>. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce(). Type: integer topic.blacklist * low Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist. Type: pattern list debug * generic, broker, topic, metadata, feature, queue, msg, protocol, cgrp, security, fetch, interceptor, plugin, consumer, admin, eos, mock, assignor, conf, telemetry, all medium A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch Type: CSV flags socket.timeout.ms * 10 .. 300000 60000 low Default timeout for network requests. Producer: ProduceRequests will use the lesser value of <code>socket.timeout.ms</code> and remaining <code>message.timeout.ms</code> for the first message in the batch. Consumer: FetchRequests will use <code>fetch.wait.max.ms</code> + <code>socket.timeout.ms</code>. Admin: Admin requests will use <code>socket.timeout.ms</code> or explicitly set <code>rd_kafka_AdminOptions_set_operation_timeout()</code> value. Type: integer socket.blocking.max.ms * 1 .. 60000 1000 low DEPRECATED No longer used. Type: integer socket.send.buffer.bytes * 0 .. 100000000 0 low Broker socket send buffer size. System default is used if 0. Type: integer socket.receive.buffer.bytes * 0 .. 100000000 0 low Broker socket receive buffer size. System default is used if 0. Type: integer socket.keepalive.enable * true, false false low Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets Type: boolean socket.nagle.disable * true, false true low Disable the Nagle algorithm (TCP_NODELAY) on broker sockets. Type: boolean socket.max.fails * 0 .. 1000000 1 low Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established. Type: integer broker.address.ttl * 0 .. 86400000 1000 low How long to cache the broker address resolving results (milliseconds). Type: integer broker.address.family * any, v4, v6 any low Allowed broker IP address families: any, v4, v6 Type: enum value socket.connection.setup.timeout.ms * 1000 .. 2147483647 30000 medium Maximum time allowed for broker connection setup (TCP connection setup as well SSL and SASL handshake). If the connection to the broker is not fully functional after this the connection will be closed and retried. Type: integer connections.max.idle.ms * 0 .. 2147483647 0 medium Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info). Type: integer reconnect.backoff.jitter.ms * 0 .. 3600000 0 low DEPRECATED No longer used. See <code>reconnect.backoff.ms</code> and <code>reconnect.backoff.max.ms</code>. Type: integer reconnect.backoff.ms * 0 .. 3600000 100 medium The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until <code>reconnect.backoff.max.ms</code> is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately. Type: integer reconnect.backoff.max.ms * 0 .. 3600000 10000 medium The maximum time to wait before reconnecting to a broker after the connection has been closed. Type: integer statistics.interval.ms * 0 .. 86400000 0 high librdkafka statistics emit interval. The application also needs to register a stats callback using <code>rd_kafka_conf_set_stats_cb()</code>. The granularity is 1000ms. A value of 0 disables statistics. Type: integer enabled_events * 0 .. 2147483647 0 low See <code>rd_kafka_conf_set_events()</code> Type: integer error_cb * low Error callback (set with rd_kafka_conf_set_error_cb()) Type: see dedicated API throttle_cb * low Throttle callback (set with rd_kafka_conf_set_throttle_cb()) Type: see dedicated API stats_cb * low Statistics callback (set with rd_kafka_conf_set_stats_cb()) Type: see dedicated API log_cb * low Log callback (set with rd_kafka_conf_set_log_cb()) Type: see dedicated API log_level * 0 .. 7 6 low Logging level (syslog(3) levels) Type: integer log.queue * true, false false low Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with <code>rd_kafka_set_log_queue()</code> and serve log callbacks or events through the standard poll APIs. NOTE: Log messages will linger in a temporary queue until the log queue has been set. Type: boolean log.thread.name * true, false true low Print internal thread name in log messages (useful for debugging librdkafka internals) Type: boolean enable.random.seed * true, false true low If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new(). Type: boolean log.connection.close * true, false true low Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive <code>connections.max.idle.ms</code> value. Type: boolean background_event_cb * low Background queue event callback (set with rd_kafka_conf_set_background_event_cb()) Type: see dedicated API socket_cb * low Socket creation callback to provide race-free CLOEXEC Type: see dedicated API connect_cb * low Socket connect callback Type: see dedicated API closesocket_cb * low Socket close callback Type: see dedicated API open_cb * low File open callback to provide race-free CLOEXEC Type: see dedicated API resolve_cb * low Address resolution callback (set with rd_kafka_conf_set_resolve_cb()). Type: see dedicated API opaque * low Application opaque (set with rd_kafka_conf_set_opaque()) Type: see dedicated API default_topic_conf * low Default topic configuration for automatically subscribed topics Type: see dedicated API internal.termination.signal * 0 .. 128 0 low Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed. Type: integer api.version.request * true, false true high DEPRECATED Post-deprecation actions: remove this configuration property, brokers &lt; 0.10.0 won't be supported anymore in librdkafka 3.x. Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version <code>broker.version.fallback</code> will be used. NOTE: Depends on broker version &gt;=0.10.0. If the request is not supported by (an older) broker the <code>broker.version.fallback</code> fallback is used. Type: boolean api.version.request.timeout.ms * 1 .. 300000 10000 low Timeout for broker API version requests. Type: integer api.version.fallback.ms * 0 .. 604800000 0 medium DEPRECATED Post-deprecation actions: remove this configuration property, brokers &lt; 0.10.0 won't be supported anymore in librdkafka 3.x. Dictates how long the <code>broker.version.fallback</code> fallback is used in the case the ApiVersionRequest fails. NOTE: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade). Type: integer broker.version.fallback * 0.10.0 medium DEPRECATED Post-deprecation actions: remove this configuration property, brokers &lt; 0.10.0 won't be supported anymore in librdkafka 3.x. Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see <code>api.version.request</code>) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for <code>api.version.fallback.ms</code>. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value &gt;= 0.10, such as 0.10.2.1, enables ApiVersionRequests. Type: string allow.auto.create.topics * true, false false low Allow automatic topic creation on the broker when subscribing to or assigning non-existent topics. The broker must also be configured with <code>auto.create.topics.enable=true</code> for this configuration to take effect. Note: the default value (true) for the producer is different from the default value (false) for the consumer. Further, the consumer default value is different from the Java consumer (true), and this property is not supported by the Java producer. Requires broker version &gt;= 0.11.0.0, for older broker versions only the broker configuration applies. Type: boolean security.protocol * plaintext, ssl, sasl_plaintext, sasl_ssl plaintext high Protocol used to communicate with brokers. Type: enum value ssl.cipher.suites * low A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for <code>ciphers(1)</code> and `SSL_CTX_set_cipher_list(3). Type: string ssl.curves.list * low The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for <code>SSL_CTX_set1_curves_list(3)</code>. OpenSSL &gt;= 1.0.2 required. Type: string ssl.sigalgs.list * low The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for <code>SSL_CTX_set1_sigalgs_list(3)</code>. OpenSSL &gt;= 1.0.2 required. Type: string ssl.key.location * low Path to client's private key (PEM) used for authentication. Type: string ssl.key.password * low Private key passphrase (for use with <code>ssl.key.location</code> and <code>set_ssl_cert()</code>) Type: string ssl.key.pem * low Client's private key string (PEM format) used for authentication. Type: string ssl_key * low Client's private key as set by rd_kafka_conf_set_ssl_cert() Type: see dedicated API ssl.certificate.location * low Path to client's public key (PEM) used for authentication. Type: string ssl.certificate.pem * low Client's public key string (PEM format) used for authentication. Type: string ssl_certificate * low Client's public key as set by rd_kafka_conf_set_ssl_cert() Type: see dedicated API ssl.ca.location * low File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to <code>probe</code>. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or <code>ssl.ca.location</code> is set to <code>probe</code> a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see <code>OPENSSLDIR</code> in <code>openssl version -a</code>). Type: string https.ca.location * low File or directory path to CA certificate(s) for verifying HTTPS endpoints, like <code>sasl.oauthbearer.token.endpoint.url</code> used for OAUTHBEARER/OIDC authentication. Mutually exclusive with <code>https.ca.pem</code>. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to <code>probe</code>. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or <code>https.ca.location</code> is set to <code>probe</code> a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see <code>OPENSSLDIR</code> in <code>openssl version -a</code>). Type: string https.ca.pem * low CA certificate string (PEM format) for verifying HTTPS endpoints. Mutually exclusive with <code>https.ca.location</code>. Optional: see <code>https.ca.location</code>. Type: string ssl.ca.pem * low CA certificate string (PEM format) for verifying the broker's key. Type: string ssl_ca * low CA certificate as set by rd_kafka_conf_set_ssl_cert() Type: see dedicated API ssl.ca.certificate.stores * Root low Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA. Type: string ssl.crl.location * low Path to CRL for verifying broker's certificate validity. Type: string ssl.keystore.location * low Path to client's keystore (PKCS#12) used for authentication. Type: string ssl.keystore.password * low Client's keystore (PKCS#12) password. Type: string ssl.providers * low Comma-separated list of OpenSSL 3.0.x implementation providers. E.g., \"default,legacy\". Type: string ssl.engine.location * low DEPRECATED Path to OpenSSL engine library. OpenSSL &gt;= 1.1.x required. DEPRECATED: OpenSSL engine support is deprecated and should be replaced by OpenSSL 3 providers. Type: string ssl.engine.id * dynamic low OpenSSL engine id is the name used for loading engine. Type: string ssl_engine_callback_data * low OpenSSL engine callback data (set with rd_kafka_conf_set_engine_callback_data()). Type: see dedicated API enable.ssl.certificate.verification * true, false true low Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb. Type: boolean ssl.endpoint.identification.algorithm * none, https https low Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL &gt;= 1.0.2 required. Type: enum value ssl.certificate.verify_cb * low Callback to verify the broker certificate chain. Type: see dedicated API sasl.mechanisms * GSSAPI high SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, OAUTHBEARER. NOTE: Despite the name only one mechanism must be configured. Type: string sasl.mechanism * GSSAPI high Alias for <code>sasl.mechanisms</code>: SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, OAUTHBEARER. NOTE: Despite the name only one mechanism must be configured. Type: string sasl.kerberos.service.name * kafka low Kerberos principal name that Kafka runs as, not including /hostname@REALM Type: string sasl.kerberos.principal * kafkaclient low This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal). Type: string sasl.kerberos.kinit.cmd * kinit -R -t \"%{sasl.kerberos.keytab}\" -k %{sasl.kerberos.principal} || kinit -t \"%{sasl.kerberos.keytab}\" -k %{sasl.kerberos.principal} low Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value. Type: string sasl.kerberos.keytab * low Path to Kerberos keytab file. This configuration property is only used as a variable in <code>sasl.kerberos.kinit.cmd</code> as <code>... -t \"%{sasl.kerberos.keytab}\"</code>. Type: string sasl.kerberos.min.time.before.relogin * 0 .. 86400000 60000 low Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0. Type: integer sasl.username * high SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms Type: string sasl.password * high SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism Type: string sasl.oauthbearer.config * low SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is \"sub\", the default value for scopeClaimName is \"scope\", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: <code>principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600</code>. In addition, SASL extensions can be communicated to the broker via <code>extension_NAME=value</code>. For example: <code>principal=admin extension_traceId=123</code> Type: string enable.sasl.oauthbearer.unsecure.jwt * true, false false low Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production. Type: boolean oauthbearer_token_refresh_cb * low SASL/OAUTHBEARER token refresh callback (set with rd_kafka_conf_set_oauthbearer_token_refresh_cb(), triggered by rd_kafka_poll(), et.al. This callback will be triggered when it is time to refresh the client's OAUTHBEARER token. Also see <code>rd_kafka_conf_enable_sasl_queue()</code>. Type: see dedicated API sasl.oauthbearer.method * default, oidc default low Set to \"default\" or \"oidc\" to control which login method to be used. If set to \"oidc\", the following properties must also be be specified: <code>sasl.oauthbearer.client.id</code>, <code>sasl.oauthbearer.client.secret</code>, and <code>sasl.oauthbearer.token.endpoint.url</code>. Type: enum value sasl.oauthbearer.client.id * low Public identifier for the application. Must be unique across all clients that the authorization server handles. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\". Type: string sasl.oauthbearer.client.credentials.client.id * low Alias for <code>sasl.oauthbearer.client.id</code>: Public identifier for the application. Must be unique across all clients that the authorization server handles. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\". Type: string sasl.oauthbearer.client.credentials.client.secret * low Alias for <code>sasl.oauthbearer.client.secret</code>: Client secret only known to the application and the authorization server. This should be a sufficiently random string that is not guessable. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\". Type: string sasl.oauthbearer.client.secret * low Client secret only known to the application and the authorization server. This should be a sufficiently random string that is not guessable. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\". Type: string sasl.oauthbearer.scope * low Client use this to specify the scope of the access request to the broker. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\". Type: string sasl.oauthbearer.extensions * low Allow additional information to be provided to the broker. Comma-separated list of key=value pairs. E.g., \"supportFeatureX=true,organizationId=sales-emea\".Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\". Type: string sasl.oauthbearer.token.endpoint.url * low OAuth/OIDC issuer token endpoint HTTP(S) URI used to retrieve token. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\". Type: string sasl.oauthbearer.grant.type * client_credentials, urn:ietf:params:oauth:grant-type:jwt-bearer client_credentials low OAuth grant type to use when communicating with the identity provider. Type: enum value sasl.oauthbearer.assertion.algorithm * RS256, ES256 RS256 low Algorithm the client should use to sign the assertion sent to the identity provider and in the OAuth alg header in the JWT assertion. Type: enum value sasl.oauthbearer.assertion.private.key.file * low Path to client's private key (PEM) used for authentication when using the JWT assertion. Type: string sasl.oauthbearer.assertion.private.key.passphrase * low Private key passphrase for <code>sasl.oauthbearer.assertion.private.key.file</code> or <code>sasl.oauthbearer.assertion.private.key.pem</code>. Type: string sasl.oauthbearer.assertion.private.key.pem * low Client's private key (PEM) used for authentication when using the JWT assertion. Type: string sasl.oauthbearer.assertion.file * low Path to the assertion file. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\" and JWT assertion is needed. Type: string sasl.oauthbearer.assertion.claim.aud * low JWT audience claim. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\" and JWT assertion is needed. Type: string sasl.oauthbearer.assertion.claim.exp.seconds * 1 .. 2147483647 300 low Assertion expiration time in seconds. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\" and JWT assertion is needed. Type: integer sasl.oauthbearer.assertion.claim.iss * low JWT issuer claim. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\" and JWT assertion is needed. Type: string sasl.oauthbearer.assertion.claim.jti.include * true, false false low JWT ID claim. When set to <code>true</code>, a random UUID is generated. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\" and JWT assertion is needed. Type: boolean sasl.oauthbearer.assertion.claim.nbf.seconds * 0 .. 2147483647 60 low Assertion not before time in seconds. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\" and JWT assertion is needed. Type: integer sasl.oauthbearer.assertion.claim.sub * low JWT subject claim. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\" and JWT assertion is needed. Type: string sasl.oauthbearer.assertion.jwt.template.file * low Path to the JWT template file. Only used when <code>sasl.oauthbearer.method</code> is set to \"oidc\" and JWT assertion is needed. Type: string plugin.library.paths * low List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically. Type: string interceptors * low Interceptors added through rd_kafka_conf_interceptor_add_..() and any configuration handled by interceptors. Type: see dedicated API group.id C high Client group id string. All clients sharing the same group.id belong to the same group. Type: string group.instance.id C medium Enable static group membership. Static group members are able to leave and rejoin a group within the configured <code>session.timeout.ms</code> without prompting a group rebalance. This should be used in combination with a larger <code>session.timeout.ms</code> to avoid group rebalances caused by transient unavailability (e.g. process restarts). Requires broker version &gt;= 2.3.0. Type: string partition.assignment.strategy C range,roundrobin medium The name of one or more partition assignment strategies. The elected group leader will use a strategy supported by all members of the group to assign partitions to group members. If there is more than one eligible strategy, preference is determined by the order of this list (strategies earlier in the list have higher priority). Cooperative and non-cooperative (eager)strategies must not be mixed. <code>partition.assignment.strategy</code> is not supported for <code>group.protocol=consumer</code>. Use <code>group.remote.assignor</code> instead. Available strategies: range, roundrobin, cooperative-sticky. Type: string session.timeout.ms C 1 .. 3600000 45000 high Client group session and failure detection timeout. The consumer sends periodic heartbeats (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance. The allowed range is configured with the broker configuration properties <code>group.min.session.timeout.ms</code> and <code>group.max.session.timeout.ms</code>. <code>session.timeout.ms</code> is not supported for <code>group.protocol=consumer</code>. It is set with the broker configuration property <code>group.consumer.session.timeout.ms</code> by default or can be configured through the AdminClient IncrementalAlterConfigs API. The allowed range is configured with the broker configuration properties <code>group.consumer.min.session.timeout.ms</code> and <code>group.consumer.max.session.timeout.ms</code>. Also see <code>max.poll.interval.ms</code>. Type: integer heartbeat.interval.ms C 1 .. 3600000 3000 low Group session keepalive heartbeat interval. <code>heartbeat.interval.ms</code> is not supported for <code>group.protocol=consumer</code>. It is set with the broker configuration property <code>group.consumer.heartbeat.interval.ms</code> by default or can be configured through the AdminClient IncrementalAlterConfigs API. The allowed range is configured with the broker configuration properties <code>group.consumer.min.heartbeat.interval.ms</code> and <code>group.consumer.max.heartbeat.interval.ms</code>. Type: integer group.protocol.type C consumer low Group protocol type for the <code>classic</code> group protocol. NOTE: Currently, the only supported group protocol type is <code>consumer</code>. <code>group.protocol.type</code> is not supported for <code>group.protocol=consumer</code> Type: string group.protocol C classic, consumer classic high Group protocol to use. Use <code>classic</code> for the original protocol and <code>consumer</code> for the new protocol introduced in KIP-848. Available protocols: classic or consumer. Default is <code>classic</code>, but will change to <code>consumer</code> in next releases. Type: enum value group.remote.assignor C medium Server side assignor to use. Keep it null to make server select a suitable assignor for the group. Available assignors: uniform or range. Default is null Type: string coordinator.query.interval.ms C 1 .. 3600000 600000 low How often to query for the current client group coordinator. If the currently assigned coordinator is down the configured query interval will be divided by ten to more quickly recover in case of coordinator reassignment. Type: integer max.poll.interval.ms C 1 .. 86400000 300000 high Maximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member. Warning: Offset commits may be not possible at this point. Note: It is recommended to set <code>enable.auto.offset.store=false</code> for long-time processing applications and then explicitly store offsets (using offsets_store()) after message processing, to make sure offsets are not auto-committed prior to processing has finished. The interval is checked two times per second. See KIP-62 for more information. Type: integer enable.auto.commit C true, false true high Automatically and periodically commit offsets in the background. Note: setting this to false does not prevent the consumer from fetching previously committed start offsets. To circumvent this behaviour set specific start offsets per partition in the call to assign(). Type: boolean auto.commit.interval.ms C 0 .. 86400000 5000 medium The frequency in milliseconds that the consumer offsets are committed (written) to offset storage. (0 = disable). This setting is used by the high-level consumer. Type: integer enable.auto.offset.store C true, false true high Automatically store offset of last message provided to application. The offset store is an in-memory store of the next offset to (auto-)commit for each partition. Type: boolean queued.min.messages C 1 .. 10000000 100000 medium Minimum number of messages per topic+partition librdkafka tries to maintain in the local consumer queue. Type: integer queued.max.messages.kbytes C 1 .. 2097151 65536 medium Maximum number of kilobytes of queued pre-fetched messages in the local consumer queue. If using the high-level consumer this setting applies to the single consumer queue, regardless of the number of partitions. When using the legacy simple consumer or when separate partition queues are used this setting applies per partition. This value may be overshot by fetch.message.max.bytes. This property has higher priority than queued.min.messages. Type: integer fetch.wait.max.ms C 0 .. 300000 500 low Maximum time the broker may wait to fill the Fetch response with fetch.min.bytes of messages. Type: integer fetch.queue.backoff.ms C 0 .. 300000 1000 medium How long to postpone the next fetch request for a topic+partition in case the current fetch queue thresholds (queued.min.messages or queued.max.messages.kbytes) have been exceded. This property may need to be decreased if the queue thresholds are set low and the application is experiencing long (~1s) delays between messages. Low values may increase CPU utilization. Type: integer fetch.message.max.bytes C 1 .. 1000000000 1048576 medium Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. If the client encounters a message larger than this value it will gradually try to increase it until the entire message can be fetched. Type: integer max.partition.fetch.bytes C 1 .. 1000000000 1048576 medium Alias for <code>fetch.message.max.bytes</code>: Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. If the client encounters a message larger than this value it will gradually try to increase it until the entire message can be fetched. Type: integer fetch.max.bytes C 0 .. 2147483135 52428800 medium Maximum amount of data the broker shall return for a Fetch request. Messages are fetched in batches by the consumer and if the first message batch in the first non-empty partition of the Fetch request is larger than this value, then the message batch will still be returned to ensure the consumer can make progress. The maximum message batch size accepted by the broker is defined via <code>message.max.bytes</code> (broker config) or <code>max.message.bytes</code> (broker topic config). <code>fetch.max.bytes</code> is automatically adjusted upwards to be at least <code>message.max.bytes</code> (consumer config). Type: integer fetch.min.bytes C 1 .. 100000000 1 low Minimum number of bytes the broker responds with. If fetch.wait.max.ms expires the accumulated data will be sent to the client regardless of this setting. Type: integer fetch.error.backoff.ms C 0 .. 300000 500 medium How long to postpone the next fetch request for a topic+partition in case of a fetch error. Type: integer offset.store.method C none, file, broker broker low DEPRECATED Offset commit store method: 'file' - DEPRECATED: local file store (offset.store.path, et.al), 'broker' - broker commit store (requires Apache Kafka 0.8.2 or later on the broker). Type: enum value isolation.level C read_uncommitted, read_committed read_committed high Controls how to read messages written transactionally: <code>read_committed</code> - only return transactional messages which have been committed. <code>read_uncommitted</code> - return all messages, even transactional messages which have been aborted. Type: enum value consume_cb C low Message consume callback (set with rd_kafka_conf_set_consume_cb()) Type: see dedicated API rebalance_cb C low Called after consumer group has been rebalanced (set with rd_kafka_conf_set_rebalance_cb()) Type: see dedicated API offset_commit_cb C low Offset commit result propagation callback. (set with rd_kafka_conf_set_offset_commit_cb()) Type: see dedicated API enable.partition.eof C true, false false low Emit RD_KAFKA_RESP_ERR__PARTITION_EOF event whenever the consumer reaches the end of a partition. Type: boolean check.crcs C true, false false medium Verify CRC32 of consumed messages, ensuring no on-the-wire or on-disk corruption to the messages occurred. This check comes at slightly increased CPU usage. Type: boolean client.rack * low A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config <code>broker.rack</code>. Type: string transactional.id P high Enables the transactional producer. The transactional.id is used to identify the same transactional producer instance across process restarts. It allows the producer to guarantee that transactions corresponding to earlier instances of the same producer have been finalized prior to starting any new transactions, and that any zombie instances are fenced off. If no transactional.id is provided, then the producer is limited to idempotent delivery (if enable.idempotence is set). Requires broker version &gt;= 0.11.0. Type: string transaction.timeout.ms P 1000 .. 2147483647 60000 medium The maximum amount of time in milliseconds that the transaction coordinator will wait for a transaction status update from the producer before proactively aborting the ongoing transaction. If this value is larger than the <code>transaction.max.timeout.ms</code> setting in the broker, the init_transactions() call will fail with ERR_INVALID_TRANSACTION_TIMEOUT. The transaction timeout automatically adjusts <code>message.timeout.ms</code> and <code>socket.timeout.ms</code>, unless explicitly configured in which case they must not exceed the transaction timeout (<code>socket.timeout.ms</code> must be at least 100ms lower than <code>transaction.timeout.ms</code>). This is also the default timeout value if no timeout (-1) is supplied to the transactional API methods. Type: integer enable.idempotence P true, false false high When set to <code>true</code>, the producer will ensure that messages are successfully produced exactly once and in the original produce order. The following configuration properties are adjusted automatically (if not modified by the user) when idempotence is enabled: <code>max.in.flight.requests.per.connection=5</code> (must be less than or equal to 5), <code>retries=INT32_MAX</code> (must be greater than 0), <code>acks=all</code>, <code>queuing.strategy=fifo</code>. Producer instantation will fail if user-supplied configuration is incompatible. Type: boolean enable.gapless.guarantee P true, false false low EXPERIMENTAL: subject to change or removal. When set to <code>true</code>, any error that could result in a gap in the produced message series when a batch of messages fails, will raise a fatal error (ERR__GAPLESS_GUARANTEE) and stop the producer. Messages failing due to <code>message.timeout.ms</code> are not covered by this guarantee. Requires <code>enable.idempotence=true</code>. Type: boolean queue.buffering.max.messages P 0 .. 2147483647 100000 high Maximum number of messages allowed on the producer queue. This queue is shared by all topics and partitions. A value of 0 disables this limit. Type: integer queue.buffering.max.kbytes P 1 .. 2147483647 1048576 high Maximum total message size sum allowed on the producer queue. This queue is shared by all topics and partitions. This property has higher priority than queue.buffering.max.messages. Type: integer queue.buffering.max.ms P 0 .. 900000 5 high Delay in milliseconds to wait for messages in the producer queue to accumulate before constructing message batches (MessageSets) to transmit to brokers. A higher value allows larger and more effective (less overhead, improved compression) batches of messages to accumulate at the expense of increased message delivery latency. Type: float linger.ms P 0 .. 900000 5 high Alias for <code>queue.buffering.max.ms</code>: Delay in milliseconds to wait for messages in the producer queue to accumulate before constructing message batches (MessageSets) to transmit to brokers. A higher value allows larger and more effective (less overhead, improved compression) batches of messages to accumulate at the expense of increased message delivery latency. Type: float message.send.max.retries P 0 .. 2147483647 2147483647 high How many times to retry sending a failing Message. Note: retrying may cause reordering unless <code>enable.idempotence</code> is set to true. Type: integer retries P 0 .. 2147483647 2147483647 high Alias for <code>message.send.max.retries</code>: How many times to retry sending a failing Message. Note: retrying may cause reordering unless <code>enable.idempotence</code> is set to true. Type: integer retry.backoff.ms * 1 .. 300000 100 medium The backoff time in milliseconds before retrying a protocol request, this is the first backoff time, and will be backed off exponentially until number of retries is exhausted, and it's capped by retry.backoff.max.ms. Type: integer retry.backoff.max.ms * 1 .. 300000 1000 medium The max backoff time in milliseconds before retrying a protocol request, this is the atmost backoff allowed for exponentially backed off requests. Type: integer queue.buffering.backpressure.threshold P 1 .. 1000000 1 low The threshold of outstanding not yet transmitted broker requests needed to backpressure the producer's message accumulator. If the number of not yet transmitted requests equals or exceeds this number, produce request creation that would have otherwise been triggered (for example, in accordance with linger.ms) will be delayed. A lower number yields larger and more effective batches. A higher value can improve latency when using compression on slow machines. Type: integer compression.codec P none, gzip, snappy, lz4, zstd none medium compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property <code>compression.codec</code>.  Type: enum value compression.type P none, gzip, snappy, lz4, zstd none medium Alias for <code>compression.codec</code>: compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property <code>compression.codec</code>.  Type: enum value batch.num.messages P 1 .. 1000000 10000 medium Maximum number of messages batched in one MessageSet. The total MessageSet size is also limited by batch.size and message.max.bytes. Type: integer batch.size P 1 .. 2147483647 1000000 medium Maximum size (in bytes) of all messages batched in one MessageSet, including protocol framing overhead. This limit is applied after the first message has been added to the batch, regardless of the first message's size, this is to ensure that messages that exceed batch.size are produced. The total MessageSet size is also limited by batch.num.messages and message.max.bytes. Type: integer delivery.report.only.error P true, false false low Only provide delivery reports for failed messages. Type: boolean dr_cb P low Delivery report callback (set with rd_kafka_conf_set_dr_cb()) Type: see dedicated API dr_msg_cb P low Delivery report callback (set with rd_kafka_conf_set_dr_msg_cb()) Type: see dedicated API sticky.partitioning.linger.ms P 0 .. 900000 10 low Delay in milliseconds to wait to assign new sticky partitions for each topic. By default, set to double the time of linger.ms. To disable sticky behavior, set to 0. This behavior affects messages with the key NULL in all cases, and messages with key lengths of zero when the consistent_random partitioner is in use. These messages would otherwise be assigned randomly. A higher value allows for more effective batching of these messages. Type: integer client.dns.lookup * use_all_dns_ips, resolve_canonical_bootstrap_servers_only use_all_dns_ips low Controls how the client uses DNS lookups. By default, when the lookup returns multiple IP addresses for a hostname, they will all be attempted for connection before the connection is considered failed. This applies to both bootstrap and advertised servers. If the value is set to <code>resolve_canonical_bootstrap_servers_only</code>, each entry will be resolved and expanded into a list of canonical names. WARNING: <code>resolve_canonical_bootstrap_servers_only</code> must only be used with <code>GSSAPI</code> (Kerberos) as <code>sasl.mechanism</code>, as it's the only purpose of this configuration value. NOTE: Default here is different from the Java client's default behavior, which connects only to the first IP address returned for a hostname.  Type: enum value enable.metrics.push * true, false true low Whether to enable pushing of client metrics to the cluster, if the cluster has a client metrics subscription which matches this client Type: boolean"}, {"location": "Librdkafka-Configuration/#topic-configuration-properties", "title": "Topic configuration properties", "text": "Property C/P Range Default Importance Description request.required.acks P -1 .. 1000 -1 high This field indicates the number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 or all=Broker will block until message is committed by all in sync replicas (ISRs). If there are less than <code>min.insync.replicas</code> (broker configuration) in the ISR set the produce request will fail. Type: integer acks P -1 .. 1000 -1 high Alias for <code>request.required.acks</code>: This field indicates the number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 or all=Broker will block until message is committed by all in sync replicas (ISRs). If there are less than <code>min.insync.replicas</code> (broker configuration) in the ISR set the produce request will fail. Type: integer request.timeout.ms P 1 .. 900000 30000 medium The ack timeout of the producer request in milliseconds. This value is only enforced by the broker and relies on <code>request.required.acks</code> being != 0. Type: integer message.timeout.ms P 0 .. 2147483647 300000 high Local message timeout. This value is only enforced locally and limits the time a produced message waits for successful delivery. A time of 0 is infinite. This is the maximum time librdkafka may use to deliver a message (including retries). Delivery error occurs when either the retry count or the message timeout are exceeded. The message timeout is automatically adjusted to <code>transaction.timeout.ms</code> if <code>transactional.id</code> is configured. Type: integer delivery.timeout.ms P 0 .. 2147483647 300000 high Alias for <code>message.timeout.ms</code>: Local message timeout. This value is only enforced locally and limits the time a produced message waits for successful delivery. A time of 0 is infinite. This is the maximum time librdkafka may use to deliver a message (including retries). Delivery error occurs when either the retry count or the message timeout are exceeded. The message timeout is automatically adjusted to <code>transaction.timeout.ms</code> if <code>transactional.id</code> is configured. Type: integer queuing.strategy P fifo, lifo fifo low EXPERIMENTAL: subject to change or removal. DEPRECATED Producer queuing strategy. FIFO preserves produce ordering, while LIFO prioritizes new messages. Type: enum value produce.offset.report P true, false false low DEPRECATED No longer used. Type: boolean partitioner P consistent_random high Partitioner: <code>random</code> - random distribution, <code>consistent</code> - CRC32 hash of key (Empty and NULL keys are mapped to single partition), <code>consistent_random</code> - CRC32 hash of key (Empty and NULL keys are randomly partitioned), <code>murmur2</code> - Java Producer compatible Murmur2 hash of key (NULL keys are mapped to single partition), <code>murmur2_random</code> - Java Producer compatible Murmur2 hash of key (NULL keys are randomly partitioned. This is functionally equivalent to the default partitioner in the Java Producer.), <code>fnv1a</code> - FNV-1a hash of key (NULL keys are mapped to single partition), <code>fnv1a_random</code> - FNV-1a hash of key (NULL keys are randomly partitioned). Type: string partitioner_cb P low Custom partitioner callback (set with rd_kafka_topic_conf_set_partitioner_cb()) Type: see dedicated API msg_order_cmp P low EXPERIMENTAL: subject to change or removal. DEPRECATED Message queue ordering comparator (set with rd_kafka_topic_conf_set_msg_order_cmp()). Also see <code>queuing.strategy</code>. Type: see dedicated API opaque * low Application opaque (set with rd_kafka_topic_conf_set_opaque()) Type: see dedicated API compression.codec P none, gzip, snappy, lz4, zstd, inherit inherit high Compression codec to use for compressing message sets. inherit = inherit global compression.codec configuration. Type: enum value compression.type P none, gzip, snappy, lz4, zstd none medium Alias for <code>compression.codec</code>: compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property <code>compression.codec</code>.  Type: enum value compression.level P -1 .. 12 -1 medium Compression level parameter for algorithm selected by configuration property <code>compression.codec</code>. Higher values will result in better compression at the cost of more CPU usage. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. Type: integer auto.commit.enable C true, false true low DEPRECATED [LEGACY PROPERTY: This property is used by the simple legacy consumer only. When using the high-level KafkaConsumer, the global <code>enable.auto.commit</code> property must be used instead]. If true, periodically commit offset of the last message handed to the application. This committed offset will be used when the process restarts to pick up where it left off. If false, the application will have to call <code>rd_kafka_offset_store()</code> to store an offset (optional). Offsets will be written to broker or local file according to offset.store.method. Type: boolean enable.auto.commit C true, false true low DEPRECATED Alias for <code>auto.commit.enable</code>: [LEGACY PROPERTY: This property is used by the simple legacy consumer only. When using the high-level KafkaConsumer, the global <code>enable.auto.commit</code> property must be used instead]. If true, periodically commit offset of the last message handed to the application. This committed offset will be used when the process restarts to pick up where it left off. If false, the application will have to call <code>rd_kafka_offset_store()</code> to store an offset (optional). Offsets will be written to broker or local file according to offset.store.method. Type: boolean auto.commit.interval.ms C 10 .. 86400000 60000 high [LEGACY PROPERTY: This setting is used by the simple legacy consumer only. When using the high-level KafkaConsumer, the global <code>auto.commit.interval.ms</code> property must be used instead]. The frequency in milliseconds that the consumer offsets are committed (written) to offset storage. Type: integer auto.offset.reset C smallest, earliest, beginning, largest, latest, end, error largest high Action to take when there is no initial offset in offset store or the desired offset is out of range: 'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' - automatically reset the offset to the largest offset, 'error' - trigger an error (ERR__AUTO_OFFSET_RESET) which is retrieved by consuming messages and checking 'message-&gt;err'. Type: enum value offset.store.path C . low DEPRECATED Path to local file for storing offsets. If the path is a directory a filename will be automatically generated in that directory based on the topic and partition. File-based offset storage will be removed in a future version. Type: string offset.store.sync.interval.ms C -1 .. 86400000 -1 low DEPRECATED fsync() interval for the offset file, in milliseconds. Use -1 to disable syncing, and 0 for immediate sync after each write. File-based offset storage will be removed in a future version. Type: integer offset.store.method C file, broker broker low DEPRECATED Offset commit store method: 'file' - DEPRECATED: local file store (offset.store.path, et.al), 'broker' - broker commit store (requires \"group.id\" to be configured and Apache Kafka 0.8.2 or later on the broker.). Type: enum value consume.callback.max.messages C 0 .. 1000000 0 low Maximum number of messages to dispatch in one <code>rd_kafka_consume_callback*()</code> call (0 = unlimited) Type: integer"}, {"location": "Librdkafka-Configuration/#cp-legend-c-consumer-p-producer-both", "title": "C/P legend: C = Consumer, P = Producer, * = both", "text": ""}, {"location": "Librdkafka-Errors/", "title": "Errors", "text": ""}, {"location": "Librdkafka-Errors/#librdkafka-errors", "title": "Librdkafka Errors", "text": "<p>Handling Non-Permanent and Permanent Errors from librdkafka</p> <p>Most errors from <code>librdkafka</code> are non-permanent and are gracefully managed by internal retries, reducing the immediate impact on your application. These errors are generally not critical and often involve temporary issues such as a <code>timed_out</code> error, which indicates a delay in response or operation but does not necessarily signify a permanent problem. Those types of errors, while reported, usually resolve themselves through subsequent retries without requiring direct intervention.</p> <p>Karafka also incorporates several recovery mechanisms that enable it to automatically reconnect and continue operations whenever possible, further enhancing the robustness of your message-handling processes.</p>"}, {"location": "Librdkafka-Errors/#errors-classification", "title": "Errors Classification", "text": "<p>Internal errors (<code>RD_KAFKA_RESP_ERR__</code>) in librdkafka originate within the client library. They are often labeled as \"Local\" issues. In contrast, the Kafka broker generates broker errors (<code>RD_KAFKA_RESP_ERR_</code>). They are typically referred to as \"Broker\" issues, indicating problems on the server side.</p> <p>Understanding whether an error is internal or from a broker is crucial for troubleshooting and resolving issues effectively:</p> <ul> <li> <p>Debugging: Knowing the error's source helps debug. If the error is internal, the issue must be resolved within the client environment (e.g., fixing configuration and updating the client library for bugs). If the error is from the broker, it might involve server configuration, network policies, or other aspects managed by the Kafka infrastructure.</p> </li> <li> <p>Scalability and Reliability: By properly handling these errors, developers and operators can ensure the creation of more robust and scalable client applications. For instance, internal errors might require code changes or updates, while broker errors could lead to changes in how the client interacts with the Kafka ecosystem.</p> </li> <li> <p>Operational Awareness: For operators and developers, the categorization of errors provides valuable insights into the health and status of both the Kafka client and the server, fostering informed operational decisions and monitoring strategies.</p> </li> </ul> <p>By categorizing errors this way, librdkafka provides a more straightforward interface for developers to handle, log, and react to various conditions affecting their applications' interaction with Apache Kafka.</p>"}, {"location": "Librdkafka-Errors/#local-errors", "title": "Local Errors", "text": "<p>Internal errors originate within the librdkafka client itself. They are not generated by interactions with the Kafka broker. Still, they are caused by issues within the client's processes and operations.</p> <p>These errors are prefixed with two underscores (<code>__</code>) in their full error code names.</p> Code Symbol Name Description <code>-199</code><code>:bad_msg</code><code>RD_KAFKA_RESP_ERR__BAD_MSG</code>Local: Bad message format <code>-198</code><code>:bad_compression</code><code>RD_KAFKA_RESP_ERR__BAD_COMPRESSION</code>Local: Invalid compressed data <code>-197</code><code>:destroy</code><code>RD_KAFKA_RESP_ERR__DESTROY</code>Local: Broker handle destroyed for termination <code>-196</code><code>:fail</code><code>RD_KAFKA_RESP_ERR__FAIL</code>Local: Communication failure with broker <code>-195</code><code>:transport</code><code>RD_KAFKA_RESP_ERR__TRANSPORT</code>Local: Broker transport failure <code>-194</code><code>:crit_sys_resource</code><code>RD_KAFKA_RESP_ERR__CRIT_SYS_RESOURCE</code>Local: Critical system resource failure <code>-193</code><code>:resolve</code><code>RD_KAFKA_RESP_ERR__RESOLVE</code>Local: Host resolution failure <code>-192</code><code>:msg_timed_out</code><code>RD_KAFKA_RESP_ERR__MSG_TIMED_OUT</code>Local: Message timed out <code>-190</code><code>:unknown_partition</code><code>RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION</code>Local: Unknown partition <code>-189</code><code>:fs</code><code>RD_KAFKA_RESP_ERR__FS</code>Local: File or filesystem error <code>-188</code><code>:unknown_topic</code><code>RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC</code>Local: Unknown topic <code>-187</code><code>:all_brokers_down</code><code>RD_KAFKA_RESP_ERR__ALL_BROKERS_DOWN</code>Local: All broker connections are down <code>-186</code><code>:invalid_arg</code><code>RD_KAFKA_RESP_ERR__INVALID_ARG</code>Local: Invalid argument or configuration <code>-185</code><code>:timed_out</code><code>RD_KAFKA_RESP_ERR__TIMED_OUT</code>Local: Timed out <code>-184</code><code>:queue_full</code><code>RD_KAFKA_RESP_ERR__QUEUE_FULL</code>Local: Queue full <code>-183</code><code>:isr_insuff</code><code>RD_KAFKA_RESP_ERR__ISR_INSUFF</code>Local: ISR count insufficient <code>-182</code><code>:node_update</code><code>RD_KAFKA_RESP_ERR__NODE_UPDATE</code>Local: Broker node update <code>-181</code><code>:ssl</code><code>RD_KAFKA_RESP_ERR__SSL</code>Local: SSL error <code>-180</code><code>:wait_coord</code><code>RD_KAFKA_RESP_ERR__WAIT_COORD</code>Local: Waiting for coordinator <code>-179</code><code>:unknown_group</code><code>RD_KAFKA_RESP_ERR__UNKNOWN_GROUP</code>Local: Unknown group <code>-178</code><code>:in_progress</code><code>RD_KAFKA_RESP_ERR__IN_PROGRESS</code>Local: Operation in progress <code>-177</code><code>:prev_in_progress</code><code>RD_KAFKA_RESP_ERR__PREV_IN_PROGRESS</code>Local: Previous operation in progress <code>-176</code><code>:existing_subscription</code><code>RD_KAFKA_RESP_ERR__EXISTING_SUBSCRIPTION</code>Local: Existing subscription <code>-175</code><code>:assign_partitions</code><code>RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS</code>Local: Assign partitions <code>-174</code><code>:revoke_partitions</code><code>RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS</code>Local: Revoke partitions <code>-173</code><code>:conflict</code><code>RD_KAFKA_RESP_ERR__CONFLICT</code>Local: Conflicting use <code>-172</code><code>:state</code><code>RD_KAFKA_RESP_ERR__STATE</code>Local: Erroneous state <code>-171</code><code>:unknown_protocol</code><code>RD_KAFKA_RESP_ERR__UNKNOWN_PROTOCOL</code>Local: Unknown protocol <code>-170</code><code>:not_implemented</code><code>RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED</code>Local: Not implemented <code>-169</code><code>:authentication</code><code>RD_KAFKA_RESP_ERR__AUTHENTICATION</code>Local: Authentication failure <code>-168</code><code>:no_offset</code><code>RD_KAFKA_RESP_ERR__NO_OFFSET</code>Local: No offset stored <code>-167</code><code>:outdated</code><code>RD_KAFKA_RESP_ERR__OUTDATED</code>Local: Outdated <code>-166</code><code>:timed_out_queue</code><code>RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE</code>Local: Timed out in queue <code>-165</code><code>:unsupported_feature</code><code>RD_KAFKA_RESP_ERR__UNSUPPORTED_FEATURE</code>Local: Required feature not supported by broker <code>-164</code><code>:wait_cache</code><code>RD_KAFKA_RESP_ERR__WAIT_CACHE</code>Local: Awaiting cache update <code>-163</code><code>:intr</code><code>RD_KAFKA_RESP_ERR__INTR</code>Local: Operation interrupted <code>-162</code><code>:key_serialization</code><code>RD_KAFKA_RESP_ERR__KEY_SERIALIZATION</code>Local: Key serialization error <code>-161</code><code>:value_serialization</code><code>RD_KAFKA_RESP_ERR__VALUE_SERIALIZATION</code>Local: Value serialization error <code>-160</code><code>:key_deserialization</code><code>RD_KAFKA_RESP_ERR__KEY_DESERIALIZATION</code>Local: Key deserialization error <code>-159</code><code>:value_deserialization</code><code>RD_KAFKA_RESP_ERR__VALUE_DESERIALIZATION</code>Local: Value deserialization error <code>-158</code><code>:partial</code><code>RD_KAFKA_RESP_ERR__PARTIAL</code>Local: Partial response <code>-157</code><code>:read_only</code><code>RD_KAFKA_RESP_ERR__READ_ONLY</code>Local: Read-only object <code>-156</code><code>:noent</code><code>RD_KAFKA_RESP_ERR__NOENT</code>Local: No such entry <code>-155</code><code>:underflow</code><code>RD_KAFKA_RESP_ERR__UNDERFLOW</code>Local: Read underflow <code>-154</code><code>:invalid_type</code><code>RD_KAFKA_RESP_ERR__INVALID_TYPE</code>Local: Invalid type <code>-153</code><code>:retry</code><code>RD_KAFKA_RESP_ERR__RETRY</code>Local: Retry operation <code>-152</code><code>:purge_queue</code><code>RD_KAFKA_RESP_ERR__PURGE_QUEUE</code>Local: Purged in queue <code>-151</code><code>:purge_inflight</code><code>RD_KAFKA_RESP_ERR__PURGE_INFLIGHT</code>Local: Purged in flight <code>-150</code><code>:fatal</code><code>RD_KAFKA_RESP_ERR__FATAL</code>Local: Fatal error <code>-149</code><code>:inconsistent</code><code>RD_KAFKA_RESP_ERR__INCONSISTENT</code>Local: Inconsistent state <code>-148</code><code>:gapless_guarantee</code><code>RD_KAFKA_RESP_ERR__GAPLESS_GUARANTEE</code>Local: Gap-less ordering would not be guaranteed if proceeding <code>-147</code><code>:max_poll_exceeded</code><code>RD_KAFKA_RESP_ERR__MAX_POLL_EXCEEDED</code>Local: Maximum application poll interval (max.poll.interval.ms) exceeded <code>-146</code><code>:unknown_broker</code><code>RD_KAFKA_RESP_ERR__UNKNOWN_BROKER</code>Local: Unknown broker <code>-145</code><code>:not_configured</code><code>RD_KAFKA_RESP_ERR__NOT_CONFIGURED</code>Local: Functionality not configured <code>-144</code><code>:fenced</code><code>RD_KAFKA_RESP_ERR__FENCED</code>Local: This instance has been fenced by a newer instance <code>-143</code><code>:application</code><code>RD_KAFKA_RESP_ERR__APPLICATION</code>Local: Application generated error <code>-142</code><code>:assignment_lost</code><code>RD_KAFKA_RESP_ERR__ASSIGNMENT_LOST</code>Local: Group partition assignment lost <code>-141</code><code>:noop</code><code>RD_KAFKA_RESP_ERR__NOOP</code>Local: No operation performed <code>-140</code><code>:auto_offset_reset</code><code>RD_KAFKA_RESP_ERR__AUTO_OFFSET_RESET</code>Local: No offset to automatically reset to <code>-139</code><code>:log_truncation</code><code>RD_KAFKA_RESP_ERR__LOG_TRUNCATION</code>Local: Partition log truncation detected <code>-138</code><code>:invalid_different_record</code><code>RD_KAFKA_RESP_ERR__INVALID_DIFFERENT_RECORD</code>Local: an invalid record in the same batch caused the failure of this message too <code>-137</code><code>:destroy_broker</code><code>RD_KAFKA_RESP_ERR__DESTROY_BROKER</code>Local: Broker handle destroyed without termination"}, {"location": "Librdkafka-Errors/#broker-errors", "title": "Broker Errors", "text": "<p>Broker errors are those reported by the Kafka broker to the librdkafka client. These errors arise from the server side of Kafka operations and are relayed back to the client during normal communication.</p> <p>These errors are prefixed with a single underscore (<code>_</code>) in their error code names.</p> Code Symbol Name Description <code>-191</code><code>:partition_eof</code><code>RD_KAFKA_RESP_ERR__PARTITION_EOF</code>Broker: No more messages <code>1</code><code>:offset_out_of_range</code><code>RD_KAFKA_RESP_ERR_OFFSET_OUT_OF_RANGE</code>Broker: Offset out of range <code>2</code><code>:invalid_msg</code><code>RD_KAFKA_RESP_ERR_INVALID_MSG</code>Broker: Invalid message <code>3</code><code>:unknown_topic_or_part</code><code>RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART</code>Broker: Unknown topic or partition <code>4</code><code>:invalid_msg_size</code><code>RD_KAFKA_RESP_ERR_INVALID_MSG_SIZE</code>Broker: Invalid message size <code>5</code><code>:leader_not_available</code><code>RD_KAFKA_RESP_ERR_LEADER_NOT_AVAILABLE</code>Broker: Leader not available <code>6</code><code>:not_leader_for_partition</code><code>RD_KAFKA_RESP_ERR_NOT_LEADER_FOR_PARTITION</code>Broker: Not leader for partition <code>7</code><code>:request_timed_out</code><code>RD_KAFKA_RESP_ERR_REQUEST_TIMED_OUT</code>Broker: Request timed out <code>8</code><code>:broker_not_available</code><code>RD_KAFKA_RESP_ERR_BROKER_NOT_AVAILABLE</code>Broker: Broker not available <code>9</code><code>:replica_not_available</code><code>RD_KAFKA_RESP_ERR_REPLICA_NOT_AVAILABLE</code>Broker: Replica not available <code>10</code><code>:msg_size_too_large</code><code>RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE</code>Broker: Message size too large <code>11</code><code>:stale_ctrl_epoch</code><code>RD_KAFKA_RESP_ERR_STALE_CTRL_EPOCH</code>Broker: StaleControllerEpochCode <code>12</code><code>:offset_metadata_too_large</code><code>RD_KAFKA_RESP_ERR_OFFSET_METADATA_TOO_LARGE</code>Broker: Offset metadata string too large <code>13</code><code>:network_exception</code><code>RD_KAFKA_RESP_ERR_NETWORK_EXCEPTION</code>Broker: Broker disconnected before response received <code>14</code><code>:coordinator_load_in_progress</code><code>RD_KAFKA_RESP_ERR_COORDINATOR_LOAD_IN_PROGRESS</code>Broker: Coordinator load in progress <code>15</code><code>:coordinator_not_available</code><code>RD_KAFKA_RESP_ERR_COORDINATOR_NOT_AVAILABLE</code>Broker: Coordinator not available <code>16</code><code>:not_coordinator</code><code>RD_KAFKA_RESP_ERR_NOT_COORDINATOR</code>Broker: Not coordinator <code>17</code><code>:topic_exception</code><code>RD_KAFKA_RESP_ERR_TOPIC_EXCEPTION</code>Broker: Invalid topic <code>18</code><code>:record_list_too_large</code><code>RD_KAFKA_RESP_ERR_RECORD_LIST_TOO_LARGE</code>Broker: Message batch larger than configured server segment size <code>19</code><code>:not_enough_replicas</code><code>RD_KAFKA_RESP_ERR_NOT_ENOUGH_REPLICAS</code>Broker: Not enough in-sync replicas <code>20</code><code>:not_enough_replicas_after_append</code><code>RD_KAFKA_RESP_ERR_NOT_ENOUGH_REPLICAS_AFTER_APPEND</code>Broker: Message(s) written to insufficient number of in-sync replicas <code>21</code><code>:invalid_required_acks</code><code>RD_KAFKA_RESP_ERR_INVALID_REQUIRED_ACKS</code>Broker: Invalid required acks value <code>22</code><code>:illegal_generation</code><code>RD_KAFKA_RESP_ERR_ILLEGAL_GENERATION</code>Broker: Specified group generation id is not valid <code>23</code><code>:inconsistent_group_protocol</code><code>RD_KAFKA_RESP_ERR_INCONSISTENT_GROUP_PROTOCOL</code>Broker: Inconsistent group protocol <code>24</code><code>:invalid_group_id</code><code>RD_KAFKA_RESP_ERR_INVALID_GROUP_ID</code>Broker: Invalid group.id <code>25</code><code>:unknown_member_id</code><code>RD_KAFKA_RESP_ERR_UNKNOWN_MEMBER_ID</code>Broker: Unknown member <code>26</code><code>:invalid_session_timeout</code><code>RD_KAFKA_RESP_ERR_INVALID_SESSION_TIMEOUT</code>Broker: Invalid session timeout <code>27</code><code>:rebalance_in_progress</code><code>RD_KAFKA_RESP_ERR_REBALANCE_IN_PROGRESS</code>Broker: Group rebalance in progress <code>28</code><code>:invalid_commit_offset_size</code><code>RD_KAFKA_RESP_ERR_INVALID_COMMIT_OFFSET_SIZE</code>Broker: Commit offset data size is not valid <code>29</code><code>:topic_authorization_failed</code><code>RD_KAFKA_RESP_ERR_TOPIC_AUTHORIZATION_FAILED</code>Broker: Topic authorization failed <code>30</code><code>:group_authorization_failed</code><code>RD_KAFKA_RESP_ERR_GROUP_AUTHORIZATION_FAILED</code>Broker: Group authorization failed <code>31</code><code>:cluster_authorization_failed</code><code>RD_KAFKA_RESP_ERR_CLUSTER_AUTHORIZATION_FAILED</code>Broker: Cluster authorization failed <code>32</code><code>:invalid_timestamp</code><code>RD_KAFKA_RESP_ERR_INVALID_TIMESTAMP</code>Broker: Invalid timestamp <code>33</code><code>:unsupported_sasl_mechanism</code><code>RD_KAFKA_RESP_ERR_UNSUPPORTED_SASL_MECHANISM</code>Broker: Unsupported SASL mechanism <code>34</code><code>:illegal_sasl_state</code><code>RD_KAFKA_RESP_ERR_ILLEGAL_SASL_STATE</code>Broker: Request not valid in current SASL state <code>35</code><code>:unsupported_version</code><code>RD_KAFKA_RESP_ERR_UNSUPPORTED_VERSION</code>Broker: API version not supported <code>36</code><code>:topic_already_exists</code><code>RD_KAFKA_RESP_ERR_TOPIC_ALREADY_EXISTS</code>Broker: Topic already exists <code>37</code><code>:invalid_partitions</code><code>RD_KAFKA_RESP_ERR_INVALID_PARTITIONS</code>Broker: Invalid number of partitions <code>38</code><code>:invalid_replication_factor</code><code>RD_KAFKA_RESP_ERR_INVALID_REPLICATION_FACTOR</code>Broker: Invalid replication factor <code>39</code><code>:invalid_replica_assignment</code><code>RD_KAFKA_RESP_ERR_INVALID_REPLICA_ASSIGNMENT</code>Broker: Invalid replica assignment <code>40</code><code>:invalid_config</code><code>RD_KAFKA_RESP_ERR_INVALID_CONFIG</code>Broker: Configuration is invalid <code>41</code><code>:not_controller</code><code>RD_KAFKA_RESP_ERR_NOT_CONTROLLER</code>Broker: Not controller for cluster <code>42</code><code>:invalid_request</code><code>RD_KAFKA_RESP_ERR_INVALID_REQUEST</code>Broker: Invalid request <code>43</code><code>:unsupported_for_message_format</code><code>RD_KAFKA_RESP_ERR_UNSUPPORTED_FOR_MESSAGE_FORMAT</code>Broker: Message format on broker does not support request <code>44</code><code>:policy_violation</code><code>RD_KAFKA_RESP_ERR_POLICY_VIOLATION</code>Broker: Policy violation <code>45</code><code>:out_of_order_sequence_number</code><code>RD_KAFKA_RESP_ERR_OUT_OF_ORDER_SEQUENCE_NUMBER</code>Broker: Broker received an out of order sequence number <code>46</code><code>:duplicate_sequence_number</code><code>RD_KAFKA_RESP_ERR_DUPLICATE_SEQUENCE_NUMBER</code>Broker: Broker received a duplicate sequence number <code>47</code><code>:invalid_producer_epoch</code><code>RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH</code>Broker: Producer attempted an operation with an old epoch <code>48</code><code>:invalid_txn_state</code><code>RD_KAFKA_RESP_ERR_INVALID_TXN_STATE</code>Broker: Producer attempted a transactional operation in an invalid state <code>49</code><code>:invalid_producer_id_mapping</code><code>RD_KAFKA_RESP_ERR_INVALID_PRODUCER_ID_MAPPING</code>Broker: Producer attempted to use a producer id which is not currently assigned to its transactional id <code>50</code><code>:invalid_transaction_timeout</code><code>RD_KAFKA_RESP_ERR_INVALID_TRANSACTION_TIMEOUT</code>Broker: Transaction timeout is larger than the maximum value allowed by the broker's max.transaction.timeout.ms <code>51</code><code>:concurrent_transactions</code><code>RD_KAFKA_RESP_ERR_CONCURRENT_TRANSACTIONS</code>Broker: Producer attempted to update a transaction while another concurrent operation on the same transaction was ongoing <code>52</code><code>:transaction_coordinator_fenced</code><code>RD_KAFKA_RESP_ERR_TRANSACTION_COORDINATOR_FENCED</code>Broker: Indicates that the transaction coordinator sending a WriteTxnMarker is no longer the current coordinator for a given producer <code>53</code><code>:transactional_id_authorization_failed</code><code>RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED</code>Broker: Transactional Id authorization failed <code>54</code><code>:security_disabled</code><code>RD_KAFKA_RESP_ERR_SECURITY_DISABLED</code>Broker: Security features are disabled <code>55</code><code>:operation_not_attempted</code><code>RD_KAFKA_RESP_ERR_OPERATION_NOT_ATTEMPTED</code>Broker: Operation not attempted <code>56</code><code>:kafka_storage_error</code><code>RD_KAFKA_RESP_ERR_KAFKA_STORAGE_ERROR</code>Broker: Disk error when trying to access log file on disk <code>57</code><code>:log_dir_not_found</code><code>RD_KAFKA_RESP_ERR_LOG_DIR_NOT_FOUND</code>Broker: The user-specified log directory is not found in the broker config <code>58</code><code>:sasl_authentication_failed</code><code>RD_KAFKA_RESP_ERR_SASL_AUTHENTICATION_FAILED</code>Broker: SASL Authentication failed <code>59</code><code>:unknown_producer_id</code><code>RD_KAFKA_RESP_ERR_UNKNOWN_PRODUCER_ID</code>Broker: Unknown Producer Id <code>60</code><code>:reassignment_in_progress</code><code>RD_KAFKA_RESP_ERR_REASSIGNMENT_IN_PROGRESS</code>Broker: Partition reassignment is in progress <code>61</code><code>:delegation_token_auth_disabled</code><code>RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_AUTH_DISABLED</code>Broker: Delegation Token feature is not enabled <code>62</code><code>:delegation_token_not_found</code><code>RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_NOT_FOUND</code>Broker: Delegation Token is not found on server <code>63</code><code>:delegation_token_owner_mismatch</code><code>RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_OWNER_MISMATCH</code>Broker: Specified Principal is not valid Owner/Renewer <code>64</code><code>:delegation_token_request_not_allowed</code><code>RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_REQUEST_NOT_ALLOWED</code>Broker: Delegation Token requests are not allowed on this connection <code>65</code><code>:delegation_token_authorization_failed</code><code>RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_AUTHORIZATION_FAILED</code>Broker: Delegation Token authorization failed <code>66</code><code>:delegation_token_expired</code><code>RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_EXPIRED</code>Broker: Delegation Token is expired <code>67</code><code>:invalid_principal_type</code><code>RD_KAFKA_RESP_ERR_INVALID_PRINCIPAL_TYPE</code>Broker: Supplied principalType is not supported <code>68</code><code>:non_empty_group</code><code>RD_KAFKA_RESP_ERR_NON_EMPTY_GROUP</code>Broker: The group is not empty <code>69</code><code>:group_id_not_found</code><code>RD_KAFKA_RESP_ERR_GROUP_ID_NOT_FOUND</code>Broker: The group id does not exist <code>70</code><code>:fetch_session_id_not_found</code><code>RD_KAFKA_RESP_ERR_FETCH_SESSION_ID_NOT_FOUND</code>Broker: The fetch session ID was not found <code>71</code><code>:invalid_fetch_session_epoch</code><code>RD_KAFKA_RESP_ERR_INVALID_FETCH_SESSION_EPOCH</code>Broker: The fetch session epoch is invalid <code>72</code><code>:listener_not_found</code><code>RD_KAFKA_RESP_ERR_LISTENER_NOT_FOUND</code>Broker: No matching listener <code>73</code><code>:topic_deletion_disabled</code><code>RD_KAFKA_RESP_ERR_TOPIC_DELETION_DISABLED</code>Broker: Topic deletion is disabled <code>74</code><code>:fenced_leader_epoch</code><code>RD_KAFKA_RESP_ERR_FENCED_LEADER_EPOCH</code>Broker: Leader epoch is older than broker epoch <code>75</code><code>:unknown_leader_epoch</code><code>RD_KAFKA_RESP_ERR_UNKNOWN_LEADER_EPOCH</code>Broker: Leader epoch is newer than broker epoch <code>76</code><code>:unsupported_compression_type</code><code>RD_KAFKA_RESP_ERR_UNSUPPORTED_COMPRESSION_TYPE</code>Broker: Unsupported compression type <code>77</code><code>:stale_broker_epoch</code><code>RD_KAFKA_RESP_ERR_STALE_BROKER_EPOCH</code>Broker: Broker epoch has changed <code>78</code><code>:offset_not_available</code><code>RD_KAFKA_RESP_ERR_OFFSET_NOT_AVAILABLE</code>Broker: Leader high watermark is not caught up <code>79</code><code>:member_id_required</code><code>RD_KAFKA_RESP_ERR_MEMBER_ID_REQUIRED</code>Broker: Group member needs a valid member ID <code>80</code><code>:preferred_leader_not_available</code><code>RD_KAFKA_RESP_ERR_PREFERRED_LEADER_NOT_AVAILABLE</code>Broker: Preferred leader was not available <code>81</code><code>:group_max_size_reached</code><code>RD_KAFKA_RESP_ERR_GROUP_MAX_SIZE_REACHED</code>Broker: Consumer group has reached maximum size <code>82</code><code>:fenced_instance_id</code><code>RD_KAFKA_RESP_ERR_FENCED_INSTANCE_ID</code>Broker: Static consumer fenced by other consumer with same group.instance.id <code>83</code><code>:eligible_leaders_not_available</code><code>RD_KAFKA_RESP_ERR_ELIGIBLE_LEADERS_NOT_AVAILABLE</code>Broker: Eligible partition leaders are not available <code>84</code><code>:election_not_needed</code><code>RD_KAFKA_RESP_ERR_ELECTION_NOT_NEEDED</code>Broker: Leader election not needed for topic partition <code>85</code><code>:no_reassignment_in_progress</code><code>RD_KAFKA_RESP_ERR_NO_REASSIGNMENT_IN_PROGRESS</code>Broker: No partition reassignment is in progress <code>86</code><code>:group_subscribed_to_topic</code><code>RD_KAFKA_RESP_ERR_GROUP_SUBSCRIBED_TO_TOPIC</code>Broker: Deleting offsets of a topic while the consumer group is subscribed to it <code>87</code><code>:invalid_record</code><code>RD_KAFKA_RESP_ERR_INVALID_RECORD</code>Broker: Broker failed to validate record <code>88</code><code>:unstable_offset_commit</code><code>RD_KAFKA_RESP_ERR_UNSTABLE_OFFSET_COMMIT</code>Broker: There are unstable offsets that need to be cleared <code>89</code><code>:throttling_quota_exceeded</code><code>RD_KAFKA_RESP_ERR_THROTTLING_QUOTA_EXCEEDED</code>Broker: Throttling quota has been exceeded <code>90</code><code>:producer_fenced</code><code>RD_KAFKA_RESP_ERR_PRODUCER_FENCED</code>Broker: There is a newer producer with the same transactionalId which fences the current one <code>91</code><code>:resource_not_found</code><code>RD_KAFKA_RESP_ERR_RESOURCE_NOT_FOUND</code>Broker: Request illegally referred to resource that does not exist <code>92</code><code>:duplicate_resource</code><code>RD_KAFKA_RESP_ERR_DUPLICATE_RESOURCE</code>Broker: Request illegally referred to the same resource twice <code>93</code><code>:unacceptable_credential</code><code>RD_KAFKA_RESP_ERR_UNACCEPTABLE_CREDENTIAL</code>Broker: Requested credential would not meet criteria for acceptability <code>94</code><code>:inconsistent_voter_set</code><code>RD_KAFKA_RESP_ERR_INCONSISTENT_VOTER_SET</code>Broker: Indicates that the either the sender or recipient of a voter-only request is not one of the expected voters <code>95</code><code>:invalid_update_version</code><code>RD_KAFKA_RESP_ERR_INVALID_UPDATE_VERSION</code>Broker: Invalid update version <code>96</code><code>:feature_update_failed</code><code>RD_KAFKA_RESP_ERR_FEATURE_UPDATE_FAILED</code>Broker: Unable to update finalized features due to server error <code>97</code><code>:principal_deserialization_failure</code><code>RD_KAFKA_RESP_ERR_PRINCIPAL_DESERIALIZATION_FAILURE</code>Broker: Request principal deserialization failed during forwarding <code>100</code><code>:unknown_topic_id</code><code>RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_ID</code>Broker: Unknown topic id <code>110</code><code>:fenced_member_epoch</code><code>RD_KAFKA_RESP_ERR_FENCED_MEMBER_EPOCH</code>Broker: The member epoch is fenced by the group coordinator <code>111</code><code>:unreleased_instance_id</code><code>RD_KAFKA_RESP_ERR_UNRELEASED_INSTANCE_ID</code>Broker: The instance ID is still used by another member in the consumer group <code>112</code><code>:unsupported_assignor</code><code>RD_KAFKA_RESP_ERR_UNSUPPORTED_ASSIGNOR</code>Broker: The assignor or its version range is not supported by the consumer group <code>113</code><code>:stale_member_epoch</code><code>RD_KAFKA_RESP_ERR_STALE_MEMBER_EPOCH</code>Broker: The member epoch is stale <code>117</code><code>:unknown_subscription_id</code><code>RD_KAFKA_RESP_ERR_UNKNOWN_SUBSCRIPTION_ID</code>Broker: Client sent a push telemetry request with an invalid or outdated subscription ID <code>118</code><code>:telemetry_too_large</code><code>RD_KAFKA_RESP_ERR_TELEMETRY_TOO_LARGE</code>Broker: Client sent a push telemetry request larger than the maximum size the broker will accept <code>129</code><code>:rebootstrap_required</code><code>RD_KAFKA_RESP_ERR_REBOOTSTRAP_REQUIRED</code>Broker: Client metadata is stale, client should rebootstrap to obtain new metadata. <p>Last modified: 2025-04-28 12:49:47</p>"}, {"location": "Librdkafka-Statistics/", "title": "Statistics", "text": ""}, {"location": "Librdkafka-Statistics/#librdkafka-statistics-metrics-details", "title": "Librdkafka Statistics Metrics Details", "text": "<p>This page is a copy of the STATISTICS.md of <code>librdkafka</code>. </p> <p>The statistics presented below are based on the raw metrics from <code>librdkafka</code>. However, Karafka and WaterDrop enhance these metrics for a more comprehensive view. By default, both frameworks have the <code>statistics.interval.ms</code> set to 5 seconds, meaning statistics are refreshed at this interval. Be mindful of this default setting when analyzing metric data.</p> <p>Last modified: 2024-05-19 21:57:23</p> <p>librdkafka may be configured to emit internal metrics at a fixed interval by setting the <code>statistics.interval.ms</code> configuration property to a value &gt; 0 and registering a <code>stats_cb</code> (or similar, depending on language).</p> <p>The stats are provided as a JSON object string.</p> <p>Note: The metrics returned may not be completely consistent between           brokers, toppars and totals, due to the internal asynchronous           nature of librdkafka.           E.g., the top level <code>tx</code> total may be less than the sum of           the broker <code>tx</code> values which it represents.</p>"}, {"location": "Librdkafka-Statistics/#general-structure", "title": "General structure", "text": "<p>All fields that contain sizes are in bytes unless otherwise noted.</p> <pre><code>{\n &lt;Top-level fields&gt;\n \"brokers\": {\n    &lt;brokers fields&gt;,\n    \"toppars\": { &lt;toppars fields&gt; }\n },\n \"topics\": {\n   &lt;topic fields&gt;,\n   \"partitions\": {\n     &lt;partitions fields&gt;\n   }\n }\n[, \"cgrp\": { &lt;cgrp fields&gt; } ]\n[, \"eos\": { &lt;eos fields&gt; } ]\n}\n</code></pre>"}, {"location": "Librdkafka-Statistics/#field-type", "title": "Field type", "text": "<p>Fields are represented as follows:  * string - UTF8 string.  * int - Integer counter (64 bits wide). Ever increasing.  * int gauge - Integer gauge (64 bits wide). Will be reset to 0 on each stats emit.  * object - Nested JSON object.  * bool - <code>true</code> or <code>false</code>.</p>"}, {"location": "Librdkafka-Statistics/#top-level", "title": "Top-level", "text": "Field Type Example Description name string <code>\"rdkafka#producer-1\"</code> Handle instance name client_id string <code>\"rdkafka\"</code> The configured (or default) <code>client.id</code> type string <code>\"producer\"</code> Instance type (producer or consumer) ts int 12345678912345 librdkafka's internal monotonic clock (microseconds) time int Wall clock time in seconds since the epoch age int Time since this client instance was created (microseconds) replyq int gauge Number of ops (callbacks, events, etc) waiting in queue for application to serve with rd_kafka_poll() msg_cnt int gauge Current number of messages in producer queues msg_size int gauge Current total size of messages in producer queues msg_max int Threshold: maximum number of messages allowed allowed on the producer queues msg_size_max int Threshold: maximum total size of messages allowed on the producer queues tx int Total number of requests sent to Kafka brokers tx_bytes int Total number of bytes transmitted to Kafka brokers rx int Total number of responses received from Kafka brokers rx_bytes int Total number of bytes received from Kafka brokers txmsgs int Total number of messages transmitted (produced) to Kafka brokers txmsg_bytes int Total number of message bytes (including framing, such as per-Message framing and MessageSet/batch framing) transmitted to Kafka brokers rxmsgs int Total number of messages consumed, not including ignored messages (due to offset, etc), from Kafka brokers. rxmsg_bytes int Total number of message bytes (including framing) received from Kafka brokers simple_cnt int gauge Internal tracking of legacy vs new consumer API state metadata_cache_cnt int gauge Number of topics in the metadata cache. brokers object Dict of brokers, key is broker name, value is object. See brokers below topics object Dict of topics, key is topic name, value is object. See topics below cgrp object Consumer group metrics. See cgrp below eos object EOS / Idempotent producer state and metrics. See eos below"}, {"location": "Librdkafka-Statistics/#brokers", "title": "brokers", "text": "<p>Per broker statistics.</p> Field Type Example Description name string <code>\"example.com:9092/13\"</code> Broker hostname, port and broker id nodeid int 13 Broker id (-1 for bootstraps) nodename string <code>\"example.com:9092\"</code> Broker hostname source string <code>\"configured\"</code> Broker source (learned, configured, internal, logical) state string <code>\"UP\"</code> Broker state (INIT, DOWN, CONNECT, AUTH, APIVERSION_QUERY, AUTH_HANDSHAKE, UP, UPDATE) stateage int gauge Time since last broker state change (microseconds) outbuf_cnt int gauge Number of requests awaiting transmission to broker outbuf_msg_cnt int gauge Number of messages awaiting transmission to broker waitresp_cnt int gauge Number of requests in-flight to broker awaiting response waitresp_msg_cnt int gauge Number of messages in-flight to broker awaiting response tx int Total number of requests sent txbytes int Total number of bytes sent txerrs int Total number of transmission errors txretries int Total number of request retries txidle int Microseconds since last socket send (or -1 if no sends yet for current connection). req_timeouts int Total number of requests timed out rx int Total number of responses received rxbytes int Total number of bytes received rxerrs int Total number of receive errors rxcorriderrs int Total number of unmatched correlation ids in response (typically for timed out requests) rxpartial int Total number of partial MessageSets received. The broker may return partial responses if the full MessageSet could not fit in the remaining Fetch response size. rxidle int Microseconds since last socket receive (or -1 if no receives yet for current connection). req object Request type counters. Object key is the request name, value is the number of requests sent. zbuf_grow int Total number of decompression buffer size increases buf_grow int Total number of buffer size increases (deprecated, unused) wakeups int Broker thread poll loop wakeups connects int Number of connection attempts, including successful and failed, and name resolution failures. disconnects int Number of disconnects (triggered by broker, network, load-balancer, etc.). int_latency object Internal producer queue latency in microseconds. See Window stats below outbuf_latency object Internal request queue latency in microseconds. This is the time between a request is enqueued on the transmit (outbuf) queue and the time the request is written to the TCP socket. Additional buffering and latency may be incurred by the TCP stack and network. See Window stats below rtt object Broker latency / round-trip time in microseconds. See Window stats below throttle object Broker throttling time in milliseconds. See Window stats below toppars object Partitions handled by this broker handle. Key is \"topic-partition\". See brokers.toppars below"}, {"location": "Librdkafka-Statistics/#window-stats", "title": "Window stats", "text": "<p>Rolling window statistics. The values are in microseconds unless otherwise stated.</p> Field Type Example Description min int gauge Smallest value max int gauge Largest value avg int gauge Average value sum int gauge Sum of values cnt int gauge Number of values sampled stddev int gauge Standard deviation (based on histogram) hdrsize int gauge Memory size of Hdr Histogram p50 int gauge 50th percentile p75 int gauge 75th percentile p90 int gauge 90th percentile p95 int gauge 95th percentile p99 int gauge 99th percentile p99_99 int gauge 99.99th percentile outofrange int gauge Values skipped due to out of histogram range"}, {"location": "Librdkafka-Statistics/#brokerstoppars", "title": "brokers.toppars", "text": "<p>Topic partition assigned to broker.</p> Field Type Example Description topic string <code>\"mytopic\"</code> Topic name partition int 3 Partition id"}, {"location": "Librdkafka-Statistics/#topics", "title": "topics", "text": "Field Type Example Description topic string <code>\"myatopic\"</code> Topic name age int gauge Age of client's topic object (milliseconds) metadata_age int gauge Age of metadata from broker for this topic (milliseconds) batchsize object Batch sizes in bytes. See Window stats\u00b7 batchcnt object Batch message counts. See Window stats\u00b7 partitions object Partitions dict, key is partition id. See partitions below."}, {"location": "Librdkafka-Statistics/#partitions", "title": "partitions", "text": "Field Type Example Description partition int 3 Partition Id (-1 for internal UA/UnAssigned partition) broker int The id of the broker that messages are currently being fetched from leader int Current leader broker id desired bool Partition is explicitly desired by application unknown bool Partition not seen in topic metadata from broker msgq_cnt int gauge Number of messages waiting to be produced in first-level queue msgq_bytes int gauge Number of bytes in msgq_cnt xmit_msgq_cnt int gauge Number of messages ready to be produced in transmit queue xmit_msgq_bytes int gauge Number of bytes in xmit_msgq fetchq_cnt int gauge Number of pre-fetched messages in fetch queue fetchq_size int gauge Bytes in fetchq fetch_state string <code>\"active\"</code> Consumer fetch state for this partition (none, stopping, stopped, offset-query, offset-wait, active). query_offset int gauge Current/Last logical offset query next_offset int gauge Next offset to fetch app_offset int gauge Offset of last message passed to application + 1 stored_offset int gauge Offset to be committed stored_leader_epoch int Partition leader epoch of stored offset committed_offset int gauge Last committed offset committed_leader_epoch int Partition leader epoch of committed offset eof_offset int gauge Last PARTITION_EOF signaled offset lo_offset int gauge Partition's low watermark offset on broker hi_offset int gauge Partition's high watermark offset on broker ls_offset int gauge Partition's last stable offset on broker, or same as hi_offset is broker version is less than 0.11.0.0. consumer_lag int gauge Difference between (hi_offset or ls_offset) and committed_offset). hi_offset is used when isolation.level=read_uncommitted, otherwise ls_offset. consumer_lag_stored int gauge Difference between (hi_offset or ls_offset) and stored_offset. See consumer_lag and stored_offset. leader_epoch int Last known partition leader epoch, or -1 if unknown. txmsgs int Total number of messages transmitted (produced) txbytes int Total number of bytes transmitted for txmsgs rxmsgs int Total number of messages consumed, not including ignored messages (due to offset, etc). rxbytes int Total number of bytes received for rxmsgs msgs int Total number of messages received (consumer, same as rxmsgs), or total number of messages produced (possibly not yet transmitted) (producer). rx_ver_drops int Dropped outdated messages msgs_inflight int gauge Current number of messages in-flight to/from broker next_ack_seq int gauge Next expected acked sequence (idempotent producer) next_err_seq int gauge Next expected errored sequence (idempotent producer) acked_msgid int Last acked internal message id (idempotent producer)"}, {"location": "Librdkafka-Statistics/#cgrp", "title": "cgrp", "text": "Field Type Example Description state string \"up\" Local consumer group handler's state. stateage int gauge Time elapsed since last state change (milliseconds). join_state string \"assigned\" Local consumer group handler's join state. rebalance_age int gauge Time elapsed since last rebalance (assign or revoke) (milliseconds). rebalance_cnt int Total number of rebalances (assign or revoke). rebalance_reason string Last rebalance reason, or empty string. assignment_size int gauge Current assignment's partition count."}, {"location": "Librdkafka-Statistics/#eos", "title": "eos", "text": "Field Type Example Description idemp_state string \"Assigned\" Current idempotent producer id state. idemp_stateage int gauge Time elapsed since last idemp_state change (milliseconds). txn_state string \"InTransaction\" Current transactional producer state. txn_stateage int gauge Time elapsed since last txn_state change (milliseconds). txn_may_enq bool Transactional state allows enqueuing (producing) new messages. producer_id int gauge The currently assigned Producer ID (or -1). producer_epoch int gauge The current epoch (or -1). epoch_cnt int The number of Producer ID assignments since start."}, {"location": "Librdkafka-Statistics/#example-output", "title": "Example output", "text": "<p>This (prettified) example output is from a short-lived producer using the following command: <code>rdkafka_performance -b localhost -P -t test -T 1000 -Y 'cat &gt;&gt; stats.json'</code>.</p> <p>Note: this output is prettified using <code>jq .</code>, the JSON object emitted by librdkafka does not contain line breaks.</p> <pre><code>{\n  \"name\": \"rdkafka#producer-1\",\n  \"client_id\": \"rdkafka\",\n  \"type\": \"producer\",\n  \"ts\": 5016483227792,\n  \"time\": 1527060869,\n  \"replyq\": 0,\n  \"msg_cnt\": 22710,\n  \"msg_size\": 704010,\n  \"msg_max\": 500000,\n  \"msg_size_max\": 1073741824,\n  \"simple_cnt\": 0,\n  \"metadata_cache_cnt\": 1,\n  \"brokers\": {\n    \"localhost:9092/2\": {\n      \"name\": \"localhost:9092/2\",\n      \"nodeid\": 2,\n      \"nodename\": \"localhost:9092\",\n      \"source\": \"learned\",\n      \"state\": \"UP\",\n      \"stateage\": 9057234,\n      \"outbuf_cnt\": 0,\n      \"outbuf_msg_cnt\": 0,\n      \"waitresp_cnt\": 0,\n      \"waitresp_msg_cnt\": 0,\n      \"tx\": 320,\n      \"txbytes\": 84283332,\n      \"txerrs\": 0,\n      \"txretries\": 0,\n      \"req_timeouts\": 0,\n      \"rx\": 320,\n      \"rxbytes\": 15708,\n      \"rxerrs\": 0,\n      \"rxcorriderrs\": 0,\n      \"rxpartial\": 0,\n      \"zbuf_grow\": 0,\n      \"buf_grow\": 0,\n      \"wakeups\": 591067,\n      \"int_latency\": {\n        \"min\": 86,\n        \"max\": 59375,\n        \"avg\": 23726,\n        \"sum\": 5694616664,\n        \"stddev\": 13982,\n        \"p50\": 28031,\n        \"p75\": 36095,\n        \"p90\": 39679,\n        \"p95\": 43263,\n        \"p99\": 48639,\n        \"p99_99\": 59391,\n        \"outofrange\": 0,\n        \"hdrsize\": 11376,\n        \"cnt\": 240012\n      },\n      \"rtt\": {\n        \"min\": 1580,\n        \"max\": 3389,\n        \"avg\": 2349,\n        \"sum\": 79868,\n        \"stddev\": 474,\n        \"p50\": 2319,\n        \"p75\": 2543,\n        \"p90\": 3183,\n        \"p95\": 3199,\n        \"p99\": 3391,\n        \"p99_99\": 3391,\n        \"outofrange\": 0,\n        \"hdrsize\": 13424,\n        \"cnt\": 34\n      },\n      \"throttle\": {\n        \"min\": 0,\n        \"max\": 0,\n        \"avg\": 0,\n        \"sum\": 0,\n        \"stddev\": 0,\n        \"p50\": 0,\n        \"p75\": 0,\n        \"p90\": 0,\n        \"p95\": 0,\n        \"p99\": 0,\n        \"p99_99\": 0,\n        \"outofrange\": 0,\n        \"hdrsize\": 17520,\n        \"cnt\": 34\n      },\n      \"toppars\": {\n        \"test-1\": {\n          \"topic\": \"test\",\n          \"partition\": 1\n        }\n      }\n    },\n    \"localhost:9093/3\": {\n      \"name\": \"localhost:9093/3\",\n      \"nodeid\": 3,\n      \"nodename\": \"localhost:9093\",\n      \"source\": \"learned\",\n      \"state\": \"UP\",\n      \"stateage\": 9057209,\n      \"outbuf_cnt\": 0,\n      \"outbuf_msg_cnt\": 0,\n      \"waitresp_cnt\": 0,\n      \"waitresp_msg_cnt\": 0,\n      \"tx\": 310,\n      \"txbytes\": 84301122,\n      \"txerrs\": 0,\n      \"txretries\": 0,\n      \"req_timeouts\": 0,\n      \"rx\": 310,\n      \"rxbytes\": 15104,\n      \"rxerrs\": 0,\n      \"rxcorriderrs\": 0,\n      \"rxpartial\": 0,\n      \"zbuf_grow\": 0,\n      \"buf_grow\": 0,\n      \"wakeups\": 607956,\n      \"int_latency\": {\n        \"min\": 82,\n        \"max\": 58069,\n        \"avg\": 23404,\n        \"sum\": 5617432101,\n        \"stddev\": 14021,\n        \"p50\": 27391,\n        \"p75\": 35839,\n        \"p90\": 39679,\n        \"p95\": 42751,\n        \"p99\": 48639,\n        \"p99_99\": 58111,\n        \"outofrange\": 0,\n        \"hdrsize\": 11376,\n        \"cnt\": 240016\n      },\n      \"rtt\": {\n        \"min\": 1704,\n        \"max\": 3572,\n        \"avg\": 2493,\n        \"sum\": 87289,\n        \"stddev\": 559,\n        \"p50\": 2447,\n        \"p75\": 2895,\n        \"p90\": 3375,\n        \"p95\": 3407,\n        \"p99\": 3583,\n        \"p99_99\": 3583,\n        \"outofrange\": 0,\n        \"hdrsize\": 13424,\n        \"cnt\": 35\n      },\n      \"throttle\": {\n        \"min\": 0,\n        \"max\": 0,\n        \"avg\": 0,\n        \"sum\": 0,\n        \"stddev\": 0,\n        \"p50\": 0,\n        \"p75\": 0,\n        \"p90\": 0,\n        \"p95\": 0,\n        \"p99\": 0,\n        \"p99_99\": 0,\n        \"outofrange\": 0,\n        \"hdrsize\": 17520,\n        \"cnt\": 35\n      },\n      \"toppars\": {\n        \"test-0\": {\n          \"topic\": \"test\",\n          \"partition\": 0\n        }\n      }\n    },\n    \"localhost:9094/4\": {\n      \"name\": \"localhost:9094/4\",\n      \"nodeid\": 4,\n      \"nodename\": \"localhost:9094\",\n      \"source\": \"learned\",\n      \"state\": \"UP\",\n      \"stateage\": 9057207,\n      \"outbuf_cnt\": 0,\n      \"outbuf_msg_cnt\": 0,\n      \"waitresp_cnt\": 0,\n      \"waitresp_msg_cnt\": 0,\n      \"tx\": 1,\n      \"txbytes\": 25,\n      \"txerrs\": 0,\n      \"txretries\": 0,\n      \"req_timeouts\": 0,\n      \"rx\": 1,\n      \"rxbytes\": 272,\n      \"rxerrs\": 0,\n      \"rxcorriderrs\": 0,\n      \"rxpartial\": 0,\n      \"zbuf_grow\": 0,\n      \"buf_grow\": 0,\n      \"wakeups\": 4,\n      \"int_latency\": {\n        \"min\": 0,\n        \"max\": 0,\n        \"avg\": 0,\n        \"sum\": 0,\n        \"stddev\": 0,\n        \"p50\": 0,\n        \"p75\": 0,\n        \"p90\": 0,\n        \"p95\": 0,\n        \"p99\": 0,\n        \"p99_99\": 0,\n        \"outofrange\": 0,\n        \"hdrsize\": 11376,\n        \"cnt\": 0\n      },\n      \"rtt\": {\n        \"min\": 0,\n        \"max\": 0,\n        \"avg\": 0,\n        \"sum\": 0,\n        \"stddev\": 0,\n        \"p50\": 0,\n        \"p75\": 0,\n        \"p90\": 0,\n        \"p95\": 0,\n        \"p99\": 0,\n        \"p99_99\": 0,\n        \"outofrange\": 0,\n        \"hdrsize\": 13424,\n        \"cnt\": 0\n      },\n      \"throttle\": {\n        \"min\": 0,\n        \"max\": 0,\n        \"avg\": 0,\n        \"sum\": 0,\n        \"stddev\": 0,\n        \"p50\": 0,\n        \"p75\": 0,\n        \"p90\": 0,\n        \"p95\": 0,\n        \"p99\": 0,\n        \"p99_99\": 0,\n        \"outofrange\": 0,\n        \"hdrsize\": 17520,\n        \"cnt\": 0\n      },\n      \"toppars\": {}\n    }\n  },\n  \"topics\": {\n    \"test\": {\n      \"topic\": \"test\",\n      \"metadata_age\": 9060,\n      \"batchsize\": {\n        \"min\": 99,\n        \"max\": 391805,\n        \"avg\": 272593,\n        \"sum\": 18808985,\n        \"stddev\": 180408,\n        \"p50\": 393215,\n        \"p75\": 393215,\n        \"p90\": 393215,\n        \"p95\": 393215,\n        \"p99\": 393215,\n        \"p99_99\": 393215,\n        \"outofrange\": 0,\n        \"hdrsize\": 14448,\n        \"cnt\": 69\n      },\n      \"batchcnt\": {\n        \"min\": 1,\n        \"max\": 10000,\n        \"avg\": 6956,\n        \"sum\": 480028,\n        \"stddev\": 4608,\n        \"p50\": 10047,\n        \"p75\": 10047,\n        \"p90\": 10047,\n        \"p95\": 10047,\n        \"p99\": 10047,\n        \"p99_99\": 10047,\n        \"outofrange\": 0,\n        \"hdrsize\": 8304,\n        \"cnt\": 69\n      },\n      \"partitions\": {\n        \"0\": {\n          \"partition\": 0,\n          \"broker\": 3,\n          \"leader\": 3,\n          \"desired\": false,\n          \"unknown\": false,\n          \"msgq_cnt\": 1,\n          \"msgq_bytes\": 31,\n          \"xmit_msgq_cnt\": 0,\n          \"xmit_msgq_bytes\": 0,\n          \"fetchq_cnt\": 0,\n          \"fetchq_size\": 0,\n          \"fetch_state\": \"none\",\n          \"query_offset\": 0,\n          \"next_offset\": 0,\n          \"app_offset\": -1001,\n          \"stored_offset\": -1001,\n          \"commited_offset\": -1001,\n          \"committed_offset\": -1001,\n          \"eof_offset\": -1001,\n          \"lo_offset\": -1001,\n          \"hi_offset\": -1001,\n          \"consumer_lag\": -1,\n          \"txmsgs\": 2150617,\n          \"txbytes\": 66669127,\n          \"rxmsgs\": 0,\n          \"rxbytes\": 0,\n          \"msgs\": 2160510,\n          \"rx_ver_drops\": 0\n        },\n        \"1\": {\n          \"partition\": 1,\n          \"broker\": 2,\n          \"leader\": 2,\n          \"desired\": false,\n          \"unknown\": false,\n          \"msgq_cnt\": 0,\n          \"msgq_bytes\": 0,\n          \"xmit_msgq_cnt\": 0,\n          \"xmit_msgq_bytes\": 0,\n          \"fetchq_cnt\": 0,\n          \"fetchq_size\": 0,\n          \"fetch_state\": \"none\",\n          \"query_offset\": 0,\n          \"next_offset\": 0,\n          \"app_offset\": -1001,\n          \"stored_offset\": -1001,\n          \"commited_offset\": -1001,\n          \"committed_offset\": -1001,\n          \"eof_offset\": -1001,\n          \"lo_offset\": -1001,\n          \"hi_offset\": -1001,\n          \"consumer_lag\": -1,\n          \"txmsgs\": 2150136,\n          \"txbytes\": 66654216,\n          \"rxmsgs\": 0,\n          \"rxbytes\": 0,\n          \"msgs\": 2159735,\n          \"rx_ver_drops\": 0\n        },\n        \"-1\": {\n          \"partition\": -1,\n          \"broker\": -1,\n          \"leader\": -1,\n          \"desired\": false,\n          \"unknown\": false,\n          \"msgq_cnt\": 0,\n          \"msgq_bytes\": 0,\n          \"xmit_msgq_cnt\": 0,\n          \"xmit_msgq_bytes\": 0,\n          \"fetchq_cnt\": 0,\n          \"fetchq_size\": 0,\n          \"fetch_state\": \"none\",\n          \"query_offset\": 0,\n          \"next_offset\": 0,\n          \"app_offset\": -1001,\n          \"stored_offset\": -1001,\n          \"commited_offset\": -1001,\n          \"committed_offset\": -1001,\n          \"eof_offset\": -1001,\n          \"lo_offset\": -1001,\n          \"hi_offset\": -1001,\n          \"consumer_lag\": -1,\n          \"txmsgs\": 0,\n          \"txbytes\": 0,\n          \"rxmsgs\": 0,\n          \"rxbytes\": 0,\n          \"msgs\": 1177,\n          \"rx_ver_drops\": 0\n        }\n      }\n    }\n  },\n  \"tx\": 631,\n  \"tx_bytes\": 168584479,\n  \"rx\": 631,\n  \"rx_bytes\": 31084,\n  \"txmsgs\": 4300753,\n  \"txmsg_bytes\": 133323343,\n  \"rxmsgs\": 0,\n  \"rxmsg_bytes\": 0\n}\n</code></pre>"}, {"location": "License/", "title": "License", "text": "<p>This License Applies Only to Wiki Content</p> <p>Please note that this license applies exclusively to the Wiki's content and does not apply to any specific Karafka ecosystem components.</p> <p>Each Karafka ecosystem component has its distinct license (or licenses), which are provided in the respective repositories. Please refer to the specific repository documentation for detailed information regarding the licensing of particular Karafka components.</p> <p>This license is intended solely to govern the use of the Wiki content, ensuring proper restrictions and permitted non-commercial use as specified above.</p>"}, {"location": "License/#all-rights-reserved", "title": "All Rights Reserved", "text": "<p>The content of this wiki, including but not limited to text, images, code snippets, and other media, is the exclusive property of the author (\"the Owner\"), unless stated otherwise. No part of this wiki may be copied, reproduced, modified, stored, transmitted, or distributed in any form or by any means, whether electronic, mechanical, photocopying, recording, or otherwise, without the Owner's prior written consent, except as outlined below.</p>"}, {"location": "License/#permitted-non-commercial-uses", "title": "Permitted Non-Commercial Uses", "text": "<p>The following uses of the content are permitted without prior written consent, provided they are non-commercial in nature and appropriate attribution is given:</p> <ol> <li>Blog Posts: You may use excerpts of the content in personal or non-commercial blogs.</li> <li>Vlogs and Video Content: You may refer to and include portions of the content in non-commercial video content (e.g., YouTube videos) as long as the content is not behind a paywall or used for monetization.</li> <li>Educational Materials: You may use the content for non-commercial educational purposes, including presentations, workshops, and study materials.</li> <li>Non-Commercial Publications: Content may be included in non-commercial publications, such as free e-books or educational guides, so long as no charge is levied for access.</li> </ol>"}, {"location": "License/#conditions-of-use", "title": "Conditions of Use", "text": "<ol> <li> <p>Attribution: Any use of the content as permitted above must include proper attribution to the Owner, including a link to the source if possible. Attribution must indicate the author and the source of the content.</p> </li> <li> <p>Modification Prohibited: You may not modify or create derivative works from the content, even for non-commercial purposes, without prior written consent.</p> </li> <li> <p>Redistribution Prohibited: Except as allowed in the non-commercial use cases above, redistribution of the original content or significant portions thereof in any form is prohibited.</p> </li> <li> <p>Commercial Use: Any use of the content for commercial purposes is strictly prohibited without explicit written consent from the Owner.</p> </li> </ol>"}, {"location": "License/#no-license-granted-beyond-permitted-use", "title": "No License Granted Beyond Permitted Use", "text": "<p>No additional license, express or implied, is granted for any use of the content beyond the permitted non-commercial use as outlined above.</p>"}, {"location": "License/#contributions-to-the-wiki", "title": "Contributions to the Wiki", "text": "<p>By sending a pull request or contributing in any other way to this Wiki, you agree to transfer the copyright of your changes to the Owner. This transfer ensures that the Owner retains exclusive control over the content of the wiki, including the right to use, modify, and distribute any contributed content as part of the wiki under the same license terms.</p>"}, {"location": "License/#enforcement", "title": "Enforcement", "text": "<p>Any unauthorized use of the content may result in legal action, including but not limited to injunctions, claims for damages, and prosecution under applicable copyright laws.</p> <p>Last modified: 2024-10-20 20:07:43</p>"}, {"location": "Monitoring-and-Logging/", "title": "Monitoring and Logging", "text": "<p>Karafka uses a simple monitor with an API compatible with <code>dry-monitor</code> and <code>ActiveSupport::Notifications</code> to which you can easily hook up with your listeners. You can use it to develop your monitoring and logging systems (for example, NewRelic) or perform additional operations during certain phases of the Karafka framework lifecycle.</p> <p>The only thing hooked up to this monitoring is the Karafka logger listener (<code>Karafka::Instrumentation::LoggerListener</code>). It is based on a standard Ruby logger or Ruby on Rails logger when used with Rails. You can find it in your <code>karafka.rb</code> file:</p> <pre><code>Karafka.monitor.subscribe(Karafka::Instrumentation::LoggerListener.new)\n</code></pre> <p>If you are looking for examples of implementing your listeners, here, you can take a look at the default Karafka logger listener implementation.</p> <p>The only thing you need to be aware of when developing your listeners is that the internals of the payload may differ depending on the instrumentation place.</p> <p>A complete list of the supported events can be found here.</p> <p>Handle Instrumentation Listener Errors Carefully</p> <p>Instrumentation listeners in Karafka must not generate errors as they can cause severe disruptions, including forcing the framework into a recovery mode or even shutting down processes. Errors within worker threads can lead to improper message acknowledgment, resulting in message loss or duplication. Always thoroughly test your instrumentation code and incorporate robust internal error handling to prevent any impact on the stability and functionality of your Karafka application.</p>"}, {"location": "Monitoring-and-Logging/#subscribing-to-the-instrumentation-events", "title": "Subscribing to the instrumentation events", "text": "<p>The best place to hook your listener is at the end of the <code>karafka.rb</code> file. This will guarantee that your custom listener will be already loaded into memory and visible for the Karafka framework.</p> <p>You should set up listeners after configuring the app because Karafka sets up its internal components right after the configuration block. That way, we can be sure everything is loaded and initialized correctly.</p>"}, {"location": "Monitoring-and-Logging/#subscribing-with-a-listener-classmodule", "title": "Subscribing with a listener class/module", "text": "<pre><code>Karafka.monitor.subscribe(MyAirbrakeListener.new)\n</code></pre>"}, {"location": "Monitoring-and-Logging/#subscribing-with-a-block", "title": "Subscribing with a block", "text": "<pre><code>Karafka.monitor.subscribe 'error.occurred' do |event|\n  type = event[:type]\n  error = event[:error]\n  details = (error.backtrace || []).join(\"\\n\")\n\n  puts \"Oh no! An error: #{error} of type: #{type} occurred!\"\n  puts details\nend\n</code></pre>"}, {"location": "Monitoring-and-Logging/#using-the-appinitialized-event-to-initialize-additional-karafka-framework-settings-dependent-libraries", "title": "Using the app.initialized event to initialize additional Karafka framework settings dependent libraries", "text": "<p>Lifecycle events can be used in various situations, for example, to configure external software or run additional one-time commands before messages receiving flow starts.</p> <pre><code># Once everything is loaded and done, assign the Karafka app logger as a Sidekiq logger\n# @note This example does not use config details, but you can use all the config values\n#   via Karafka::App.config method to setup your external components\nKarafka.monitor.subscribe('app.initialized') do |_event|\n  Sidekiq::Logging.logger = Karafka::App.logger\nend\n</code></pre>"}, {"location": "Monitoring-and-Logging/#using-the-karafkamonitor-for-application-specific-events", "title": "Using the <code>Karafka.monitor</code> for application specific events", "text": "<p><code>Karafka.monitor</code> can be used for monitoring Karafka's internal events and for instrumenting and observing custom, application-specific events. Allowing developers to register and monitor their own events provides a unified way to instrument Karafka and custom business operations within Karafka.</p>"}, {"location": "Monitoring-and-Logging/#registering-and-instrumenting-custom-events", "title": "Registering and Instrumenting Custom Events", "text": "<p>To register your custom event with the Karafka monitor, you can use the <code>#register_event</code> method:</p> <pre><code># Register the event\nKarafka.monitor.notifications_bus.register_event('app.custom.event')\n</code></pre> <p>After registering an event, you can instrument with it as follows:</p> <pre><code>Karafka.monitor.instrument('app.custom.event') do\n  puts 'This is my instrumented custom logic!'\nend\n</code></pre> <p>You can subscribe to those events the same way as you subscribe to the internal Karafka events:</p> <pre><code># Via a block:\nKarafka.monitor.subscribe('app.custom.event') do |event|\n  puts \"Custom logic was executed. Details: #{event}\"\nend\n\n# Or by using a listener with `#on_app_custom_event` method:\nKarafka.monitor.subscribe(AppEventsListener.new)\n</code></pre>"}, {"location": "Monitoring-and-Logging/#use-cases-for-custom-events", "title": "Use Cases for Custom Events", "text": "<p>Here are some examples where instrumenting custom events can be beneficial:</p> <ul> <li> <p>Performance Monitoring: If your application has a particular business operation or function that you suspect might be a performance bottleneck, you can instrument it and measure its execution time.</p> </li> <li> <p>External Service Calls: If your application interacts with external APIs or services, you can instrument events to monitor the success, failure, and response time of these external calls.</p> </li> <li> <p>Data Flow Monitoring: In data-intensive applications, you can instrument events to monitor data flow as it's ingested, processed, transformed, or exported.</p> </li> </ul>"}, {"location": "Monitoring-and-Logging/#naming-considerations-for-custom-events", "title": "Naming Considerations for Custom Events", "text": "<p>Ensuring that your custom events' names don't clash with Karafka's internal events is essential. As a best practice, consider prefixing your event names with a unique identifier like <code>app.</code> or any other prefix that distinguishes your events from Karafka's. This approach prevents naming conflicts and provides clarity when observing and debugging events.</p> <p>For example, a custom event to monitor external API calls could be named <code>app.external_api_call</code>:</p> <pre><code>Karafka.monitor.notifications_bus.register_event('app.external_api_call')\n</code></pre>"}, {"location": "Monitoring-and-Logging/#usage-statistics-and-subscribing-to-statisticsemitted-event", "title": "Usage Statistics and Subscribing to <code>statistics.emitted</code> Event", "text": "<p>Always keep <code>statistics.emitted</code> handlers concise and non-blocking</p> <p>When subscribing to <code>statistics.emitted</code>, ensure your code is concise and non-blocking, as this runs every 5 seconds and during active processing. Long-running handlers can impede the polling process, affecting message consumption. Rigorously test your handlers - failures in processing these statistics can lead to critical exceptions that disrupt your consumption process.</p> <p>Karafka emits metrics every 5 seconds by default, governed by the Kafka setting <code>statistics.interval.ms</code>. Metrics are also published during processing and long polling. Whether you are processing data or waiting on more information being shipped from Kafka, metrics publishing will occur.</p> <p>Karafka may be configured to emit internal metrics at a fixed interval by setting the <code>kafka</code> <code>statistics.interval.ms</code> configuration property to a value &gt; <code>0</code>. Once that is done, emitted statistics are available after subscribing to the <code>statistics.emitted</code> publisher event. By default this setting is set to 5 seconds.</p> <p>The statistics include all of the metrics from <code>librdkafka</code> (full list here) as well as the diff of those against the previously emitted values.</p> <p>For several attributes like <code>rxmsgs</code>, <code>librdkafka</code> publishes only the totals. In order to make it easier to track the progress (for example number of messages received between statistics emitted events) and state changes, Karafka compares all the numeric values against previously available numbers enriching the original payload with following values:</p> <ul> <li><code>METRIC_KEY_d</code> - delta computed as a difference between current and previous value - useful for trends.</li> <li><code>METRIC_KEY_fd</code> - freeze duration. Informs how long (in milliseconds) the given metric did not change - helpful for staleness detection.</li> </ul> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': 'localhost:9092',\n      # Emit statistics every second\n      'statistics.interval.ms': 1_000\n    }\n  end\nend\n\nKarafka::App.monitor.subscribe('statistics.emitted') do |event|\n  sum = event[:statistics]['rxmsgs']\n  diff = event[:statistics]['rxmsgs_d']\n\n  p \"Received messages: #{sum}\"\n  p \"Messages received from last statistics report: #{diff}\"\nend\n</code></pre>"}, {"location": "Monitoring-and-Logging/#web-ui-monitoring-and-error-tracking", "title": "Web UI monitoring and error tracking", "text": "<p>Karafka Web UI is a user interface for the Karafka framework. The Web UI provides a convenient way for developers to monitor and manage their Karafka-based applications, without the need to use the command line or third party software. It does not require any additional database beyond Kafka itself.</p> <p> </p> <p>You can read more about its features here, and the installation documentation can be found here.</p>"}, {"location": "Monitoring-and-Logging/#appsignal-metrics-and-error-tracking", "title": "AppSignal Metrics and Error Tracking", "text": "<p>AppSignal has had and continues to have, a tremendous impact on the Karafka ecosystem. Without their invaluable contributions and support, the progress and evolution of this ecosystem would not have been possible. For those searching for a top-notch monitoring system for Ruby and Rails applications, AppSignal stands out as a prime choice. Karafka officially recommends AppSignal as the supported integration for its community and users.</p> <p>Karafka's integration with AppSignal offers comprehensive support for error reporting and performance monitoring, making it a seamless solution for monitoring your Kafka-based applications.</p> <p></p> <p>The Karafka AppSignal integration provides an extensive set of metrics with both per-topic and per-partition resolution. This granularity allows you to drill down into specific aspects of your Kafka processing pipeline.</p> <p>Key Metrics Include:</p> <ul> <li> <p>Performance Metrics: Monitor the performance of your Karafka consumers, ensuring optimal message processing times.</p> </li> <li> <p>Error Reporting: Gain insights into errors and exceptions within your Karafka application. AppSignal will help you identify and diagnose issues quickly, including asynchronous operation-related errors.</p> </li> <li> <p>Dead Letter Queue: Keep an eye on messages that have failed to be processed and understand why they ended up in the dead letter queue.</p> </li> </ul> <p>By using the Karafka AppSignal integration, you can proactively manage your Kafka-based applications, ensuring they operate smoothly and reliably.</p> <p>When setting up listeners for both metrics and errors, it's crucial to subscribe to the error listener first and then the metrics listener. Doing so in reverse may result in incorrect propagation of namespace and transaction details, leading to potential data inconsistencies. Ensure the correct sequence for accurate monitoring and data integrity.</p>"}, {"location": "Monitoring-and-Logging/#error-tracking", "title": "Error Tracking", "text": "<p>Monitoring errors in Karafka consumers and producers is as critical as tracking performance and stability. Doing so provides a holistic view of system health, ensuring no issues or anomalies are overlooked. With the integration of Appsignal, you gain an additional layer of instrumentation specifically for this purpose. Appsignal integration tracks and reports all errors, including the internal asynchronous ones that might arise while working with Kafka. This comprehensive error tracking ensures timely detection and resolution, safeguarding your Kafka operations' integrity and reliability.</p> <p>Below, you can find instructions on how to enable the errors instrumentation:</p> <pre><code># First configure your app in karafka.rb\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # setup goes here...\n  end\nend\n\n# require appsignal errors listener as it is not loaded by default\nrequire 'karafka/instrumentation/vendors/appsignal/errors_listener'\n\n# Create an appsignal errors listener\nappsignal_errors_listener = ::Karafka::Instrumentation::Vendors::Appsignal::ErrorsListener.new\n\n# Subscribe with your errors listener to Karafka and its producer and you should be ready to go!\nKarafka.monitor.subscribe(appsignal_errors_listener)\nKarafka.producer.monitor.subscribe(appsignal_errors_listener)\n\n# setup the metrics listener here if you want\n</code></pre> <p></p>"}, {"location": "Monitoring-and-Logging/#metrics-instrumentation", "title": "Metrics Instrumentation", "text": "<p>The AppSignal integration offers comprehensive instrumentation, ensuring that you have a clear view of your application's performance and other vital metrics. In addition, a ready-to-import dashboard has been made available for instant insights. You can access and explore this dashboard here.</p> <p>Below, you can find instructions on how to enable the metrics instrumentation:</p> <pre><code># First configure your app in karafka.rb\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # setup goes here...\n  end\nend\n\n# require appsignal metrics listener as it is not loaded by default\nrequire 'karafka/instrumentation/vendors/appsignal/metrics_listener'\n\n# Create an appsignal metrics listener\nappsignal_metrics_listener = ::Karafka::Instrumentation::Vendors::Appsignal::MetricsListener.new\n\n# Subscribe with your listener to Karafka and you should be ready to go!\nKarafka.monitor.subscribe(appsignal_metrics_listener)\n</code></pre> <p>Remember to import the Appsignal ready-to-import dashboard that you can find here.</p>"}, {"location": "Monitoring-and-Logging/#sentry-error-tracking-integration", "title": "Sentry Error Tracking Integration", "text": "<p>If you are using Sentry and want to track errors that occurred in Karafka for both consumptions as well as any errors happening in the background threads, all you need to do is to connect to the <code>error.occurred</code> using Sentry <code>#capture_exception</code> API:</p> <pre><code>Karafka.monitor.subscribe 'error.occurred' do |event|\n  Sentry.capture_exception(event[:error])\nend\n</code></pre>"}, {"location": "Monitoring-and-Logging/#datadog-and-statsd-integration", "title": "Datadog and StatsD integration", "text": "<p>Two DataDog Integration Options Available</p> <p>Karafka offers two DataDog integration options: a native Karafka-maintained instrumentation (documented below) and DataDog's own Ruby APM integration. You cannot use both simultaneously as they may conflict with each other. The native Karafka integration provides Kafka-specific metrics and is maintained by us to ensure timely updates and compatibility with new Karafka releases, independent of DataDog's release cycles. DataDog's APM offers broader Ruby application monitoring. Choose the one that best fits your monitoring needs.</p> <p>Enable WaterDrop Instrumentation Separately</p> <p>WaterDrop has a separate instrumentation layer that you need to enable if you want to monitor both the consumption and production of messages. Please go here for more details.</p>"}, {"location": "Monitoring-and-Logging/#karafka-native-datadog-integration", "title": "Karafka Native DataDog Integration", "text": "<p>Karafka comes with (optional) full Datadog and StatsD integration that you can use. To use it:</p> <pre><code># First configure your app in karafka.rb\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # setup goes here...\n  end\nend\n\n# require datadog/statsd and the listener as it is not loaded by default\nrequire 'datadog/statsd'\nrequire 'karafka/instrumentation/vendors/datadog/metrics_listener'\n\n# initialize the listener with statsd client\ndd_listener = ::Karafka::Instrumentation::Vendors::Datadog::MetricsListener.new do |config|\n  config.client = Datadog::Statsd.new('localhost', 8125)\n  # Publish host as a tag alongside the rest of tags\n  config.default_tags = [\"host:#{Socket.gethostname}\"]\nend\n\n# Subscribe with your listener to Karafka and you should be ready to go!\nKarafka.monitor.subscribe(dd_listener)\n</code></pre> <p>You can also find here a ready-to-import Datadog dashboard configuration file that you can use to monitor your consumers.</p> <p></p>"}, {"location": "Monitoring-and-Logging/#tracing-consumers-using-datadog-logger-listener", "title": "Tracing Consumers using Datadog Logger Listener", "text": "<p>If you are interested in tracing your consumers' work with Datadog, you can use our Datadog logger listener:</p> <pre><code># you need to add ddtrace to your Gemfile\nrequire 'ddtrace'\nrequire 'karafka/instrumentation/vendors/datadog/logger_listener'\n\n# Initialize the listener\ndd_logger_listener = Karafka::Instrumentation::Vendors::Datadog::LoggerListener.new do |config|\n  config.client = Datadog::Tracing\nend\n\n# Use the DD tracing only for staging and production\nKarafka.monitor.subscribe(dd_logger_listener) if %w[staging production].include?(Rails.env)\n</code></pre> <p></p> <p>Tracing capabilities were added by Bruno Martins.</p>"}, {"location": "Monitoring-and-Logging/#async-producer-tracing-with-the-consumption-context", "title": "Async Producer Tracing With The Consumption Context", "text": "<p>Tracing asynchronous producer operations in data consumption requires a mechanism to persist the trace context from consumer to producer. This ensures that a message's lifecycle - from consumption to its asynchronous production and delivery is fully traceable coherently. This is crucial for systems where you must maintain traceability across distributed systems and ensure that messages produced asynchronously are linked to their consumption traces.</p> <p>One powerful tool to facilitate this traceability in Karafka using Datadog is the WaterDrop Labeling API. It allows you to attach consumer trace information directly to messages being produced, preserving the trace context. This enables Datadog to accurately associate the producer actions with the consumer context, without prematurely finalizing the trace.</p> <p>Here's how you can modify the message payload to include trace context information:</p> <pre><code># Check if there is an active parent span already (for example from consumer)\nparent_span = Datadog::Tracing.active_span\nspan = Datadog::Tracing.trace('karafka.producer.produce')\n# Set some defaults on your span if needed\nset_span_tags(span, message, DEFAULT_SPAN_TAGS)\n\nlabel = { span: span }\n\n# If there is a parent span that is from the consumer, include it in the label so\n# it gets passed into the producer flow\nif parent_span &amp;&amp; parent_span.name.start_with?('karafka.consumer')\n  label[:parent_span] = parent_span\nend\n\nkafka_message[:label] = label\n\nKarafka.producer.produce_async(kafka_message)\n</code></pre> <p>To complete the tracing lifecycle, you must handle the acknowledgment of message delivery or manage delivery failures. This involves finalizing the spans you initiated during the production step.</p> <p>Here's how to handle the finalization of the trace when a message is acknowledged or when an error occurs:</p> <pre><code>def on_message_acknowledged(event)\n  report = event[:delivery_report]\n  label = report.label\n\n  if label\n    span = label[:span]\n\n    if span\n      # update offset and partition tags\n      span.set_tag('offset', report.offset)\n      span.set_tag('partition', report.partition)\n      span.finish\n    end\n\n    # close parent span (karafka.consumer trace)\n    label[:parent_span]&amp;.finish\n  end\nend\n\ndef on_error_occurred(event)\n  if event[:type] == 'librdkafka.dispatch_error'\n    report = event[:delivery_report]\n    label = report.label\n\n    if label\n      span = label[:span]\n      message = label[:message]\n      err = report.error\n      # set the error in case it's not set\n      span.set_error(err) if span.status.zero?\n      span.finish\n\n      # close parent span (karafka.consumer trace)\n      label[:parent_span]&amp;.finish\n\n      # additional error handling behavior follows\n      # ...\n    end\n  end\nend\n</code></pre> <p>In these methods, the <code>#on_message_acknowledged</code> is responsible for finalizing the span when the message is successfully delivered, updating the trace with the offset and partition information. The <code>#on_error_occurred</code> method handles situations where a delivery error occurs, ensuring that the span is marked with the error and then finished.</p> <p>By leveraging these mechanisms, you can maintain a continuous trace from the point of message consumption to its final acknowledgment in the production process, providing a comprehensive view of your data's lifecycle within the distributed system.</p>"}, {"location": "Monitoring-and-Logging/#datadog-ruby-apm-integration", "title": "DataDog Ruby APM Integration", "text": "<p>DataDog also provides their own Ruby APM integration that can automatically instrument Karafka applications. </p> <p>This integration offers broader Ruby application monitoring capabilities beyond just Kafka metrics. However, since this instrumentation is maintained by DataDog, we do not control its release cycles or compatibility updates with new Karafka versions.</p> <p>For setup instructions and configuration details, refer to the DataDog Ruby APM documentation.</p> <p>Do Not Use Both Integrations Simultaneously</p> <p>Do not use this integration alongside the native Karafka DataDog integration as they may conflict with each other. Choose the one that best fits your monitoring needs.</p>"}, {"location": "Monitoring-and-Logging/#kubernetes", "title": "Kubernetes", "text": "<p>Kubernetes is an open-source platform for automating the deployment and management of containerized applications. For integrating Karafka with Kubernetes, including liveness probe setup, detailed guidance is provided in the Deployment section.</p>"}, {"location": "Monitoring-and-Logging/#opentelemetry", "title": "OpenTelemetry", "text": "<p>WaterDrop has a separate instrumentation layer that you need to enable if you want to monitor both the consumption and production of messages. You can use the same approach as Karafka and WaterDrop share the same core monitoring library.</p> <p>OpenTelemetry does not support async tracing in the same way that Datadog does. Therefore it is impossible to create a tracer that will accept reporting without the code running from within a block nested inside the <code>#in_span</code> method.</p> <p>Because of this, you need to subclass the default <code>Monitor</code> and inject the OpenTelemetry tracer into it. Below is an example that traces the <code>consumer.consumed</code> event. You can use this approach to trace any events Karafka publishes:</p> <pre><code>class MonitorWithOpenTelemetry &lt; ::Karafka::Instrumentation::Monitor\n  # Events we want to trace with OpenTelemetry\n  TRACEABLE_EVENTS = %w[\n    consumer.consumed\n  ].freeze\n\n  def instrument(event_id, payload = EMPTY_HASH, &amp;block)\n    # Always run super, so the default instrumentation pipeline works\n    return super unless TRACEABLE_EVENTS.include?(event_id)\n\n    # If event is trackable, run it inside the opentelemetry tracer\n    MyAppTracer.in_span(\n      \"karafka.#{event_id}\",\n      attributes: extract_attributes(event_id, payload)\n    ) { super }\n  end\n\n  private\n\n  # Enrich the telemetry with custom attributes information\n  def extract_attributes(event_id, payload)\n    payload_caller = payload[:caller]\n\n    case event_id\n    when 'consumer.consumed'\n      {\n        'topic' =&gt; payload_caller.topic.name,\n        'consumer' =&gt; payload_caller.class.to_s\n      }\n    else\n      raise ArgumentError, event_id\n    end\n  end\nend\n</code></pre> <p>Once created, assign it using the <code>config.monitor</code> setting:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.monitor = MonitorWithOpenTelemetry.new\n  end\nend\n</code></pre>"}, {"location": "Monitoring-and-Logging/#example-listener-with-errbitairbrake-support", "title": "Example Listener with Errbit/Airbrake Support", "text": "<p>Here's a simple example of a listener used to handle errors logging into Airbrake/Errbit.</p> <pre><code># Example Airbrake/Errbit listener for error notifications in Karafka\nmodule AirbrakeListener\n  def on_error_occurred(event)\n    Airbrake.notify(event[:error])\n  end\nend\n</code></pre>"}, {"location": "Monitoring-and-Logging/#publishing-karafka-and-waterdrop-notifications-events-using-activesupportnotifications", "title": "Publishing Karafka and WaterDrop notifications events using <code>ActiveSupport::Notifications</code>", "text": "<p>If you already use <code>ActiveSupport::Notifications</code> for notifications event tracking, you may also want to pipe all the Karafka and WaterDrop notifications events there.</p> <p>To do so, subscribe to all Karafka and WaterDrop events and publish those events via <code>ActiveSupport::Notifications</code>:</p> <pre><code># Karafka subscriptions piping\n::Karafka::Instrumentation::Notifications::EVENTS.each do |event_name|\n  ::Karafka.monitor.subscribe(event_name) do |event|\n    # Align with ActiveSupport::Notifications default naming convention\n    event = (event_name.split('.').reverse &lt;&lt; 'karafka').join('.')\n\n    # Instrument via ActiveSupport\n    ::ActiveSupport::Notifications.instrument(event_name, **event.payload)\n  end\nend\n</code></pre> <pre><code># WaterDrop subscriptions piping\n::WaterDrop::Instrumentation::Notifications::EVENTS.each do |event_name|\n  ::Karafka.producer.subscribe(event_name) do |event|\n    # Align with ActiveSupport::Notifications default naming convention\n    event = (event_name.split('.').reverse &lt;&lt; 'waterdrop').join('.')\n\n    ::ActiveSupport::Notifications.instrument(event_name, **event.payload)\n  end\nend\n</code></pre> <p>Once that is done, you can subscribe directly to the events published there:</p> <pre><code># Note that the events naming is reverted to follow ActiveSupport::Notifications conventions\nActiveSupport::Notifications.subscribe('consumed.consumer.karafka') do |event|\n  Rails.logger.info \"[consumer.consumed]: #{event.inspect}\"\nend\n</code></pre> <p>Please note that each Karafka producer has its instrumentation instance, so if you use more producers, you need to pipe each of them independently.</p>"}, {"location": "Monitoring-and-Logging/#monitor-wrapping-and-replacement", "title": "Monitor Wrapping and Replacement", "text": "<p>Karafka's monitor can be replaced or wrapped to add custom instrumentation while preserving core functionality. This allows distributed tracing, custom metrics, or enhanced error tracking to be added in case async tracing is not available.</p> <pre><code>class CustomMonitor &lt; ::Karafka::Instrumentation::Monitor\n  # Events where we want custom handling\n  INTERCEPTED_EVENTS = %w[\n    consumer.consumed\n    consumer.heartbeat\n    consumer.polling.started\n  ].freeze\n\n  def instrument(event_id, payload = EMPTY_HASH, &amp;block)\n    return super unless INTERCEPTED_EVENTS.include?(event_id)\n\n    # Pre-processing\n    MyLogger.info(\"Starting #{event_id}\")\n\n    # Maintain core functionality\n    super\n  ensure \n    # Post-processing (runs even after errors)\n    MyMetrics.increment(\"karafka.#{event_id}\")\n  end\nend\n\n# Use custom monitor\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.monitor = CustomMonitor.new\n  end\nend\n</code></pre> <p>Such a custom monitor will intercept specific events while delegating to the parent monitor to maintain framework functionality. This pattern enables proper error handling, cleanup, and integration with external monitoring systems that require execution to be wrapped in a block.</p> <p>Avoid Calling super Multiple Times in Custom Monitors</p> <p>When overriding Karafka\u2019s instrumentation monitor (<code>Karafka::Instrumentation::Monitor</code>), it's crucial not to invoke <code>super</code> more than once within the <code>instrument</code> method. Calling <code>super</code> multiple times will cause the original wrapped block of code to execute repeatedly. This can lead to severe and unintended side-effects, such as duplicated message processing, incorrect consumption behaviors, and unexpected logic execution. Always ensure your custom monitor calls <code>super</code> exactly once per event, guarding against unintended duplicate invocations.</p>"}, {"location": "Monitoring-and-Logging/#implications-of-broken-instrumentation-listenerslisteners-causing-errors", "title": "Implications of Broken Instrumentation listeners/listeners Causing Errors", "text": "<p>Instrumentation and monitoring listeners are essential components in Karafka-based applications as they provide insight into the app's performance and behavior. They are critical in collecting metrics, measuring response times, and tracking other performance data. When functioning correctly, they enable efficient identification of issues and performance optimization. However, their malfunctioning could lead to several challenges and problems.</p> <p>The first significant impact of broken instrumentation and monitoring listeners is the loss of visibility into the application's internal workings. This obscures your understanding of its performance and makes debugging more difficult. Such listeners are instrumental in spotting errors, bottlenecks, and irregular behaviors in your Karafka applications. Their malfunctioning can thus complicate the identifying of the root causes of issues and effective debugging.</p> <p>Secondly, faulty listeners can adversely affect your Karafka application's performance. Their roles include collecting metrics and measuring response times, among other performance-related tasks. Any malfunctioning might result in missing essential performance bottlenecks, leading to performance degradation like decreased throughput and increased latency.</p> <p>In specific scenarios, instrumentation errors in the Kafka listener threads can force Karafka into a recovery mode, causing continuous attempts to reconnect to Kafka and triggering rebalances. This can temporarily halt message consumption and impact workload distribution among consumer instances. Furthermore, instrumentation listener errors in worker threads responsible for processing messages might prevent proper acknowledgment of work or cause double processing of messages, resulting in issues like message loss or duplicate processing.</p> <p>For those using custom instrumentation listeners, it's vital to ensure they are thoroughly tested and not performing heavy or error-prone tasks. These listeners can introduce additional complexity, and maintaining a balance between gathering valuable insights and keeping the listeners lightweight and error-free is essential.</p> <p>To avert these issues, it's crucial to ensure your Karafka applications' instrumentation and monitoring listeners function correctly.</p> <p>In conclusion, maintaining the stability, performance, and reliability of Karafka-based applications requires the proper functioning of any custom instrumentation and monitoring listeners.</p> <p>Last modified: 2025-06-12 10:26:20</p>"}, {"location": "Multi-Cluster-Setup/", "title": "Multi-Cluster Setup", "text": "<p>Karafka is a robust framework that allows applications to interact with multiple Kafka clusters simultaneously. This provides enhanced scalability, redundancy, and flexibility, enabling developers to optimize data processing and manage message streams across various clusters with ease. </p>"}, {"location": "Multi-Cluster-Setup/#overview", "title": "Overview", "text": "<ol> <li> <p>Karafka and Multiple Clusters: Karafka can be used both to consume from and produce to multiple Kafka clusters.</p> </li> <li> <p>Default Configuration: By default, Karafka is set up to operate with a single Kafka cluster, which is the most common case.</p> </li> <li> <p>Primary Cluster Reference:</p> <ul> <li> <p>The <code>Karafka.producer</code> will always refer to the primary cluster defined unless overwritten.</p> </li> <li> <p>ActiveJob jobs, when scheduled with Karafka's ActiveJob backend, will also always go to the primary cluster.</p> </li> </ul> </li> <li> <p>Admin Operations and Web UI:</p> <ul> <li> <p>All admin operations are performed on the primary cluster.</p> </li> <li> <p>While the Web UI can work with multiple clusters, both reporting and processing of the Web UI data will take place on the primary cluster unless <code>Karafka::Web.producer</code> is redefined.</p> </li> </ul> </li> </ol>"}, {"location": "Multi-Cluster-Setup/#configuration", "title": "Configuration", "text": "<p>Configuring Karafka for multiple clusters requires attention to two primary areas: consumer settings and producer settings.</p>"}, {"location": "Multi-Cluster-Setup/#consumer-settings", "title": "Consumer Settings", "text": "<p>To consume data from multiple clusters, the configuration within the <code>kafka</code> scope needs to be updated per topic. Specifically, for topics that originate from a secondary cluster, you need to ensure that they are correctly pointed to the appropriate cluster. By doing this, you enable Karafka to know from which cluster to fetch the messages for a particular topic:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Primary cluster\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092'\n    }\n\n    # Other settings...\n  end\n\n  routes.draw do\n    # This topic will be consumed from the primary cluster\n    topic :example do\n      consumer ExampleConsumer\n    end\n\n    topic :example2 do\n      consumer Example2Consumer\n      # This topic will be consumed from a secondary cluster\n      kafka('bootstrap.servers': 'example.org:9092')\n    end\n  end\nend\n</code></pre> <p>Karafka intelligently groups topics targeting different clusters into distinct subscription groups. This approach optimizes and conserves connections to Kafka, ensuring efficient resource utilization and streamlined data consumption across clusters.</p>"}, {"location": "Multi-Cluster-Setup/#producer-settings", "title": "Producer Settings", "text": "<p>While the consumption settings ensure Karafka knows where to pull messages from, the production settings dictate where Karafka sends the outbound messages. An extra setup is essential when producing messages to multiple clusters to ensure that the messages are directed to the correct location.</p> <p>Remember, a correct configuration is crucial for the efficient and error-free operation of Karafka when working with multiple Kafka clusters.</p> <p>If you want to configure the primary <code>Karafka.producer</code> to write messages to a cluster different than the default one, please refer to this documentation section.</p> <p>To produce data across multiple clusters, set up individual producers for each targeted cluster. However, it's essential to manually integrate instrumentation and error tracking, as these producers won't be monitored by the default Karafka Web UI.</p> <pre><code># Just create producers targeting other clusters\nSECONDARY_CLUSTER_PRODUCER = WaterDrop::Producer.new do |config|\n  config.deliver = true\n  config.kafka = {\n    'bootstrap.servers': 'example.com:9092',\n    'request.required.acks': 1\n  }\nend\n</code></pre>"}, {"location": "Multi-Cluster-Setup/#common-mistakes", "title": "Common Mistakes", "text": "<ul> <li> <p>Ignoring Primary Default: Forgetting that <code>Karafka.producer</code> and ActiveJob jobs default to the primary cluster can lead to unexpected routing of messages.</p> </li> <li> <p>Mismatched Cluster Configuration: Ensure that all specified clusters in the configuration have the correct broker addresses.</p> </li> <li> <p>Web UI Assumption: Assuming that the Web UI processes data on the cluster it shows. Remember, data processing is done on the primary cluster unless overridden.</p> </li> <li> <p>Overcomplicating Setup: Using multiple clusters can add complexity to your setup. Ensure there's a clear need for this before diving in.</p> </li> <li> <p>Monitoring Challenges: Monitoring and alerting can become challenging with multiple clusters. Ensure you have a solid monitoring strategy.</p> </li> </ul>"}, {"location": "Multi-Cluster-Setup/#example-use-cases", "title": "Example use-cases", "text": "<ul> <li> <p>Geographical Distribution: For businesses operating in different regions, separate clusters can help localize data processing closer to where data is produced or consumed.</p> </li> <li> <p>Disaster Recovery: A secondary cluster can be crucial for backup and recovery, ensuring business continuity even if the primary cluster fails.</p> </li> <li> <p>Data Segregation: For businesses handling data with different security or compliance requirements, other clusters can segregate data based on these requirements.</p> </li> <li> <p>Load Distribution: High-traffic applications can distribute their load across multiple clusters to ensure no single cluster becomes a bottleneck.</p> </li> <li> <p>Multi-Tenancy: For businesses offering multi-tenant solutions, different clusters can cater to various tenants, ensuring isolation and independent scalability.</p> </li> </ul>"}, {"location": "Multi-Cluster-Setup/#conclusion", "title": "Conclusion", "text": "<p>While Karafka's ability to operate with multiple clusters offers flexibility and scalability, it's essential to understand the nuances and potential pitfalls of such a setup. Plan your configuration carefully, and ensure you're leveraging the multi-cluster configuration for valid business reasons.</p> <p>Last modified: 2023-11-05 14:30:40</p>"}, {"location": "Offset-management/", "title": "Offset management (checkpointing)", "text": "<p>By default, Karafka handles offset commit management for you. The offset is automatically committed:</p> <ul> <li>frequently (defaults to once every 5 seconds) - defined by the <code>auto.commit.interval.ms</code> setting on the <code>kafka</code> level.</li> <li>during the shutdown after all processing is done.</li> <li>during the rebalance after all the blocking processing and before the new assignment distribution.</li> </ul> <p>This approach is excellent for most cases and should provide a minimum number of scenarios where reprocessing would happen during normal operations. However, there are some situations where you might need better control over offset management.</p>"}, {"location": "Offset-management/#manual-offset-management", "title": "Manual offset management", "text": "<p>There are several cases in which this API can be helpful:</p> <ul> <li>In memory of the DDD sagas realization,</li> <li>Buffering,</li> <li>Simulating transactions.</li> </ul>"}, {"location": "Offset-management/#configuring-karafka-not-to-mark-messages-as-consumed-automatically", "title": "Configuring Karafka not to mark messages as consumed automatically", "text": "<p>To use this API, you need to switch the <code>manual_offset_management</code> setting to <code>true</code> on a per topic basis:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    consumer_group :events do\n      # manual_offset_management false\n\n      topic :user_events do\n        consumer EventsConsumer\n        manual_offset_management true\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Offset-management/#marking-messages-as-consumed-and-committing-offsets", "title": "Marking messages as consumed and committing offsets", "text": "<p>To mark a certain message as consumed (so in case of a crash or restart, it won't be consumed again), you can use one of two marking methods:</p> <ul> <li><code>#mark_as_consumed</code> - for a non-blocking eventual offset commitment.</li> <li><code>#mark_as_consumed!</code> - for a blocking offset commitment that will stop the processing flow to ensure that the offset has been stored. This is not recommended for most scenarios, as Karafka will automatically commit the most recent offsets upon rebalance and shutdown.</li> </ul> <pre><code>def consume\n  # Do something with messages\n  EventStore.store(messages.payloads)\n  # And now mark the last message as consumed,\n  # so we won't consume any of the already processed messages again\n  mark_as_consumed! messages.last\nend\n</code></pre> <p>Both <code>#mark_as_consumed</code> and <code>#mark_as_consumed!</code> return a boolean value indicating whether your consumer instance still owns the given topic partition. If there was a rebalance and the partition is no longer owned by a consumer, the value returned will be <code>false</code>. You can use this result to stop processing early:</p> <pre><code>def consume\n  messages.each do |message|\n    puts \"Processing #{message.topic}/#{message.partition} #{message.offset}\"\n\n    # Do not process further if we no longer own partition\n    return unless mark_as_consumed(message)\n  end\nend\n</code></pre> <p>Karafka offers two additional methods to commit already stored but not committed offsets to Kafka: <code>#commit_offsets</code> and <code>#commit_offsets!</code>. </p> <p>These two methods allow you to manage your consumer's offsets, ensuring Kafka knows the last message your consumer has processed.</p> <ul> <li><code>#commit_offsets</code>: This method is asynchronous. It sends a request to the Kafka brokers to commit the offsets but immediately gets confirmation. Instead, it returns immediately, which allows your consumer to continue processing other messages without delay. The result of the commit request (whether successful or not) is not immediately known but can be checked using the <code>#revoked?</code> method.</li> <li><code>#commit_offsets!</code>: In contrast, this method is synchronous. It sends a request to commit the offsets and waits for a response from the Kafka brokers. This means your consumer pauses and waits for the brokers to acknowledge the commit request. The method will return a boolean value indicating the operation's success - true if the commit was successful and false if it was not. This can be helpful when you need to ensure that the offsets have been committed before moving forward.</li> </ul> <p>Remember, both <code>#commit_offsets</code> and <code>#commit_offsets!</code> only commit offsets that have already been stored. Storing an offset signifies that a message has been processed, so ensure you have correctly stored the offsets before attempting to commit them to Kafka.</p>"}, {"location": "Offset-management/#example-buffer-implementation-with-shutdown-db-flush", "title": "Example buffer implementation with <code>shutdown</code> DB flush", "text": "<p>When manually controlling the moment of marking the message as consumed, it is also worth taking into consideration graceful application termination process.</p> <p>For some cases, it might be a moment in which, for example, you want to flush the buffer regardless of it not reaching the desired threshold. You can use the <code>#mark_as_consumed</code> also from the <code>#shutdown</code> method:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  # Flush to DB only in 1k batches\n  FLUSH_SIZE = 1000\n\n  def consume\n    # Unparse and add to buffer\n    messages.each { |message| buffer &lt;&lt; message }\n\n    # If buffer exceeds the FLUSH_SIZE, it's time to put data into the DB\n    if buffer.size &gt;= FLUSH_SIZE\n      data = buffer.shift(FLUSH_SIZE)\n      p \"importing: #{data.count}\"\n      # Once importing is done, we can mark last message from the imported set\n      # as consumed\n      mark_as_consumed!(data.last)\n    end\n  end\n\n  # Before we stop, if there is anything in the buffer, let's import it despite\n  # the fact, that it didn't reach the FLUSH_SIZE\n  def shutdown\n    unless buffer.empty?\n      p \"importing: #{buffer.count}\"\n      # Mark last message as consumed, as they are all in the DB\n      mark_as_consumed!(buffer.last)\n    end\n  end\n\n  def buffer\n    @buffer ||= []\n  end\nend\n</code></pre>"}, {"location": "Offset-management/#mixing-manual-and-automatic-offset-management", "title": "Mixing manual and automatic offset management", "text": "<p>You can still take advantage of this API even when using automatic offset management. For example, you may want to commit the offset manually after a certain number of messages are consumed from a batch:</p> <pre><code>class CountersConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each_with_index do |message, index|\n      # Some business logic here\n      # Commit every 10 messages processed\n      mark_as_consumed(message) if index % 10 == 0\n    end\n  end\nend\n</code></pre> <p>Last modified: 2023-11-03 14:36:20</p>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/", "title": "Pausing, Seeking and Rate-Limiting", "text": "<p>Karafka Pro provides an excellent filtering and rate-limiting APIs, making it a highly recommended option over manually managing message processing flow. By using Karafka Pro, developers can easily configure filtering and rate limiting on a per-topic basis, which allows them to fine-tune the message processing flow according to the requirements of their application.</p> <p>Using Karafka Pro for filtering and rate limiting also eliminates the need for developers to manually manage message processing, which can be time-consuming and error-prone. With Karafka Pro, you can rely on a robust and efficient system that automatically takes care of these tasks.</p> <p>Overall, using Karafka Pro for filtering and rate limiting not only simplifies the development process but also ensures that message processing is handled in a reliable and scalable manner.</p> <p>Karafka allows you to pause processing for a defined time. This can be used, for example, to apply a manual back-off policy or throttling. To pause a given partition from within the consumer, you need to use the <code>#pause</code> method that accepts the pause offset (what should be the first message to get again after resuming) and the time for which the pause should be valid.</p>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#using-the-pause-api", "title": "Using the <code>#pause</code> API", "text": "<pre><code>def consume\n  messages.each do |message|\n    # Sends requests to an API that can be throttled\n    result = DispatchViaHttp.call(message)\n\n    next unless result.throttled?\n\n    # Pause and resume from the first message that was throttled\n    # Pause based on our fake API throttle backoff information\n    pause(message.offset, result.backoff)\n\n    # We need to return, otherwise the messages loop would continue sending messages\n    return\n  end\nend\n</code></pre> <p>It is important to remember that the <code>#pause</code> invocation does not stop the processing flow. You need to do it yourself.</p> <p>BAD:</p> <p>Without stopping the processing, the <code>messages#each</code> loop will continue:</p> <pre><code>def consume\n  messages.each do |message|\n    # Wait for 10 seconds and try again if we've received messages\n    # that are younger than 1 minute\n    pause(message.offset, 10.seconds * 1_000) if message.timestamp &gt;= 1.minute.ago\n\n    save_to_db(message)\n  end\nend\n</code></pre> <p>GOOD:</p> <p>Invoking <code>return</code> after <code>#pause</code> will ensure no consecutive messages are processed. They will be processed after pause has expired:</p> <pre><code>def consume\n  messages.each do |message|\n    if message.timestamp &gt;= 1.minute.ago\n      pause(message.offset, 10.seconds * 1_000)\n      # After pausing do not continue processing consecutive messages\n      return\n    end\n\n    save_to_db(message)\n  end\nend\n</code></pre> <p>GOOD:</p> <p>Another good approach is by using the <code>#find</code> on messages to detect if throttling is needed and what was the message that was throttled:</p> <pre><code>def consume\n  throttled = messages.find do |message|\n    DispatchViaHttp.call(message).throttled?\n  end\n\n  # Done if nothing was throttled\n  return unless throttled\n\n  # Try again in 5 seconds\n  pause(throttled.offset, 5_000)\nend\n</code></pre>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#using-the-seek-api", "title": "Using the <code>#seek</code> API", "text": "<p>In the context of Apache Kafka, \"seeking\" refers to moving the consumer's offset to a specific position within a topic's partition. In essence, it allows you to dictate where in the partition the consumer begins (or resumes) reading messages.</p>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#seeking-to-an-offset", "title": "Seeking to an Offset", "text": "<p>Inside your consumer's <code>#consume</code> method, you can use the <code>#seek</code> method to set the offset for a specific partition:</p> <pre><code>def consume\n  results = []\n\n  messages.each do |message|\n    results &lt;&lt; Processor.call(message.payload)\n  end\n\n  return if results.all? { |result| result == true }\n\n  # Get back by 100 messages and reprocess data if anything went badly\n  seek(messages.last.offset - 100)\nend\n</code></pre>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#seeking-to-a-point-in-time", "title": "Seeking to a Point in Time", "text": "<p>In Karafka, aside from the traditional offset-based seeking using #seek, you can seek by timestamp as follows:</p> <pre><code>def consume\n  results = []\n\n  messages.each do |message|\n    results &lt;&lt; Processor.call(message.payload)\n  end\n\n  return if results.all? { |result| result == true }\n\n  # Get back by an hour of data and reprocess all\n  seek(Time.now.utc - 60 * 60)\nend\n</code></pre>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#seeking-the-earliest-and-latest-positions", "title": "Seeking the Earliest and Latest Positions", "text": "<p>Karafka provides convenient symbols to seek to special positions within a partition without needing to know specific offsets or timestamps. These symbols allow you to jump to the beginning or end of a partition's message log.</p>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#seeking-to-earliest", "title": "Seeking to <code>:earliest</code>", "text": "<p>The <code>:earliest</code> symbol moves the consumer to the very beginning of the partition, starting from the first available message. This is particularly useful when you must reprocess all available data from the start of the topic's retention period.</p> <pre><code>def consume\n  # Process current batch of messages\n  messages.each do |message|\n    result = ProcessMessage.call(message)\n\n    # If we detect data corruption or need full reprocessing\n    if result.requires_full_reprocess?\n      # Start from the very beginning of the partition\n      seek(:earliest)\n      return\n    end\n  end\nend\n</code></pre>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#seeking-to-latest", "title": "Seeking to <code>:latest</code>", "text": "<p>The <code>:latest</code> symbol is more nuanced than it might initially appear. It does not seek to the last (most recent) available message in the partition. Instead, it seeks to the high water mark offset, which represents the position where the next new message will be written.</p> <p>This means that after seeking to <code>:latest</code>, the consumer will wait for new messages to arrive rather than processing existing ones. The high water mark is essentially the \"end\" of the current log, pointing to a message that doesn't exist yet but will be the first to arrive after seeking.</p> <pre><code>def consume\n  messages.each do |message|\n    # Check if we're processing stale data\n    if message.timestamp &lt; 1.hour.ago\n      # Skip all existing messages and wait for new ones\n      # This moves to the high water mark, not the last available message\n      seek(:latest)\n      return\n    end\n\n    ProcessRealtimeMessage.call(message)\n  end\nend\n</code></pre>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#seeking-vs-offset-position", "title": "Seeking vs. Offset Position", "text": "<p>When utilizing the <code>#seek</code> API in Karafka, it's crucial to understand the behavior of offsets and how this method interacts with them. The <code>#seek</code> method lets you move the consumer's offset to a specific position within a topic's partition. This capability is essential for controlling exactly where the consumer begins or resumes reading messages in the partition.</p> <p>By default, when you invoke the <code>#seek</code> method, the in-memory offset position (also known as the seek offset) is not reset. This means that the position to which you're seeking won't automatically update the current offset in memory.</p> <p>Additionally, Karafka implements a safeguard to ensure data consistency and integrity. By default, it prevents committing offsets earlier than the highest offset committed on a consumer instance. This mechanism helps avoid scenarios where a consumer might read and process messages out of order, potentially leading to data duplication or loss.</p> <p>To address scenarios where you need to explicitly move the consumer's offset and update the in-memory position, the <code>#seek</code> method accepts an additional flag: <code>reset_offset</code>. When this flag is set to true, Karafka will move the consumer to the specified location and update the in-memory offset to match this new position.</p> <p>This is particularly useful in cases where you need to process messages from a specific point, regardless of previous commits, or when managing complex consumer behaviors that require precise control over message processing orders.</p> <pre><code>def consume\n  results = []\n\n  messages.each do |message|\n    results &lt;&lt; Processor.call(message.payload)\n  end\n\n  return if results.all? { |result| result == true }\n\n  # Get back by 100 messages and reprocess data if anything went badly\n  # Move the offset position together so when reprocessing, things can be marked again despite\n  # the most recent offset committed till this point being ahead\n  seek(messages.last.offset - 100, reset_offset: true)\nend\n</code></pre>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#seeking-use-cases", "title": "Seeking Use Cases", "text": "<ul> <li> <p>Reprocessing Messages: If there's a need to reprocess certain messages due to application logic changes or errors, you can seek back to an earlier offset to re-read and reprocess the messages.</p> </li> <li> <p>Skipping Faulty Messages: In scenarios where specific messages might cause processing issues (e.g., due to data corruption), the consumer can skip these by seeking ahead to a subsequent offset.</p> </li> <li> <p>Time-based Processing: If your messages have a timestamp and you wish to process or reprocess messages from a specific time, you can seek the offset corresponding to that timestamp.</p> </li> <li> <p>Consumer Recovery: In the event of consumer crashes or restarts, you can use seek to ensure that the consumer starts processing from where it left off, ensuring no missed messages.</p> </li> <li> <p>Testing and Debugging: During development or debugging, you might want to read specific messages multiple times. Seeking allows you to jump to those messages easily.</p> </li> <li> <p>Conditional Processing: Based on some real-time conditions or configurations, you might want to jump over specific messages or go back to previous ones.</p> </li> </ul>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#smart-seek-capabilities", "title": "Smart Seek Capabilities", "text": "<p>At its core, the Smart Seek feature is a mechanism that prevents unnecessary <code>#seek</code> operations. In typical Kafka clients, invoking the <code>#seek</code> method can lead to the purging of prefetch buffers, which in turn can result in increased network usage as discarded messages are re-fetched. However, not every <code>#seek</code> operation results in a change in cursor position.</p> <p>Karafka's Smart Seek feature checks whether a <code>#seek</code> operation would effectively change the cursor position. If the operation doesn't change the position, the <code>#seek</code> is ignored. This is not just an optimization to reduce unnecessary operations but also plays a crucial role in preserving the integrity and efficiency of prefetch buffers.</p> <p>Benefits of Smart Seek:</p> <ul> <li> <p>Buffer Integrity: One of the most significant benefits is preserving prefetched messages. Since unnecessary <code>#seek</code> operations are ignored, the prefetch buffers aren't purged without cause. This results in better utilization of fetched messages and reduces the need to re-fetch them, thus saving network resources.</p> </li> <li> <p>Reduced Network Traffic: By preventing unnecessary #seek operations, Karafka ensures that there's less need to re-fetch messages from the broker, which subsequently leads to reduced network traffic.</p> </li> <li> <p>Efficiency and Performance: Ignoring #seek operations that don't change the cursor's position means fewer operations for the consumer to handle, leading to more efficient processing and reduced latency.</p> </li> <li> <p>Smart Pausing: The Smart Seek logic extends to the <code>#pause</code> functionality. Just like with seeking, Karafka checks if pausing would be redundant and, if so, ignores the operation. This smart behavior ensures optimal performance even when applications try to pause consumption frequently.</p> </li> </ul>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#pause-and-seek-usage-potential-networking-impact", "title": "<code>#pause</code> and <code>#seek</code> Usage Potential Networking Impact", "text": "<p>When using the <code>#pause</code> or <code>#seek</code> method in Karafka, you're essentially instructing the system to halt the fetching of messages for a specific topic partition. However, this is not just a simple \"pause\" in the regular sense of the word.</p> <p>When one of those methods is invoked, Karafka stops fetching new messages and purges its internal buffer that holds messages from that specific partition. It's essential to recognize that Karafka, by default, pre-buffers 1MB of data per topic partition for efficiency reasons. This buffer ensures that there is always a consistent supply of messages ready for processing without constantly waiting for new fetches.</p> <p>The challenge arises here: If you use the <code>#pause</code> or <code>#seek</code> method frequently and for short durations, you might inadvertently create substantial network traffic. Every time you resume from a pause or seek to a location, Karafka will attempt to re-buffer the 1MB of data, which can result in frequently re-fetching the same data, thereby causing redundant network activity.</p>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#the-impact-of-purging-prefetched-messages", "title": "The Impact of Purging Prefetched Messages", "text": "<ul> <li> <p>Network Traffic Increase: Whenever <code>#seek</code> or <code>#pause</code> is called, all prefetched messages for the partition are discarded. If a consumer then needs to read messages that were in the now-purged prefetch cache, it has to request them from the broker again, resulting in additional network traffic.</p> </li> <li> <p>Increased Latency: As previously mentioned, prefetched messages reduce consumer latency. Purging them means the consumer might need to wait for the broker to deliver messages that it once had in its prefetch cache.</p> </li> <li> <p>Wasted Resources: Messages prefetched but never consumed due to a <code>#seek</code> operation represent wasted resources. These messages were fetched from the broker, occupying network bandwidth, and were stored in memory, consuming RAM, all for no benefit.</p> </li> <li> <p>Broker Load: Continuously bringing messages from the broker due to prefetch purges increases the broker's work. The broker has to handle more fetch requests and serve more data, potentially affecting its performance.</p> </li> </ul>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#potential-solutions", "title": "Potential Solutions", "text": "<ul> <li> <p>Adjust Buffer Size: If you're pausing and resuming often, consider adjusting the <code>fetch.message.max.bytes</code> setting for affected topics. This will lower the buffer size to reduce the volume of redundant data fetched, but do note that this might affect performance during regular operations.</p> </li> <li> <p>Optimize Pause Usage: Reevaluate your use cases for the <code>#pause</code> method. Perhaps there are ways to minimize its usage or extend the duration of pauses to reduce the frequency of data re-fetches.</p> </li> <li> <p>Monitor and Alert: Set up alerts to notify you of a spike in network traffic or frequent use of the <code>#pause</code> method. This way, you can quickly address any issues or misconfigurations.</p> </li> </ul>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#cost-implications-with-third-party-providers", "title": "Cost Implications with Third-party Providers", "text": "<p>It's crucial to be aware, especially if you're using a third-party Kafka provider that charges based on the number of messages sent, that frequent pausing and resuming can inflate costs. This is due to the aforementioned frequent prefetching of the same data, which can result in the same messages being counted multiple times for billing purposes. Always ensure alignment and configuration are optimized to prevent unnecessary financial implications.</p> <p> </p> <p>        *This example showcases the consequences of improperly using <code>#seek</code> (as seen on the left) versus the improved results after consulting with the Karafka team. The daily network traffic resulting from such misuse came with a hefty price tag of USD 2,500 daily.    </p>"}, {"location": "Pausing-Seeking-and-Rate-Limiting/#summary", "title": "Summary", "text": "<p>In conclusion, while the <code>#pause</code> and <code>#seek</code> methods in Karafka provide valuable functionalities, it's vital to understand their implications regarding system performance and potential costs. Proper configuration and mindful usage can help leverage its benefits while mitigating downsides.</p> <p>Last modified: 2025-05-22 18:43:25</p>"}, {"location": "Pro-Adaptive-Iterator/", "title": "Adaptive Iterator", "text": "<p>The Adaptive Iterator is a Karafka Pro feature designed to proactively monitor the processing of messages within a batch to prevent exceeding Kafka's <code>max.poll.interval.ms</code>. By estimating the processing cost of each message and halting processing when the remaining time is insufficient, the Adaptive Iterator ensures smooth operation without exceeding Kafka's poll interval limit. This feature is particularly useful in environments where message processing times vary, and occasional long-processing times risk reaching the <code>max.poll.interval.ms</code>.</p> <p>Consider the Adaptive Iterator as a solution when:</p> <ul> <li>Your application experiences occasional spikes in processing times that risk exceeding the Kafka poll interval.</li> <li>You want to ensure the consumer does not exceed the <code>max.poll.interval.ms</code>, preventing rebalancing and potential downtimes.</li> <li>You are processing messages one after another, as the Adaptive Iterator is designed to handle sequential message processing efficiently.</li> </ul> <p>Do not use the Adaptive Iterator if:</p> <ul> <li>Your processing cost of a single message can exceed the <code>max.poll.interval.ms</code>.</li> <li>You are processing messages in batches instead of one after another, as the Adaptive Iterator is designed for individual message processing.</li> <li>You are using the Virtual Partitions feature, as it is not compatible with the Adaptive Iterator.</li> <li>Your application consistently handles long-running jobs. In that case, use the Long-Running Jobs feature.</li> <li>Your consumer requires strict control over offset management outside the default provided by the Adaptive Iterator.</li> </ul> <p> </p> <p> *Illustration presenting how Adaptive Iterator work.    </p> <p>Handling Consistent Long-Running Jobs</p> <p>Consider using Karafka's Long-Running Jobs feature instead for consistently long-running jobs, as the Adaptive Iterator is primarily designed to handle sporadic long-processing cases.</p>"}, {"location": "Pro-Adaptive-Iterator/#benefits-of-adaptive-iterator", "title": "Benefits of Adaptive Iterator", "text": "<ul> <li> <p>Prevents Poll Interval Expiry: Dynamically monitors the remaining time to avoid exceeding the Kafka poll interval, ensuring consumer group stability.</p> </li> <li> <p>Seeks Back on Timeout: When it determines that processing further messages in a batch could exceed the poll interval, it stops and seeks back, allowing the next poll to reset the timer.</p> </li> <li> <p>Automated Handling: Provides built-in capabilities for operations such as stopping processing if the consumer is revoked or auto-marking messages based on your configuration.</p> </li> <li> <p>Configurable Safety Margin: Allows configuring a safety margin to leave a buffer for post-processing activities, ensuring smooth handling of messages.</p> </li> </ul>"}, {"location": "Pro-Adaptive-Iterator/#using-adaptive-iterator", "title": "Using Adaptive Iterator", "text": "<p>To use the Adaptive Iterator in your consumer, you need to configure it at the topic level using the <code>adaptive_iterator</code> method. Here's how to do it:</p>"}, {"location": "Pro-Adaptive-Iterator/#enabling-the-adaptive-iterator", "title": "Enabling the Adaptive Iterator", "text": "<p>The first step is to activate the Adaptive Iterator within the topic configuration in your consumer class. Below is an example:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      adaptive_iterator(\n        active: true,\n        safety_margin: 15, # 15% of time\n        clean_after_yielding: true\n      )\n    end\n  end\nend\n</code></pre> <p>In this example, the Adaptive Iterator is activated with specific parameters. The <code>a</code>daptive_iterator` method takes several configuration options:</p> <ul> <li><code>active</code>: Set this to true to enable the Adaptive Iterator for the topic.</li> <li><code>safety_margin</code>: Defines the percentage of the total poll interval to reserve as a buffer. For instance, setting this to 15 leaves 15% of the time for post-processing, stopping further processing when 85% of the poll interval is used. The default is 10.</li> <li><code>marking_method</code>: Specifies how messages are marked as consumed. The default method is <code>:mark_as_consumed</code>, but you can set it to <code>:mark_as_consumed!</code>.</li> <li><code>clean_after_yielding</code>: Indicates whether to clean up after processing each message using the Cleaner API. Set this to true if you want automatic cleanup after yielding.</li> </ul>"}, {"location": "Pro-Adaptive-Iterator/#processing-messages-with-the-adaptive-iterator", "title": "Processing Messages with the Adaptive Iterator", "text": "<p>Once configured, the Adaptive Iterator wraps around the <code>#each</code> method used in your consume method. It monitors the time taken to process each message and calculates the remaining time within the Kafka poll interval. If it determines that there isn't enough time left to safely process more messages, it will stop and seek back to prevent exceeding the <code>max.poll.interval.ms</code>.</p> <p>Here's an example of how you might use it:</p> <pre><code>class MyConsumer &lt; ApplicationConsumer\n  def consume\n    each do |message|\n      # This block is monitored by the Adaptive Iterator for processing time\n      process_message(message)\n    end\n  end\n\n  private\n\n  def process_message(message)\n    # Your custom message processing logic here\n  end\nend\n</code></pre> <p>In this example, the iterator is configured with a safety margin of 20%, allowing a buffer to handle post-processing without exceeding the poll interval. It automatically marks messages as consumed and cleans up after each message, depending on the configuration.</p> <p>Correct Iteration Method for Adaptive Iterator</p> <p>When using the Adaptive Iterator, always use the consumer's <code>#each</code> method directly, as shown in the example, instead of iterating over <code>messages#each</code>. The Adaptive Iterator wraps both the messages and the consumer context, allowing it to accurately monitor processing time and handle tasks like stopping, seeking, and offset marking. Using <code>messages#each</code> bypasses this wrapping, preventing the Adaptive Iterator from functioning.</p>"}, {"location": "Pro-Adaptive-Iterator/#stopping-and-seeking-back-implications", "title": "Stopping and Seeking Back Implications", "text": "<p>The Adaptive Iterator is designed to monitor message processing times and stop processing when it approaches the Kafka poll interval limit, seeking back to the last unprocessed message. While this mechanism helps prevent exceeding the poll interval, it may impact performance and networking.</p>"}, {"location": "Pro-Adaptive-Iterator/#impact-on-performance", "title": "Impact on Performance", "text": "<p>The primary impact on performance arises when the Adaptive Iterator stops processing frequently and initiates a seek operation. Seeking is not a lightweight operation; it involves the consumer resetting its position in the partition, introducing a delay. If seeking happens with every batch, especially when <code>the max.poll.interval.ms</code> is set to a lower value (default is 5 minutes), this overhead can become significant.</p> <p>However, it's important to note the relative impact of seeking in the context of typical configurations. Assuming that seeking back takes approximately 5 seconds, this delay is still only around 2% of the total processing time if the safety margin is set to 10% of the poll interval. In cases where the interval is longer and seeking happens infrequently, this impact is minimal.</p> <p>The performance hit becomes more noticeable if seeking occurs with every batch, which can lead the consumer to spend a disproportionate amount of time managing offsets instead of processing messages, ultimately reducing throughput. Thus, the key consideration is the frequency of seeking and the <code>configured max.poll.interval.ms</code> - the lower the interval, the higher the relative cost of frequent seeks. Proper configuration of the safety margin is crucial to balance processing efficiency against the risk of exceeding the poll interval.</p>"}, {"location": "Pro-Adaptive-Iterator/#impact-on-networking", "title": "Impact on Networking", "text": "<p>Frequent use of the Adaptive Iterator may have significant implications for networking, primarily due to increased communication with Kafka brokers. Each time the iterator stops processing and seeks back, it requires an update to the consumer's offset and the refetching of messages. In high-throughput environments, these operations can introduce a considerable increase in network traffic, as each one involves a network round trip between the consumer and the broker.</p> <p>Moreover, the Adaptive Iterator's frequent stopping and resetting cause the consumer to poll the Kafka broker more often, generating additional network activity. If the safety margin is too tight or if message processing times are highly variable, this can result in a large volume of polling requests, further adding to the network load. This behavior not only impacts the network usage of individual consumers but also places additional strain on Kafka brokers. The cumulative effect of frequent seeks and polling can affect the overall responsiveness and stability of the consumer group, particularly in high-traffic environments where multiple consumers are using the Adaptive Iterator simultaneously.</p>"}, {"location": "Pro-Adaptive-Iterator/#summary", "title": "Summary", "text": "<p>While the Adaptive Iterator is an effective tool for managing sporadic long-processing messages, it introduces trade-offs in terms of performance and networking. The frequent stopping and seeking back can reduce processing efficiency, increase network traffic, and place a higher load on Kafka brokers. To minimize these impacts, it's crucial to carefully configure the safety margin and use the Adaptive Iterator in situations where processing times are relatively predictable, with only occasional spikes. For environments with consistently long processing times or high variability, consider using other features like Long-Running Jobs to maintain optimal performance and network usage.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Cleaner-API/", "title": "Cleaner API", "text": "<p>Cleaner API is a feature designed to enhance the performance and efficiency of batch-processing tasks by promptly freeing up memory once a message payload is no longer needed. By addressing potential memory spikes, the Cleaner API ensures a more stable and efficient processing environment, especially beneficial when handling payloads of size 10KB and larger.</p>"}, {"location": "Pro-Cleaner-API/#how-does-it-work", "title": "How does it work", "text": "<p>This functionality extends both the message batch and individual message objects. Messages can indicate when their payload is no longer necessary. Once a message conveys this information, the Cleaner API completely removes both the payload and the raw payload from memory. </p> <p>Remarkably, this memory clearance occurs before the entire batch is processed, effectively allowing only a single deserialized payload to be kept in memory. This mechanism is particularly advantageous when sequentially processing messages, especially within large batches containing hundreds of messages or more. This ensures optimal memory management and faster processing, as no longer needed data is eradicated immediately after processing, rather than waiting for the entire batch to conclude.</p> <p>The below example illustrates how the Cleaner API releases both <code>payload</code> and <code>raw_payload</code> after each of the processed messages.</p> <p> </p> <p>Cleaner API and Kafka Message Immutability</p> <p>The data stored in Kafka remains unaltered and intact, regardless of any actions taken using the Cleaner API. The Cleaner API is exclusively designed to manage and optimize the memory utilization of the running process. When the API removes a message's payload, it only clears that data from the application's active memory. This operation does not, in any manner, affect the original message data residing in Kafka.</p>"}, {"location": "Pro-Cleaner-API/#using-cleaner-api", "title": "Using Cleaner API", "text": "<p>When utilizing the Cleaner API, it is paramount to understand the implications on the message's data.</p> <p>Once <code>#clean!</code> is invoked on a message, the message's <code>payload</code> and <code>raw_payload</code> are permanently removed from memory. As a result, these data become irretrievable and inaccessible. Please ensure that, under any circumstances, you do not try to use this data after it has been cleaned.</p>"}, {"location": "Pro-Cleaner-API/#cleaning-one-message-at-a-time", "title": "Cleaning One Message at a Time", "text": "<p>After processing a message, you can explicitly call the <code>#clean!</code> method on that message. This allows you to have granular control over which messages are cleaned from memory.</p> <p>By default, both the payload and metadata (headers and key) are removed. If you want to retain the metadata while discarding only the payload and raw payload, you can pass <code>metadata: false</code>.</p> <pre><code>def consume\n  messages.each do |message|\n    DataStore.persist!(message.payload)\n\n    # Always make sure to mark as consumed before cleaning\n    # in case you are using the Dead Letter Queue\n    mark_as_consumed(message)\n\n    # Clean only the payload, keep headers and key\n    # Defaults to cleaning also metadata\n    message.clean!(metadata: false)\n  end\nend\n</code></pre> <p>Metadata Cleaning Behavior</p> <p>By default, the Cleaner API removes both the message\u2019s payload and its metadata (<code>headers</code>, <code>key</code>). If you only want to clean the payload and keep the metadata available, use <code>message.clean!(metadata: false)</code>.</p>"}, {"location": "Pro-Cleaner-API/#automatic-cleaning-with-each", "title": "Automatic Cleaning with <code>#each</code>", "text": "<p>The Cleaner API allows you to automate the cleanup process for a more streamlined approach. By providing the <code>clean: true</code> parameter to the <code>#each</code> method, each message's payload is automatically cleaned from memory as soon as it's processed.</p> <pre><code>def consume\n  # Clean each message after we're done processing it\n  messages.each(clean: true) do |message|\n    DataStore.persist!(message.payload)\n\n    # Always make sure to mark as consumed before cleaning\n    # in case you are using the Dead Letter Queue\n    mark_as_consumed(message)\n  end\nend\n</code></pre>"}, {"location": "Pro-Cleaner-API/#checking-message-state", "title": "Checking Message State", "text": "<p>To assess whether a message has undergone the cleaning process, you can use the <code>#cleaned?</code> method directly on the message object. This method provides a straightforward way to check the cleaning status of the message, returning a boolean value (<code>true</code> if cleaned, <code>false</code> otherwise). This functionality is particularly useful for conditional operations or logging scenarios.</p> <pre><code>if message.cleaned?\n  puts 'The message has been cleaned.'\nelse\n  puts 'The message has not been cleaned yet.'\nend\n</code></pre> <p>Regardless of where you are in your code, as long as the message object is accessible, you can leverage the <code>#cleaned?</code> method to determine the cleaning state of the message and proceed accordingly.</p>"}, {"location": "Pro-Cleaner-API/#benefits", "title": "Benefits", "text": "<ul> <li> <p>Memory Efficiency: Cleaner API optimizes memory usage by promptly releasing unused messages payloads, ensuring that your application uses memory judiciously.</p> </li> <li> <p>Performance Boost: By avoiding significant memory spikes and congestion, applications can run smoother and process batches faster.</p> </li> <li> <p>Cost Savings: Efficient memory use can directly influence hosting costs, especially in cloud environments where you pay for the resources you use.</p> </li> <li> <p>Enhanced Reliability: By proactively managing memory, the chances of application crashes due to out-of-memory exceptions become much lower.</p> </li> </ul>"}, {"location": "Pro-Cleaner-API/#statistics", "title": "Statistics", "text": "<p>The efficacy and benefits of the Cleaner API are closely linked to the sizes of the raw payload and the deserialized payload of messages. </p> <p>The magnitude of memory management gains directly correlates with the average size of these payloads and the number of messages obtained in a single batch.</p> <p>In scenarios where the raw payload of a message is under 1KB, the advantages derived from using the Cleaner API tend to be minimal and may even appear negligible. This is because the memory space occupied by smaller messages is relatively minor. However, as the size of messages increases, the benefits of using the Cleaner API become increasingly pronounced. Large messages, when retained in memory after processing, can lead to significant memory congestion, and clearing them out promptly can free up sizable memory chunks, enhancing overall system performance.</p> <p>This size dependency is precisely why the statistics below offer insights into multiple scenarios, capturing the varying benefits across different payload sizes.</p> <p>The examples provided here illustrate the memory management across three styles of processing:</p> <ol> <li> <p>Standard: In this approach, the processing loop deserializes one message at a time, making it a sequential and straightforward method.</p> </li> <li> <p>Immediate: This method first invokes the <code>#payloads</code> function, deserializing all messages before any further processing occurs. This pre-deserialization can influence the overall memory consumption as all messages get loaded into memory up front.</p> </li> <li> <p>Cleaned: This processing style uses the <code>#each</code> loop but with an added parameter <code>cleaned: true</code>. This ensures that after each message is processed, its payload is promptly cleaned from memory, thus potentially reducing memory spikes and promoting efficient memory use.</p> </li> </ol> <p>For these examples, the primary metric used to measure memory consumption is RSS or Resident Set Size. RSS represents the portion of a process's memory that is held in RAM. This measurement clearly indicates the actual memory footprint of the process during its execution. In the context of these examples, the RSS value is displayed in megabytes (MBs).</p> <p>The structure of the code primarily focuses on deserializing data and then waiting and was similar to the one below:</p> <pre><code>def consume\n  # messages.payloads\n  messages.each(clean: true) do |message|\n    message.payload\n    sleep(0.1)\n  end\nend\n</code></pre> <p>Each fetched batch contained at most 500 messages.</p>"}, {"location": "Pro-Cleaner-API/#message-size-of-1mb", "title": "Message size of 1MB", "text": "<p>Conclusion: For messages approximately 1MB in size, the Cleaner API proves invaluable. It drastically cuts memory usage and stabilizes memory consumption patterns, reducing fluctuations and ensuring smoother, more efficient operations.</p>"}, {"location": "Pro-Cleaner-API/#message-size-of-100kb", "title": "Message size of 100KB", "text": "<p>Conclusion: For messages around 100KB in size, the Cleaner API still demonstrates a notable impact. While the memory savings might not be as dramatic as with 1MB messages, the reduction in memory usage and stabilization of consumption patterns remain significant, underscoring the API's effectiveness also at this size.</p>"}, {"location": "Pro-Cleaner-API/#message-size-of-10kb", "title": "Message size of 10KB", "text": "<p>Conclusion: When processing messages of approximately 10KB, the Cleaner API's influence is more nuanced. The memory savings hover around 4-5%, which might seem modest compared to larger payloads. However, the standout benefit lies in the considerable reduction in standard deviation. This reduction means memory usage is more predictable, leading to improved and more consistent operational performance.</p>"}, {"location": "Pro-Cleaner-API/#message-size-of-1kb", "title": "Message size of 1KB", "text": "<p>Conclusion: For messages approximating 1KB in size, the impact of employing the Cleaner API is virtually nonexistent. Its application doesn't notably affect the memory usage metrics for such small messages. However, it's worth noting that even if most messages are of this size, there could be occasional or periodic inflows of larger payloads. In such scenarios, the Cleaner API can help manage these sporadic spikes in memory usage. Therefore, even with predominantly 1KB messages, integrating the Cleaner API can be a prudent strategy if there's an anticipation of intermittently receiving messages with increased payloads.</p>"}, {"location": "Pro-Cleaner-API/#limitations", "title": "Limitations", "text": "<p>The Cleaner API offers several advantages, especially when it comes to efficiently managing memory for larger payloads. However, as with any tool or feature, there are certain limitations that users should be aware of:</p> <ul> <li> <p>Overhead: Despite its design focus on being lightweight and efficient, the Cleaner API introduces a small overhead. This is associated with the operation of releasing the data from memory.</p> </li> <li> <p>Not Suitable for Tiny Payloads: The efficacy of the Cleaner API is more pronounced for larger message payloads, typically those sized 10KB and above. When dealing with tiny payloads, the memory management benefits are negligible, and the overhead might overshadow any gains.</p> </li> <li> <p>Cleaned Messages and DLQ: Messages that have been cleaned using the Cleaner API can't be dispatched to the Dead Letter Queue (DLQ). This underscores the importance of ensuring that any <code>#mark_as_consumed</code> operations happen strictly after complete processing.</p> </li> <li> <p>Marking as Consumed When Cleaning and DLQ: When using the Cleaner API in conjunction with a topic with a Dead Letter Queue configured, it's required to ensure each message is <code>#marked_as_consumed</code> before invoking the <code>#clean!</code> method. Please do so to avoid inconsistencies and potential errors, as cleaned messages cannot be dispatched to the DLQ.</p> </li> <li> <p>Payload Availability for Metrics and Reporting: Once a message has been cleaned, its payload and raw payload are no longer accessible. If you depend on these payloads for metrics, logging, or reporting purposes, you must gather and store this information before invoking the cleaning operation.</p> </li> <li> <p>Lifecycle Hooks Limitations: When working with lifecycle hooks like <code>#shutdown</code> or <code>#revoked</code>, it's crucial to approach carefully if Cleaner API has been used on the messages. The payloads for these cleaned messages will be unavailable from these hooks, which could affect operations relying on them.</p> </li> </ul> <p>Understanding these limitations is essential for users to effectively and efficiently leverage the Cleaner API without encountering unforeseen issues or complications in their processes.</p>"}, {"location": "Pro-Cleaner-API/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Log Aggregation and Analysis Systems: Many enterprises gather vast amounts of log data from various sources like applications, servers, and network devices. Processing this data in real time often involves deserializing large chunks of data to analyze patterns, anomalies, or security breaches. After processing, the payload data often remains in memory until the whole batch is analyzed, causing inefficiencies.</p> </li> <li> <p>E-Commerce Transaction Processing: E-commerce platforms process millions of daily transactions, including user data, product information, and payment details. Each transaction can be a sizeable chunk of data.</p> </li> <li> <p>IoT Data Ingestion: IoT devices can send vast amounts of data, especially in smart cities or industrial IoT scenarios. This data often contains sensor readings, device status updates, and more. The Cleaner API can help quickly clean up processed data, ensuring efficient memory usage as millions of messages pour in.</p> </li> <li> <p>Financial Data Analysis: Financial institutions process large datasets daily, including stock market feeds, transactions, and trading data. These data packets can vary in size and come in rapid succession. To ensure that analysis tools and algorithms function at peak efficiency, the Cleaner API can be employed to release memory as soon as a data packet has been processed, maintaining system responsiveness.</p> </li> </ul> <p>In all these use cases, the key value of the Cleaner API is in enhancing memory management, ensuring that systems maintain optimal performance even when dealing with substantial or varied data loads with various message sizes.</p> <p>Last modified: 2025-03-26 10:50:57</p>"}, {"location": "Pro-Delayed-Topics/", "title": "Delayed Topics", "text": "<p>Karafka's Delayed Topics is a feature that enables delaying message processing from specific topics for a specified time. It can be beneficial to delay message processing for various reasons. For example, to allow additional processing or validation time, avoid overloading the system during high-traffic periods, or provide a retry mechanism for failed messages. Delayed Topics offer greater flexibility and control over message processing.</p> <p>One of the benefits of the Karafka Delayed Topic feature is that it allows for arbitrary delay without impacting the processing of other topics. </p> <p>Delay is implemented by pausing the consumption of the partitions for a specified amount of time. This means that there is no explicit sleep or anything of that nature involved that would clog or impact other topics' operations, and all the available resources are free to process messages that are not expected to be delayed.</p> <p>This makes the Delayed Topic feature a great choice for applications that need to delay the processing of specific messages without impacting the processing of other messages in the system. By using Karafka's built-in partition pausing mechanism, delayed messages can be processed in a way that is both efficient and reliable.</p> <p> </p> <p> *Illustration presenting how Delayed Topics delays too young messages.    </p>"}, {"location": "Pro-Delayed-Topics/#enabling-delayed-topics", "title": "Enabling Delayed Topics", "text": "<p>To enable the Delayed Topics feature in Karafka, you need to add the <code>delay_by</code> option to your Karafka routing configuration. Here's an example of how to do that:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders do\n      consumer OrdersConsumer\n      # Always delay processing messages from the orders topic by 1 minute\n      delay_by(60_000)\n    end\n  end\nend\n</code></pre> <p>Please keep in mind, that the delay time needs to be provided in milliseconds</p>"}, {"location": "Pro-Delayed-Topics/#delayed-topics-vs-inline-sleep-invocation", "title": "Delayed Topics vs. inline <code>#sleep</code> invocation", "text": "<p>Using <code>#sleep</code> inside consumers is not recommended. Sleep can heavily impact other topics and partitions and cause additional lags even for those, that do not invoke it. Below you can find a potential impact of sleeping on one of the topic partitions vs. using the delayed topics functionality instead:</p> <p>Using <code>#sleep</code> on one of the partitions:</p> <p> </p> <p> *Sleep can affect other topics and partitions running in parallel in other threads.    </p> <p>vs. delaying via the <code>#delay_by</code> API:</p> <p> </p> <p> <code>#delay_by</code> is affecting only the desired topic/partition while others are processed as fast as possible.    </p>"}, {"location": "Pro-Delayed-Topics/#limitations", "title": "Limitations", "text": "<p>While the Karafka Delayed Topics feature provides a valuable way to delay message processing, it does have some limitations to keep in mind.</p> <p>One significant limitation is that the delay is not always millisecond-precise. This is because the Delayed Topics feature works by pausing the processing of a given partition for a specified amount of time and then unpausing it after that time has elapsed. However, the unpausing happens before the polling happens, so there can be a slight delay between when the partition is unpaused and when the delayed message is processed.</p> <p>This means that if you need millisecond-precise timing for your application, there may be better choices than Delayed Topics. However, this limitation is unlikely to be a significant issue for most use cases.</p> <p>This limitation also means that messages may be delayed slightly more than the requirement minimum but will never be delayed less than expected.</p> <p>Below is an example distribution of the extra lag beyond the tested and expected ten seconds. In most cases, it is equal to or less than 10% of Karafkas' max wait time.</p> <p> </p>"}, {"location": "Pro-Delayed-Topics/#revocation-and-shutdown", "title": "Revocation and Shutdown", "text": "<p>When using the Delayed Topics feature in Karafka, it is essential to note that both the <code>#shutdown</code> and <code>#revocation</code> methods may be executed without the prior <code>#consume</code> running. This is because Delayed Topics may delay the processing of the first set of messages, which means that the messages batch may be empty, and the first and last offsets taken from metadata will be equal to <code>-1001</code>.</p> <p>In such scenarios, using the <code>#used?</code> method when relying on them in <code>#revocation</code> and <code>#shutdown</code> is always recommended. This can be done using a conditional statement that checks if there was even a single batch consumed or scheduled for consumption.</p> <p>Below you can find an example scenario where a check is needed on <code>#shutdown</code> to verify that there are messages in the messages batch before committing the offset. This scenario occurs when the Karafka server is started and quickly shut down before the first batch of messages is old enough to be processed.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders do\n      consumer OrdersConsumer\n      delay_by(60_000)\n      manual_offset_management true\n    end\n  end\nend\n\nclass OrdersConsumer &lt; ApplicationConsumer\n  def consume\n    puts messages.payloads\n  end\n\n  def shutdown\n    # When using `#delay_by`, make sure that there is any message\n    # we want to mark as consumed upon shutdown\n    #\n    # Please note, you do not need this check if you are not using\n    # filtering API or delayed topics functionalities.\n    return unless used?\n\n    mark_as_consumed messages.last\n  end\nend\n</code></pre> <p>It is important to note that Delayed Topics can be a powerful tool for managing message processing, as it allows for messages to be processed in a controlled manner. However, it is essential to understand the potential side effects and implement appropriate error-handling mechanisms to ensure the system remains stable and reliable.</p>"}, {"location": "Pro-Delayed-Topics/#example-use-cases", "title": "Example Use Cases", "text": "<p>Here are some potential use cases for Delayed Topics:</p> <ul> <li> <p>General: By using Delayed Topics in conjunction with a Dead Letter Queue, you can create a more robust and dynamic system that is capable of handling a variety of failure scenarios and providing a more efficient and effective message processing system.</p> </li> <li> <p>Data crawling: The Delayed Topics feature can be helpful in data crawling applications, where immediately published data may not be immediately available due to HTTP caches. In such cases, it may be beneficial to delay the processing of messages for a fixed period, to ensure that all the caches have expired and the data is fully available. By using delayed processing, you can avoid processing incomplete or stale data and ensure that your application works with the latest, fully available information.</p> </li> <li> <p>E-commerce: Delay processing of orders for a short period to allow for cancellation or modification of orders by customers. During this delay, additional validation can be performed, such as stock availability or fraud detection. If there is a problem with processing an order, it can be moved to a Dead Letter Queue for reprocessing later.</p> </li> <li> <p>Social Media: Delay processing of user-generated content to allow for moderation by human teams before publishing. During the delay, messages can be stored in a separate queue and notified to moderators for review. If approved, the message can be moved to the main processing queue for publishing. The message can be moved to a Dead Letter Queue for further action if not approved.</p> </li> <li> <p>Finance: Delay processing of high-risk transactions to provide additional time for fraud detection. If a transaction is flagged as suspicious, it can be moved to a separate queue for human review. If a transaction fails to process due to a system error, it can be moved to a Dead Letter Queue for reprocessing at a later time.</p> </li> <li> <p>Real-Time Monitoring: Delay processing of real-time monitoring data to enable better performance and reduce network congestion. By batching data and delaying processing, it can be processed more efficiently and effectively, and results can be displayed to users in a more timely manner.</p> </li> <li> <p>Machine Learning: Delay processing of machine learning training data to avoid training on stale or inaccurate data. By delaying data processing, it is possible to ensure that the data used for training is up-to-date and accurate.</p> </li> </ul>"}, {"location": "Pro-Delayed-Topics/#summary", "title": "Summary", "text": "<p>Karafka's Delayed Topics is a powerful feature that allows for arbitrary delays when processing messages from specific topics. It can be used in various use cases, such as e-commerce, social media moderation, and finance. By delaying message processing, you can perform additional processing or validation, moderate user-generated content, and introduce a retry mechanism for failed messages. By using Delayed Topics in conjunction with a Dead Letter Queue, you can create a more robust and dynamic system that can handle various business and failure scenarios.</p> <p>Last modified: 2023-12-12 12:18:06</p>"}, {"location": "Pro-Direct-Assignments/", "title": "Direct Assignments", "text": "<p>The Direct Assignments feature allows precise control over Kafka topic and partition consumption. This feature bypasses the standard consumer group partition assignment mechanism, allowing you to manually specify which partitions and topics each consumer should process. This can be particularly useful for building complex data pipelines and applications that require explicit partition handling.</p> <p>Direct Assignments enable scenarios where automatic partition assignments could be suboptimal or inappropriate. With direct assignments, you can explicitly define partition ownership, ensuring that specific consumers only process data from specified partitions.</p>"}, {"location": "Pro-Direct-Assignments/#key-features", "title": "Key Features", "text": "<ul> <li> <p>Partition Specific Processing: Assign specific partitions to specific consumers to ensure data locality and processing are optimized for performance and correctness.</p> </li> <li> <p>Bypass Consumer Groups: Directly assign partitions without relying on Kafka's built-in consumer group mechanics. This can reduce rebalance times and increase stability in environments with high partition counts.</p> </li> <li> <p>Enhanced Control: Greater control over which partitions are processed by which consumers, allowing for tailored processing logic that can adapt to the nuances of your data and application requirements.</p> </li> <li> <p>Optimized Data Locality: Direct Assignments can enhance data locality by assigning partitions to consumers based on where data is stored, potentially reducing network traffic and increasing overall system efficiency.</p> </li> <li> <p>Tailored Processing Logic: They facilitate complex processing logic that might be necessary for advanced use cases, such as maintaining state or processing partitions in a specific sequence.</p> </li> <li> <p>Unaffected by <code>max.poll.interval.ms</code>: By using Direct Assignments, consumers are not subjected to the constraints of <code>max.poll.interval.ms</code>, which defines the maximum time between poll calls before a consumer is considered unresponsive.</p> </li> </ul>"}, {"location": "Pro-Direct-Assignments/#configuring-direct-assignments", "title": "Configuring Direct Assignments", "text": "<p>To utilize Direct Assignments, you specify which partitions a consumer should subscribe to directly in your Karafka routing configuration. Here is an example of how to configure a consumer to only process specific partitions:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic 'my_topic' do\n      consumer MyConsumer\n      # Directly assign partitions 0 and 1 to this consumer\n      assign [0, 1]\n    end\n  end\nend\n</code></pre> <p>In case you would want to assign all partitions, for example for repartitioning or other cross-partition operations, you can pass <code>true</code> to the <code>#assign</code> method:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic 'my_topic' do\n      consumer MyConsumer\n      # Directly assign all partitions of this topic to the consumer\n      assign true\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Direct-Assignments/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Complex Data Pipeline Integration: In complex data pipelines, certain data processing stages may need to operate on specific partitions due to dependencies or processing requirements. With Direct Assignments, you can ensure that these requirements are explicitly handled, reducing the risk of processing errors and inefficiencies.</p> </li> <li> <p>High-Performance Requirements: For applications requiring high throughput and low latency, managing partition assignments directly can minimize rebalances and optimize data locality, leading to faster processing times.</p> </li> <li> <p>Predictable Scaling: When scaling out consumers in a Kafka application, Direct Assignments allow for predictable performance by explicitly controlling partition distribution among consumers. This is especially useful in scenarios where adding more consumers must be meticulously planned to avoid performance degradation.</p> </li> <li> <p>Data Isolation for Security or Compliance: Certain use cases might require data isolation for security or compliance reasons. Direct Assignments allow you to segregate data processing to specific consumers, ensuring that sensitive data is only accessible to authorized processes.</p> </li> <li> <p>Stateful Stream Processing: Applications requiring stateful operations across messages can benefit from Direct Assignments, which guarantee that specific consumers always process certain partitions. This setup is crucial for systems like event sourcing or complex event processing (CEP), where maintaining order and state consistency is vital.</p> </li> <li> <p>Stream Merging from Multiple Topics: One advanced use case of Direct Assignments is stream merging, where data from different topics or partitions needs to be combined or synchronized. This is particularly useful in scenarios where related data streams from multiple sources must be processed together to produce a coherent output. For example, merging user actions from one topic with user profiles from another to generate a comprehensive activity log.</p> </li> </ul>"}, {"location": "Pro-Direct-Assignments/#conclusion", "title": "Conclusion", "text": "<p>Karafka Pro's Direct Assignments feature is a powerful tool that, when used effectively, can greatly enhance the performance, reliability, and security of Kafka-based applications. It allows for precise control over partition processing, enabling sophisticated data processing strategies that go beyond the capabilities of standard Kafka consumer groups. Whether you are building high-throughput data pipelines, implementing complex processing logic, or must comply with strict data security standards, Direct Assignments can provide the tools to achieve these goals efficiently and reliably.</p> <p>Last modified: 2024-04-26 13:03:30</p>"}, {"location": "Pro-Enhanced-Active-Job/", "title": "Enhanced Active Job", "text": "<p>While Kafka is not a message queue, it has certain features that make it a great fit for Active Job, especially when strict ordering and scaling are desired.</p> <p>Enhanced Active Job adapter provides extra capabilities to regular Active Job to elevate the combination of Active Job and Kafka.</p>"}, {"location": "Pro-Enhanced-Active-Job/#enabling-enhanced-active-job", "title": "Enabling Enhanced Active Job", "text": "<p>No action needs to be taken. Please follow the Active Job setup instructions, and the moment you enable Karafka Pro, it will use the Enhanced Active Job components.</p>"}, {"location": "Pro-Enhanced-Active-Job/#ordered-jobs", "title": "Ordered Jobs", "text": "<p>With the Karafka Enhanced Active Job adapter, you can ensure jobs processing order. This means that with proper <code>partitioner</code> usage, you can ensure that for a given resource, only one job runs at a time and that jobs will run in the order in which they were enqueued.</p> <p>You can tell Karafka to which partition send a given job based on the job arguments. For it to work, Karafka provides two <code>karafka_options</code> options you can set:</p> <ul> <li><code>partitioner</code> - a callable that accepts the job as the argument</li> <li><code>partition_key_type</code> - either <code>:key</code> (default), <code>:partition_key</code> or <code>:partition</code></li> </ul> <p>Jobs sent to the same partition will always be processed in the order. This can be useful when you process data of objects for which you need to apply your logic sequentially without risking any concurrency problems. For example for applying updates in a consistent order.</p> <pre><code># An example job that updates user attributes in the background job\nclass Job &lt; ActiveJob::Base\n  queue_as TOPIC\n\n  karafka_options(\n    # Make sure that all jobs related to a given user are always dispatched to the same partition\n    partitioner: -&gt;(job) { job.arguments.first },\n    partition_key_type: :key\n  )\n\n  def perform(user_id, attributes)\n    User.find(user_id).update!(attributes)\n  end\nend\n</code></pre> <p>The above code will ensure that jobs related to the same user will always be dispatched to the same consumer.</p> <p>We recommend using the <code>:key</code> as then it can be used for combining Enhanced Active Job with Virtual Partitions.</p> <p> </p> <p> *This example illustrates the end distribution of jobs based on the user id.    </p>"}, {"location": "Pro-Enhanced-Active-Job/#scheduled-jobs", "title": "Scheduled Jobs", "text": "<p>Karafka supports job scheduling via the Scheduled Messages feature, providing a robust framework for setting future execution times for tasks, akin to capabilities seen in other Rails Active Job adapters. This feature integrates seamlessly with Karafka's infrastructure, allowing users to schedule and manage tasks directly within the Kafka ecosystem.</p> <p>To utilize the Scheduled Jobs functionality in Karafka, you must:</p> <ol> <li> <p>Configure the Scheduled Messages Feature: Ensure the Scheduled Messages feature is properly configured within your Karafka setup. This involves setting up the necessary Kafka topics and ensuring Karafka knows these configurations.</p> </li> <li> <p>Configure the Job Class: Each job class that requires scheduling must have the scheduled_messages_topic configured. This setting informs Karafka about the specific Kafka topic that serves as the proxy for handling the scheduling of these messages.</p> </li> </ol> <pre><code>class ExampleJob &lt; ActiveJob::Base\n  queue_as :default\n\n  karafka_options(\n    scheduled_messages_topic: 'scheduled_jobs_topic'\n  )\n\n  def perform(*args)\n    # Job execution logic here\n  end\nend\n</code></pre> <ol> <li>Schedule Jobs: After these configurations are in place, jobs can be scheduled using the standard ActiveJob APIs by specifying the execution time:</li> </ol> <pre><code>ExampleJob.set(wait_until: Date.tomorrow.noon).perform_later(user_id)\n</code></pre> <p>This integration not only simplifies the management of timed tasks but also enhances the reliability and scalability of job execution, making Karafka an ideal platform for complex, time-sensitive job scheduling needs in large-scale applications.</p>"}, {"location": "Pro-Enhanced-Active-Job/#custom-producervariant-usage", "title": "Custom Producer/Variant Usage", "text": "<p>When using ActiveJob with Karafka, you can customize the dispatch of Active Jobs by leveraging custom producers or producer variants. This customization allows for more granular control over how jobs are produced and managed within Kafka, which can be crucial for applications with specific performance, scalability, or reliability requirements.</p> <p>To utilize a custom producer or variant with ActiveJob, specify a <code>:producer</code> option within the <code>#karafka_options</code>. This option should be set to a callable object (such as a lambda or a proc) that accepts the job as an argument. This callable is expected to return a producer or a variant that will be used to dispatch the job's message to Kafka.</p> <p>Here is an example that demonstrates how to integrate a custom producer variant within an ActiveJob setup:</p> <pre><code># Define a custom producer variant for high-priority jobs\nHIGH_RELIABILITY_PRODUCER = Karafka.producer.with(topic_config: { 'acks': 'all' })\n\n# Define an ActiveJob class that uses this custom producer variant\nclass HighPriorityJob &lt; ActiveJob::Base\n  queue_as :critical_events\n\n  karafka_options(\n    # Job is accepted as an argument for dynamic producer selection\n    producer: -&gt;(_job) { HIGH_RELIABILITY_PRODUCER }\n  )\n\n  def perform(event_data)\n    # Job implementation\n  end\nend\n</code></pre> <p>In the above example, <code>HighPriorityJob</code> is configured to use a specifically tailored producer variant for critical events. This producer variant is configured with a higher acknowledgment setting (<code>all</code>), ensuring that all replicas confirm each message before it is successfully delivered. This setup is particularly beneficial for jobs where data loss or delivery failure is unacceptable.</p> <p>Allowing each job class to specify its producer offers the flexibility to tailor message production characteristics according to the job's requirements. Whether it's adjusting the acknowledgment levels, managing timeouts, or utilizing specific compression settings, custom producers and variants can significantly enhance the robustness and efficiency of your Karafka-based messaging system within ActiveJob, opening up new possibilities for system optimization and performance improvement.</p>"}, {"location": "Pro-Enhanced-Active-Job/#routing-patterns", "title": "Routing Patterns", "text": "<p>Pro ActiveJob adapter supports the Routing Patterns capabilities. You can read more about it here.</p>"}, {"location": "Pro-Enhanced-Active-Job/#execution-warranties", "title": "Execution Warranties", "text": "<p>Same execution warranties apply as for standard Active Job adapter.</p>"}, {"location": "Pro-Enhanced-Active-Job/#behaviour-on-errors", "title": "Behaviour on Errors", "text": "<p>When using the ActiveJob adapter with Virtual Partitions, upon any error in any of the Virtual Partitions, all the not-started work in any of the Virtual Partitions will not be executed. The not-executed work will be then executed upon the retry. This behavior minimizes the number of jobs that must be re-processed upon an error.</p> <p>For non-VP setup, same error behaviors apply as for standard Active Job adapter.</p> <p>Please keep in mind that if you use it in combination with Virtual Partitions, marking jobs as consumed (done) will happen only after all virtually partitioned consumers finished their work collectively. There is no intermediate marking in between jobs in that scenario.</p>"}, {"location": "Pro-Enhanced-Active-Job/#behaviour-on-revocation", "title": "Behaviour on Revocation", "text": "<p>Enhanced Active Job adapter has revocation awareness. That means that Karafka will stop processing other pre-buffered jobs upon discovering that a given partition has been revoked. In a scenario of a longer job where the revocation happened during the job execution, only at most one job per partition will be processed twice. You can mitigate this scenario with static group memberships.</p>"}, {"location": "Pro-Enhanced-Active-Job/#behaviour-on-shutdown", "title": "Behaviour on Shutdown", "text": "<p>When using the ActiveJob adapter with Virtual Partitions, Karafka will not early break processing and will continue until all the work is done. This is needed to ensure that all the work is done before committing the offsets.</p> <p>For a non-VP setup, the same shutdown behavior applies as for standard Active Job adapter.</p> <p>Last modified: 2024-09-09 13:17:18</p>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/", "title": "Enhanced Dead Letter Queue", "text": "<p>Enhanced Dead Letter Queue feature provides additional functionalities and warranties to the regular Dead Letter Queue feature. It aims to complement it with additional dispatch warranties and additional messages metadata information.</p> <p>This documentation only covers extra functionalities enhancing the Dead Letter Queue feature.</p> <p>Please refer to the Dead Letter Queue documentation for more details on its core principles.</p>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#using-enhanced-dead-letter-queue", "title": "Using Enhanced Dead Letter Queue", "text": "<p>There are no extra steps needed. If you are using Karafka Pro, Enhanced Dead Letter Queue is configured the same way as the regular one:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: 'dead_messages',\n        max_retries: 2\n      )\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#delaying-the-dlq-data-processing", "title": "Delaying the DLQ Data Processing", "text": "<p>In some cases, it can be beneficial to delay the processing of messages dispatched to a Dead Letter Queue (DLQ) topic. This can be useful when a message has failed to be processed multiple times, and you want to avoid overwhelming the system with repeated processing attempts. By delaying the processing of these messages, you can avoid consuming valuable resources and prevent potential system failures or downtime.</p> <p>If you are processing data dispatched to the DLQ topic, all you need to do to make it delayed is to add <code>delay_by</code> to your DLQ topic routing definition as follows:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: :failed_orders_dlq,\n        max_retries: 2\n      )\n    end\n\n    topic :failed_orders_dlq do\n      consumer FailedOrdersRecoveryConsumer\n      # Try to process failed orders messages with 5 minutes of a delay\n      delay_by(5 * 60_000)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#disabling-dispatch", "title": "Disabling Dispatch", "text": "<p>For some use cases, you may want to skip messages after retries without dispatching them to an alternative topic.</p> <p>To do this, you need to set the DLQ <code>topic</code> attribute value to <code>false</code>:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: false,\n        max_retries: 2\n      )\n    end\n  end\nend\n</code></pre> <p>When that happens, Karafka will retry two times and continue processing despite errors.</p> <p>Critical Configuration Notice</p> <p>Setting the <code>topic</code> value to <code>nil</code> when disabling dispatch will fully disable the Dead Letter Queue. If you intend to turn off dispatch but still want DLQ functionality, ensure the <code>topic</code> value is set to <code>false</code> instead of <code>nil</code>. This ensures that DLQ handling remains active while preventing message dispatch.</p>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#dispatch-warranties", "title": "Dispatch Warranties", "text": "<p>Enhanced Dead Letter Queue ensures that messages moved to the DLQ topic will always reach the same partition and in order, even when the DLQ topic has a different number of partitions. This means that you can implement pipelines for processing broken messages and rely on the ordering warranties from the original topic.</p> <p> </p> <p> *This example illustrates how Enhanced DLQ preserves order of messages from different partitions.    </p> <p>The DLQ topic does not have to have the same number of partitions as the topics from which the broken messages come. Karafka will ensure that all the messages from the same origin partition will end up in the same DLQ topic partition.</p>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#additional-headers-for-increased-traceability", "title": "Additional Headers For Increased Traceability", "text": "<p>Karafka Pro, upon transferring the message to the DLQ topic, aside from preserving the <code>payload</code>, and the <code>headers</code> will add a few additional headers that allow for increased traceability of broken messages:</p> <ul> <li><code>source_topic</code> - topic from which the message came</li> <li><code>source_partition</code> - partition from which the message came</li> <li><code>source_offset</code> - offset of the transferred message</li> <li><code>source_key</code> - key of the transferred message</li> <li><code>source_consumer_group</code> - id of the consumer group that was consuming this message</li> <li><code>source_trace_id</code> - distributed tracing identifier from the original message processing. Can be used to correlate dispatches with errors visible in the Web UI</li> </ul> <p>String Headers</p> <p>Karafka headers values are always strings.</p> <p>This can be used for debugging or for example when you want to have a single DLQ topic with per topic strategies:</p> <pre><code>class DlqConsumer\n  def consume\n    messages.each do |broken_message|\n      source_topic = broken_message.headers['source_topic']\n      source_partition = broken_message.headers['source_partition'].to_i\n      source_offset = broken_message.headers['source_offset'].to_i\n      payload = broken_message.raw_payload\n\n      case source_topic\n      when 'orders_events'\n        BrokenOrders.create!(\n          payload: payload,\n          source_partition: source_partition,\n          source_offset: source_offset\n        )\n      when 'users_events'\n        NotifyDevTeam.call(\n          payload: payload,\n          source_partition: source_partition,\n          source_offset: source_offset\n        )\n      else\n        raise StandardError, \"Unsupported original topic: #{source_topic}\"\n      end\n\n      mark_as_consumed(broken_message)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#adding-custom-details-to-the-dlq-message", "title": "Adding Custom Details To the DLQ Message", "text": "<p>If you want to add some extra information or change anything in the message that will be dispatched to the DLQ topic, you can do it by defining a custom method called <code>#enhance_dlq_message</code> in your consumer class.</p> <p>It accepts two arguments:</p> <ul> <li><code>dlq_message</code> - a hash with all the details of the DLQ message that will be dispatched</li> <li><code>skippable_message</code> - Karafka message that we skip via the DLQ feature</li> </ul> <p>Let's say you want to add some headers and alter the payload. You can do it in the following way:</p> <pre><code>class MyConsumer\n  def consume\n    # some code that can raise an error...\n  end\n\n  private\n\n  def enhance_dlq_message(dlq_message, skippable_message)\n    # Replace the DLQ message payload with a hash containing the original raw payload as well as\n    # process pid\n    #\n    # Note that payload here needs to be a string\n    dlq_message[:payload] = {\n      source_raw_payload: skippable_message.raw_payload,\n      process_pid: Process.pid\n    }.to_json\n\n    # Add one extra header to the message headers\n    dlq_message[:headers]['extra-header'] = 'yes'\n  end\nend\n</code></pre> <p>No routing changes are needed to make it work.</p>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#dlq-message-key-enhancements-for-a-compacted-dlq-topic", "title": "DLQ Message <code>key</code> Enhancements For a Compacted DLQ Topic", "text": "<p>If you use a <code>compact</code> value for Kafka <code>log.cleanup.policy</code>, you may lose messages dispatched to the DLQ topic due to the DLQ compacting limitations.</p> <p>You can mitigate this by enhancing the DLQ message with a unique key using the <code>#enhance_dlq_message</code> consumer method:</p> <pre><code>class MyConsumer\n  def consume\n    # some code that can raise an error...\n  end\n\n  private\n\n  def enhance_dlq_message(dlq_message, skippable_message)\n    dlq_message[:key] = [\n      topic.name,\n      skippable_message.partition,\n      skippable_message.offset\n    ].join('-')\n  end\nend\n</code></pre>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#disabling-transactions-during-dlq-dispatches", "title": "Disabling Transactions During DLQ Dispatches", "text": "<p>Karafka, by default, uses transactions to atomically dispatch messages to a Dead-Letter Queue (DLQ) and mark them as consumed when a transactional producer is available. However, you might prefer to handle these actions independently, especially when minimizing transactional overhead is a priority.</p> <p>To turn off transactional behavior for DLQ dispatches, set the transactional option to false in the DLQ routing configuration. Here's how to apply this setting:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: 'dead_messages',\n        max_retries: 2,\n        # Do not use transactions for DLQ dispatches\n        transactional: false\n      )\n    end\n  end\nend\n</code></pre> <p>This adjustment ensures that messages dispatched to the DLQ and the marking of their consumption are processed separately, providing you with the flexibility to align Karafka's behavior with your system's needs.</p>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#advanced-error-tracking", "title": "Advanced Error Tracking", "text": "<p>Karafka Pro maintains a log of the last 100 errors during message processing, retaining this error history until a successful processing. This feature allows developers to leverage historical error data to inform recovery strategies, ensuring a nuanced approach to handling errors based on past failures.</p> <p>The errors_tracker API in Karafka's ErrorsTracker class is designed to accumulate and manage a history of errors during the <code>#consume</code> method execution. It tracks up to the last 100 errors to prevent memory leaks from endless error loops.</p> <p>You can access the errors tracker from the consumer by invoking the <code>#errors_tracker</code> consumer method:</p> <pre><code>def consume\n  if retrying?\n    skip_first = nil\n\n    # Use the last (most recent) error details for advanced error handling\n    case errors_tracker.last\n    when DbTimeoutError\n      skip_first = false\n    when FormatError\n      skip_first = true\n    else\n      skip_first = false\n    end\n\n    messages.each_with do |message, index|\n      if index.zero? &amp;&amp; skip_first\n        mark_as_consumed(message)\n        next \n      end\n\n      DbStorage.save!(message.payload)\n      mark_as_consumed(message)\n    end\n  else\n    messages.each do |message|\n      DbStorage.save!(message.payload)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#error-tracking-with-virtual-partitions", "title": "Error Tracking with Virtual Partitions", "text": "<p>When using Virtual Partitions, which operate in parallel within a single Kafka partition, Karafka aggregates errors across all virtual partitions. This aggregation means that errors from all virtual partitions are available during the recovery phase, providing a comprehensive view of the issues encountered. This capability is crucial for implementing effective recovery strategies, as it ensures that the error-handling logic can account for the diverse range of errors that may occur across parallel processing threads.</p>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#accessing-error-context-in-dlq-message-enhancement", "title": "Accessing Error Context in DLQ Message Enhancement", "text": "<p>When customizing DLQ messages using the <code>#enhance_dlq_message</code> method, you may want to include information about the specific error that caused the message to be dispatched to the DLQ. This can be particularly useful for debugging, monitoring, or implementing error-specific recovery strategies.</p> <p>You can access the error information through the <code>#errors_tracker</code> method within your <code>#enhance_dlq_message</code> implementation. The errors tracker provides access to the history of errors that occurred during message processing, allowing you to include error details in the DLQ message headers or payload.</p> <p>Here's an example of how to add error information to your DLQ messages:</p> <pre><code>class MyConsumer\n  def consume\n    # some code that can raise an error...\n    raise StandardError, \"Database connection failed\"\n  end\n\n  private\n\n  def enhance_dlq_message(dlq_message, skippable_message)\n    # Add error class information to headers\n    dlq_message[:headers]['error_class'] = errors_tracker.last.class.to_s\n\n    # Add the number of processing attempts\n    dlq_message[:headers]['attempt_count'] = errors_tracker.size.to_s\n\n    # You can also enhance the payload with error context\n    enhanced_payload = {\n      source_payload: skippable_message.raw_payload,\n      error_details: {\n        class: errors_tracker.last.class.to_s,\n        message: errors_tracker.last.message,\n        backtrace: errors_tracker.last.backtrace&amp;.first(5), # First 5 lines of stack trace\n        total_attempts: errors_tracker.size\n      }\n    }\n\n    dlq_message[:payload] = enhanced_payload.to_json\n  end\nend\n</code></pre> <p>This approach enables you to:</p> <ul> <li>Debug failures more effectively by having immediate access to error details when processing DLQ messages</li> <li>Implement error-specific recovery logic in your DLQ consumer based on the error type</li> <li>Monitor and alert on specific types of errors by examining DLQ message headers</li> <li>Track processing attempts to identify messages that consistently fail</li> </ul> <p>Header Value Type Conversion</p> <p>Remember that Kafka header values are always strings, so ensure any non-string values are converted appropriately when adding them to headers.</p>"}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#custom-context-aware-recovery-strategies", "title": "Custom Context-Aware Recovery Strategies", "text": "<p>Karafka allows for implementing custom DLQ handling and recovery strategies, leveraging the flexibility to respond to errors based on specific conditions like the number of attempts or the nature of the errors encountered. This approach enables tailored error handling, improving the resilience and reliability of your application. Custom strategies can differentiate between errors, deciding to retry, skip, or dispatch messages to a DLQ based on predefined logic, such as retrying database-related errors indefinitely, skipping non-recoverable errors immediately, or applying a limited number of retries for recoverable errors.</p> <p>This method offers significant benefits, including more efficient processing, reduced noise from non-recoverable errors, and enhanced opportunity for successful message recovery, leading to a more robust and error-tolerant system.</p> <p>For a practical implementation, consider a scenario where you define custom error classes for different error types and a strategy class that decides the action based on the last error and attempt number. This setup enables nuanced control over how your application responds to specific errors, optimizing your processing logic for efficiency and effectiveness.</p> <p>To integrate a custom DLQ strategy, define your strategy class with the necessary logic in the <code>#call</code> method. Then, in your Karafka routing configuration, assign your custom strategy under the <code>#dead_letter_queue</code> option for the relevant topic:</p> <pre><code>class OrdersDlqStrategy\n  # @param errors_tracker [Karafka::Pro::Processing::Coordinators::ErrorsTracker] errors tracker\n  #   that collects errors that occurred during processing until another successful processing run\n  # @param attempt [Integer] attempt of processing of given messages\n  def call(errors_tracker, attempt)\n    #...\n  end\nend\n\nclass KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: 'dead_messages',\n        # When defining strategy, `max_retries` is not needed\n        strategy: OrdersDlqStrategy.new\n      )\n    end\n  end\nend\n</code></pre> <p>After an error occurs and Karafka decides on what to do with the error, it will invoke the <code>#call</code> method from the strategy with the following arguments:</p> Argument Type Description <code>errors_tracker</code> <code>Karafka::Pro::Processing::Coordinators::ErrorsTracker</code> Tracks the history of errors for the current messages until another successful run. <code>attempt</code> Integer Indicates the current attempt of processing. <p>When implementing a custom DLQ strategy in Karafka, the <code>#call</code> method is expected to return specific symbols indicating the next action for a message: <code>:retry</code>, <code>:dispatch</code>, or <code>:skip</code>. Each symbol represents a distinct pathway for handling messages that have encountered processing issues, guiding the system on whether to attempt reprocessing, move the message to a dead-letter queue, or bypass further attempts.</p> Return Result Explanation <code>:retry</code> The processing should be retried. This is typically used when the error is considered transient and successful processing is possible in a subsequent attempt. <code>:dispatch</code> The message should be moved to the DLQ. This is used when the message cannot be processed successfully after several attempts or when specific error conditions are met. <code>:skip</code> The message should be skipped and not retried or dispatched to the DLQ. This is typically used for non-recoverable errors where retrying or dispatching is not appropriate."}, {"location": "Pro-Enhanced-Dead-Letter-Queue/#dynamic-dlq-target-topic", "title": "Dynamic DLQ Target Topic", "text": "<p>Karafka Pro also supports the dynamic determination of the DLQ target topic. This feature is useful when the target DLQ topic may vary depending on runtime conditions or message metadata.</p> <p>To enable dynamic DLQ target topics, set the <code>topic:</code> option to <code>:strategy</code> in your routing configuration. Your strategy class's <code>#call</code> method should then return an array instead of a single symbol:</p> <ul> <li>The first element is the symbol representing the action (<code>:retry</code>, <code>:dispatch</code>, <code>:skip</code>).</li> <li>The second element specifies the dynamically determined target DLQ topic.</li> </ul> <pre><code>class DynamicDlqStrategy\n  def call(errors_tracker, attempt)\n    if errors_tracker.last.is_a?(SpecialError)\n      [:dispatch, 'dlq_topic_for_specials']\n    else\n      [:dispatch, 'dlq_topic_for_anything_else']\n    end\n  end\nend\n\nclass KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      dead_letter_queue(\n        topic: :strategy,\n        strategy: DynamicDlqStrategy.new\n      )\n    end\n  end\nend\n</code></pre> <p>Last modified: 2025-06-16 14:39:05</p>"}, {"location": "Pro-Enhanced-Inline-Insights/", "title": "Enhanced Inline Insights", "text": "<p>Inline Insights is a feature of Karafka that provides a way to enhance your data processing capabilities by allowing your consumers to adjust their actions based on real-time metrics.</p> <p>Not all applications and services need insights at all times. However, for critical applications, these insights can be crucial. With the <code>required</code> option, developers can ensure that their consumers only process the data when the associated insights metrics are available.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      # Ensures that `#insights` are always present during processing\n      inline_insights(required: true)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Enhanced-Inline-Insights/#how-does-it-work", "title": "How does it work?", "text": "<p>When the <code>required</code> option is activated in Karafka Pro's Inline Insights, it automatically verifies the data processing workflow. Karafka immediately checks to ascertain if the corresponding insights are available after receiving messages for a particular topic partition. If these insights are unavailable, Karafka temporarily pauses processing for that specific topic partition rather than proceeding with potentially incomplete data processing. This pause continues until the necessary <code>#insights</code> become available. With this feature in place, developers are spared the chore of manually verifying the presence of insights using the <code>#insights?</code> method, as the system intelligently and autonomously ensures that every piece of data is paired with its analytical insights before processing.</p>"}, {"location": "Pro-Enhanced-Inline-Insights/#summary", "title": "Summary", "text": "<p>In sum, the <code>required</code> option in Karafka Pro's Inline Insights is an invaluable asset for developers aiming to bolster and assure the reliability of their metrics availability.</p> <p>Last modified: 2023-10-17 13:18:13</p>"}, {"location": "Pro-Enhanced-Reliability/", "title": "Enhanced Reliability", "text": ""}, {"location": "Pro-Enhanced-Reliability/#enhanced-execution-stability", "title": "Enhanced Execution Stability", "text": "<p>Karafka Pro comes with Enhanced Execution Stability improvements that significantly reduce the risks associated with involuntary revocations, which occur when partition ownership is transferred among consumers.</p> <p>This mechanism minimizes the number of jobs in flight during such reassignments, thereby preventing potential race conditions and ensuring a stable, predictable execution environment, even during high-volume data processing and dynamic consumer scenarios.</p>"}, {"location": "Pro-Enhanced-Reliability/#enhanced-scheduler", "title": "Enhanced Scheduler", "text": "<p>Karafka Pro comes shipped with an Enhanced Scheduler.</p> <p>The default scheduler schedules work in a FIFO (First-In, First-Out) order.</p> <p>The Enhanced Scheduler uses a non-preemptive LJF (Longest Job First) algorithm.</p> <p>This scheduler is designed to optimize execution times, especially on jobs that perform IO operations. When computing the order, it considers the potential time cost of executing jobs based on the in-process p95 time and number of messages.</p> <p>For IO intense jobs, where the number of jobs exceeds the number of threads, this can provide gains up to 20%.</p>"}, {"location": "Pro-Enhanced-Reliability/#enhanced-memory-utilization", "title": "Enhanced Memory Utilization", "text": "<p>Karafka Pro provides the Cleaner API. This feature is specifically designed to optimize memory management for message payloads, especially those exceeding 10KB.</p> <p>This mechanism allows users to achieve memory savings of up to 80%, ensuring that applications run more efficiently and with reduced risk. Moreover, the memory usage patterns become substantially more stable with the Cleaner API, offering robust protection against unexpected out-of-memory exceptions.</p> <p>For a deeper dive into the nuances and technicalities of this feature, please refer to the dedicated Cleaner API documentation.</p>"}, {"location": "Pro-Enhanced-Reliability/#enhanced-supervision-in-swarm-mode", "title": "Enhanced Supervision in Swarm Mode", "text": "<p>Karafka Pro enhances the stability and performance of distributed systems with its Enhanced Supervision for Swarm Nodes. This feature targets the detection and graceful management of hanging or bloated worker nodes within the swarm. By monitoring and identifying nodes that are either unresponsive or consuming excessive memory, Karafka Pro ensures that such nodes are shut down and restarted gracefully. This process not only preserves the integrity of ongoing tasks but also optimizes system resources by preventing memory leaks and ensuring efficient allocation of processing power.</p> <p>For more detailed information on how Enhanced Supervision for Swarm Nodes works and how to implement it in your Karafka Pro setup, please refer to the Enhanced Swarm documentation.</p> <p>Last modified: 2024-02-17 19:16:48</p>"}, {"location": "Pro-Enhanced-Swarm-Multi-Process/", "title": "Enhanced Swarm / Multi-Process Mode", "text": "<p>Karafka's Enhanced Swarm / Multi-Process mode introduces Pro enhancements that extend the capabilities of the standard Swarm Mode, offering advanced features for greater control, efficiency, and reliability in processing Kafka messages. These enhancements cater to enterprise-level needs, where complex and high-volume message processing requires sophisticated management strategies.</p> <p>This documentation only covers extra functionalities enhancing the Swarm feature.</p> <p>Please refer to the Swarm documentation for more details on its core principles.</p>"}, {"location": "Pro-Enhanced-Swarm-Multi-Process/#enhanced-liveness-listener", "title": "Enhanced Liveness Listener", "text": "<p>The Pro Liveness Listener is a significant enhancement in Karafka Pro, designed to ensure the highest system health and efficiency level. This feature goes beyond traditional liveness checks by allowing developers to specify the maximum memory allowed for each node and the criteria for processing and polling liveness. The supervisor will gracefully restart the misbehaving swarm node if a node exceeds memory limits or fails to meet processing or polling criteria.</p> <p>This listener provides following benefits:</p> <ul> <li> <p>System Stability: Memory leaks or prolonged processing times can lead to system instability or degradation. The Pro Liveness Listener proactively addresses these issues, ensuring nodes operate within defined parameters.</p> </li> <li> <p>Efficient Resource Utilization: By monitoring and restarting nodes that exceed memory usage or fail to process or poll efficiently, the system conserves resources and maintains optimal performance.</p> </li> <li> <p>Fault Tolerance: The ability to automatically identify and restart problematic nodes minimizes the impact of individual node failures on the overall system, enhancing the fault tolerance of the swarm.</p> </li> </ul> <p>This enhancement is crucial for maintaining a high-performance Kafka processing environment, especially in scenarios with stringent resource constraints or high throughput requirements.</p> <p>To use the Enhanced Liveness Listener in your Karafka application, you need to subscribe to the listener within your <code>karafka.rb</code> configuration file:</p>"}, {"location": "Pro-Enhanced-Swarm-Multi-Process/#usage", "title": "Usage", "text": "<pre><code># Put this at the end of karafka.rb\nKarafka.monitor.subscribe(\n  Karafka::Pro::Swarm::LivenessListener.new(\n    memory_limit: 2048, # Memory limit in MB (e.g., 10GB)\n    consuming_ttl: 5 * 60 * 1_000, # 5 minutes in ms\n    polling_ttl: 5 * 60 * 1_000 # 5 minutes in ms\n  )\n)\n</code></pre> <p>Once the listener subscribes, it will actively report any abnormalities to the supervisor.</p>"}, {"location": "Pro-Enhanced-Swarm-Multi-Process/#configuration-parameters", "title": "Configuration Parameters", "text": "<p>The Enhanced Liveness Listener accepts several parameters to customize its behavior. Here\u2019s a table outlining the arguments, their expected types, default values, and descriptions:</p> Argument Expected Type Default Description <code>memory_limit</code> Integer nil Max memory in MB for a process to be considered healthy. Set to <code>nil</code> to disable monitoring. <code>consuming_ttl</code> Integer Matches <code>max.poll.interval.ms</code> Time in ms to consider consumption hanging. Defines the max consumption time after which the supervisor should consider a process as hanging. <code>polling_ttl</code> Integer Matches <code>max.poll.interval.ms</code> Max time in ms for polling. If polling does not happen often enough, the process will be considered dead."}, {"location": "Pro-Enhanced-Swarm-Multi-Process/#failure-statuses", "title": "Failure Statuses", "text": "<p>The listener reports to the supervisor the following failure statuses for monitored conditions:</p> Status Code Description 1 Node reported insufficient polling from Kafka (Pro only). 2 Consumer is consuming a batch longer than expected (Pro only). 3 Node exceeded the allocated memory limit (Pro only)."}, {"location": "Pro-Enhanced-Swarm-Multi-Process/#node-assignments", "title": "Node Assignments", "text": "<p>The Node Assignments feature in Karafka's Enhanced Swarm / Multi-Process Mode addresses the need for more granular control over topic processing across different nodes within the swarm. By default, Karafka Swarm assigns all topics to all nodes uniformly. This means each node attempts to connect to and subscribe to the same set of topics. This approach ensures that the processing load is distributed across all available nodes, providing a balanced workload under typical conditions. However, this can lead to inefficiencies in specific scenarios.</p> <p>Granular control over node assignments becomes crucial when topics have varying loads, message volumes, or numbers of partitions. </p> <p>Allocating specific topics to specific nodes allows for more efficient resource utilization and can significantly enhance performance by:</p> <ul> <li> <p>Aligning Resource Allocation: Directing high-volume topics to nodes with more processing power or assigning them exclusively can prevent bottlenecks and ensure smoother processing across the swarm.</p> </li> <li> <p>Optimizing for Partitions: Topics with different numbers of partitions may benefit from being processed by a specific subset of nodes, enabling more effective load balancing and reducing cross-node communication overhead.</p> </li> <li> <p>Improving Performance: Tailoring node assignments can help optimize the processing time by ensuring that nodes are not overwhelmed by attempting to subscribe and process messages from topics that are too resource-intensive for their capacity.</p> </li> </ul> <p>In Karafka, configuring node assignments is straightforward within the routing setup, utilizing the <code>#swarm</code> method to direct topic subscriptions to specified nodes. Nodes are indexed starting at <code>0</code>, allowing for individual or ranges of nodes to be targeted. Without explicit assignments, topics default to being accessible by all nodes. Below is a configuration example to demonstrate node assignment usage:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n    # Run 8 processes\n    config.swarm.nodes = 8\n  end\n\n  routes.draw do\n    consumer_group :group_name do\n      topic :example do\n        swarm(nodes: [0, 1, 2])\n        consumer ExampleConsumer\n      end\n\n      topic :example2 do\n        swarm(nodes: 4..7)\n        consumer ExampleConsumer2\n      end\n    end\n\n    consumer_group :group_name2 do\n      topic :example3 do\n        consumer Example2Consumer3\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Enhanced-Swarm-Multi-Process/#direct-assignments", "title": "Direct Assignments", "text": "<p>Direct Assignments allow you to specify which nodes should handle which topics and partitions.</p> <p>Configuring Direct Assignments involves specifying the partitions and the nodes that should handle them within the Karafka routing setup. Here's how you can define direct assignments for your topics:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n    # Run 8 processes\n    config.swarm.nodes = 8\n  end\n\n  routes.draw do\n    topic 'financial_data' do\n      consumer FinancialDataConsumer\n      # Directly assign partitions 0, 1, and 2\n      assign(0, 1, 2)\n      # Specify that only nodes 0 and 1 should handle these partitions\n      # with the 0 node receiving partition 0 and node 1 receiving\n      # partitions 1 and 2\n      swarm(nodes: { 0 =&gt; [0], 1 =&gt; [1, 2] })\n    end\n\n    topic 'user_activity' do\n      consumer UserActivityConsumer\n      assign [0, 1]  # Directly assign partitions 0 and 1\n      swarm(nodes: [2])  # Assign these partitions to node 2\n    end\n  end\nend\n</code></pre> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Enterprise-Workshop-Session/", "title": "Enterprise Workshop Session", "text": "<p>Dive deep into the world of event-driven applications and supercharge your development skills with our exclusive Kafka and Karafka workshop. As part of our Enterprise offering, this live, remote, five to six hour-long event delivers a hands-on and immersive learning experience for up to 6 participants per session.</p>"}, {"location": "Pro-Enterprise-Workshop-Session/#why-invest-in-this-workshop", "title": "Why Invest in This Workshop?", "text": "<p>By investing a few hours, you're setting yourself up to save weeks of collective team efforts later. Grasp the intricate nuances of Karafka and sidestep the common pitfalls and potential bugs that could be harmful later. The complexities of Karafka and event-driven architecture demand a precise, methodical introduction \u2014 and our workshop is tailor-made to deliver just that.</p>"}, {"location": "Pro-Enterprise-Workshop-Session/#what-we-offer", "title": "What We Offer", "text": "<ul> <li> <p>Holistic Introduction: Begin with an overview of the session, establishing a foundational understanding of the learning path ahead.</p> </li> <li> <p>Deep Dive into Event-Driven Architecture &amp; Kafka: Understand the potential of event-driven architecture, its terminologies, and its invaluable benefits. Also, discover the powerful capabilities of Apache Kafka, exploring its architecture, message handling, use cases, and the industry's rapid adoption rate.</p> </li> <li> <p>In-Depth Exploration with Karafka: Transition from Kafka to Karafka. Learn the core functionalities of the Karafka framework and how it facilitates building top-tier event-driven Ruby applications. This segment equips you with hands-on knowledge, from Karafka setup to mastering its user interface.</p> </li> <li> <p>Crafting with Karafka: Theoretical knowledge bears fruit only when put into practice. Experience a live coding session, where you'll forge a rudimentary Karafka application with consistent guidance and support from our seasoned instructor.</p> </li> <li> <p>Mastering Advanced Karafka Concepts: No stone goes unturned. Delve into the intricate features of Karafka, including virtual partitions, retries, batch processing, and more.</p> </li> <li> <p>Wrap-Up with Key Insights: As we conclude, we ensure that you're equipped with a comprehensive understanding of the journey traversed. We'll share additional resources and guide you on the subsequent steps, empowering you to navigate the Karafka landscape confidently.</p> </li> </ul>"}, {"location": "Pro-Enterprise-Workshop-Session/#schedule", "title": "Schedule", "text": "<ol> <li>Introduction (15 minutes)<ul> <li>Welcome and introduction to other attendees</li> <li>Brief overview of the workshop's goals and structure</li> </ul> </li> <li>Event-Driven Architecture and Kafka Overview (60 minutes)<ul> <li>Overview of event-driven architecture and its benefits</li> <li>Explanation of event-driven patterns and terminology</li> <li>Introduction to Apache Kafka and its key features</li> <li>Explanation of Kafka's architecture and how it handles messages</li> <li>Discussion of Kafka use cases and industry adoption</li> </ul> </li> <li>Break (10 minutes)</li> <li>Karafka Introduction and Setup (45 minutes)<ul> <li>Introduction to Karafka and its features</li> <li>Explanation of how Karafka can be used to build event-driven Ruby applications</li> <li>Walkthrough of Karafka installation and setup</li> <li>Karafka Web UI installation</li> </ul> </li> <li>Building Your First Karafka Application (60 minutes)<ul> <li>Hands-on coding session where attendees build a basic Karafka application</li> <li>Guidance and support from instructor as attendees work through the exercise</li> </ul> </li> <li>Break (10 minutes)</li> <li>Advanced Karafka Concepts (45 minutes)<ul> <li>Explanation of advanced Karafka features such as virtual partitions, retries, batch processing and others</li> <li>Discussion of best practices and common pitfalls when using Karafka</li> <li>Q&amp;A session with instructor to answer any remaining questions</li> </ul> </li> <li>Conclusion and Next Steps (15 minutes)<ul> <li>Recap of the workshop's key takeaways and learnings</li> <li>Discussion of resources and next steps for attendees to continue learning and using Karafka</li> <li>Closing remarks and thank you</li> </ul> </li> </ol>"}, {"location": "Pro-Enterprise-Workshop-Session/#terms-conditions", "title": "Terms &amp; Conditions", "text": "<ol> <li>Karafka Enterprise Subscription: You must have an active paid Karafka Enterprise subscription.</li> <li>Advance Notice: Submit your workshop session request at least two weeks before the desired date.</li> <li>Attendee Limit: The session can accommodate up to 6 people.</li> <li>Experience: While you don\u2019t need any prior knowledge of Kafka or Karafka, attendees should understand Ruby and Ruby on Rails to get the most out of the workshop.</li> <li>Technical Requirements:<ul> <li>Operating System: Mac/Linux.</li> <li>Software: Docker and Ruby 3.2.</li> <li>Docker Kafka setup: This should already be in place. (Instructions will be provided.)</li> </ul> </li> </ol> <p>Last modified: 2023-11-21 09:07:24</p>"}, {"location": "Pro-Enterprise/", "title": "Karafka Enterprise", "text": "<p>When deciding between Karafka Pro and Enterprise, it's vital to understand that the technical features of the Karafka ecosystem remain the same across both. The primary distinctions lie not in the technical functionality but in licensing, usage permissions, payment methods, and support for compliance documentation. For larger organizations that prioritize legal compliance and aim to minimize any legal impact of using third-party software, the Enterprise license agreement is tailored to meet such needs.</p> <p>Additionally, the offline/embedded mode available in the Enterprise version is specifically designed to reduce the supply chain footprint. This can be especially crucial for organizations subject to rigorous security checks and compliance requirements.</p>"}, {"location": "Pro-Enterprise/#key-differences-between-karafka-pro-and-enterprise", "title": "Key Differences between Karafka Pro and Enterprise", "text": ""}, {"location": "Pro-Enterprise/#workshop-session", "title": "Workshop Session", "text": "<p>Our Enterprise Subscription includes a remote 4-hour-long live workshop specifically tailored for those embarking on their journey with Kafka and Karafka. This session provides a comprehensive dive into the event-driven architecture, Kafka and Karafka. This live, interactive session dedicated to both novice and seasoned developers will provide you with the knowledge needed to quickly kick-start your projects, sidestep common pitfalls, and truly harness the full power of the Karafka framework. You can read more about this workshop here.</p>"}, {"location": "Pro-Enterprise/#extended-contingent-rights", "title": "Extended Contingent Rights", "text": "<p>With the Enterprise version, organizations have the privilege to continue using the version of Karafka available on the day their license expires or subscription terminates. This means that even if you decide not to renew or end your subscription, you can still legally use the existing version without any subsequent updates.</p>"}, {"location": "Pro-Enterprise/#offline-embedded-usage-rights", "title": "Offline / Embedded Usage Rights", "text": "<p>Enterprise users are not tied to our gem server for license gem download and verification. This grants organizations the flexibility and independence to utilize Karafka without any external dependencies.</p>"}, {"location": "Pro-Enterprise/#private-fork-usage", "title": "Private Fork Usage", "text": "<p>One of the standout features of the Enterprise version is the extended permission granted to users. You can continue using your private fork of Karafka even after your license expires or your subscription concludes. This ensures that your operations remain unaffected, granting you more control over your ecosystem.</p>"}, {"location": "Pro-Enterprise/#flexible-payment-options", "title": "Flexible Payment Options", "text": "<p>Understanding the diverse needs of large organizations, the Enterprise version offers the option of making payments via invoicing. This makes the financial transaction process more streamlined and adaptable to corporate financial workflows.</p>"}, {"location": "Pro-Enterprise/#compliance-documentation-support", "title": "Compliance Documentation Support", "text": "<p>Legal compliance, especially in today's complex regulatory environment, is paramount. With the Enterprise version, I am available to assist your organization in crafting any security or compliance documentation. This bespoke service ensures that your usage of Karafka aligns with industry standards and meets organizational requirements.</p>"}, {"location": "Pro-Enterprise/#custom-license-agreement", "title": "Custom License Agreement", "text": "<p>While Karafka Pro is sold with its set terms and conditions, Enterprise customers may request contract changes. Recognizing that enterprise businesses might have specific requirements or concerns, we offer the flexibility for Enterprise users to request changes to the terms and conditions of the license agreement. This level of customization ensures that your organization's unique needs and preferences are met.</p>"}, {"location": "Pro-Enterprise/#who-should-choose-karafka-enterprise", "title": "Who Should Choose Karafka Enterprise?", "text": "<p>Karafka Enterprise offering is designed to meet the specific needs of larger organizations and enterprises that require more than just the features offered by Karafka Pro. Below are key scenarios and organizational needs that make Karafka Enterprise the ideal choice:</p> <ul> <li>Organizations that Need Invoicing</li> </ul> <p>Enterprise offers the flexibility of invoicing for payments, which is essential for many large organizations with strict financial processes and workflows. This option simplifies the financial transactions and aligns with corporate payment practices.</p> <ul> <li>Organizations Betting Their Business on Karafka:</li> </ul> <p>For companies that plan to heavily rely on Karafka for their core business operations, Enterprise provides extended contingency rights. This ensures that even if the subscription expires, the organization can continue using the existing version of Karafka, thereby minimizing operational risks.</p> <ul> <li>Organizations with Strict Security Requirements:</li> </ul> <p>Enterprise caters to organizations with stringent security needs by offering a fully offline license mode. This feature allows Karafka to be used without any external dependencies, which is crucial for environments with rigorous security checks and compliance requirements.</p> <ul> <li>Organizations Needing Custom License Agreements:</li> </ul> <p>Unlike Karafka Pro, which comes with standard terms and conditions, Karafka Enterprise provides the flexibility to negotiate and customize license agreements. This is particularly beneficial for enterprises with specific legal or operational requirements.</p> <ul> <li>Organizations Requiring Special Documentation:</li> </ul> <p>Enterprise includes support for creating custom compliance and security documentation. This service ensures that the organization\u2019s use of Karafka meets industry standards and regulatory requirements, which is essential for legal and compliance audits.</p> <p>By choosing Karafka Enterprise, organizations can ensure that their Kafka and Karafka implementations are robust, secure, and aligned with their specific business and operational needs.</p> <p>Last modified: 2024-05-17 13:23:26</p>"}, {"location": "Pro-Expiring-Messages/", "title": "Expiring Messages", "text": "<p>Karafka's Expiring Messages feature allows messages to be excluded from processing automatically in case they are too old. This feature is helpful in scenarios where messages become irrelevant or outdated after a specific time frame.</p> <p>To use the Expiring Messages feature in Karafka, you can specify the message expiration time in your routing. Once the specified time has elapsed, the message is automatically ignored and will not reach the consumer. Karafka provides the ability to configure the default message expiration time for all messages in a topic.</p>"}, {"location": "Pro-Expiring-Messages/#how-does-it-work", "title": "How does it work", "text": "<p>Karafka's Expiring Messages filtering process takes place before the virtual partitioning (if applicable) and dispatching of messages to consumers. This helps optimize resource utilization, particularly CPU usage, as consumers receive sets of messages that are already filtered.</p> <p>By filtering messages before they are partitioned and dispatched, Karafka reduces the number of messages that need to be processed by each consumer. This approach ensures that only relevant and recent messages are dispatched to consumers, making it easier for them to process the data and reducing the overall processing load on the system. This optimization helps in improving the performance of the overall system and enables more efficient data processing.</p> <p> </p> <p> *Illustration presenting how Expiring Messages filter out too old messages.    </p>"}, {"location": "Pro-Expiring-Messages/#enabling-expiring-messages", "title": "Enabling Expiring Messages", "text": "<p>To enable the Expiring Messages feature in Karafka, you need to add the <code>expire_on</code> option to your Karafka routing configuration. Here's an example of how to do that:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders do\n      consumer OrdersConsumer\n      # Skip processing of messages that would be older than 1 hour\n      expire_in(60 * 60_000)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Expiring-Messages/#behaviour-on-errors", "title": "Behaviour on errors", "text": "<p>Karafka's Expiring Messages feature ensures that failed messages are reprocessed after a short period. However, if the failed messages become too old, Karafka will skip them. This is because the Expiring Messages feature in Karafka automatically filters out messages that are older than the defined period. Therefore, if a failed message becomes older than the expiry period, it will not be included in the batch of messages that Karafka processes again. This is a design decision made to optimize resource utilization and prevent the processing of stale or irrelevant data.</p> <p>It is important to note that this behavior can be adjusted by changing the expiry period for messages in the configuration settings. It is also essential to ensure that the message expiry period is set to a value appropriate for the use case. For example, suppose the processing time for messages is expected to be longer than the expiry period. In that case, it may be necessary to increase the expiry period to ensure that failed messages are not skipped.</p>"}, {"location": "Pro-Expiring-Messages/#limitations", "title": "Limitations", "text": "<p>When a Karafka consumer process is heavily saturated, and there are more jobs in the internal queue than threads available, processing lag is risky. This means there may be a delay between when a message is polled and when it is processed. In some cases, the delay can be long enough that messages that were polled but not yet processed can go beyond the expiration time.</p> <p>The delay occurs due to the nature of the Karafka consumer processing flow. When messages are polled, they are subject to filtering before they are dispatched for processing. This filtering process, along with the dispatching and processing itself, can take some time, mainly when a large number of messages are being processed at once. If the number of jobs in the internal queue exceeds the number of available threads, then some messages will need to wait for processing, leading to processing lag. This lag can be mitigated by increasing the number of threads available for processing, but this may not be possible in some cases, such as with resource constraints.</p> <p>If case of such scenarios, we recommend running second-stage filtering to ensure that at the moment of processing particular messages, they are not expired.</p>"}, {"location": "Pro-Expiring-Messages/#expiring-messages-vs-using-logretentionms", "title": "Expiring Messages vs. using <code>log.retention.ms</code>", "text": "<p>The Karafka Expiring Messages feature and Kafka <code>log.retention.ms</code> setting serve different purposes in managing data retention. While both provide mechanisms to \"exclude\" messages, their scope and implications differ significantly.</p> <p>Karafka's Expiring Messages feature allows you to set a maximum age for a message to be processable. If a message exceeds the time limit, it is excluded from being processed by the consumer. However, it remains in Kafka, where it can be consumed by other consumers or applications that might still find it relevant. This feature enables the processing of new data without getting stuck on old messages that might no longer be relevant to the specific consumer.</p> <p>On the other hand, Kafka's <code>log.retention.ms</code> setting allows for the complete removal of old data from Kafka. This setting specifies the maximum time a message can remain in a Kafka topic before being removed. Once the retention time has passed, Kafka deletes the messages from the topic, freeing up space for new data. This setting is useful in scenarios where the data has a limited lifetime and is no longer needed after a certain period.</p> <p>However, it's important to understand the limitations and behavior of <code>log.retention.ms</code>:</p> <ol> <li> <p>Cleanup Interval: Kafka's log retention cleanup doesn't run continuously. By default, the <code>log.cleanup.interval.ms</code> is set to 5 minutes (300000 ms). Even if you set a short retention period, the cleanup thread will only check and delete eligible messages every 5 minutes.</p> </li> <li> <p>Deletion Conditions: The actual deletion process is subject to several conditions:</p> </li> </ol> <ul> <li>Kafka won't delete segments that are currently active (being written to)</li> <li>There must be at least one segment remaining after deletion</li> <li>The segment file must be fully rolled before it can be considered for deletion</li> </ul> <p>For scenarios requiring precise message expiration control, especially with short periods, Karafka's Expiring Messages feature is more suitable as it provides immediate control over message processing. Properly tuned <code>log.retention.ms</code> settings can be used to manage topic storage for long-term data retention management.</p> <p>In summary, while both features deal with message lifecycle management, they operate at different levels:</p> <ul> <li>Karafka's Expiring Messages feature controls message processing at the consumer level</li> <li>Kafka's <code>log.retention.ms</code> manages physical storage cleanup at the broker level</li> </ul>"}, {"location": "Pro-Expiring-Messages/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Email dispatch: In email dispatch applications, the expiring consumption of Kafka messages can be used to prevent sending emails based on old events. For example, skipping messages that would dispatch emails that would no longer be relevant or useful, such as promotions or marketing campaigns that have already ended.</p> </li> <li> <p>Push notifications: In e-commerce applications, the expiring consumption of Kafka messages can be used to prevent sending push notifications based on old events. For example, skipping events that would trigger outdated push notifications to the user, such as a reminder to complete a no longer-relevant purchase.</p> </li> <li> <p>Log analysis: In log analysis applications, the expiring consumption of Kafka messages can be used to prevent the processing of old logs that are no longer useful. For example, skipping logs that are older than a certain age or logs that have already been analyzed and processed.</p> </li> </ul> <p>Last modified: 2025-02-15 12:44:43</p>"}, {"location": "Pro-FAQ/", "title": "Karafka Pro FAQ", "text": "<p>Karafka Pro is an enhanced version of the Karafka framework, adding more functionalities and providing additional customer support options.</p> <ol> <li>To become a Karafka Pro user, do I need to follow the LICENSE-COMM terms?</li> <li>Is there a trial version?</li> <li>What is the license?</li> <li>How does Pro licensing work?</li> <li>Do I require to change source of the package?</li> <li>What happens if my subscription lapses?</li> <li>Do I need to replace the license for my running processes?</li> <li>How do I buy Karafka Pro?</li> <li>Can I distribute Karafka Pro to my customers?</li> <li>Can I use Karafka Pro in my public project?</li> <li>Can you transfer a license?</li> <li>Do I have to share the credentials with all of my developers?</li> <li>Can I get a refund?</li> <li>Can I accidentally use Pro because it is in the same repository?</li> <li>Why do I see a \"Bad username or password\" message when trying to bundle install?</li> <li>Where can I find my license credentials page URL?</li> <li>Do you require any personal information, financial data, confidential/sensitive data, government data, etc.?</li> <li>How do you ensure the security of your gem server?</li> <li>What is Karafka Data Collection and GDPR Policy?</li> <li>Does Karafka Pro stores, processes or transmits Personal Health Information?</li> <li>Does Karafka Pro stores, processes or transmits Personally Identifiable Information?</li> <li>Will Karafka store, process, or transmit company confidential information and data?</li> <li>How do you store, process, or transmit Payment Cardholder Information?</li> <li>What specific PHI, PII, PCI data fields, or company confidential information and data do you collect?</li> <li>Where is the hosting infrastructure located?</li> <li>Can I use Karafka Pro with an offline license without using the Karafka gem server?</li> <li>How can I change the email associated with my subscription?</li> <li>How can I change the credit card associated with my subscription?</li> <li>Does the Pro license require me to use the \"Source URL\" in my Gemfile to fetch the license?</li> <li>Can I use Karafka Pro with a private gem server / private registry?</li> <li>Can I pay via invoice and purchase order?</li> <li>Can you fill out my security or compliance documentation?</li> <li>Can I request a change to the license terms?</li> <li>Are there any feature related differences in between Karafka Pro and Enterprise?</li> <li>What is the response and resolution time for priority support with Karafka?</li> <li>Where can I find a list of OSS components that the Karafka ecosystem uses?</li> <li>Can I use one Karafka license across multiple companies within a corporate group?</li> <li>Ethics, Privacy, and Information Usage</li> <li>Contact Info</li> </ol>"}, {"location": "Pro-FAQ/#to-become-a-karafka-pro-user-do-i-need-to-follow-the-license-comm-terms", "title": "To become a Karafka Pro user, do I need to follow the LICENSE-COMM terms?", "text": "<p>Yes, it is required to accept the LICENSE-COMM terms to become a Karafka Pro user.</p>"}, {"location": "Pro-FAQ/#is-there-a-trial-version", "title": "Is there a trial version?", "text": "<p>Yes. For free, you can obtain temporary credentials from our website.</p> <p>Those credentials will be valid for 14 days for every environment.</p> <p>The trial license does not grant you our Pro commercial support.</p>"}, {"location": "Pro-FAQ/#what-is-the-license", "title": "What is the license?", "text": "<p>See LICENSE-COMM in the root of the Karafka repo.</p>"}, {"location": "Pro-FAQ/#how-does-pro-licensing-work", "title": "How does Pro licensing work?", "text": "<p>Every organization running Karafka Pro on its servers must purchase a subscription. There's no limit to the number of servers or environments used by that organization. Your subscription will automatically renew every year.</p>"}, {"location": "Pro-FAQ/#do-i-require-to-change-source-of-the-package", "title": "Do I require to change source of the package?", "text": "<p>No. All Karafka Pro code is stored in the same package and only included and used when a valid license gem is present.</p>"}, {"location": "Pro-FAQ/#what-happens-if-my-subscription-lapses", "title": "What happens if my subscription lapses?", "text": "<p>If we cannot charge your card, we will email you and try three more times over a week. If it still fails, your subscription will be canceled.</p> <p>You'll lose access to the gem server and priority support, and Karafka Pro won't work anymore.</p>"}, {"location": "Pro-FAQ/#do-i-need-to-replace-the-license-for-my-running-processes", "title": "Do I need to replace the license for my running processes?", "text": "<p>No. The production environment that is already started will not be affected (until the next deployment).</p>"}, {"location": "Pro-FAQ/#how-do-i-buy-karafka-pro", "title": "How do I buy Karafka Pro?", "text": "<p>Follow the instructions on our website.</p>"}, {"location": "Pro-FAQ/#can-i-distribute-karafka-pro-to-my-customers", "title": "Can I distribute Karafka Pro to my customers?", "text": "<p>This is a common requirement for \"on-site installs\" or \"appliances\" sold to large corporations.</p> <p>The standard license is only appropriate for SaaS usage as it does not allow distribution. Karafka Pro has an Appliance license option which does allow you to distribute it. The Appliance license is $12,995/yr. It allows you to distribute the Pro gem as part of your application and each of your customers to run Karafka Pro as part of your application only. Email contact@karafka.io to purchase.</p>"}, {"location": "Pro-FAQ/#can-i-use-karafka-pro-in-my-public-project", "title": "Can I use Karafka Pro in my public project?", "text": "<p>By default, neither the Karafka Pro license nor the Appliance license allows for usage in publicly accessible source code, whether it's an open-source project or a commercial project with public repositories. This restriction applies to both open-source initiatives and commercial projects that maintain public codebases. This is because both licenses are designed for commercial use within defined organizational boundaries and include restrictions on redistribution and public sharing of the codebase.</p> <p>However, there are various scenarios where organizations might want to maintain public repositories while using Karafka Pro features, whether for open-source contributions or transparency in commercial projects. If you want to use Karafka Pro in your public source project, please contact us at contact@karafka.io to discuss your use case. We can explore potential custom agreements that could include:</p> <ul> <li>Limited usage rights for specific Pro features</li> <li>Special licensing terms for public source code integration</li> <li>Custom arrangements for commercial projects with public repositories</li> <li>Collaborative opportunities that benefit both the project and the Karafka community</li> </ul> <p>Each case will be evaluated individually, considering factors such as the project's scope, impact on the community, alignment with Karafka's goals, and potential business implications. We need to carefully assess each case to ensure proper license key management and mitigate possible risks of the public codebase being used to bypass Karafka Pro licensing requirements. While we're open to supporting meaningful public source initiatives, whether open-source or commercial, we must maintain appropriate controls to prevent misuse of our commercial features through public code access.</p>"}, {"location": "Pro-FAQ/#can-you-transfer-a-license", "title": "Can you transfer a license?", "text": "<p>Licenses are not transferrable to another company. It is strongly recommended that you buy the license using a group email address so the license is not attached to any one employee's email address.</p>"}, {"location": "Pro-FAQ/#do-i-have-to-share-the-credentials-with-all-of-my-developers", "title": "Do I have to share the credentials with all of my developers?", "text": "<p>In general, yes. The credentials are required to download the gems and your developers will need the gems to use the commercial features.</p>"}, {"location": "Pro-FAQ/#can-i-get-a-refund", "title": "Can I get a refund?", "text": "<p>No. We offer a 14 days trial during which you can check out Karafka Pro capabilities.</p>"}, {"location": "Pro-FAQ/#can-i-accidentally-use-pro-because-it-is-in-the-same-repository", "title": "Can I accidentally use Pro because it is in the same repository?", "text": "<p>No. The Pro code is never loaded unless a valid <code>karafka-license</code> is detected.</p>"}, {"location": "Pro-FAQ/#why-do-i-see-a-bad-username-or-password-message-when-trying-to-bundle-install", "title": "Why do I see a \"Bad username or password\" message when trying to bundle install?", "text": "<p>If you are seeing the following error when trying to <code>bundle install</code>:</p> <pre><code>Fetching source index from https://gems.karafka.io/\n\nBad username or password for https://LOGIN@gems.karafka.io/.\nPlease double-check your credentials and correct them.\n</code></pre> <ol> <li> <p>Check the account email's Spam/Junk folder for any billing or payment emails. You would need to purchase a new subscription if your subscription was canceled.</p> </li> <li> <p>Double-check your login and password to the gem server.</p> </li> <li> <p>Upgrade Bundler. Versions before 2.3.x are buggy with the gem server.</p> </li> </ol>"}, {"location": "Pro-FAQ/#where-can-i-find-my-license-credentials-page-url", "title": "Where can I find my license credentials page URL?", "text": "<p>You can find it in the email you received from us when you requested the Pro license via our gems UI.</p>"}, {"location": "Pro-FAQ/#do-you-require-any-personal-information-financial-data-confidentialsensitive-data-government-data-etc", "title": "Do you require any personal information, financial data, confidential/sensitive data, government data, etc.?", "text": "<p>No. We only collect enough customer information to fill out a standard invoice for billing purposes, and customer information is never shared or sold to anyone.</p> <p>Karafka license gem contains only the organization name.</p>"}, {"location": "Pro-FAQ/#how-do-you-ensure-the-security-of-your-gem-server", "title": "How do you ensure the security of your gem server?", "text": "<p>Please read our Security statement for details on this manner.</p>"}, {"location": "Pro-FAQ/#what-is-karafka-data-collection-and-gdpr-policy", "title": "What is Karafka Data Collection and GDPR Policy?", "text": "<p>We understand the importance of user privacy and data protection, especially in today's digital world. For this reason, we want to provide absolute clarity regarding the data practices associated with Karafka and Karafka Pro.</p> <ul> <li> <p>Data Collection: Karafka and Karafka Pro do NOT collect any data from the servers they operate on. Our primary concern is to deliver functionality without infringing on the privacy of our users.</p> </li> <li> <p>Logging: The only data Karafka collects is the IP address from which the license gem request was made during the bundling process. This is purely for logging and license validation purposes. Furthermore, the date of the last request is noted, but no other identifiable information is ever collected or stored.</p> </li> <li> <p>GDPR and Data Processing: As per the General Data Protection Regulation (GDPR), a \"data processor\" is an entity that processes personal data on behalf of a data controller. Since Karafka does not collect or process any data from its users, it does not act as a data processor, and therefore, GDPR-related concerns in that context do not apply to Karafka.</p> </li> <li> <p>Future Changes: Our commitment to data privacy is unwavering. We hereby affirm that this policy is not subject to change. Karafka will only introduce features or capabilities that collect data if explicitly requested by the community. Even then, all Pro and Enterprise users will be informed about that, and any such features will be turned off by default. We are and will remain committed to ensuring the privacy and trust of our users.</p> </li> </ul>"}, {"location": "Pro-FAQ/#does-karafka-pro-stores-processes-or-transmits-personal-health-information", "title": "Does Karafka Pro stores, processes or transmits Personal Health Information?", "text": "<p>No. Karafka and Karafka Pro do not store, process or transmit any information to me. Never.</p>"}, {"location": "Pro-FAQ/#does-karafka-pro-stores-processes-or-transmits-personally-identifiable-information", "title": "Does Karafka Pro stores, processes or transmits Personally Identifiable Information?", "text": "<p>No. Karafka and Karafka Pro do not store, process or transmit any information to me. Never.</p>"}, {"location": "Pro-FAQ/#will-karafka-store-process-or-transmit-company-confidential-information-and-data", "title": "Will Karafka store, process, or transmit company confidential information and data?", "text": "<p>No., it will not store, process, or transmit company confidential information and data.</p>"}, {"location": "Pro-FAQ/#how-do-you-store-process-or-transmit-payment-cardholder-information", "title": "How do you store, process, or transmit Payment Cardholder Information?", "text": "<p>We do not hold your credit card information. All subscription management is done via Stripe.</p>"}, {"location": "Pro-FAQ/#what-specific-phi-pii-pci-data-fields-or-company-confidential-information-and-data-do-you-collect", "title": "What specific PHI, PII, PCI data fields, or company confidential information and data do you collect?", "text": "<p>No. Karafka and Karafka Pro do not store, process or transmit any information to me. Never.</p>"}, {"location": "Pro-FAQ/#where-is-the-hosting-infrastructure-located", "title": "Where is the hosting infrastructure located?", "text": "<p>Karafka Pro gem license server is located in Hetzner, Germany, Frankfurt. Karafka and Karafka Pro are served directly from RubyGems.</p>"}, {"location": "Pro-FAQ/#can-i-use-karafka-pro-with-an-offline-license-without-using-the-karafka-gem-server", "title": "Can I use Karafka Pro with an offline license without using the Karafka gem server?", "text": "<p>We understand that some companies have strict policies regarding their open-source supply chain, and we are happy to provide a solution that meets those needs.</p> <p>Karafka can be used with an embedded/offline license without relying on our gem server. It is important to note that this mode of operation requires an Enterprise agreement.</p> <p>Upon agreement, we will provide the license gem sources with installation instructions, so you can start using Karafka Pro with ease without reliance on our third-party source.</p>"}, {"location": "Pro-FAQ/#how-can-i-change-the-email-associated-with-my-subscription", "title": "How can I change the email associated with my subscription?", "text": "<p>If you need to change the email associated with your subscription, you have two ways to proceed:</p> <ol> <li> <p>Email: You can send us a direct request by emailing us at contact@karafka.io. Ensure to include your current email address, the new one you want to use, and any relevant information about your subscription.</p> </li> <li> <p>Pro Private Slack Channel: If you're part of the Pro private Slack channel, you can also use this platform to send your request. Just write a message explaining that you want to change the email associated with your subscription. Include your current email address and the new one you want to use.</p> </li> </ol>"}, {"location": "Pro-FAQ/#how-can-i-change-the-credit-card-associated-with-my-subscription", "title": "How can I change the credit card associated with my subscription?", "text": "<p>If you need to update the credit card associated with your subscription, you have two ways to request it:</p> <ol> <li> <p>Email: You can send us a direct request by emailing us at contact@karafka.io.</p> </li> <li> <p>Pro Private Slack Channel: If you're part of the Pro private Slack channel, you can also use this platform to send your request. After receiving your request, we will generate a temporary link for you to update your payment method. This link will be generated through Stripe, our payment processor.</p> </li> </ol> <p>This temporary link will be sent directly from Stripe to the billing email associated with your subscription. This email will include instructions for updating your credit card information.</p> <p>You will receive a confirmation email once you've successfully updated your payment method.</p> <p>Please check your spam or junk folders if you don't see the email in your inbox. If you encounter any issues, feel free to contact us for further assistance.</p>"}, {"location": "Pro-FAQ/#does-the-pro-license-require-me-to-use-the-source-url-in-my-gemfile-to-fetch-the-license", "title": "Does the Pro license require me to use the \"Source URL\" in my Gemfile to fetch the license?", "text": "<p>Yes, with the Karafka Pro license, you must fetch the license from the Source URL specified in your Gemfile. This is the standard procedure for validating and activating your Pro license subscription.</p> <p>However, the Enterprise license would be more suitable if your use case requires total offline usage or involves private gem registries.</p> <p>Moreover, the Enterprise license comes with additional legal benefits. These include contingency warranties, which provide safeguards against unforeseen or unexpected events, and post-contract usage warranties, which ensure that you're covered even after your contract has ended. </p>"}, {"location": "Pro-FAQ/#can-i-use-karafka-pro-with-a-private-gem-server-private-registry", "title": "Can I use Karafka Pro with a private gem server / private registry?", "text": "<p>Yes, however, you need a specific Enterprise agreement allowing you to leverage this capability.</p> <p>Once the Enterprise agreement is made, you'll be given offline access to the license and necessary instructions on how to proceed. This type of agreement comes with additional legal benefits. These include contingency warranties, which provide safeguards against unforeseen or unexpected events, and post-contract usage warranties, which ensure that you're covered even after your contract has ended.</p>"}, {"location": "Pro-FAQ/#can-i-pay-via-invoice-and-purchase-order", "title": "Can I pay via invoice and purchase order?", "text": "<p>Karafka Pro is credit card only, no exceptions.</p> <p>Karafka Enterprise can be purchased via invoice and purchase order if needed. Email contact@karafka.io for more details.</p>"}, {"location": "Pro-FAQ/#can-you-fill-out-my-security-or-compliance-documentation", "title": "Can you fill out my security or compliance documentation?", "text": "<p>Only for Karafka Enterprise tier.</p>"}, {"location": "Pro-FAQ/#can-i-request-a-change-to-the-license-terms", "title": "Can I request a change to the license terms?", "text": "<p>Karafka Pro is sold as is, no change to terms. Karafka Enterprise customers can ask for changes to the terms and conditions. Email your concerns and we can negotiate something.</p>"}, {"location": "Pro-FAQ/#are-there-any-feature-related-differences-in-between-karafka-pro-and-enterprise", "title": "Are there any feature related differences in between Karafka Pro and Enterprise?", "text": "<p>Not in terms of technical features. The primary differences between Pro and Enterprise pertain to licensing, usage permissions, payment methods, workshop session, and support for compliance documentation. With Enterprise:</p> <ol> <li> <p>You have permission to continue to use Karafka in the version available on the day of license expiration or subscription termination without any subsequent updates.</p> </li> <li> <p>You can legally utilize Karafka without needing my gem server for license gem download and verification.</p> </li> <li> <p>You are granted extended permission to continue using your private fork of Karafka even after your license expires or your subscription ends.</p> </li> <li> <p>Payments can be made via invoicing.</p> </li> <li> <p>I am available to assist with any security or compliance documentation required for your organization's needs. </p> </li> <li> <p>You can request a 4-hour-long live workshop specifically tailored for those embarking on their journey with Kafka and Karafka</p> </li> </ol> <p>You can read more about those differences here.</p>"}, {"location": "Pro-FAQ/#what-is-the-response-and-resolution-time-for-priority-support-with-karafka", "title": "What is the response and resolution time for priority support with Karafka?", "text": "<p>With Karafka's priority support, you'll receive an initial assessment and reply within a maximum of 2 working days. While most issues are diagnosed, reproduced, and fixed within seven days of the report acknowledgment, complex cases might take up to a few months. Every case is unique and addressed individually.</p> <p>Please note that our software is provided \"as is.\" We recommend utilizing the trial period to thoroughly test it, as we cannot guarantee it will be entirely bug-free or that all issues will be resolved. That said, we always strive to deliver the best, and historically, there have been no unresolved bugs. However, given Kafka's complexity, situations can vary.</p>"}, {"location": "Pro-FAQ/#where-can-i-find-a-list-of-oss-components-that-the-karafka-ecosystem-uses", "title": "Where can I find a list of OSS components that the Karafka ecosystem uses?", "text": "<p>Karafka maintains a documentation page with its runtime Software Bill of Materials (SBOM). This page lists Open Source Software (OSS) components utilized within the Karafka ecosystem.</p>"}, {"location": "Pro-FAQ/#can-i-use-one-karafka-license-across-multiple-companies-within-a-corporate-group", "title": "Can I use one Karafka license across multiple companies within a corporate group?", "text": "<p>Both Pro and Enterprise licenses ares limited to a single legal entity that purchased it. Using the license across multiple companies, even if they are part of your group or owned by your organization, requires either:</p> <ol> <li>Each company purchasing its own license, or</li> <li>Upgrading to an Appliance license ($12,995/yr), or</li> <li>Requesting a custom group companies amendment that explicitly permits usage across affiliated companies with defined ownership threshold.</li> </ol> <p>Contact us to discuss group licensing options.</p>"}, {"location": "Pro-FAQ/#ethics-privacy-and-information-usage", "title": "Ethics, Privacy, and Information Usage", "text": "<p>We only collect enough customer information to fill out a standard invoice for billing purposes. Customer information is never shared or sold to anyone.</p> <p>The Karafka software runs on your servers. Karafka never has access to any private user data.</p>"}, {"location": "Pro-FAQ/#contact-info", "title": "Contact Info", "text": "<pre><code>Maciej Mensfeld, Karafka.io\nSikorskiego 31/12\n34-400 Nowy Targ\nPoland\n\nAll billing/support inquiries: contact@karafka.io\nNIP (VAT-ID): PL 735 261 5885\n</code></pre> <p>Last modified: 2025-02-17 20:04:31</p>"}, {"location": "Pro-FIPS-Support/", "title": "FIPS Support", "text": "<p>Recommendation: Related Regulatory Documentation</p> <p>If you're working in healthcare or handling sensitive personal information, we recommend also reviewing our HIPAA, PHI, PII Support documentation. Many organizations need to comply with both FIPS and HIPAA requirements, and the referenced document provides complementary guidance on securing protected health information and personally identifiable information within Karafka deployments.</p> <p>Karafka aligns with FIPS 140-2 cryptographic module expectations to ensure secure operations in environments that require adherence to Federal Information Processing Standards. This documentation outlines how Karafka supports FIPS requirements.</p> <p>No Warranty</p> <p>While Karafka strives to maintain compatibility with FIPS 140-2 requirements, this compatibility is not warranted or guaranteed. We do our best to ensure compliance with FIPS regulations but recommend thorough testing in your specific environment. If you encounter any issues or find areas where FIPS support could be improved, please contact us so we can enhance the codebase accordingly.</p> <p>OSS Version Limitations</p> <p>The open-source (OSS) version of Karafka lacks several advanced security features. It is not recommended for environments requiring FIPS compatibility. Karafka Pro or Enterprise editions should be used for regulatory needs.</p>"}, {"location": "Pro-FIPS-Support/#what-is-fips-140-2", "title": "What is FIPS 140-2?", "text": "<p>Federal Information Processing Standard Publication 140-2 (FIPS 140-2) is a U.S. government standard that defines security requirements for cryptographic modules. Developed by the National Institute of Standards and Technology (NIST), it is mandatory for federal agencies and contractors working with sensitive but unclassified (SBU) information.</p> <p>FIPS 140-2 specifies four security levels:</p> <ul> <li>Level 1: Basic security requirements for a cryptographic module</li> <li>Level 2: Adds requirements for physical tamper-evidence and role-based authentication</li> <li>Level 3: Adds requirements for physical tamper-resistance and identity-based authentication</li> <li>Level 4: Adds stringent physical security and environmental protection</li> </ul> <p>Karafka supports FIPS 140-2 Level 3 requirements in specific configurations, as librdkafka supports identity-based authentication through certificates, and Karafka Web UI can be configured with identity-based authentication. Additionally, Karafka's integrity verification mechanisms provide a form of tamper resistance.</p>"}, {"location": "Pro-FIPS-Support/#karafkas-fips-implementation", "title": "Karafka's FIPS Implementation", "text": "<p>Karafka achieves FIPS compatibility through:</p> <ul> <li>Use of FIPS-validated cryptographic modules: Karafka leverages librdkafka, which can be configured to use OpenSSL in FIPS mode for all - cryptographic operations</li> <li>Secure communications: All network traffic can be encrypted using TLS/SSL with FIPS-approved algorithms</li> <li>In-flight encryption: Data transmitted between Karafka clients and Kafka brokers uses FIPS-compatible encryption algorithms</li> <li>At-rest encryption: Data stored by Karafka can be encrypted using FIPS-approved algorithms (SHA-256 for hashing, AES for encryption)</li> <li>Identity-based authentication: Through certificate-based authentication and Karafka Web UI's customizable authentication systems</li> <li>Integrity verification: Provides tamper resistance by validating message integrity through fingerprinting</li> </ul>"}, {"location": "Pro-FIPS-Support/#cryptographic-standards-used", "title": "Cryptographic Standards Used", "text": "<p>Karafka exclusively employs FIPS-approved cryptographic algorithms:</p> <ul> <li>Hash Functions: SHA-256 only (MD5 is explicitly disabled)</li> <li>Symmetric Encryption: AES-128, AES-192, and AES-256</li> <li>Asymmetric Encryption: RSA with key sizes \u2265 2048 bits</li> <li>Random Number Generation: Uses Ruby's OpenSSL FIPS-compatible random number generators when Ruby is running in FIPS mode</li> </ul> <p>These algorithms are implemented through FIPS-validated cryptographic modules, ensuring all cryptographic operations meet federal standards.</p>"}, {"location": "Pro-FIPS-Support/#messages-at-rest-encryption", "title": "Messages At Rest Encryption", "text": "<p>Karafka Pro provides transparent encryption of message payloads, ensuring sensitive data at rest in Kafka cannot be accessed by unauthorized parties. This is crucial for meeting FIPS requirements. You can read more about the at-rest encryption here.</p>"}, {"location": "Pro-FIPS-Support/#custom-headers-deserializer-and-encryption", "title": "Custom Headers Deserializer and Encryption", "text": "<p>When using Karafka's encryption features, be aware that encryption may not work as expected if you use a custom headers deserializer. Custom deserialization of headers can alter how encryption headers are processed, potentially leading to issues in correctly encrypting or decrypting messages. In cases where custom headers deserialization is necessary, it is recommended to consult with Karafka Pro support for guidance.</p>"}, {"location": "Pro-FIPS-Support/#message-fingerprinting-for-integrity", "title": "Message Fingerprinting for Integrity", "text": "<p>Karafka includes a fingerprinting feature that provides tamper resistance for messages to prevent the processing of incorrectly decrypted messages.</p> <p>Use SHA-256 for Fingerprinting</p> <p>For FIPS compatibility, use SHA-256 as your fingerprinter algorithm instead of MD5, which is not FIPS-approved.</p>"}, {"location": "Pro-FIPS-Support/#supply-chain-security", "title": "Supply Chain Security", "text": "<p>All dependencies have been reviewed for FIPS compatibility regarding cryptographic hashing algorithms to ensure they don't use non-approved methods like MD5. For a complete listing of all dependencies and their security status, please refer to our Software Bill of Materials (SBOM) document.</p> <p>For more comprehensive information about Karafka's security posture, including our approach to dependency management, vulnerability handling, and secure coding practices, please consult our Security Guidelines documentation.</p>"}, {"location": "Pro-FIPS-Support/#librdkafka-fips-support", "title": "librdkafka FIPS Support", "text": "<p>Karafka relies on librdkafka for Kafka communication. For FIPS compatibility, librdkafka must be compiled with the following considerations:</p> <ul> <li>Uses OpenSSL with FIPS mode enabled</li> <li>No MD5 hash functions in the build process</li> <li>Proper verification of FIPS-compatible algorithms during runtime</li> </ul>"}, {"location": "Pro-FIPS-Support/#limitations-and-unsupported-features", "title": "Limitations and Unsupported Features", "text": "<p>When operating in FIPS mode, the following limitations apply:</p> <ul> <li>Legacy Authentication Methods: Only SASL/SCRAM and SSL certificate authentication are supported</li> <li>Custom Compression Codecs: Some compression algorithms may not be FIPS-compatible</li> <li>Third-party Extensions: Plugins not specifically designed for FIPS environments may not function correctly</li> <li>Custom Headers Deserializer: When using encryption features, custom headers deserializers may interfere with proper encryption/decryption</li> </ul>"}, {"location": "Pro-FIPS-Support/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li>Government Agencies: Agencies handling sensitive but unclassified information need to ensure all data processing systems meet FIPS requirements.</li> <li>Healthcare Systems: Medical institutions processing patient data that must adhere to both HIPAA and FIPS (for government contracts) can use Karafka to ensure data security.</li> <li>Financial Services: Banks and financial institutions working with government entities need FIPS-compatible processing for financial transactions.</li> <li>Defense Contractors: Companies working with the Department of Defense can use Karafka's FIPS capabilities to process sensitive information securely.</li> <li>Critical Infrastructure: Systems supporting utilities and critical infrastructure that need to meet federal security standards.</li> </ul>"}, {"location": "Pro-FIPS-Support/#summary", "title": "Summary", "text": "<p>Karafka Pro and Enterprise editions provide comprehensive support for FIPS 140-2 requirements through their implementation of validated cryptographic modules, secure communication channels, and robust at-rest encryption. </p> <p>While we strive to maintain FIPS compatibility, we encourage users to contact us with any issues or suggestions for improvement to help us continue enhancing Karafka's security features.</p> <p>Last modified: 2025-02-25 14:41:17</p>"}, {"location": "Pro-Features-Compatibility/", "title": "Karafka Pro Features Compatibility", "text": "<p>Karafka provides several features that can work together. Unless explicitly stated otherwise, Karafka Pro features should work with each other without any limitations</p>"}, {"location": "Pro-Features-Compatibility/#long-running-jobs-virtual-partitions", "title": "Long Running Jobs + Virtual Partitions", "text": "<p>Long-Running Jobs work together with Virtual Partitions. All the Virtual Partitions consumers will respond to <code>#revoked?</code> if the partition is lost, similar to regular consumers.</p>"}, {"location": "Pro-Features-Compatibility/#enhanced-active-job-virtual-partitions", "title": "Enhanced Active Job + Virtual Partitions", "text": "<p>Virtual Partitions can be used with Active Job without any limitations. The only thing worth keeping in mind is that the message payload for Active Job contains serialized job details and should not be deserialized in the partitioner.</p> <p>The recommended approach is to use the Enhanced Active Job headers support to add a key that can be used for partitioning:</p> <pre><code>class Job &lt; ActiveJob::Base\n  queue_as :jobs\n\n  karafka_options(\n    dispatch_method: :produce_async,\n    partitioner: -&gt;(job) { job.arguments.first[0] }\n  )\nend\n\nclass KarafkaApp &lt; Karafka::App\n  routes.draw do\n    active_job_topic :jobs do\n      virtual_partitions(\n        partitioner: -&gt;(job) { job.key }\n      )\n    end\n  end\nend\n</code></pre> <p>Please keep in mind that with Virtual Partitions, the offset will be committed after all the Virtual Partitions work is done. There is no \"per job\" marking as processed.</p>"}, {"location": "Pro-Features-Compatibility/#enhanced-dead-letter-queue-virtual-partitions", "title": "Enhanced Dead Letter Queue + Virtual Partitions", "text": "<p>Virtual Partitions can be used together with the Dead Letter Queue. This can be done due to Virtual Partitions' ability to collapse upon errors.</p> <p>The only limitation when combining Virtual Partitions with the Dead Letter Queue is the minimum number of retries. It needs to be set to at least <code>1</code>:</p> <pre><code>routes.draw do\n  topic :orders_states do\n    consumer OrdersStatesConsumer\n    virtual_partitions(\n      partitioner: -&gt;(message) { message.headers['order_id'] }\n    )\n    dead_letter_queue(\n      topic: 'dead_messages',\n      # Minimum one retry because VPs needs to switch to the collapsed mode\n      max_retries: 1\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Features-Compatibility/#virtual-partitions-transactions", "title": "Virtual Partitions + Transactions", "text": "<p>Due to the Virtual Partitions' nature, message production transactions work entirely as expected. However, transactions involving offset storage operate in a simulated mode. This means that even if <code>#mark_as_consumed</code> is used within a transaction, it doesn't become part of the transaction itself. Instead, it's committed right after the transaction successfully ends. This creates an edge case: there could be inconsistencies if a consumer is killed or loses its assignment right after the Kafka transaction completes but before the consumer offset is sent to Kafka.</p> <p>This behavior aligns with the principles of the underlying Virtual Offset Management system. This system is crafted to handle offsets in a way distinct from Kafka's native offset handling due to the underlying parallelization process. As a result, certain operations, like <code>#mark_as_consumed</code>, are executed outside the main transaction scope, which is a direct consequence of the design and functionality of the Virtual Offset Management.</p> <p>The transactional behavior aligns with standard expectations in a collapsed virtual partition flow scenario. The associated offset is included in the transaction when a message is marked as consumed.</p> <pre><code>class VirtualPartitionedEventsConsumer &lt; ApplicationConsumer\n  def consume\n    transaction do\n      produce topic: totals, payload: messages.payloads.sum(&amp;:count).to_s\n\n      # if this topic uses virtual partitions this will NOT be part of the transaction and will be\n      # executed right after the transaction has ended.\n      #\n      # In case `#collapsed?` would be true, this will behave like a regular transaction\n      mark_as_consumed messages.last\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Features-Compatibility/#virtual-partitions-offset-metadata-storage", "title": "Virtual Partitions + Offset Metadata Storage", "text": "<p>In Karafka's Virtual Partitions, the offset_metadata_strategy setting, configurable during routing, dictates whether the system should use the most recent (<code>:current</code>) or the exact (<code>:exact</code>) metadata associated with a materialized offset, a choice crucial for aligning processing logic with data consistency requirements. For detailed usage and configuration, refer to this Karafka documentation section.</p>"}, {"location": "Pro-Features-Compatibility/#routing-patterns-dead-letter-queue", "title": "Routing Patterns + Dead Letter Queue", "text": "<p>While Karafka's Routing Patterns feature integrates seamlessly with the Dead Letter Queue (DLQ) mechanism, developers are advised to exercise caution. Specifically, there's a potential issue where an imprecisely crafted regular expression could inadvertently match both primary and DLQ topics. This misconfiguration might result in messages from the DLQ being consumed in an unexpected loop, especially if the DLQ topic isn't explicitly targeted for consumption.</p> <p>You can read more about this issue here.</p>"}, {"location": "Pro-Features-Compatibility/#messages-at-rest-encryption-custom-headers-deserializer-and-encryption", "title": "Messages At Rest Encryption + Custom Headers Deserializer and Encryption", "text": "<p>When using Karafka's encryption features, it's important to note that encryption may not work as expected if you use a custom headers deserializer. Custom deserialization of headers can alter how encryption headers are processed, potentially leading to issues in correctly encrypting or decrypting messages. In cases where custom headers deserialization is necessary, it is recommended to consult with Karafka Pro support for guidance to ensure that encryption functionalities are properly integrated and maintained within your application.</p>"}, {"location": "Pro-Features-Compatibility/#adaptive-iterator-long-running-jobs", "title": "Adaptive Iterator + Long-Running Jobs", "text": "<p>Adaptive Iterator should not be used with Long-Running Jobs because their operating principles are fundamentally incompatible. Long-Running Jobs have their own resource lifecycle management, while Adaptive Iterator dynamically adjusts batch sizes based on processing speed. This combination leads to conflicting resource allocation, disrupted timeout mechanisms, and inconsistent offset commitment patterns.</p> <p>Last modified: 2025-04-30 10:04:57</p>"}, {"location": "Pro-Features-List/", "title": "Karafka Pro Features List", "text": "<p>Karafka Pro is a commercial version of the open-source Karafka framework for building Ruby Kafka-based applications.</p> <p>Below you can find the list of the Pro features with their brief description:</p> <ul> <li> <p>Enhanced Web UI - The Enhanced Web UI offers additional features and capabilities that are not available in the free version, making it a better option for those looking for more robust monitoring and management capabilities for their Karafka applications.</p> </li> <li> <p>Transactions - Transactions ensure that a series of produce and consume operations are either all successfully executed or none are, maintaining data integrity even in the face of system failures or crashes. It allows for coupling, consuming, and producing messages to multiple topics together, ensuring that either all succeed or none.</p> </li> <li> <p>Offset Metadata Storage - Offset Metadata Storage allows attaching custom metadata to message offsets during commit to Kafka. This feature enriches message processing by providing additional data annotations and enhancing system capabilities, traceability, and intelligence through retrievable metadata.</p> </li> <li> <p>Virtual Partitions - Virtual Partitions allow you to parallelize data processing from a single partition. This can drastically increase throughput when IO operations are involved.</p> </li> <li> <p>Parallel Segments - Parallel Segments enable processing data from a single topic partition across multiple independent processes simultaneously. This approach allows for horizontal scaling by filtering and distributing messages at the consumer group level, making it particularly effective when data clustering renders Virtual Partitions ineffective.</p> </li> <li> <p>Delayed Topics - Delayed Topics feature allows for arbitrary delay in processing messages from specified topics without impacting the processing of other topics.</p> </li> <li> <p>Long-Running Jobs - Long-Running Jobs are jobs that run continuously and handle messages from a Kafka topic over an extended time beyond <code>max.poll.interval.ms</code>. These jobs are designed to handle tasks requiring longer execution times, such as data processing, transformation, and analysis.</p> </li> <li> <p>Non-Blocking Jobs - Non-Blocking Jobs optimize performance by allowing continuous data polling from multiple partitions without blocking processing, ensuring efficient and timely data handling.</p> </li> <li> <p>Adaptive Iterator - TBA</p> </li> <li> <p>Periodic Jobs - Periodic Jobs enable consumers to perform operations at regular intervals, even without new data. This feature allows for advanced window-based operations and ensures consumers remain active and ready, facilitating consistent processing capabilities regardless of data inflow.</p> </li> <li> <p>Expiring Messages - Karafka's Expiring Messages feature allows messages to be excluded from processing automatically in case they are too old.</p> </li> <li> <p>Routing Patterns - Karafka's Routing Patterns feature allows users to define routes using regular expressions. When a Kafka topic matches the specified pattern, Karafka automatically initiates consumption, streamlining the handling of dynamically created topics without manual configuration.</p> </li> <li> <p>Rate Limiting - Rate limiting allows you to control the pace at which messages are consumed.</p> </li> <li> <p>Filtering API - The Filtering API allows users to filter messages based on specific criteria, reducing the amount of data that needs to be processed downstream. It also provides advanced ways of altering the consumption flow by allowing for explicit pausing and seeking before the actual processing happens.</p> </li> <li> <p>Scheduling API - Scheduling API was designed to control when specific consumption jobs are placed on the processing queue. This API is not just a simple timer-based mechanism but a sophisticated controller that allows for precise and intelligent scheduling of tasks.</p> </li> <li> <p>Iterator - Iterator API allows you to quickly subscribe to selected topics and partitions and perform data lookups without having to start a <code>karafka server</code> and create consumers.</p> </li> <li> <p>Granular Backoffs - Granular Backoffs provide heightened control over backing off, pausing, and retrying processing upon errors. This feature offers per-topic customization of the error-handling strategy.</p> </li> <li> <p>Cleaner API - The Cleaner API efficiently releases messages payloads from memory after processing, optimizing memory management during batch operations.</p> </li> <li> <p>Multiplexing - This feature allows a single process to establish multiple independent connections to the same Kafka topic, enhancing parallel processing and throughput. This capability enables more efficient data handling and improved performance in consuming messages.</p> </li> <li> <p>Piping - Feature allowing applications to forward processing results seamlessly to subsequent stages or other applications.</p> </li> <li> <p>Recurring Tasks - Supports defining tasks that run at specific times on a regular basis, similar to cron jobs. It is ideal for automating periodic operations directly within Karafka, ensuring consistent and reliable execution.</p> </li> <li> <p>Scheduled Messages - Allows the scheduling of Kafka messages for future processing, allowing users to control precise timing for message delivery.</p> </li> <li> <p>Messages At Rest Encryption - Karafka Pro supports transparent encryption of the message's payload, so sensitive data at rest in Kafka cannot be seen.</p> </li> <li> <p>Enhanced Swarm / Multi Process - Provides extra capabilities for managing worker nodes, ensuring resilience by automatically detecting and handling hanging or memory-intensive nodes, and maintaining system efficiency and stability.</p> </li> <li> <p>Enhanced Dead Letter Queue - Enhanced Dead Letter Queue feature provides additional functionalities and warranties to the regular Dead Letter Queue feature. It aims to complement it with other dispatch warranties and additional messages metadata information.</p> </li> <li> <p>Enhanced Active Job - Enhanced Active Job adapter provides extra capabilities to regular Active Job to elevate the combination of Active Job and Kafka.</p> </li> <li> <p>Enhanced Reliability - Enhanced Reliability provides improvements to achieve better performance and stability, especially on jobs that perform IO operations.</p> </li> <li> <p>Commercial Friendly License - Besides its useful functionalities, buying Karafka Pro grants your organization a Karafka commercial license instead of the GNU LGPL, avoiding any legal issues your lawyers might raise. Please see the Pro FAQ for further licensing details, including options for distributing Karafka Pro with your products.</p> </li> </ul> <p>Last modified: 2025-07-08 17:55:56</p>"}, {"location": "Pro-Filtering-API/", "title": "Filtering API", "text": "<p>The Filtering API allows users to filter messages based on specific criteria, reducing the amount of data that needs to be processed downstream. It also provides advanced ways of altering the consumption flow by allowing for explicit pausing and seeking before the actual processing happens.</p> <p>With the Karafka Pro Filtering API, users can set up rules to filter messages based on the message content, key, headers, other metadata, or even external information. For example, users can filter messages based on a specific value in the message body or based on the message header.</p> <p>This feature is handy in scenarios where a high volume of messages is being sent and received, and processing all the downstream messages may be too resource-intensive or unnecessary. By filtering messages at the source, users can ensure that only the relevant data is being processed, which can improve system performance and reduce costs. It also helps with scenarios requiring custom logic and context-aware pausing or reprocessing where pausing and seeking need to be used.</p>"}, {"location": "Pro-Filtering-API/#creating-filters", "title": "Creating Filters", "text": "<p>Karafka filters need to inherit from the <code>Karafka::Pro::Processing::Filters::Base</code> and need to at least respond to two methods:</p> <ul> <li><code>#apply!</code> - a method that accepts an array of messages from a single topic partition for filtering. This array needs to be mutated using methods like <code>#delete_if</code>.</li> <li><code>#applied?</code> - did the filter limit the input messages array in any way? This should be <code>true</code> also in the case of no-altering but when post-execution action altering is required.</li> </ul> <p>If you plan to implement action-altering filters, you need to define two additional methods:</p> <ul> <li><code>action</code> - that needs to respond with <code>:skip</code>, <code>:pause</code> or <code>:seek</code> to inform Karafka what action to take after the batch processing.</li> <li><code>timeout</code> - <code>nil</code> in case of non-pause actions or pause time in milliseconds.</li> </ul> <p>Timeout Should Always Return <code>nil</code> for Non-Pause Actions</p> <p>When implementing filters with action-altering capabilities, ensure that the <code>#timeout</code> method always returns <code>nil</code> for non-<code>:pause</code> actions. </p> <p>Do not set <code>0</code> as a default value for <code>#timeout</code>, as this may interfere with the behavior of other filters when multiple filters are in use. Returning <code>nil</code> explicitly prevents unintended interactions between filters and ensures the correct application of pause logic in scenarios where multiple filters contribute to action selection.</p> <p>Always validate your filters to confirm that <code>#timeout</code> behaves as expected to avoid unexpected delays or conflicts.</p> <p>It is essential to remember that post-processing actions may also be applied when no data is left after filtering.</p> <p>Below is an example implementation of a filter that continuously removes messages with odd offsets. This filter sets the <code>@applied</code> in case even one message has been removed. </p> <pre><code>class OddRemoval &lt; Karafka::Pro::Processing::Filters::Base\n  def apply!(messages)\n    @applied = false\n\n    messages.delete_if do |message|\n      remove = !(message.offset % 2).zero?\n      @applied = true if remove\n      remove\n    end\n  end\nend\n</code></pre> <p>If you are looking for more extensive examples, you can check out the implementations of:</p> <ul> <li><code>Karafka::Pro::Processing::Filters::Delayer</code> - used as a part of the Delayed Jobs feature.</li> <li><code>Karafka::Pro::Processing::Filters::Expirer</code> - used as a part of the Expiring Messages feature</li> <li><code>Karafka::Pro::Processing::Filters::Throttler</code> - used as a part of the Rate Limiting feature.</li> </ul>"}, {"location": "Pro-Filtering-API/#filters-lifecycle", "title": "Filters Lifecycle", "text": "<p>Filter instance is created when Karafka encounters a given topic partition for the first time and is long-lived. While their primary responsibility is to filter the incoming data, they can also alter the flow behavior. Hence it is essential to remember that part of their operations happens after all the data is being processed at the moment of post-execution strategy application. This means that there may be a significant delay between the filtering and the invocation of <code>#action</code> that is equal to the collective processing time of all the data of a given topic partition.</p> <p> </p> <p> *Lifecycle of filters, illustrating their post-processing usage for action altering.    </p>"}, {"location": "Pro-Filtering-API/#post-execution-action-altering", "title": "Post Execution Action Altering", "text": "<p>Thoughtfully test your filters before usage</p> <p>Using <code>#seek</code> and <code>#pause</code> within a Filter requires a clear understanding of their implications. Misuse can result in unexpected behavior and performance issues.</p> <p>For full details and best practices, refer to the pausing and seeking documentation. Ensure you're informed before integrating these operations.</p> <p>By default, filters applied to messages do not alter the execution or polling behavior of Karafka. This means that even if a message is filtered out, Karafka will continue to poll for messages at the same rate. However, it is possible to alter this behavior by overwriting the <code>#action</code> method in a custom consumer. This method is responsible for executing the logic of a given message. By overwriting it, developers can modify the behavior of their Karafka application based on the result of the filtering. For example, they might choose to pause processing or resume from a particular message.</p> <p>Each action consists of three elements that need to be present in case there is expectation on non-default post-execution action:</p> <ul> <li><code>action</code> - defines how Karafka should behave after the data is processed or upon idle job execution. Karafka can either:<ul> <li><code>:skip</code> - in which case the default strategy action will be applied, like the filters would not exist.</li> <li><code>:pause</code> - will pause processing for the <code>timeout</code> period.</li> <li><code>:seek</code> - will move the offset to the desired location or time taken from the <code>:cursor</code> message or set manually.</li> </ul> </li> <li><code>timeout</code> - value applicable for the <code>:pause</code> action that describes how long we should pause the consumption on a given topic partition.</li> <li><code>cursor</code> - The first message we need to get next time we poll or nil if not applicable.</li> </ul> <p>For example, in case you want to pause the processing, you need to return the following:</p> <ul> <li><code>:pause</code> as an <code>#action</code> result.</li> <li>number of milliseconds to pause under <code>#timeout</code>.</li> <li>message containing the desired offset from which to start processing after un-pausing under <code>#cursor</code>.</li> </ul> <p>User actions always take precedence over Filtering API automatic actions. This means that even if you issue a <code>:pause</code> action request, in case of a user manual pause, it will be applied and not the filter one. Same applies to the <code>:seek</code> logic.</p>"}, {"location": "Pro-Filtering-API/#priority-based-action-selection", "title": "Priority Based Action Selection", "text": "<p>To make the most of the Filtering API, it is crucial to have a deep understanding of how Karafka selects actions and the factors that determine their priority. While this may be a challenging aspect of the API to master, it is essential to build robust and efficient filters that can alter polling behaviors.</p> <p>Since each of the filters can impact the behavior of given topic partition polling, we need to ensure that they collectively do not collide with each other. This is done by applying the algorithm described below that selects proper action parameters.</p> <p>Here are the rules that the action selection follows:</p> <ol> <li>If none of the filters were applied, the action is always <code>:skip</code>.</li> <li>If any filter action is <code>:pause</code>, collectively, <code>:pause</code> will be applied.</li> <li>If any filter action is <code>:seek</code>, collectively, <code>:seek</code> will be applied.</li> <li>If no filters define action other than <code>:skip</code>, <code>:skip</code> will be applied.</li> <li>For <code>:pause</code>, minimum timeout out of the recommended will be selected.</li> <li>The message with the lowest offset always represents the <code>cursor</code> value.</li> </ol> <p>This algorithm ensures that all the expectations and constraints from any of the filters are always applicable collectively.</p>"}, {"location": "Pro-Filtering-API/#idle-runs", "title": "Idle Runs", "text": "<p>After applying filters to the messages batch, no data may be left to process. In such cases, Karafka may run an idle job to apply proper action and perform housekeeping work. The idle job will initialize the consumer instance and may invoke <code>#pause</code> or <code>seek</code> commands if needed.</p> <p>Idle jobs do not run any end-user code except strategy applications based on the Filtering API.</p>"}, {"location": "Pro-Filtering-API/#filters-based-marking", "title": "Filters Based Marking", "text": "<p>This feature in the Filtering API enhances the ability to manage offsets directly from within filters. This allows for fine-grained control over the consumption flow, which is especially useful in high-volume data scenarios.</p> <p>This feature enables marking messages as consumed based on filter logic, ensuring that specific messages, even if filtered out, are acknowledged and their offsets committed. This helps in scenarios where:</p> <ul> <li>You want to reduce lag reporting inaccuracies.</li> <li>Filtered messages should still be considered processed for offset management purposes.</li> <li>You aim to avoid processing large amounts of irrelevant data repeatedly.</li> </ul> <p>To implement filtering-based marking as consumed, filters must override additional methods beyond the basic <code>#apply!</code> and <code>#applied?</code>. These methods include:</p> <ul> <li><code>#mark_as_consumed?</code>: Indicates whether the filter requests marking offsets as consumed (<code>false</code> by default).</li> <li><code>#marking_method</code>: Specifies the marking method, either <code>:mark_as_consumed</code> or <code>:mark_as_consumed!</code> (<code>:mark_as_consumed</code> by default).</li> </ul> <p>When <code>#mark_as_consumed?</code> returns <code>true</code>, Karafka will mark the offset of the message returned by <code>#cursor</code> as consumed, even if the message is filtered out. This prevents the lag from growing unnecessarily and ensures that the Kafka consumer group offset is updated correctly.</p> <p>Below is an example filter that marks messages as consumed when all of them were filtered out:</p> <pre><code>class MarkingFilter &lt; Karafka::Pro::Processing::Filters::Base\n  def apply!(messages)\n    @applied = false\n    @cursor = nil\n    @all_filtered = false\n\n    messages.delete_if do |message|\n      next false if message.headers['source'] != 'internal'\n\n      @applied = true\n      @cursor = message\n\n      true\n    end\n\n    @all_filtered = messages.empty?\n  end\n\n  def applied?\n    @applied\n  end\n\n  def mark_as_consumed?\n    @all_filtered\n  end\nend\n</code></pre> <p>Marking Based on Applied Filters</p> <p>Only filters that have been applied will mark offsets as consumed when requested. Filters that were not applied will not be considered for offset marking.</p> <p>This ensures that only the relevant filters' logic affects the offset management, maintaining the intended processing flow and accuracy.</p>"}, {"location": "Pro-Filtering-API/#registering-filters", "title": "Registering Filters", "text": "<p>When registering filters using the Karafka routing API, a factory must be provided instead of a class or filter instance. This is because a new filter instance is built for each topic partition. The factory is responsible for creating a new filter object for each partition, ensuring each partition has its independent instance. By using a factory, Karafka Pro can ensure that each filter instance is thread-safe and can handle messages concurrently without interfering with each other. Therefore, when registering filters using Karafka routing API, it is essential to provide a factory that creates new instances of the filter for each topic partition to ensure proper handling of incoming messages.</p> <p>The factory is expected to respond to <code>#call</code> and will be provided with two arguments:</p> <ul> <li><code>Karafka::Routing::Topic</code> - routing topic</li> <li><code>Integer</code> - partition</li> </ul> <p>Those arguments can be used to implement long-living filters that will not be regenerated after rebalances or to use the topic/partition context to alter the behavior of generic filters.</p> <p>Below you can find code snippets of registering filters for various scenarios.</p> <p>Example of a topic/partition agnostic filter registration:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :example do\n      consumer ExampleConsumer\n      # \n      filter -&gt;(*) { MyCustomFilter.new }\n    end\n  end\nend\n</code></pre> <p>Example of registration of a filter that accepts topic and partition as arguments:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :example do\n      consumer ExampleConsumer\n      # \n      filter -&gt;(topic, partition) { MyCustomFilter.new(topic, partition) }\n    end\n  end\nend\n</code></pre> <p>Example of a persistent factory that uses topic and partition information to provide long-living filter instances:</p> <pre><code>class Factory\n  include Singleton\n\n  MUTEX = Mutex.new\n\n  def initialize\n    @cache = Hash.new { |h, k| h[k] = {} }\n  end\n\n  def call(topic, partition)\n    MUTEX.synchronize do\n      @cache[topic][partition] ||= MyCustomFilter.new\n    end\n  end\nend\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :example do\n      consumer ExampleConsumer\n      # Use a factory object instead of a proc\n      filter Factory.instance\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Filtering-API/#ordering", "title": "Ordering", "text": "<p>Filtering API allows you to register multiple filters for a consumer group or topic. When multiple filters are registered, they are executed in the order in which they were registered. This means that the filter registered later will receive data already pre-filtered by the previously registered one.</p> <p>For example, let's say you have two filters registered for a topic:</p> <pre><code>routes.draw do\n  topic :example do\n    consumer ExampleConsumer\n    filter -&gt;(*) { FilterOne }\n    filter -&gt;(*) { FilterTwo }\n  end\nend\n</code></pre> <p>In this case, <code>FilterOne</code> will be executed first, and its output will be passed as input to <code>FilterTwo</code>. The output of <code>FilterTwo</code> will then be used in the further steps of work distribution.</p> <p>This order dependency allows you to chain multiple filters together to create more complex filtering logic. For example, you could have a first filter that removes all messages with invalid headers, followed by a second filter that removes messages that don't match a specific pattern in the payload.</p> <p>Remember that the order in which you register filters can affect the performance of your Karafka Pro application. If you have many filters registered, each filter adds overhead to message processing. Therefore, design your filter logic carefully and only register the filters you need to achieve the desired functionality.</p>"}, {"location": "Pro-Filtering-API/#best-practices", "title": "Best Practices", "text": "<ul> <li> <p>Keep Filters Simple: Filters should be kept as simple as possible to ensure they execute quickly and without error. A filter should only perform one specific task, such as filtering messages with invalid headers or messages that match a specific pattern. Keeping filters simple also makes them easier to maintain and update as your application evolves.</p> </li> <li> <p>Use Factories to Create Filter Instances: When registering filters using Karafka routing API, use factories to create filter instances instead of class or filter instances. This ensures that each partition has its independent filter instance, making them thread-safe and able to handle messages concurrently without interfering with each other.</p> </li> <li> <p>Register Filters in the Correct Order: When registering multiple filters, register them in the correct order. Filters are executed in the order in which they are registered, and each filter adds overhead to message processing. Therefore, it's crucial to design your filter logic carefully and only register the filters you need to achieve the desired functionality. It would be best if you also considered their performance to filter out the biggest number of messages using the fastest one.</p> </li> <li> <p>Monitor Filter Performance: Monitoring filter performance is essential to ensure that your Karafka Pro application processes messages efficiently. Keep an eye on the performance metrics for your filters, and keep in mind that slow filters will add additional lag to the consumption process.</p> </li> <li> <p>Test Filters Thoroughly: Before deploying your Karafka Pro application, test your filters thoroughly to ensure they are working as expected. Use sample messages to test each filter individually and in combination with other filters. This ensures that your filters are working as expected and not causing unintended side effects.</p> </li> </ul> <p>By following these best practices, you can ensure that your Karafka Pro Filtering API implementation is robust, efficient, and scalable.</p>"}, {"location": "Pro-Filtering-API/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Throttling: You can use filters to throttle and rate limit messages, including process or system-wide limitation solutions. </p> </li> <li> <p>Data Reliability: You can use the Filtering API to build a transactional offset management system, improving the reliability of your processing pipelines.</p> </li> <li> <p>Data Validation: You can use filters to validate the data of incoming messages. For example, you can check whether a message contains all required fields and has valid values and reject invalid.</p> </li> <li> <p>Data Deduplication: You can use filters to prevent duplicate messages from a single batch from being processed. For example, you can remove messages with the same identifiers or timestamps.</p> </li> <li> <p>Data Redaction: Filters can be used to remove messages containing sensitive data.</p> </li> <li> <p>Data Sampling: Filters can be used to sample incoming messages. For example, you can randomly select a subset of messages for further processing or analysis.</p> </li> <li> <p>Data Quality: Filters can be used to measure the quality of incoming messages. For example, you can check the completeness or accuracy of messages and log or discard those that don't meet a certain threshold.</p> </li> <li> <p>Data Filtering: Finally, filters can be used to filter out unwanted messages. For example, you can discard messages that contain spam or malware or messages that don't match specific criteria.</p> </li> </ul>"}, {"location": "Pro-Filtering-API/#summary", "title": "Summary", "text": "<p>Karafka Filtering API is a powerful tool that allows developers to process incoming messages in real time and perform various actions based on their content. With the Filtering API, users can register multiple filters to validate, filter out messages as they arrive, and alter the polling process by pausing or starting from a different offset. The order of the filters is essential, as each filter receives the data that the previous filters have already processed. Following best practices, such as using lightweight filters, avoiding complex logic, and testing filters thoroughly, can ensure that the system remains performant and reliable.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Getting-Started/", "title": "Getting Started with Karafka Pro", "text": ""}, {"location": "Pro-Getting-Started/#configuration", "title": "Configuration", "text": "<p>To activate Karafka Pro, you need to do three things:</p> <ol> <li> <p>Follow the standard Karafka installation procedure.</p> </li> <li> <p>Obtain credentials to a registry hosting a custom <code>karafka-license</code> gem. This gem contains all the code for Karafka to detect the Pro components. You can get them here.</p> </li> <li> <p>Add this to your Gemfile and <code>bundle install</code>:</p> </li> </ol> <pre><code>source 'https://USERNAME:PASSWORD@gems.karafka.io' do\n  gem 'karafka-license', 'LICENSE_ID'\nend\n\ngem 'karafka'\n# other gems...\n</code></pre> <ol> <li>(Optionally) Enable Bundler native checksum verification by running: <code>bundle lock --add-checksums</code> or enabling it for all your projects via <code>bundle config lockfile_checksums true</code>.</li> </ol> <p>Crucial Gemfile Tip for Karafka Pro Users</p> <p>You still need to have the standard <code>gem 'karafka'</code> definition in your <code>Gemfile</code>. License gem is just providing the license. </p> <p>Enterprise License Required for Offline or Private Registry Setup</p> <p>An Enterprise license is required if you are looking into hosting the license key offline within the app or via a private registry. This mode of operation allows you to use Karafka Pro without relying on our gem server. An Enterprise Agreement will grant you access to the license gem sources and installation instructions, ensuring you can proceed smoothly with your preferred setup.</p>"}, {"location": "Pro-Getting-Started/#license-gem-integrity-verification", "title": "License Gem Integrity Verification", "text": "<p>Checksum Verification in Enterprise Mode Not Needed</p> <p>When using Karafka with an offline Enterprise license, license gem checksum verification is not required. The Enterprise license is fully offline and not subject to change. It is also not being fetched from the Karafka gem server, thus making the checksum verification pointless.</p> <p> Checksum Not Available on Web UI</p> <p>Due to security reasons, license checksum is not available through the license Web UI. It is only sent once via email.</p>"}, {"location": "Pro-Getting-Started/#with-bundler-26", "title": "With Bundler 2.6+", "text": "<p>From Bundler 2.6, gem checksum verification is natively supported and built directly into the <code>Gemfile.lock</code> file. This eliminates the need for external scripts or manual verification processes, as Bundler will automatically ensure the integrity of all gems during installation as long as checksum verification is active. By enabling checksum verification, you enhance the security of the <code>karafka-license</code> gem and protect all other dependencies in your project.</p> <p>To enable checksum verification with Bundler 2.6 or newer, follow these steps:</p> <ol> <li>Activate checksum management in your lockfile:</li> </ol> <p>Run the following command to add a <code>CHECKSUMS</code> section to your <code>Gemfile.lock</code>:</p> <pre><code>bundle lock --add-checksums\n</code></pre> <ol> <li>(First-time setup only) Verify the karafka-license gem checksum:</li> </ol> <p>After running <code>bundle lock --add-checksums</code>, locate the checksum of the karafka-license gem in the Gemfile.lock file and ensure it matches the checksum provided in the email you received. This ensures that the correct gem version is being used and matches the one issued with your license.</p> <p>Use the following command to extract the checksum of the karafka-license gem from the Gemfile.lock:</p> <pre><code>grep -A 1 \"karafka-license\" Gemfile.lock | grep sha256\n</code></pre> <ol> <li>(Optionally) Enable checksum verification globally:</li> </ol> <p>If you want all new lockfiles to include checksum verification by default, run:</p> <pre><code>bundle config lockfile_checksums true\n</code></pre> <p>This will ensure that every new project or lockfile you create will have checksums included.</p>"}, {"location": "Pro-Getting-Started/#protecting-all-gems-not-just-karafka-license", "title": "Protecting All Gems, Not Just <code>karafka-license</code>", "text": "<p>One of the biggest advantages of Bundler 2.6+ is that checksum verification applies to all gems listed in your <code>Gemfile.lock</code>, not just <code>karafka-license</code>. This means your entire dependency chain is protected from tampering, enhancing the overall security of your application.</p> <p>When Bundler downloads a gem (or installs it from a local cache), it verifies that the checksum matches the value stored in the <code>Gemfile.lock</code>. If there is a mismatch, Bundler halts the installation process and raises an error, ensuring no compromised or altered gems are installed.</p>"}, {"location": "Pro-Getting-Started/#with-bundler-25-or-older-deprecated", "title": "With Bundler 2.5 or Older (Deprecated)", "text": "<p>Upgrade to Bundler 2.6+ for Hassle-Free Checksum Protection</p> <p>We highly recommend upgrading to Bundler 2.6 or higher rather than implementing the manual checksum verification flow. Bundler 2.6 introduces native support for gem checksum verification directly in the <code>Gemfile.lock</code>, making manual verification unnecessary.</p> <p>This feature protects all your dependencies from tampering by verifying that the <code>.gem</code> file's checksum matches the one recorded in the lockfile before installation. Consider this upgrade as a vital step to enhance the integrity of your Open Source Supply Chain.</p> <p>Using Karafka Pro means we are part of your Open Source Supply Chain. We take this exceptionally seriously, and that is why we encourage you to verify the integrity of the license gem we provide in your CI/CD.</p> <p>You can do it with the following script:</p> <pre><code>#!/usr/bin/env bash\n\nset -e\n\nKARAFKA_PRO_USERNAME='PROVIDE-USERNAME'\nKARAFKA_PRO_PASSWORD='PROVIDE-PASSWORD'\nKARAFKA_PRO_LICENSE_ID='PROVIDE-LICENSE-ID'\nKARAFKA_PRO_LICENSE_CHECKSUM='PROVIDE-CHECKSUM'\n\nif [ \"$MODE\" != \"after\" ]; then\n  # Check the remote license prior to bundle installing\n  curl \\\n    --fail \\\n    --retry 5 \\\n    --retry-delay 1 \\\n    -u $KARAFKA_PRO_USERNAME:$KARAFKA_PRO_PASSWORD \\\n    https://gems.karafka.io/gems/karafka-license-$KARAFKA_PRO_LICENSE_ID.gem \\\n    -o ./karafka-license.gem\nelse\n  # Check the local cached one after bundle install\n  cache_path=`ruby -e 'puts \"#{Gem.dir}/cache/\"'`\n  cp \"$cache_path/karafka-license-$KARAFKA_PRO_LICENSE_ID.gem\" ./karafka-license.gem\nfi\n\ndetected=`sha256sum ./karafka-license.gem | awk '{ print $1 }'`\n\nrm ./karafka-license.gem\n\necho -n \"Karafka Pro license artifact checksum verification result: \"\n\nif [ \"$detected\" = \"$KARAFKA_PRO_LICENSE_CHECKSUM\" ]; then\n  echo \"Success\"\nelse\n  echo -e \"\\033[0;31mFailure!\\033[0m\"\n  exit 1\nfi\n</code></pre> <p>Due to the nature of how Bundler works, it is recommended to run this script twice in the CI/CD:</p> <ol> <li>First, before <code>bundle install</code> is executed, to ensure that the gem server is serving the correct data.</li> <li>Second time after <code>bundle install</code> to ensure consistency of the fetched package.</li> </ol> <p>To use it:</p> <ol> <li> <p>Store above script in your repository preferably under <code>bin/verify_karafka_license_checksum</code>.</p> </li> <li> <p>Set the <code>KARAFKA_PRO_USERNAME</code>, <code>KARAFKA_PRO_PASSWORD</code>, <code>KARAFKA_PRO_LICENSE_ID</code> and <code>KARAFKA_PRO_LICENSE_CHECKSUM</code> based on data provided to you in the license issuing email or set those values as your CI/CD ENV variables.</p> </li> <li> <p>Run <code>MODE=before bin/verify_karafka_license_checksum</code> as part of your CI/CD before running <code>bundle install</code>.</p> </li> <li> <p>Run <code>bundle install</code></p> </li> <li> <p>Run <code>MODE=after bin/verify_karafka_license_checksum</code> to ensure that the stored artefact was not compromised.</p> </li> </ol> <p>In case the verification fails, script will exit with the exit code <code>1</code>.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Granular-Backoffs/", "title": "Granular Backoffs", "text": "<p>Granular Backoffs is a feature in Karafka that provides a heightened level of control over the error handling backoffs. It offers per-topic customization of pause, backoff, and retry time settings. This functionality allows for more personalized management of how your application backs off after errors, as you can define specific conditions for each topic.</p> <p>When a consumer faces an issue while processing a message from a topic (e.g., a temporary network outage, an intermittently unavailable resource, or a processing error), it pauses for a specified duration before trying to process the message again - a behavior known as \"backoff\".</p> <p>The Granular Backoffs feature enables you to customize backoff settings for each topic. That is, for different topics, you can outline distinct backoff policies. Having such granular control over these settings allows you to optimize message processing based on the unique characteristics of the specific topics, such as its size, update frequency, and relevance to your application.</p>"}, {"location": "Pro-Granular-Backoffs/#usage", "title": "Usage", "text": "<p>Karafka, by default, includes three configuration-level settings for computing pause time:</p> <ol> <li> <p><code>pause_timeout</code>: This setting determines the waiting period after a processing error. The wait time is expressed in milliseconds, and the default is set to 1000 milliseconds (or 1 second).</p> </li> <li> <p><code>pause_max_timeout</code>: This is the maximum time to wait in an exponential backoff scenario. The wait time is in milliseconds; by default, it's set to 30,000 milliseconds (or 30 seconds).</p> </li> <li> <p><code>pause_with_exponential_backoff</code>: This Boolean setting determines whether or not the system should use exponential backoff. The default setting is true, meaning that the system will utilize an exponential backoff approach by default.</p> </li> </ol> <p>However, these default settings aren't set in stone. You can override them on a per-topic basis using the routing <code>#pause</code> method. This method accepts the following keyword arguments:</p> <ol> <li> <p><code>timeout</code>: this argument allows you to set the pause time after a processing error.</p> </li> <li> <p><code>max_timeout</code>: this lets you set the maximum time for the system to wait in case of an exponential backoff.</p> </li> <li> <p><code>with_exponential_backoff</code>: when set to false, this argument will tell the system not to use an exponential backoff approach.</p> </li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # This topic will use the setup defaults\n    topic :example do\n      consumer ExampleConsumer\n    end\n\n    # This topic has a custom pause times\n    topic :externals do\n      consumer ExternalsConsumer\n      pause(\n        # Wait for at least 5 seconds after an error\n        timeout: 5_000,\n        # Wait for at most 1 minute after error\n        max_timeout: 60_000,\n        # Backoff exponentially on consecutive attempts\n        with_exponential_backoff: true\n      )\n    end\n\n    # This topic is expected to recover really fast\n    topic :events do\n      consumer EventsConsumer\n      pause(\n        # 100 ms at most and no exponential\n        timeout: 100,\n        with_exponential_backoff: false\n      )\n    end\n  end\nend\n</code></pre> <p>If the <code>#pause</code> method is called without any of these keyword arguments, the system will fall back on using the default settings for those parameters. This means you have a great deal of flexibility and can adjust the system's behavior on a topic-by-topic basis, depending on your specific needs and use cases.</p>"}, {"location": "Pro-Granular-Backoffs/#usefulness", "title": "Usefulness", "text": "<p>Granular Backoffs can be valuable in numerous situations, including:</p> <ol> <li> <p>High-Importance Topics: For a topic of high relevance to your application (e.g., containing updates that need immediate processing), you should assign a shorter backoff time and a higher retry count. This ensures any processing issues related to this topic are addressed promptly.</p> </li> <li> <p>Low-Importance Topics: In contrast, for a topic of low relevance to your application, you should assign a longer backoff time and a lower retry count. This can help alleviate system load as less processing power is expended in resolving issues with this topic.</p> </li> <li> <p>High-Frequency Updates: For a topic that updates frequently, you should assign a shorter pause time and a higher retry count. This ensures your consumer can keep pace with the update rate.</p> </li> <li> <p>Resource-Intensive Topics: For a topic that requires substantial resources to process (e.g., containing messages that necessitate complex computations), you should assign a longer backoff time. This ensures your consumer has adequate time to free up resources before retrying to process a message from this topic.</p> </li> <li> <p>Topics Making External HTTP Calls: Topics involving external HTTP calls could benefit from a longer backoff time. This ensures external systems, especially those encountering temporary issues, have enough recovery time before the subsequent request.</p> </li> </ol>"}, {"location": "Pro-Granular-Backoffs/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>E-commerce Applications: An e-commerce application may have a high-priority topic for order processing and a low-priority topic for recommendation updates. Using granular backoffs, the application can prioritize order processing and avoid overwhelming the system with recommendation updates.</p> </li> <li> <p>Real-time Analytics: In a real-time analytics application, a topic with high-frequency updates, like user clickstreams, may require a different pause and retry strategy compared to a topic with batched daily updates, such as database backups.</p> </li> <li> <p>Financial Applications: In financial applications, topics with real-time trading information may require shorter backoff times and higher retry counts to maintain market competitiveness. Conversely, topics dealing with less time-sensitive information, such as user account updates, can have longer backoff times.</p> </li> <li> <p>IoT Applications: IoT applications may have multiple topics receiving data from various types of sensors. Depending on the sensor's importance, data frequency, and processing requirements, different backoff, pause, and retry settings could be beneficial.</p> </li> <li> <p>Distributed Systems: In distributed systems with a high level of microservice communication, some services might be more critical than others. Granular backoffs allow adjusting the pause and retry parameters based on each service's importance and load.</p> </li> <li> <p>Systems Making External HTTP Calls: Systems that interact with external services via HTTP calls can use longer backoff times for these topics. This gives the external system adequate recovery time in case of issues, improving the overall success rate of requests.</p> </li> </ul>"}, {"location": "Pro-Granular-Backoffs/#summary", "title": "Summary", "text": "<p>Granular Backoffs is a powerful feature, allowing for the per-topic customization of pause, backoff, and retry time settings. This ensures a flexible and tailored approach to handling and processing messages based on their respective topic characteristics.</p> <p>Last modified: 2023-12-12 12:18:06</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/", "title": "HIPAA, PHI, PII Support", "text": "<p>OSS Version Limitations for Regulated Environments</p> <p>This document references several Pro features not available in the open-source (OSS) version of Karafka. Due to the lack of these advanced security and compliance features in the OSS version, its use is not recommended in environments governed by HIPAA, PHI, or PII regulations.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#overview", "title": "Overview", "text": "<p>Organizations that handle sensitive information, such as Protected Health Information (PHI) and Personally Identifiable Information (PII), must comply with stringent regulatory frameworks like the Health Insurance Portability and Accountability Act (HIPAA). These regulations require rigorous data security, privacy practices, and access controls to protect sensitive information from unauthorized access and misuse. Ensuring compliance in software environments that handle PHI and PII involves safeguarding data and managing how software systems interact with processes and present that data.</p> <p>Karafka and Karafka Web UI can be adapted for use in HIPAA, PHI, and PII-compliant environments. This document outlines how Karafka and Karafka Web UI can meet the stringent requirements of such regulatory frameworks and what features make them particularly suited for these high-compliance environments.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#regulatory-context-hipaa-phi-and-pii", "title": "Regulatory Context: HIPAA, PHI, and PII", "text": ""}, {"location": "Pro-HIPAA-PHI-PII-Support/#understanding-hipaa-compliance", "title": "Understanding HIPAA Compliance", "text": "<p>HIPAA requires organizations handling PHI to implement administrative, physical, and technical safeguards to ensure sensitive information's confidentiality, integrity, and availability. These safeguards include controlling access to data, encrypting sensitive information both in transit and at rest, auditing access, and providing data masking or anonymization mechanisms.</p> <p>While HIPAA does not prescribe specific software configurations, it demands that all systems and processes that handle PHI meet particular privacy and security standards. This means any tool used in a HIPAA-compliant environment must support robust data encryption, fine-grained access controls, audit logging, and data minimization.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#phi-and-pii-management", "title": "PHI and PII Management", "text": "<p>PHI is a specific subset of PII, which encompasses a broader range of sensitive personal information, including identifiers such as names, addresses, social security numbers, and health records. Managing this data securely requires strict controls on who can access, view, or process such information. Auditing data access, anonymizing information, and limiting exposure to sensitive fields is essential for compliance with healthcare-specific (HIPAA) and general privacy regulations (e.g., GDPR, CCPA).</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#karafka-and-hipaaphipii-compliance-core-capabilities", "title": "Karafka and HIPAA/PHI/PII Compliance: Core Capabilities", "text": "<p>Karafka and Karafka Web UI come equipped with a set of features designed to facilitate compliance with the privacy and security requirements of HIPAA, PHI, and PII regulations. These capabilities ensure that sensitive information is adequately protected at every stage of its lifecycle, from ingestion and processing to presentation and logging.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#1-data-encryption", "title": "1. Data Encryption", "text": ""}, {"location": "Pro-HIPAA-PHI-PII-Support/#at-rest-data-encryption", "title": "At-Rest Data Encryption", "text": "<p>Karafka supports data encryption at rest, ensuring that any stored information, including sensitive data, is protected against unauthorized access. This aligns with HIPAA's requirement for securing PHI, as encryption is one of the key safeguards for preventing data breaches. When data is written to storage by Karafka, it is encrypted using strong encryption algorithms (e.g., AES-256), providing a vital layer of security in environments where data privacy is paramount.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other config options...\n\n    config.encryption.active = true\n    config.encryption.version = '1'\n    config.encryption.public_key = ENV['PUBLIC_PEM_KEY']\n    config.encryption.private_keys = { '1' =&gt; ENV['PRIVATE_PEM_KEY'] }\n  end\nend\n</code></pre>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#in-transit-encryption", "title": "In-Transit Encryption", "text": "<p>Data in transit must be protected as it moves between components in a distributed system. Karafka supports secure communication using Transport Layer Security (TLS/SSL), which encrypts data while it is transmitted between clients, brokers, and the Web UI. This protection is essential to meet HIPAA's technical safeguard requirements, ensuring that PHI or PII is not exposed to unauthorized interception during network communication.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#2-access-control-and-authentication", "title": "2. Access Control and Authentication", "text": "<p>Karafka Web UI does not include a built-in access control or authentication system. Instead, it provides abstract code APIs that allow developers to implement custom access control mechanisms tailored to their specific requirements.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#role-based-access-control-rbac", "title": "Role-Based Access Control (RBAC)", "text": "<p>Karafka Web UI features a policies engine that enables administrators to enforce granular control over user actions, supporting a robust Role-Based Access Control (RBAC) system. This RBAC implementation allows for precise management of what specific users can view and interact with within the Web UI, thereby adhering to the principle of least privilege.</p> <pre><code>class MyCustomRequestsPolicy\n  # @param env [Hash] rack env object that we can use to get request details\n  # @return [Boolean] should this request be allowed or not\n  def allow?(env)\n    # Example logic: Allow access only if the user is an admin\n    user = env['rack.session'][:user]\n    user &amp;&amp; user.admin?\n  end\nend\n</code></pre> <p>You can learn more about the Polices here.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#integration-with-authentication-providers", "title": "Integration with Authentication Providers", "text": "<p>Since Karafka Web UI is fundamentally a Rack application, it can be mounted directly into Rails routes. By leveraging this capability, you can integrate Karafka Web UI with Rails' existing authentication mechanisms, whether they are standard username/password systems, Single Sign-On (SSO) providers, or multi-factor authentication (2FA).</p> <p>Additionally, the provided code APIs enable you to define custom access policies, ensuring that only authorized users can interact with specific components of the Web UI.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#3-granular-data-presentation", "title": "3. Granular Data Presentation", "text": "<p>One of the core privacy principles under HIPAA and other privacy regulations is to minimize the exposure of sensitive information. Karafka Web UI enables administrators to define which data fields are displayed and how they are presented. To protect their content, sensitive fields, such as patient identifiers or personal data, can be masked or redacted. This granular control over data presentation prevents unauthorized viewing of sensitive information and aligns with the requirements for data minimization.</p> <p>You can learn more about the Partial Payload Sanitization here.</p> <p> </p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#4-logging-and-auditing", "title": "4. Logging and Auditing", "text": "<p>Since Karafka Web UI is built as a Rack application, it can leverage Rack's middleware capabilities to log every action taken within the interface accurately. Additionally, using its Request Policies, Karafka Web UI can implement granular logging for each request, capturing detailed information about system interactions. These interactions include:</p> <ul> <li>Access to specific pages and data views</li> <li>Administrative changes, such as configuration updates or modifications to access controls</li> <li>Error occurrences and exceptions within the application</li> </ul> <p>By logging these activities, Karafka Web UI provides comprehensive audit trails crucial for compliance with HIPAA's auditing requirements.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#5-offline-operation-with-karafka-enterprise", "title": "5. Offline Operation with Karafka Enterprise", "text": "<p>Full control over software components and data processing is imperative for organizations operating in environments with the strictest data governance requirements, such as healthcare systems subject to HIPAA. In such cases, Karafka Enterprise is the recommended solution.</p> <p>Karafka Enterprise operates fully offline, independently of the public Karafka gem server. This means that all software components can be sourced, managed, and deployed within the organization's private infrastructure without relying on external servers. This offline operation reduces the risk of unauthorized data exposure or dependency on third-party services, enhancing overall security.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#private-registries-and-dependency-management", "title": "Private Registries and Dependency Management", "text": "<p>Although HIPAA, PHI, and PII regulations do not explicitly require the use of private registries, controlling software sources is a best practice to minimize risks related to supply chain vulnerabilities. With Karafka Enterprise, you can manage your software dependencies using private registries, ensuring only verified and trusted components are used in their environments. This control helps maintain compliance by preventing the introduction of unvetted software that could compromise sensitive information.</p>"}, {"location": "Pro-HIPAA-PHI-PII-Support/#summary", "title": "Summary", "text": "<p>Karafka and Karafka Web UI offer a secure and adaptable platform that can be configured to meet the stringent requirements of environments governed by HIPAA, PHI, and PII regulations. Key features such as at-rest and in-transit data encryption, role-based access control, granular data presentation, comprehensive logging, and configurable data access policies make Karafka a robust solution for managing sensitive data streams in compliance with privacy laws.</p> <p>Last modified: 2024-09-18 12:34:47</p>"}, {"location": "Pro-Iterator-API/", "title": "Iterator API", "text": "<p>Iterator API allows developers to subscribe to Kafka topics and perform data lookups from various Ruby processes, including Rake tasks, custom scripts, and the Rails console. This API provides a powerful and flexible way to access Kafka data without the need for complex setup, configuration, <code>karafka server</code> processes deployment or creating consumers.</p> <p>The Iterator API is designed to be simple and easy to use. It allows developers to subscribe to specific Kafka topics and partitions and perform data lookups using a simple and intuitive Ruby interface. </p> <p>Developers can customize their data processing logic and perform data lookups according to their specific needs, such as retrieving data from a particular offset, processing data from the beginning of the topic, or processing only the most recent messages.</p> <p>One of the major benefits of the Iterator API is its flexibility. You can use it from any Ruby process, including Rake tasks, custom scripts, and the Rails console. This means you can easily integrate Kafka data processing into their existing workflows and automation tasks. It also makes it easy to perform ad-hoc data processing and analysis without complex infrastructure.</p> <p>Iterator API and Compacted Messages</p> <p>When using Karafka's Iterator API to access Kafka data, please keep in mind, that it skips compacted messages and transactions-related messages during reading. However, these skipped messages are still included in the overall count. For instance, if you request the last 10 messages and all are transaction-related or compacted, the API will return no data, but they're counted in the total.</p>"}, {"location": "Pro-Iterator-API/#usage", "title": "Usage", "text": "<p>The iterator API requires you to create an instance of the <code>Karafka::Pro::Iterator</code> with the following arguments:</p> <ul> <li><code>topics</code> - A topic name or a hash with topics subscriptions settings</li> <li><code>settings</code> (keyword argument) - settings to pass to the consumer. Allow for altering the EOF behavior and other low-level settings.</li> <li><code>yield_nil</code> (keyword argument) - indicates if <code>nil</code> values should also be yielded. Useful for long-living scenarios. Defaults to <code>false</code>.</li> </ul> <pre><code># A simple example to stream all partitions from beginning till the end\niterator = Karafka::Pro::Iterator.new('my_topic')\n\niterator.each do |message|\n  puts message.payload\nend\n</code></pre> <p>Please read the sections below for more details. </p>"}, {"location": "Pro-Iterator-API/#subscription-modes", "title": "Subscription Modes", "text": "<p>Iterator accepts topic lists in various formats to support multiple use cases. Depending on your specific needs, you can pass in topics as a single string, a list of strings, or a hash with options. This allows developers to easily subscribe to one or multiple Kafka topics and partitions and perform data lookups according to their requirements.</p>"}, {"location": "Pro-Iterator-API/#subscribing-to-all-topic-partitions", "title": "Subscribing to All Topic Partitions", "text": "<p>The easiest way to subscribe to a topic or few is by providing their names. In a scenario like this, Karafka will subscribe to all the topics' partitions and consume messages from the earliest available message.</p> <p>You can either provide a topic name or array with all the topics you want to subscribe to:</p> <pre><code>iterator = Karafka::Pro::Iterator.new(['my_topic1', 'my_topic2'])\n\niterator.each do |message|\n  puts message.payload\nend\n</code></pre>"}, {"location": "Pro-Iterator-API/#subscribing-to-fetch-the-last-n-messages", "title": "Subscribing to Fetch the Last N Messages", "text": "<p>One everyday use case for Karafka Pro Iterator API is to fetch the last N messages from each topic partition and process them instead of starting from the beginning. This can be useful when you need to perform data analysis or processing on the most recent data without dealing with the entire dataset. By using the Iterator API to fetch the last N messages from each partition, you can save time and resources and focus on processing only the most relevant data.</p> <p>When subscribing with a negative offset, Karafka will compute the offset from which it should start for each partition independently, ensuring that at least the requested number of messages is being processed.</p> <pre><code># Read and iterate over the last 10 000 messages available in each\n# partition of the topic users_events\niterator = Karafka::Pro::Iterator.new(\n  {\n    'users_events' =&gt; -10_000 \n  }\n)\n\niterator.each do |message|\n  puts message.payload\nend\n</code></pre> <p>Differentiating Negative Offsets in Karafka</p> <p>In Karafka, you may encounter a negative offset of <code>-1001</code> in the context of statistics reporting, and this does not represent the same concept as the Iterator negative offsets lookup. In the context of Karafka emitted statistics, the <code>-1001</code> means that the offset information is not yet available.</p> <p>Handling Compacted Topics with Negative Lookups</p> <p>Negative lookups operate based on watermark offsets, not actual message counts. So, for compacted topics (where redundant data is removed), this could result in fetching fewer messages than requested, as the specified offset might include removed data.</p>"}, {"location": "Pro-Iterator-API/#subscribing-to-fetch-messages-from-a-certain-point-in-time", "title": "Subscribing to Fetch Messages From a Certain Point in Time", "text": "<p>This functionality is handy when you need to analyze data from a specific period or start processing from a certain point in the Kafka topic, but you do not know the offset.</p> <p>To do this, you must provide a timestamp instead of a numerical offset when setting up your subscription. This timestamp should represent the exact time you wish to start processing messages. The Karafka Iterator API will then fetch all messages that were produced after this timestamp.</p> <pre><code># Read and iterate over the last 60 seconds of messages available in each\n# partition of the topic users_events\niterator = Karafka::Pro::Iterator.new(\n  {\n    'users_events' =&gt; Time.now - 60\n  }\n)\n\niterator.each do |message|\n  puts message.payload\nend\n</code></pre> <p>This feature enables a more intuitive way of accessing and processing historical Kafka data. Instead of calculating or estimating offsets, you can directly use real-world time to navigate through your data. Just like with offsets, remember that you can only fetch messages still stored in Kafka according to its data retention policy.</p>"}, {"location": "Pro-Iterator-API/#subscribing-to-particular-partitions", "title": "Subscribing to Particular Partitions", "text": "<p>One reason it may be worth subscribing only to particular partitions of a topic using the iterator API is to reduce resource consumption. Consuming all topic partitions can be resource-intensive, especially when dealing with large amounts of data. By subscribing only to specific partitions, you can significantly reduce the amount of data that needs to be processed and reduce the overall resource consumption.</p> <p>Another reason subscribing only to particular partitions can be helpful is to save time. When consuming all partitions of a topic, the iterator needs to search through all the partitions to find the data that matches the consumer's criteria. If you know to which partition the data you are looking for goes, you can skip the unnecessary search in other partitions, which can save a lot of time.</p> <p>To do so, you need to provide the list of the partitions with the initial offset or time. You can set the initial offset to <code>0</code> if you want to start from the beginning. If the <code>0</code> offset is unavailable, Karafka will seek to the beginning of the partition. You may also use negative per-partition offsets similar to how they use them for whole-topic subscriptions.</p> <pre><code># Go through two partitions: 0, 5 and 7\n# Get 100 most recent messages for partition 0\n# Get 10 000 most recent messages for partition 5\n# Get messages from last 60 seconds from partition 7\n# Get messages from partition 9 starting from the beginning\niterator = Karafka::Pro::Iterator.new(\n  {\n    'users_events' =&gt; {\n      0 =&gt; -100,\n      5 =&gt; -10_000,\n      7 =&gt; Time.now - 60,\n      # Below requires setting 'auto.offset.reset': 'beginning'\n      9 =&gt; true\n    }\n  }\n)\n\niterator.each do |message|\n  puts message.payload\nend\n</code></pre>"}, {"location": "Pro-Iterator-API/#stopping-the-iterator", "title": "Stopping the Iterator", "text": "<p>When working with the Karafka Pro Iterator, there may be scenarios where you need to halt the iteration process entirely. Using the <code>#stop</code> method is recommended in such cases. This method provides a clean and graceful termination of the iterator, ensuring that all resources are properly managed and released. This approach is recommended over simply breaking out of the iteration loop, as it allows for a more controlled and efficient shutdown process.</p> <p>Using <code>#stop</code> is straightforward. Once invoked, the method sets an internal flag that indicates the iterator should cease processing as soon as possible. This check is performed internally within the iterator's loop, ensuring that the iteration stops cleanly after the current message processing completes.</p> <p>Here\u2019s an example of how to use <code>#stop</code> effectively:</p> <pre><code>iterator.each do |message, iterator|\n  process_message(message)\n\n  # A condition that determines when to stop iterating\n  # No need to break is the iterator will not yield more messages\n  iterator.stop if should_stop_iteration?\nend\n</code></pre>"}, {"location": "Pro-Iterator-API/#marking-as-consumed", "title": "Marking As Consumed", "text": "<p>In scenarios where precise tracking of message consumption is crucial, the Karafka Pro Iterator provides functionality similar to that of a traditional Karafka consumer. This allows for marking messages as consumed, which is essential for managing the offsets of processed messages. This feature is handy to ensure that messages are not reprocessed unintentionally on subsequent iterations, maintaining the integrity and accuracy of data processing.</p> <p>The Iterator can mark messages as consumed using two methods:</p> <ul> <li><code>mark_as_consumed</code></li> <li><code>mark_as_consumed!</code> (blocking)</li> </ul> <p>Example usage:</p> <pre><code>iterator.each do |message, iterator|\n  process_message(message)\n\n  # Mark message as consumed non-blockingly\n  iterator.mark_as_consumed(message)\nend\n</code></pre>"}, {"location": "Pro-Iterator-API/#long-living-iterators", "title": "Long-Living Iterators", "text": "<p>By default iterator instance will finish its work when it reaches end of data on all the partitions. This however may not be desired if you want to process data as it comes.</p> <p>You can alter this behavior by setting the <code>enable.partition.eof</code> to <code>false</code> and setting the <code>yield_nil</code> to <code>true</code>. Yielding nil is required because you need a way to exit the iterator even if no messages are being produced to the topic you are iterating on.</p> <pre><code>iterator = Karafka::Pro::Iterator.new(\n  # Start from the last message available\n  { 'system_events' =&gt; -1 },\n  settings: { 'enable.partition.eof': false },\n  yield_nil: true\n)\n\n# Stop iterator when 100 messages are accumulated\nlimit = 100\nbuffer = []\n\niterator.each do |message|\n  break if buffer.count &gt;= limit\n\n  # Message may be a nil when `yield_nil` is set to true  \n  buffer &lt;&lt; message if message\nend\n</code></pre> <p>Recommended Approach for Long-Living Iterators</p> <p>If you find yourself working with long-living iterators that operate for a long time, we do recommend using the <code>karafka server</code> default consumption API as it provides all the needed features and components for robust and long-running consumption. </p>"}, {"location": "Pro-Iterator-API/#routing-awareness", "title": "Routing Awareness", "text": "<p>If you are iterating over topics defined in your <code>karafka.rb</code>, including those marked as inactive, the iterator will know what deserializer to use and will operate accordingly. If you are iterating over an unknown topic, defaults will be used.</p> <pre><code># karafka.rb\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic 'events' do\n      active false\n      deserializers(\n        payload: XmlDeserializer\n      )\n    end\n  end\nend\n\n# Your iterator script\n\niterator = Karafka::Pro::Iterator.new('events')\n\niterator.each do |message|\n  # Karafka will know, to use the XmlDeserializer\n  puts message.payload\nend\n</code></pre>"}, {"location": "Pro-Iterator-API/#partition-consumption-early-stop", "title": "Partition Consumption Early Stop", "text": "<p>There may be situations when using the iterator where you may want to stop consuming data from specific partitions while continuing to consume data from other partitions. This can be useful in scenarios where you were looking for pieces of information in each of the partitions, and in some, you've already found it. In such scenarios, further processing of those partitions will not provide any benefits and will only consume resources.</p> <p>To early stop one partition without stopping the iterator process, you can use the <code>#stop_partition</code> or <code>#stop_current_partition</code> methods.</p> <pre><code>iterator = Karafka::Pro::Iterator.new(\n  {\n    'users_events' =&gt; -10_000 \n  }\n)\n\ndata_per_partition = Hash.new { |h, k| h[k] = [] }\nlimit_per_partition = 20\n\niterator.each do |message, iterator|\n  data_per_partition[message.partition] &lt;&lt; message.payload\n\n  # Continue data collection for each partition until enough data\n  next if data_per_partition[message.partition].size &lt; limit_per_partition\n\n  # Stop current partition from being fetched\n  iterator.stop_current_partition\nend\n</code></pre>"}, {"location": "Pro-Iterator-API/#iterating-with-the-cleaner-api", "title": "Iterating with the Cleaner API", "text": "<p>The Cleaner API is designed to enhance batch processing efficiency by promptly freeing up memory once a message's payload is no longer needed. This functionality is especially beneficial when working with large payloads (10KB and above) and can help manage memory usage more effectively.</p> <p>The Cleaner API can be integrated with the Iterator API to ensure optimal memory management during long-running iterations. When processing large datasets or streaming data over extended periods, it is essential to keep memory usage under control to avoid performance degradation or crashes due to memory overload.</p> <p>Here's how you can use the Cleaner API with the Iterator API to process messages and clean up memory efficiently:</p> <pre><code># Initialize the iterator for a specific topic\niterator = Karafka::Pro::Iterator.new(['my_topic'])\n\niterator.each do |message|\n  # Process the message\n  process_message(message)\n\n  # Clean the message payload from memory after processing\n  # Message may be a nil when `yield_nil` is set to true  \n  message.clean!\nend\n</code></pre> <p>In this example, the <code>#clean!</code> method is called on each message after processing, immediately removing the payload from memory. This helps maintain a low memory footprint throughout the iteration process.</p>"}, {"location": "Pro-Iterator-API/#integration-with-ruby-processes", "title": "Integration with Ruby Processes", "text": "<p>The Karafka Pro Iterator API is designed to be simple and easy to use, and there is nothing special needed to get started with it. There are also no special recommendations when using it from any specific Ruby process type.</p> <p>However, it is important to remember that the Iterator API is designed for lightweight Kafka operations and should not be used to perform extensive Kafka operations during HTTP requests or similar. This is because performing extensive Kafka operations during requests can impact the application's performance and result in slower response times.</p> <p>Additionally, it is important to note that the Iterator API does not manage the offsets. This means that when you subscribe to a Kafka topic and partition, you must provide the initial offsets yourself.</p>"}, {"location": "Pro-Iterator-API/#scalability-and-performance", "title": "Scalability and Performance", "text": "<p>It's important to note that the Karafka Pro iterator API is designed to be a straightforward way to access Kafka data using Ruby. However, it is a single-threaded API, meaning it does not provide any form of parallelizing data. This means that any data processing or analysis will be performed sequentially, which may impact performance when dealing with large amounts of data or performing extensive IO operations. While this can be limiting in some cases, the Iterator API's simplicity and ease of use make it an attractive option for developers looking for a quick and easy way to access Kafka data without the need for complex configuration or deployment of additional processes. If parallelization is required, alternative approaches, such as using the Karafka consumers feature, may need to be explored.</p> <p>The iterator should handle up to 100 000 messages per second.</p>"}, {"location": "Pro-Iterator-API/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Custom Kafka data processing: The iterator API allows developers to create custom scripts that process Kafka data in a specific way. For example, you can extract specific fields from Kafka data, transform them into a different format, or perform calculations on the data.</p> </li> <li> <p>Custom data analytics: With the iterator API, developers can perform custom data analytics on Kafka data, such as trend analysis, forecasting, or anomaly detection. This can be useful for detecting patterns in data that might take time to be noticeable.</p> </li> <li> <p>Automated testing: With the iterator API, developers can automate the testing of Kafka data and verify that data is flowing correctly between different components of an application.</p> </li> <li> <p>Custom reporting: By subscribing to specific Kafka topics and partitions, developers can create custom reports that provide insights into Kafka data. This can be useful for identifying trends or outliers in data.</p> </li> <li> <p>Debugging: The iterator API can be used to quickly diagnose and fix issues with Kafka data by providing a simple way to inspect Kafka data in real time.</p> </li> <li> <p>Data Analysis: You can use the iterator API to quickly check your message sizes and compute needed distributions.</p> </li> </ul>"}, {"location": "Pro-Iterator-API/#iterator-usage-for-message-size-distribution-computation", "title": "Iterator Usage for Message Size Distribution Computation", "text": "<p>Below is an example of how the Iterator can be used to compute and analyze message sizes.</p> <pre><code>iterator = Karafka::Pro::Iterator.new(\n  {\n    'karafka_consumers_reports' =&gt; -10_000\n  },\n  settings: { 'enable.partition.eof': true }\n)\n\nparts = {}\n\niterator.each do |message, iterator|\n  scope = parts[message.partition] ||= []\n  scope &lt;&lt; message.raw_payload.bytesize\n  iterator.stop_current_partition if scope.size &gt;= 10000\n  message.clean!\nend\n\ndef percentile(values, percent)\n  sorted = values.sort\n  rank = (percent * (sorted.size - 1)).round\n  sorted[rank]\nend\n\nparts.each do |part, values|\n  avg = values.sum / values.size.to_f\n  p95 = percentile(values, 0.95)\n  p99 = percentile(values, 0.99)\n  mean = values.sum.to_f / values.size\n\n  puts \"Partition: #{part}:\"\n  puts \"  avg: #{avg} bytes\"\n  puts \"  p95: #{p95} bytes\"\n  puts \"  p99: #{p99} bytes\"\n  puts \"  mean: #{mean} bytes\"\nend\n\n# Partition: 0:\n#   avg: 808.4875 bytes\n#   p95: 903 bytes\n#   p99: 907 bytes\n#   mean: 808.4875 bytes\n</code></pre>"}, {"location": "Pro-Iterator-API/#summary", "title": "Summary", "text": "<p>Overall, the Karafka Pro Iterator API provides a powerful and flexible way to access and process Kafka data from various Ruby processes. Its simplicity, flexibility, and scalability make it an essential tool for developers who need to work with Kafka data quickly and efficiently.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Long-Running-Jobs/", "title": "Long-Running Jobs", "text": "<p>When working with Kafka, there is a setting called <code>max.poll.interval.ms</code>.</p> <p>It is the maximum delay between invocations of <code>poll()</code> commands. This places an upper bound on the time the consumer can wait before fetching more records.</p> <p>After exceeding this time, an error will be raised, the process will be removed from the group, and you may notice the following message:</p> <pre><code>Maximum poll interval (300000ms) exceeded by 255ms \n(adjust max.poll.interval.ms for long-running message processing): leaving group\n</code></pre> <p>This value is effectively the maximum time you can spend processing messages fetched in a single <code>poll</code> even if they come from different partitions. Once this is exceeded, the given process will be removed from the group. This can cause the group to become unstable due to frequent rebalances.</p> <p> </p> <p> *Standard processing flow requires all the data to be processed before polling another batch of messages.    </p> <p>To mitigate this, you can do a few things:</p> <ol> <li>Extend the <code>max.poll.interval.ms</code>.</li> <li>Decrease <code>max_messages</code>.</li> <li>Use Virtual Partitions to parallelize the work further.</li> <li>Use Adaptive Iterator to mitigate occasional spikes in processing time.</li> <li>Use Long-Running Jobs and not worry about that.</li> </ol> <p>The strategy you want to go with heavily depends on your data and processing patterns. If you encounter this issue while maintaining a sane number of messages and decent <code>max.poll.interval.ms</code>, you should further parallelize the processing or enable this feature.</p> <p>Long-Running Jobs feature follows the Confluent recommended strategy of pausing a given partition for the time of the processing and resuming processing of the partition once the work is done.</p> <p>That way, as long as no rebalances occur during the processing that would cause the partition to be revoked from the given process, polling can happen within the boundaries of <code>max.poll.interval.ms</code>.</p> <p> </p> <p> *With Long-Running Job enabled, the given partition is paused for the processing time, and polling happens independently from processing.    </p> <p>This feature is great for scenarios where your processing may last for a longer time period. For example, when you need to communicate with external systems, their performance periodically is not deterministic.</p> <p>Alternatives to Long-Running Jobs: Direct Assignments and Iterator</p> <p>An alternative to using subscription group-based assignments to handle long-running jobs in Karafka is the Direct Assignments API. This API provides a flexible way to manage Kafka partitions directly, bypassing the <code>max.poll.interval.ms</code> constraint. The Iterator API also presents a viable alternative for scenarios requiring fine-tuned control over message consumption. These approaches can be instrumental in systems where long processing times are common and must be managed efficiently.</p>"}, {"location": "Pro-Long-Running-Jobs/#using-long-running-jobs", "title": "Using Long-Running Jobs", "text": "<p>The only thing you need to add to your setup is the <code>long_running_job</code> option in your routing section:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      long_running_job true\n    end\n  end\nend\n</code></pre> <p>Long-Running Job Impact on Internal Queues</p> <p>When using Long-Running Jobs, be aware that pausing to manage <code>max.poll.interval.ms</code> will purge your internal message queue. This is due to <code>#pause</code> acting as a fencing mechanism, invalidating all messages currently in the queue. To avoid extensive network traffic from message re-fetching, it's recommended to reduce <code>queued.max.messages.kbytes</code>. This ensures a smaller pre-fetched message queue, which is crucial if you frequently seek, helping to optimize bandwidth usage. You can read more about this here.</p>"}, {"location": "Pro-Long-Running-Jobs/#processing-during-revocation", "title": "Processing during revocation", "text": "<p>Upon a group rebalance, there are three scenarios affecting the paused partition you are processing:</p> <ol> <li>Partition is not revoked because <code>cooperative-sticky</code> assignment strategy is in use.</li> <li>Partition is revoked and re-assigned to the same process.</li> <li>Partition is revoked and assigned to a different process.</li> </ol> <p>The <code>#revoked?</code> method value changes independently from the workers' occupation. This means that the revocation status will be updated even if all the workers are busy processing long-running jobs.</p> <p>Revocation jobs are also non-blocking for long-running jobs. If the internal workers' batch is full, they will not block polling.</p>"}, {"location": "Pro-Long-Running-Jobs/#cooperative-sticky-rebalance", "title": "<code>cooperative-sticky</code> rebalance", "text": "<p>Using the <code>cooperative-sticky</code> assignment strategy is recommended when using Long-Running Jobs. This can increase overall stability by not triggering revocation of partitions upon rebalances when partition would be re-assigned back:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'partition.assignment.strategy': 'cooperative-sticky'\n    }\n  end\nend\n</code></pre>"}, {"location": "Pro-Long-Running-Jobs/#revocation-and-re-assignment", "title": "Revocation and re-assignment", "text": "<p>In the case of scenario <code>2</code>, there is nothing you need to do. Karafka will continue processing your messages and resume partition after it is done with the work.</p>"}, {"location": "Pro-Long-Running-Jobs/#revocation-without-re-assignment", "title": "Revocation without re-assignment", "text": "<p>If partition becomes assigned to a different process, this process will pick up the same messages you are currently working with. To mitigate this, Karafka has a <code>#revoked?</code> method you can periodically check to ensure that a given process still owns the partition you are working with.</p> <p>This method, in the case of the Long-Running Jobs feature, does not require marking messages as consumed or taking any other actions. Group state is updated asynchronously alongside the work being done.</p> <pre><code>def consume\n  messages.each do |message|\n    # Stop sending messages to the external service if we no longer own the partition\n    return if revoked?\n\n    ExternalSystemDispatcher.new.call(message)\n  end\nend\n</code></pre>"}, {"location": "Pro-Long-Running-Jobs/#processing-during-shutdown", "title": "Processing during shutdown", "text": "<p>Karafka will wait for your Long-Running Jobs to finish within the limits of <code>shutdown_timeout</code>. Either set it to a value big enough for the jobs to finish or implement periodic shutdown checks and enable manual offset management. Otherwise, Karafka may forcefully stop workers in the middle of processing after the <code>shutdown_timeout</code> is exceeded.</p> <p>During the shutdown, polling occurs, so there is no risk of exceeding the <code>max.poll.interval.ms</code>.</p> <pre><code># Note that for this to work, you need to manage offsets yourself\n# Otherwise, automatic offset management will commit offset of the last message\ndef consume\n  messages.each do |message|\n    # Stop sending messages if the app is stopping\n    return if Karafka::App.stopping?\n\n    ExternalSystemDispatcher.new.call(message)\n\n    mark_message_as_consumed(message)\n  end\nend\n</code></pre>"}, {"location": "Pro-Long-Running-Jobs/#using-long-running-jobs-alongside-regular-jobs-in-the-same-subscription-group", "title": "Using Long-Running Jobs alongside regular jobs in the same subscription group", "text": "<p>By default, Long-Running Jobs defined alongside regular jobs will be grouped in a single subscription group. This means they will share an underlying connection to Kafka and be subject to the same blocking polling limitations.</p> <p>In case of a regular job blocking beyond <code>max.poll.interval.ms</code>, Kafka will revoke the regular jobs and the defined Long-Running Jobs.</p> <p>If you expect that your regular jobs within the same subscription group may cause Kafka rebalances or any other issues, separating them into different subscription groups is worth doing. This will ensure that external factors do not influence Long-Running Jobs's stability.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # By providing this, all the long running jobs will get a separate Kafka connection that\n    # won't be affected by other topics consumption in any way\n    subscription_group 'long_running_jobs' do\n      topic :orders_states do\n        consumer OrdersStatesConsumer\n\n        long_running_job true\n      end\n    end\n\n    topic :deliveries_states do\n      consumer DeliveriesStatesConsumer\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Long-Running-Jobs/#pausing-in-long-running-jobs", "title": "Pausing in Long-Running Jobs", "text": "<p>When using Karafka Long-Running Jobs, it is not recommended to use manual pausing because it can lead to unexpected behaviors and errors in the system. Long-Running Jobs automatically pause and resume topic partitions based on the consumption flow. In case of a manual pause that operates for a shorter duration than the consumer processing time, the partition may be resumed before the consumer finishes its processing, which can cause unexpected behaviors and errors in the system.</p> <p>If a manual pause is needed, it is recommended to compute its duration based on the following formula:</p> <p><code>max_remaining_processing_time + 2 * max_wait_time</code></p> <p>This will ensure that the consumer has enough time to process all the messages in the batch before the partition is resumed.</p> <p>Overall, it is crucial to be mindful of the potential risks and issues associated with manual pausing when using Karafka Long-Running Jobs. By following best practices and leveraging the built-in features of the framework, we can ensure that the system remains reliable, scalable, and performs as expected.</p>"}, {"location": "Pro-Long-Running-Jobs/#non-blocking-jobs-complementing-long-running-job-pausing", "title": "Non-Blocking Jobs: Complementing Long-Running Job Pausing", "text": "<p>The Long-Running Jobs feature is designed to handle tasks that take longer to process than the standard Kafka message batch. Using the pausing strategy allows for efficient parallel processing across multiple partitions of the same topic. This approach helps manage longer tasks without risking the stability of the consumer group due to frequent rebalances. For users looking to optimize their parallel processing further, reviewing the Non-Blocking Jobs documentation for additional strategies and best practices is recommended.</p>"}, {"location": "Pro-Long-Running-Jobs/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>External Service Calls: In some cases, processing messages may require making HTTP requests to external services, which can take a long time to complete. For example, processing messages to perform payment processing, geocoding, or weather data retrieval may require making requests to external services that can take a significant amount of time to return a response.</p> </li> <li> <p>IoT Data Processing: With the rise of the Internet of Things (IoT), data processing and analysis of IoT-generated data has become increasingly important. The processing of messages may involve analyzing sensor data, predicting equipment failure, and optimizing operations.</p> </li> <li> <p>Complex Database Operations: Performing complex database operations such as joins, aggregations, or subqueries can take a significant amount of time, especially when dealing with large datasets. The processing of messages may involve performing such operations on the incoming data.</p> </li> <li> <p>Data Cleaning and Preprocessing: Data cleaning and preprocessing can take significant time, especially when dealing with large datasets. The processing of messages may involve tasks such as data validation, data normalization, or data standardization.</p> </li> </ul> <p>These are just a few examples of how Long-Running Jobs can benefit different industries where processing messages takes a significant amount of time. Karafka's Long-Running Jobs feature can be used to develop and manage these jobs, enabling continuous data processing and analysis.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Messages-At-Rest-Encryption/", "title": "Messages At Rest Encryption", "text": "<p>Specific industries have strong regulations around the storage of personal data. Private medical data, financial data, social security numbers, and credit card numbers are all sensitive. Karafka Pro supports transparent encryption of the message's payload, so sensitive data at rest in Kafka cannot be seen.</p> <p>Karafka uses RSA asymmetric encryption, so your producers do not have to have the capability to decrypt data.</p> <p>Custom Headers Deserializer and Encryption</p> <p>When using Karafka's encryption features, it's important to note that encryption may not work as expected if you use a custom headers deserializer. Custom deserialization of headers can alter how encryption headers are processed, potentially leading to issues in correctly encrypting or decrypting messages. In cases where custom headers deserialization is necessary, it is recommended to consult with Karafka Pro support for guidance to ensure that encryption functionalities are properly integrated and maintained within your application.</p>"}, {"location": "Pro-Messages-At-Rest-Encryption/#enabling-encryption", "title": "Enabling Encryption", "text": "<p>Encryption has its dedicated section in the configuration called <code>encryption</code>. To enable encryption, you need to:</p> <ol> <li>Set the <code>encryption.active</code> key to <code>true</code>.</li> <li>Set the <code>encryption.version</code> to a uniquely identifiable string, and it will be used in produced messages headers to match against the private key.</li> <li>Set the <code>encryption.public_key</code> with a public PEM key value.</li> <li>Set the <code>encryption.private_keys</code> using a hash, where the key is the version name matching the encryption version, and the value is the PEM private key that should be used to decrypt messages.</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other config options...\n\n    config.encryption.active = true\n    config.encryption.version = '1'\n    config.encryption.public_key = ENV['PUBLIC_PEM_KEY']\n    config.encryption.private_keys = { '1' =&gt; ENV['PRIVATE_PEM_KEY'] }\n  end\nend\n</code></pre> <p>Once everything is configured, Karafka will automatically produce encrypted messages and decrypt them before their usage.</p> <p>Karafka keeps messages encrypted until their deserialization.</p> <p>Karafka encrypts only the message payload. All other things are cleartext to aid with debugging. Do not store any sensitive information in message keys or headers.</p>"}, {"location": "Pro-Messages-At-Rest-Encryption/#handling-of-unencrypted-messages-with-encryption-enabled", "title": "Handling of Unencrypted Messages with Encryption Enabled", "text": "<p>Karafka automatically recognizes unencrypted messages and does not attempt to decrypt them. This means you can gradually enable and roll out encryption without worrying about previously unencrypted data.</p>"}, {"location": "Pro-Messages-At-Rest-Encryption/#producing-encrypted-messages-without-private-key-configuration", "title": "Producing Encrypted Messages without Private Key Configuration", "text": "<p>If you do not plan to consume messages from some of your applications, you may skip the <code>private_keys</code> definition:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other config options...\n\n    config.encryption.active = true\n    config.encryption.version = '1'\n    config.encryption.public_key = ENV['PUBLIC_PEM_KEY']\n  end\nend\n</code></pre> <p>That way, the given application can produce messages but not decrypt them. This is especially useful when you are building bigger systems where you want to provide limited granular permissions.</p>"}, {"location": "Pro-Messages-At-Rest-Encryption/#rotating-public-and-private-keys", "title": "Rotating Public and Private Keys", "text": "<p>When you upgrade your keys, please remember to update the <code>config. encryption.version</code>, so Karafka can recognize the correct key pair.</p> <p>If you have yet to consume messages using an old public key, do not remove the old private key.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other config options...\n\n    config.encryption.active = true\n    config.encryption.version = '2'\n    config.encryption.public_key = ENV['PUBLIC_PEM_KEY_V2']\n    config.encryption.private_keys = {\n      '1' =&gt; ENV['PRIVATE_PEM_KEY_V1'],\n      '2' =&gt; ENV['PRIVATE_PEM_KEY_V2']\n    }\n  end\nend\n</code></pre> <p>Karafka will automatically detect and use the correct private key to decrypt messages encrypted with the old public key.</p>"}, {"location": "Pro-Messages-At-Rest-Encryption/#using-multiple-public-and-private-keys-to-support-multiple-customers", "title": "Using Multiple Public and Private Keys to Support Multiple Customers", "text": "<p>There are scenarios where you may want your customers to publish messages directly to your Kafka cluster. You can ensure that this communication is also private and, At-Rest encrypted.</p> <p>All you need to do for this to happen is:</p> <ol> <li>Generate a key pair and give the public key to your customer.</li> <li>Use a unique identifier as a version in <code>private_keys</code> so Karafka knows which private key to use.</li> <li>Ask the customer to include an <code>encryption</code> header in each message containing the identifier.</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other config options...\n\n    config.encryption.active = true\n    config.encryption.version = 'internal_1'\n    config.encryption.public_key = ENV['PUBLIC_PEM_KEY_INTERNAL_1']\n    config.encryption.private_keys = {\n      'internal_1' =&gt; ENV['PRIVATE_PEM_KEY_INTERNAL_1'],\n      'customer_1' =&gt; ENV['PRIVATE_PEM_KEY_CUSTOMER_1'],\n      'customer_2' =&gt; ENV['PRIVATE_PEM_KEY_CUSTOMER_2']\n    }\n  end\nend\n</code></pre> <p>Such a pattern should only be used when working with trusted entities.</p>"}, {"location": "Pro-Messages-At-Rest-Encryption/#messages-fingerprinting", "title": "Messages Fingerprinting", "text": "<p>In Ruby, decrypting a message with an incorrect but semantically valid key may complete the decryption process successfully but result in nonsensical data. This behavior poses a significant risk in systems lacking stringent consistency checks, as they might inadvertently persist corrupted data to a database, assuming the decryption was successful.</p> <p>This situation can arise during key rotation processes or when encryption keys are changed. Karafka introduces a feature for enhancing message integrity through fingerprinting to mitigate such risks. By setting the <code>fingerprinter</code> option to an object that responds to the <code>#hexdigest</code> method, Karafka appends an additional <code>encryption_fingerprint</code> header to each message before sending. This fingerprint is then used after decrypting the message to verify its integrity. If the integrity check fails, indicating that the message was not decrypted with the correct key or was tampered with, Karafka will raise an error, preventing corrupted data from being processed further.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other config options...\n\n    config.encryption.active = true\n    config.encryption.version = '1'\n    config.encryption.public_key = ENV['PUBLIC_PEM_KEY']\n    config.encryption.private_keys = { '1' =&gt; ENV['PRIVATE_PEM_KEY'] }\n\n    # Set this to any system or custom digest engine that responds to `#hexdigest`\n    config.encryption.fingerprinter = Digest::MD5\n  end\nend\n</code></pre> <p>This mechanism of ensuring message integrity is distinct from the CRC (Cyclic Redundancy Check) integrity check provided by Kafka. While Kafka's CRC check ensures that a message has not been corrupted in transit, providing a form of transport-level integrity, it does not protect against decryption with the wrong key. Karafka's fingerprinting feature addresses this gap by offering an additional layer of security that ensures the decrypted message is the exact message that was encrypted initially, thereby safeguarding against processing corrupted data due to key mismanagement or other errors.</p> <p>This feature is especially critical in industries subject to stringent regulations around handling sensitive data, such as healthcare, finance, and government. By ensuring the integrity of decrypted messages, Karafka helps organizations maintain compliance with regulations like HIPAA and GDPR, which mandate strict controls over the confidentiality and integrity of sensitive information.</p> <p>Selection of Fingerprinting Algorithm</p> <p>The choice of fingerprinting algorithm is critical and should be made with care. Each message processed by Karafka will have a fingerprint header attached based on the selected algorithm. This inclusion can significantly increase the size of each message, especially for smaller messages, potentially impacting overall throughput and storage efficiency. Additionally, the process of computing these fingerprints is CPU-intensive. This could lead to increased processing times and higher CPU usage, affecting the performance of your system. It's essential to weigh these considerations when selecting a fingerprinting algorithm to ensure it aligns with your application's performance and resource utilization requirements.</p>"}, {"location": "Pro-Messages-At-Rest-Encryption/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Healthcare: In the healthcare industry, patient data like medical history, diagnoses, and prescriptions are stored in Kafka. At-rest encryption can help ensure the confidentiality and integrity of this sensitive information, helping to meet regulatory requirements like HIPAA and GDPR.</p> </li> <li> <p>E-commerce: E-commerce companies may store sensitive customer data like login credentials, shipping addresses, and credit card details in Kafka. At-rest encryption can help prevent data breaches and protect the privacy of customers, which can improve their trust in the company and increase sales.</p> </li> <li> <p>Government: Government agencies often store sensitive information like personally identifiable information (PII) and classified data in Kafka. At-rest encryption can help protect this data from unauthorized access or theft, ensuring national security and compliance with regulations.</p> </li> <li> <p>IoT: Internet of Things (IoT) devices often send and receive sensitive data like sensor readings, user locations, and device configurations via Kafka. At-rest encryption can help prevent unauthorized access to this data, ensuring the privacy and security of users and devices.</p> </li> <li> <p>Human Resources: Human resources departments may store sensitive employee data such as social security numbers, payroll information, and performance reviews in Kafka. At-rest encryption can help protect this data from unauthorized access or theft, ensuring regulatory compliance and maintaining employee trust.</p> </li> </ul> <p>Karafka Pro's at-rest encryption is worth using because it provides a layer of security to sensitive data stored in Kafka, ensuring that even if the data is compromised, it cannot be viewed by unauthorized users. The encryption is transparent, meaning that it doesn't require any changes to the application code or Kafka configuration, making it easy to implement. Moreover, it supports key rotation and management, enabling the organization to have complete control over the encryption keys and ensuring data is always secure. This makes it an essential feature for businesses that deal with sensitive data and want to protect their customers' privacy and maintain regulatory compliance.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Multiplexing/", "title": "Multiplexing", "text": "<p>Karafka Multiplexing is designed to boost the efficiency and performance of message processing in Kafka. Allowing a single process to establish multiple independent connections to the same Kafka topic significantly enhances parallel processing and throughput.</p> <p>Multiplexing in Karafka enables more effective data handling and improves performance by dividing the workload across several connections. This approach ensures quicker data processing, better resource utilization, and increased fault tolerance, making your Kafka-based systems more robust and responsive.</p> <p>Multiplexing increases throughput and significantly enhances processing capabilities in scenarios with multi-partition lags. When a single process subscribes to multiple partitions, it can swiftly address lags in any of them, ensuring more consistent performance across your system. This advantage becomes particularly prominent in IO-intensive workloads where efficient data handling and processing are crucial.</p> <p> </p> <p> *This example illustrates the performance difference for IO intense work, where the IO cost of processing a single message is 1ms and a total lag of 500 000 messages in five partitions.    </p>"}, {"location": "Pro-Multiplexing/#enabling-multiplexing", "title": "Enabling Multiplexing", "text": "<p>To enable multiplexing in Karafka, a simple yet crucial step must be taken when defining your subscription groups. By providing the <code>multiplex</code> option within your subscription group definition, you instruct Karafka to initiate multiplexing for that particular group:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # Always establish two independent connections to this topic from every\n    # single process. They will be able to poll and process data independently\n    subscription_group 'events' do\n      multiplexing(max: 2)\n\n      topic :events do\n        consumer EventsConsumer\n      end\n    end\n  end\nend\n</code></pre> <p>Multiplexing also works for subscription groups with multiple topics:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # Always establish two independent connections to those topics from every\n    # single process. They will be able to poll and process data independently\n    subscription_group 'main' do\n      multiplexing(max: 2)\n\n      topic :events do\n        consumer EventsConsumer\n      end\n\n      topic :notifications do\n        consumer NotificationsConsumer\n      end\n    end\n  end\nend\n</code></pre> <p>Proper Placement of <code>#multiplexing</code> configuration</p> <p>The <code>#multiplexing</code> method must be used exclusively within a <code>#subscription_group</code> block. It is not suitable for routing without an explicit subscription group definition.</p> <pre><code>routes.draw do\n  # This will NOT work - will raise undefined method\n  multiplexing(max: 2)\n\n  # Always define your subscription group and apply multiplexing directly on it\n  subscription_group :main do\n    multiplexing(max: 2)\n\n    topic :events do\n      consumer EventsConsumer\n    end\n  end\nend</code></pre> <p>Once you have configured multiplexing in your routing settings, no additional steps are required for it to function. Your application will start processing messages from multiple partitions simultaneously, leveraging the benefits of multiplexing immediately and seamlessly.</p> <p>The following configuration options are available for <code>#multiplexing</code>:</p> Option Type Default Description <code>min</code> Integer, nil <code>nil</code> (sets to <code>max</code>) The minimum multiplexing count. Setting this to <code>nil</code> or not setting it at all will set it to the <code>max</code> value, effectively turning off dynamic multiplexing and ensuring a stable number of multiplexed connections. <code>max</code> Integer 1 The maximum multiplexing count. This defines the upper limit for the number of connections that can be multiplexed. <code>boot</code> Integer, nil <code>nil</code> (defaults to half of <code>max</code> or <code>min</code> if <code>min</code> is set) Specifies how many listeners should be started during the boot process by default in the dynamic mode. If not set, it picks half of <code>max</code> as long as possible. Otherwise, it goes with <code>min</code>."}, {"location": "Pro-Multiplexing/#multiplexing-vs-virtual-partitions", "title": "Multiplexing vs. Virtual Partitions", "text": "<p>Deciding between Multiplexing and Virtual Partitions isn't a strict either/or scenario; in fact, they can be complementary. While Virtual Partitions parallelize single-topic processing without extra Kafka connections, Multiplexing increases throughput by allowing a consumer to handle multiple partitions. Together, they can enhance capabilities, especially in IO-intensive workloads. For instance, with a concurrency of ten, you might use two Kafka connections and virtualize each into five virtual partitions. This approach leverages both strategies for optimal performance, adapting to your application's needs, data intricacies, and anticipated load.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    subscription_group do\n      multiplexing(max: 2)\n\n      topic :events do\n        consumer EventsConsumer\n\n        virtual_partitions(\n          partitioner: -&gt;(message) { message.headers['order_id'] },\n          max_partitions: 5\n        )\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Multiplexing/#dynamic-multiplexing", "title": "Dynamic Multiplexing", "text": "<p>Aside from fixed connections multiplexing, Karafka provides a mode called \"Dynamic\". Dynamic multiplexing in Karafka is designed to optimize resource utilization and enhance system efficiency. Karafka's dynamic multiplexing adjusts the number of in-process connections based on partition assignments. This assignment-based scaling ensures a balanced and efficient distribution of resources.</p> <p>It is essential to know the following facts about this mode of operations:</p> <ul> <li> <p>Assignment-Based Scaling: The system dynamically adjusts connections based on the distribution of partition assignments. This ensures a more effective and targeted scaling strategy, directly aligning with Kafka's data structure and flow.</p> </li> <li> <p>Selective Scaling vs. Partition Assignment: When scaling, Karafka aims to balance the load across connections by evenly distributing partitions. For example, with 20 partitions and 2 processes set with multiplexing to 2, each connection manages 5 partitions. If scaled up to 20 processes, Karafka deactivates one connection from each process as each now manages one partition, ensuring efficient resource use and performance.</p> </li> <li> <p>Delayed Decision Post-Rebalance: To ensure stability and avoid premature scaling, any decision to change the number of connections is delayed by at least one minute following the cluster state change. This delay helps accommodate transient changes in partition assignments and maintains system equilibrium.</p> </li> <li> <p>Resource Preservation: By aligning the number of active connections with the number of partitions, Karafka prevents running extra threads and connections that are not needed, conserving vital system resources such as memory and CPU.</p> </li> </ul>"}, {"location": "Pro-Multiplexing/#how-does-it-work", "title": "How Does It Work?", "text": "<p>Here's a breakdown of how it operates when the dynamic mode is enabled:</p> <ul> <li> <p>Initial Connection Setup: Upon startup, Karafka initiates <code>boot</code> connections to Kafka or if not defined, half of available connections. This initial number is based on the configuration set for the multiplexing feature, representing the starting point for the dynamic scaling process.</p> </li> <li> <p>Stabilization Period: After establishing the initial connections, Karafka enters a stabilization period. It waits for at least one minute following the last rebalance. This waiting period allows the system to stabilize and ensures that decisions to scale down are not made prematurely, which might otherwise lead to unnecessary fluctuations and inefficiencies.</p> </li> <li> <p>Selective Connection Shutdown: Once the system has stabilized, Karafka begins monitoring the usage of each connection. If it identifies a connection that is not being used (i.e., no partitions are assigned), it will shut it down to conserve resources. However, it's crucial to note that Karafka is designed always to maintain at least <code>min</code> active connections, even if no partitions are currently assigned. This ensures that a line is always open to Kafka, ready to take on assignments if the need arises quickly.</p> </li> <li> <p>Adaptive Scaling Up: If any subscription group receives multiple assignments and the system has yet to reach the <code>max</code> number of active connections, Karafka will adaptively scale up the connections, one at a time. This gradual increase helps efficiently handle the increased load while avoiding abrupt changes that could lead to instability.</p> </li> <li> <p>Guaranteed Connectivity: At no point will there be a scenario where a given consumer group is left without connections. This persistent connectivity ensures that the consumer group is always able to receive and process messages, maintaining the flow of data and the system's reliability.</p> </li> </ul> <p>Dynamic Multiplexing in Karafka is about smartly adapting to the system's needs. It scales down to conserve resources when the load is low but remains ready to scale up as soon as the demand increases. This balance ensures that resources are used efficiently without compromising the system's ability to handle incoming data effectively. </p> <p> </p> <p> *This diagram illustrates the upscaling and downscaling flow for dynamic multiplexing.     </p> <p>Persistent Connection in Dynamic Mode</p> <p>Even when operating in dynamic mode, Karafka maintains a minimum of <code>min</code> active connections to Kafka at all times for each multiplexed subscription group. This ensures continuous communication and readiness to handle assignments, even when no partitions are currently assigned to it. Karafka's design guarantees that at least one line is open to Kafka, preventing complete shutdown of connections and ensuring stable, ongoing operation.</p> <p>Controlled Connection Adjustments</p> <p>Karafka uses a controlled approach to connection adjustments to maintain system stability when using Dynamic Multiplexing. Karafka will perform at most one change per minute for each consumer group. This deliberate pacing ensures that the system does not destabilize from rapid, frequent changes. As a result, while adapting to new conditions, the entire cluster may take some time to reach a stable and consistent state. This methodical approach is crucial for preserving the integrity and performance of the system as it dynamically adjusts to changing demands.</p>"}, {"location": "Pro-Multiplexing/#enabling-dynamic-multiplexing", "title": "Enabling Dynamic Multiplexing", "text": "<p>There are two things you need to do to fully facilitate dynamic multiplexing:</p> <ol> <li>Make sure you use <code>cooperative-sticky</code> rebalance strategy either globally or within the selected subscription group:</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      # Other kafka settings...\n      'partition.assignment.strategy': 'cooperative-sticky'\n    }\n  end\n\n  routes.draw do\n    # ...\n  end\nend\n</code></pre> <p>Always Use Cooperative-Sticky Rebalance with the Dynamic Mode</p> <p>The <code>cooperative-sticky</code> rebalance strategy is strongly recommended for optimal performance in dynamic mode. Without it, every change in connection count (upscaling or downscaling) will trigger a consumer group-wide rebalance, potentially causing processing delays. </p> <p><code>cooperative-sticky</code> strategy minimizes these disruptions by allowing more gradual and efficient rebalancing, ensuring smoother operation and more consistent throughput.</p> <p>Caution Against Using Dynamic Connection Multiplexing with Long-Running Jobs</p> <p>Avoid dynamic connection multiplexing for long-running jobs to prevent frequent rebalances, which can disrupt processing and extend execution times. Use stable, dedicated connections for better reliability and efficiency.</p> <ol> <li>Configure the <code>multiplexing</code> feature with a <code>min</code> flag set to a minimum number of connections you want to keep:</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # Establish at most three connections and shut down two if not needed. Start with 2.\n    subscription_group 'events' do\n      multiplexing(min: 1, max: 3, boot: 2)\n\n      topic :events do\n        consumer EventsConsumer\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Multiplexing/#benefits-of-the-dynamic-approach", "title": "Benefits of the Dynamic Approach:", "text": "<ul> <li> <p>Efficiency and Performance: By optimizing resource use and ensuring each connection has a balanced share of partitions, Karafka maintains high efficiency and performance even as data volumes and structures change.</p> </li> <li> <p>Cost-Effectiveness: Reduces operational costs by using resources only when necessary and as dictated by the structure and distribution of Kafka partitions.</p> </li> <li> <p>Scalability: Supports the dynamic and fluctuating nature of distributed data systems without manual intervention, ensuring that the system can seamlessly adapt to varying partition loads.</p> </li> <li> <p>Improved Parallelism: Enhances the system's ability to process data concurrently across multiple partitions and topics, resulting in faster processing times and higher throughput.</p> </li> </ul>"}, {"location": "Pro-Multiplexing/#limitations", "title": "Limitations", "text": "<p>Below, you can find specific considerations and recommendations related to using dynamic mode multiplexing in Karafka.</p>"}, {"location": "Pro-Multiplexing/#static-group-membership-and-dynamic-mode-multiplexing", "title": "Static Group Membership and Dynamic Mode Multiplexing", "text": "<p>We do not recommend using static group membership with Multiplexing operating in Dynamic mode. Multiplexing in Dynamic mode involves frequent changes in group composition, which conflicts with the nature of static group membership that relies on stable consumer identities. This can lead to increased complexity and more prolonged assignment lags.</p> <p>However, Multiplexing can be used without issues if Dynamic mode is not enabled. In this configuration, consumers maintain a more predictable group composition, which aligns well with the principles of static group membership and ensures a more stable and efficient operation.</p>"}, {"location": "Pro-Multiplexing/#conclusion", "title": "Conclusion", "text": "<p>Dynamic Multiplexing feature in Karafka represents a refined approach to managing connections with Kafka clusters. By focusing on partition assignments, Karafka ensures that resources are utilized efficiently, balancing performance needs with cost and resource conservation. This feature is handy for large-scale, distributed applications where partition loads vary significantly.</p>"}, {"location": "Pro-Multiplexing/#multiplexing-memory-usage-implications", "title": "Multiplexing Memory Usage Implications", "text": "<p>When employing Multiplexing, one must be aware of the potential impact on memory usage. Each additional connection established by a process consumes more memory as it maintains its own set of buffers, offsets, and other metadata related to its subscribed topics. In scenarios where many connections are established to handle high volumes of data, the memory footprint can increase significantly.</p> <p>This increased memory usage is particularly notable when messages are large or when a high volume of messages is being processed. The system needs to keep track of each message until it's successfully processed and acknowledged. We highly recommend using Multiplexing together with the Cleaner API when possible.</p>"}, {"location": "Pro-Multiplexing/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>High-Volume Data Streams: For applications dealing with massive influxes of data, multiple connections can fetch data concurrently, reducing latency and preventing bottlenecks.</p> </li> <li> <p>Resource Optimization: Distribute the load across multiple connections to utilize system resources more effectively, ensuring no single connection becomes a strain point.</p> </li> <li> <p>Improved Fault Tolerance: With multiple connections, if one fails, others continue processing, providing higher availability and reliability.</p> </li> <li> <p>Enhanced Throughput: For systems requiring high throughput, multiple connections can collectively handle more messages per second than a single connection.</p> </li> </ul>"}, {"location": "Pro-Multiplexing/#summary", "title": "Summary", "text": "<p>Karafka Multiplexing is a powerful tool designed to enhance the performance of your Kafka-based applications, especially under heavy load. Allowing multiple connections to the same topic from a single process provides a robust solution for handling high-volume data streams, optimizing resources, and ensuring high availability. Whether you're dealing with fluctuating workloads, aiming for high throughput, or seeking to improve fault tolerance, Multiplexing can provide significant benefits.</p> <p>Last modified: 2024-07-30 14:22:06</p>"}, {"location": "Pro-Non-Blocking-Jobs/", "title": "Non-Blocking Jobs", "text": "<p>Non-blocking jobs do not block polling of the underlying listener for other topic partitions. This ensures that a single Kafka connection can efficiently poll data from multiple topics and partitions.</p>"}, {"location": "Pro-Non-Blocking-Jobs/#using-non-blocking-jobs", "title": "Using Non-Blocking Jobs", "text": "<pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      non_blocking_job true\n    end\n  end\nend\n</code></pre> <p>Setting <code>non_blocking_job</code> to <code>true</code> within a route configuration indicates that the job should execute without blocking data polling from other topic partitions that utilize the same connection.</p>"}, {"location": "Pro-Non-Blocking-Jobs/#non-blocking-vs-long-running-jobs", "title": "Non-Blocking vs. Long-Running Jobs", "text": "<p>Non-Blocking Jobs are Long-Running Jobs, and <code>#non_blocking_job</code> is just an alias to <code>long_running_job</code>. The dual terminology, Long-Running Jobs and Non-Blocking Jobs, provides clarity and specificity in codebases. Using these terms appropriately allows developers to:</p> <ul> <li>Communicate the primary characteristics of the job directly through code.</li> <li>Make informed decisions about system architecture based on the job's nature.</li> <li>Maintain a self-documenting codebase that is easier to understand and manage.</li> </ul> <p>Please refer to the Long-Running Jobs documentation to better understand this feature.</p> <p>Last modified: 2024-01-17 18:31:44</p>"}, {"location": "Pro-Offset-Metadata-Storage/", "title": "Offset Metadata Storage", "text": "<p>Offset Metadata Storage is a feature within the Karafka framework allowing the addition of metadata to offsets. At its core, Offset Metadata Storage enables developers to attach custom metadata to message offsets when they are committed to the Kafka broker. This metadata, essentially a form of annotation or additional data, can then be retrieved and used for many purposes, enhancing message processing systems' capability, traceability, and intelligence.</p> <p>In traditional Kafka consumption, a message's offset indicates its position within a partition. While this is crucial for ensuring messages are processed in order, and no message is missed or duplicated, the standard offset mechanism doesn't provide context or additional information about the processing state or the nature of the message. Offset Metadata Storage fills this gap by allowing developers to store custom, context-rich data alongside these offsets.</p> <p>This feature can be compelling in complex processing scenarios where understanding the state or history of a message's processing is crucial. For instance, in a distributed system where messages undergo multiple stages of processing, Offset Metadata Storage can be used to attach processing stage information, timestamps, or identifiers of the services that have already processed the message. This additional layer of information opens up new possibilities for monitoring, debugging, and orchestrating complex message flows.</p>"}, {"location": "Pro-Offset-Metadata-Storage/#enabling-offset-metadata-storage", "title": "Enabling Offset Metadata Storage", "text": "<p>This feature is always enabled, ensuring you can leverage Offset Metadata Storage's benefits without additional setup. However, the behavior of Offset Metadata Storage can be fine-tuned using specific settings that control its caching and deserialization behavior. You can alter this feature behavior in the routing, similar to other features:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n      offset_metadata(\n        deserializer: JsonOffsetMetadataDeserializer.new\n      )\n    end\n  end\nend\n</code></pre> <p>If you plan to use same offset metadata deserializer throughout the whole system, we recommend using the <code>#defaults</code> API:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    defaults do\n      offset_metadata(\n        deserializer: JsonOffsetMetadataDeserializer.new\n      )\n    end\n\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n    end\n\n    topic :users_actions do\n      consumer UsersActionsConsumer\n    end\n  end\nend\n</code></pre> <p>The following configuration options are available for <code>#offset_metadata</code>:</p> Option Type Default Description <code>cache</code> Boolean <code>true</code>          Determines whether the metadata should be cached until a rebalance occurs. When set to <code>true</code>, the metadata will be cached, reducing the need to fetch or compute it until the consumer group rebalances repeatedly.        <code>deserializer</code> Object (Proc, Lambda, or any object responding to <code>#call</code>) String deserializer Specifies the deserializer that will process the raw metadata data. This deserializer should be an object that responds to the <code>#call</code> method. It receives the raw metadata and is responsible for returning the deserialized metadata. This setting is crucial when interpreting or processing the offset metadata in a specific format or structure."}, {"location": "Pro-Offset-Metadata-Storage/#working-with-offsets-metadata", "title": "Working with Offsets Metadata", "text": ""}, {"location": "Pro-Offset-Metadata-Storage/#writing-offset-metadata-alongside-marking", "title": "Writing Offset Metadata Alongside Marking", "text": "<p>Writing offset metadata is a straightforward process that can be achieved by passing a second string type argument to the <code>#mark_as_consumed</code> or <code>#mark_as_consumed!</code> method. This string can represent serialized data (like JSON) that contains the metadata you want to store or any other plain-text-based information:</p> <p>For instance, you can store the process identifier and processing details as part of its metadata like this:</p> <pre><code>def consume\n  messages.each do |message|\n    EventsStore.call(message)\n    @aggregator.mark(message)\n\n    mark_as_consumed(\n      message,\n      # Make sure that this argument is a string and in case of a JSON, do not\n      # forget to define a custom deserializer\n      {\n        process_id: Process.uid,\n        aggregated_state: @aggregator.to_h, \n      }.to_json\n    )\n  end\nend\n</code></pre> <p>In this example, <code>#mark_as_consumed!</code> marks a message as consumed and simultaneously stores its offset within the metadata. In this case, the metadata is a JSON string representing a hash with details that can be later used in a process that would receive this partition after the reassignment.</p>"}, {"location": "Pro-Offset-Metadata-Storage/#storing-offset-metadata-for-the-upcoming-marking", "title": "Storing Offset Metadata for the Upcoming Marking", "text": "<p>Automatic flows in Karafka refer to scenarios where offset marking is not explicitly invoked by the user but is handled internally by the system. This could be the case when automatic offset management is enabled or when certain features like the Dead Letter Queue (DLQ) are enabled. The system may implicitly use marking methods like <code>#mark_as_consumed</code> or <code>#mark_as_consumed!</code> without direct user intervention.</p> <p>The main challenge in automatic flows is ensuring that the relevant offset metadata is correctly associated with the message being processed, even though the user does not explicitly invoke the marking method. This is crucial for maintaining a coherent state and enabling more intelligent processing.</p> <p>To address this challenge, Karafka provides the <code>#store_offset_metadata</code> method. This method allows you to store offset metadata in advance, ensuring that it will be used with the following marking, regardless of whether the marking is triggered by user-based actions or by automatic internal processes.</p> <pre><code>def consume\n  messages.each do |message|\n    EventsStore.call(message)\n    @aggregator.mark(message)\n  end\n\n  # Store offset metadata alongside the next automatic marking as consumed\n  store_offset_metadata(\n    {\n      process_id: Process.uid,\n      aggregated_state: @aggregator.to_h, \n    }.to_json\n  )\nend\n</code></pre> <p>Automatic Application of Stored Offset Metadata</p> <p>Please be aware that the Karafka system will apply the stored offset metadata to the next message offset marked as consumed, regardless of whether the marking is manual or automatic, including automatic dispatches to the Dead Letter Queue (DLQ). This behavior might lead to unexpected metadata associations with messages, particularly in high-throughput or automated processing scenarios. It is highly recommended to thoroughly test and fully understand the implications of storing offset metadata in your specific use case, ensuring its application aligns with your message processing logic and does not disrupt the intended flow.</p>"}, {"location": "Pro-Offset-Metadata-Storage/#reading-offset-metadata", "title": "Reading Offset Metadata", "text": "<p>To retrieve the offset metadata, you can use the <code>#offset_metadata</code> method within your consumer. This method fetches the offset metadata and deserializes it using the configured deserializer.</p> <pre><code>def consume\n  # Use offset metadata only on the first run on the consumer\n  unless @recovered\n    @recovered = true\n\n    metadata = offset_metadata\n\n    # Do nothing if `#offset_metadata` was false. It means we have lost the assignment\n    return unless metadata\n\n    # Use the metadata from previous process to recover internal state\n    @aggregator.recover(\n      metadata.fetch('aggregated_state')\n    )\n  end\n\n  # Rest of the processing here...\nend\n</code></pre> <p>It's important to note the behavior of the <code>#offset_metadata</code> method about the <code>:cache</code> configuration option:</p> <ul> <li> <p>If <code>:cache</code> is set to <code>true</code>, the metadata will be cached until a rebalance occurs, preventing unnecessary round trips to Kafka and ensuring better performance.</p> </li> <li> <p>If <code>:cache</code> is set to <code>false</code>, each invocation of the <code>#offset_metadata</code> method will make a round trip to Kafka to fetch the data. It's generally not recommended to set <code>:cache</code> to <code>false</code> unless necessary.</p> </li> </ul> <p>The primary use case for offset metadata is to pass stateful information that can be crucial during rebalances or when the assignment of partitions changes. For example, in a distributed system where multiple consumers work on different partitions, a rebalance might change the partition assignment of consumers. In such cases, the offset metadata can provide the necessary context or state information to the newly assigned consumer, allowing it to pick up the processing exactly where the previous consumer left off.</p> <p>One crucial aspect is that the <code>#offset_metadata</code> method may return <code>false</code> if the given partition is no longer part of the consumer's assignment. This safety mechanism ensures that your consumer does not act on stale or irrelevant metadata. Always check the return value of <code>#offset_metadata</code> and handle the false case appropriately in your application logic.</p>"}, {"location": "Pro-Offset-Metadata-Storage/#offset-metadata-usage-from-within-filters", "title": "Offset Metadata Usage From Within Filters", "text": "<p>Offset Metadata Storage in Karafka enhances consumer instances' capability to manage and utilize message offsets and extends this functionality beyond the scope of a single consumer. This feature is particularly beneficial when you need to access offset metadata from different application components, such as within the Filtering API, to make more context-aware decisions based on the metadata associated with specific offsets.</p> <p>In scenarios where you need to retrieve offset metadata outside of the consumer instance, for instance, within Filters, to leverage the Filtering API, Karafka provides a flexible solution. This is especially useful when your processing logic requires insight into the message offsets' metadata at different stages or components of your application, not just within the consumer itself.</p> <p>As long as the current process retains the assignment of the given topic partition, you can retrieve the offset metadata from places other than the consumer instance. This means that even in filters or other parts of your Karafka application, you can access the metadata associated with any offset, ensuring a seamless and cohesive processing flow.</p> <p>To do so, you need to utilize the <code>Karafka::Pro::Processing::OffsetMetadata::Fetcher</code> object as follows:</p> <pre><code>offset_metadata = Karafka::Pro::Processing::OffsetMetadata::Fetcher.find(\n  # Karafka::Routing::Topic expected and NOT a string\n  topic,\n  # Partition id integer: 0, 1, 2, etc\n  partition\n)\n\nputs offset_metadata\n</code></pre> <p>Proper Arguments expectations for <code>Fetcher#find</code></p> <p>When using the <code>Fetcher#find</code> method, passing a <code>Karafka::Routing::Topic</code> object is essential, not just a string name of the topic. This specificity is required because a single topic might be associated with multiple consumer groups, each holding distinct offset metadata. Providing a <code>Karafka::Routing::Topic</code> object ensures accurate metadata retrieval by uniquely identifying the topic within its consumer group context, preventing any mix-up in metadata due to topic name overlaps across different consumer groups.</p>"}, {"location": "Pro-Offset-Metadata-Storage/#interaction-with-virtual-partitions", "title": "Interaction with Virtual Partitions", "text": "<p>Virtual Partitions in Karafka introduce an additional layer of complexity to how work is distributed from one topic partition to multiple virtual partitions. This intricacy extends to the behavior of Offset Metadata Storage, which can be configured to operate in two distinct ways. This behavior is governed by the <code>offset_metadata_strategy</code> flag, set during the configuration of virtual partitions in routing.</p> <p>When the Virtual Offset Manager is in the process of materializing an offset to store, it faces a choice regarding which metadata to use. The decision is based on the <code>offset_metadata_strategy</code> setting:</p> <ol> <li> <p>Current Strategy (<code>:current</code>) / Default: When set to current, the system uses the most recently provided metadata for the offset being materialized. This approach assumes that the latest metadata is the most relevant or accurate for the current processing state. However, it may only sometimes precisely align with the specific offset, especially in high-throughput or rapidly changing environments.</p> </li> <li> <p>Exact Strategy (<code>:exact</code>): Conversely, when the <code>offset_metadata_strategy</code> is set to <code>:exact</code>, the system uses the metadata associated with the exact offset being materialized. This ensures a tight coupling between an offset and its metadata, providing precision and ensuring each offset is associated with its specific contextual data. This can be particularly important in scenarios where the metadata's relevance is closely tied to the particular position within the partition.</p> </li> </ol> <p>The choice between <code>:current</code> and <code>:exact</code> strategies should be made based on the specific needs of your application and the nature of your message processing logic.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      virtual_partitions(\n        partitioner: -&gt;(message) { message.headers['order_id'] },\n        max_partitions: 5,\n        # Use the exact matching offset metadata strategy\n        offset_metadata_strategy: :exact\n      )\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Offset-Metadata-Storage/#limitations", "title": "Limitations", "text": "<p>Offset Metadata Storage comes with certain limitations you should be aware of:</p> <ol> <li> <p>Immutability of Stored Offset Metadata: Once offset metadata is stored for a specific offset value, it cannot be updated or overwritten. This means the system retains only the first set of metadata associated with a particular offset. If your processing logic involves updating or changing metadata over time, you must design your system to accommodate this limitation.</p> </li> <li> <p>Limitations on Storing Metadata for Previous Offsets: Storing metadata for an offset that precedes the current position is more complex. Suppose your processing logic requires you to associate metadata with a message that has already been processed (or an offset already stored); you would need to use the <code>#seek</code> method with the <code>reset_offset: true</code> parameter. This approach effectively rewinds the consumer to the desired offset, allowing you to store metadata.</p> </li> <li> <p>Offset Metadata Cache and Non-Persistent Consumers in Development: In a development environment, the offset metadata cache may only function as expected if consumers are set to operate in a persistent mode. Non-persistent consumers may lose the context or state between runs, leading to inconsistencies or the inability to retrieve cached metadata.</p> </li> <li> <p>Usage of <code>#store_offset_metadata</code> with Non-Persistent Consumers: Similar to the caching issue, the <code>#store_offset_metadata</code> method may face challenges if consumers are not operating in persistent mode. This method is designed to store metadata in anticipation of future marking, and its proper functioning relies on the consumer maintaining a consistent state. If the consumers are not persistent, the stored metadata might not be associated with the intended offset, leading to unexpected behavior.</p> </li> </ol> <p>Understanding and accommodating these limitations is essential for effectively leveraging the Offset Metadata Storage feature in your Karafka-based applications.</p>"}, {"location": "Pro-Offset-Metadata-Storage/#example-use-cases", "title": "Example Use Cases", "text": "<p>Here are some use cases from where Karafka's offset metadata storage feature can be beneficial:</p> <ol> <li> <p>Complex Event Processing: In systems where events pass through multiple stages, offset metadata can store state or stage information, helping to orchestrate the event processing flow and ensuring each event is handled appropriately at each stage.</p> </li> <li> <p>Tracing and Debugging: Attach unique trace IDs or log contextual information to messages as metadata, making it easier to trace the flow of specific messages through the system and debug complex distributed systems.</p> </li> <li> <p>Transactional Workflows: In workflows that mimic transactions across distributed services, offset metadata stores transaction states or IDs, ensuring consistency and recoverability across service boundaries.</p> </li> <li> <p>Failure Recovery and Retries: Store retry counts or error information as metadata, providing context for failure recovery mechanisms and enabling intelligent retry strategies based on the history of each message.</p> </li> <li> <p>Checkpointing in Stream Processing: Store the latest processed offset as metadata to create a checkpoint. In case of a system failure, the processing can resume from the last checkpoint, ensuring no data loss.</p> </li> <li> <p>Rebalance Recovery: During consumer group rebalances, use offset metadata to store context about the processing state. This allows the newly assigned consumer to resume work seamlessly without losing track of the processing state.</p> </li> <li> <p>Selective Message Replay: Use offset metadata to mark specific offsets for replaying messages for scenarios like error recovery or reprocessing after code changes without affecting the entire message stream.</p> </li> <li> <p>Conditional Processing Flow: Store indicators or flags as offset metadata to control the processing flow, turning on or off specific processing paths based on the offset metadata.</p> </li> </ol>"}, {"location": "Pro-Offset-Metadata-Storage/#summary", "title": "Summary", "text": "<p>Offset Metadata Storage enriches message processing by allowing custom metadata to be attached to message offsets, providing additional context for enhanced monitoring, debugging, and complex workflow orchestration. This feature supports flexible metadata access within consumer instances and across application components, ensuring precise retrieval and management of offset metadata.</p> <p>Last modified: 2024-01-17 16:20:15</p>"}, {"location": "Pro-Parallel-Segments/", "title": "Parallel Segments", "text": "<p>Parallel Segments are a feature in Karafka that enables you to process data from a single topic partition across multiple processes simultaneously. This approach allows you to achieve horizontal scaling for CPU-intensive workloads and those that cannot be effectively parallelized using Virtual Partitions alone.</p> <p>Unlike Virtual Partitions, which operate within a single consumer group and are optimized for IO-bound operations, Parallel Segments create multiple independent consumer groups that each process a subset of messages from the same topic partition. This makes it particularly effective for CPU-intensive processing scenarios where the computational overhead is the primary bottleneck, as well as in cases where data clustering makes Virtual Partitions ineffective.</p>"}, {"location": "Pro-Parallel-Segments/#how-parallel-segments-work", "title": "How Parallel Segments Work", "text": "<p>Parallel Segments operate by splitting a single consumer group into multiple sub-groups, each identified by a unique segment ID. Each sub-group processes only the messages assigned to it based on a partitioning strategy you define. This allows multiple processes to work on different segments of the same partition's data simultaneously.</p> <p>The key difference from Virtual Partitions is that each consumer group in the Parallel Segments setup maintains its connection to Kafka and downloads all messages from the topic partition. A filtering mechanism then determines which messages each segment should process based on your partitioning logic. </p>"}, {"location": "Pro-Parallel-Segments/#when-to-use-parallel-segments", "title": "When to Use Parallel Segments", "text": "<p>Parallel Segments are most beneficial in the following scenarios:</p> <ul> <li>CPU-Intensive Processing: When your message processing is primarily CPU-bound rather than IO-bound, and you need to distribute computational load across multiple processes</li> <li>Complex Computations: For workloads involving heavy mathematical calculations, data transformations, or algorithms that require significant CPU resources</li> <li>High-Volume Processing: When you need to process large volumes of messages and have multiple CPU cores or machines available</li> <li>Grouped Message Processing: When your batches contain large groups of messages with the same key (e.g., <code>user_id</code>, <code>session_id</code>) that cannot be effectively distributed via Virtual Partitions.</li> </ul> <p>Independent Error Handling Across Segments</p> <p>Since Parallel Segments distribute processing across multiple independent consumer groups, an error affecting one segment will not impact the processing of other segments. While one segment pauses and retries due to an error, the remaining segments will continue processing their assigned messages normally.</p> <p>This behavior differs from Virtual Partitions, where an error in any virtual partition affects the entire processing. Whether this independent error handling is desirable depends on your use case - it can provide better fault isolation. Still, it may also lead to processing lag between segments if errors are frequent in specific segments.</p> <p>Consider this characteristic when designing your error handling strategy and monitoring approach.</p>"}, {"location": "Pro-Parallel-Segments/#basic-configuration", "title": "Basic Configuration", "text": "<p>To enable Parallel Segments, you configure them at the consumer group level using the <code>parallel_segments</code> method:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Your configuration\n  end\n\n  routes.draw do\n    consumer_group :analytics do\n      parallel_segments(\n        count: 4,\n        partitioner: -&gt;(message) { message.headers['user_id'] }\n      )\n\n      topic :user_events do\n        consumer UserEventsConsumer\n      end\n    end\n  end\nend\n</code></pre> <p>Migration from Existing Consumer Groups</p> <p>Parallel Segments are not plug-and-play when migrating from an existing consumer group to a parallel segments setup. If you're converting an existing consumer group to use parallel segments, you must use the CLI <code>distribute</code> command to ensure that the parallel segments start processing from the correct offsets for each topic and partition.</p> <p>Without running the distribution command, the parallel segment consumer groups will start from the beginning of each topic (or from the latest offset, depending on your configuration), potentially causing message reprocessing or missing messages.</p> <p>This CLI step is only required for migrations. If you're creating a new consumer group with parallel segments enabled from the start, no additional setup is needed.</p>"}, {"location": "Pro-Parallel-Segments/#configuration-options", "title": "Configuration Options", "text": "<p>The <code>parallel_segments</code> method accepts the following options:</p> Parameter Type Default Description <code>count</code> Integer 1 Number of parallel consumer groups to create. Must be at least 1. Setting to 1 disables parallel segments. <code>partitioner</code> <code>#call</code> nil A callable that determines which segment a message should be assigned to. Must respond to <code>#call</code> and accept a message as argument. <code>reducer</code> <code>#call</code> Auto-generated A callable that maps the partitioner result to a segment ID (0 to count-1). If not provided, defaults to <code>-&gt;(key) { key.to_s.sum % count }</code>. <code>merge_key</code> String <code>-parallel-</code> The string used to generate unique consumer group names for each segment."}, {"location": "Pro-Parallel-Segments/#multi-topic-consumer-groups", "title": "Multi-Topic Consumer Groups", "text": "<p>Since the <code>parallel_segments</code> configuration applies at the consumer group level, when your consumer group contains multiple topics, you must implement a partitioner that can distinguish between topics. Use <code>message#topic</code> to route different topics using different partitioning strategies:</p> <pre><code>consumer_group :analytics do\n  parallel_segments(\n    count: 4,\n    partitioner: -&gt;(message) {\n      case message.topic\n      when 'user_events'\n        message.headers['user_id']\n      when 'order_events'\n        message.headers['order_id']\n      else\n        message.key\n      end\n    }\n  )\n\n  topic :user_events do\n    consumer UserEventsConsumer\n  end\n\n  topic :order_events do\n    consumer OrderEventsConsumer\n  end\nend\n</code></pre> <p>Without topic-aware partitioning, all topics in the consumer group would use the same partitioning logic, which may not be appropriate for different data types. This approach ensures that each topic can use its own optimal partitioning strategy while still benefiting from the parallel processing capabilities of Parallel Segments.</p> <p>For more complex scenarios, you can also extract the partitioning logic into a dedicated class:</p> <pre><code>class MultiTopicPartitioner\n  def call(message)\n    case message.topic\n    when 'user_events'\n      message.headers['user_id']\n    when 'order_events'\n      message.headers['order_id']\n    when 'session_events'\n      message.headers['session_id']\n    else\n      # Fallback strategy for unknown topics\n      message.key || message.offset\n    end\n  end\nend\n\nconsumer_group :analytics do\n  parallel_segments(\n    count: 4,\n    partitioner: MultiTopicPartitioner.new\n  )\n\n  # Multiple topics using the same partitioner\n  topic :user_events do\n    consumer UserEventsConsumer\n  end\n\n  topic :order_events do\n    consumer OrderEventsConsumer\n  end\n\n  topic :session_events do\n    consumer SessionEventsConsumer\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#partitioning-strategies", "title": "Partitioning Strategies", "text": "<p>The effectiveness of Parallel Segments depends heavily on your partitioning strategy. Since Parallel Segments work by filtering messages at the consumer group level prior to work delegation, choosing an optimal partitioner is crucial for both performance and proper data distribution.</p> <p>All-at-Once Deployment Required for Partitioner/Reducer Changes</p> <p>Any modifications to the <code>partitioner</code> or <code>reducer</code> configuration must be deployed using a non-rolling (full restart) deployment strategy. These components are critical to message routing logic, and changing them during a rolling deployment can lead to serious data consistency issues.</p> <p>Potential Issues with Rolling Deployments:</p> <ul> <li>Double Processing: Messages may be processed by both old and new segment assignments</li> <li>Missing Data: Some messages may not be processed by any segment during the transition</li> <li>Inconsistent State: Different consumer instances using different routing logic simultaneously</li> </ul> <p>Safe Deployment Process:</p> <ol> <li>Stop all consumer processes for the affected consumer group</li> <li>Wait for all in-flight processing to complete</li> <li>Deploy the updated partitioner/reducer configuration</li> <li>Start all consumer processes with the new configuration</li> </ol> <p>This ensures all parallel segments use consistent message routing logic from the moment processing resumes.</p>"}, {"location": "Pro-Parallel-Segments/#performance-considerations", "title": "Performance Considerations", "text": "<p>Parallel Segments work the best, when messages can be filtered prior to deserialization, which minimizes CPU overhead during the filtering process. To maximize this benefit, your partitioner should ideally use data that doesn't require payload deserialization.</p>"}, {"location": "Pro-Parallel-Segments/#using-message-headers-recommended", "title": "Using Message Headers (Recommended)", "text": "<p>The most efficient approach is to use Kafka message headers, which are available without triggering Karafka's lazy deserialization:</p> <pre><code>consumer_group :user_analytics do\n  parallel_segments(\n    count: 4,\n    # Efficient - uses headers, no deserialization required\n    partitioner: -&gt;(message) { message.headers['user_id'] }\n  )\n\n  topic :user_events do\n    consumer UserEventsConsumer\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#using-message-key", "title": "Using Message Key", "text": "<p>The message key is another efficient option as it's readily available:</p> <pre><code>consumer_group :order_processing do\n  parallel_segments(\n    count: 3,\n    # Efficient - message key is immediately available\n    partitioner: -&gt;(message) { message.key }\n  )\n\n  topic :orders do\n    consumer OrderProcessor\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#avoiding-payload-based-partitioning", "title": "Avoiding Payload-Based Partitioning", "text": "<p>While possible, using the message payload for partitioning defeats part of the performance benefits of Parallel Segments since it forces deserialization in the main thread:</p> <pre><code>consumer_group :analytics do\n  parallel_segments(\n    count: 4,\n    # Inefficient - forces deserialization before filtering\n    partitioner: -&gt;(message) { message.payload['user_id'] }\n  )\n\n  topic :user_events do\n    consumer UserEventsConsumer\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#testing-and-validating-distribution", "title": "Testing and Validating Distribution", "text": "<p>It's crucial to test your partitioning strategy because the combination of your partitioner and the reducer may not distribute data evenly.</p>"}, {"location": "Pro-Parallel-Segments/#understanding-the-default-reducer", "title": "Understanding the Default Reducer", "text": "<p>Parallel Segments use a two-step process:</p> <ol> <li>Partitioner: Extracts a partition key from each message</li> <li>Reducer: Maps the partition key to a segment ID (0 to count-1)</li> </ol> <p>The default reducer is: <code>-&gt;(partition_key) { partition_key.to_s.sum % count }</code></p> <p>This can lead to sub-optimal behaviours where different partition keys map to the same segment.</p>"}, {"location": "Pro-Parallel-Segments/#example-of-reducer-collision", "title": "Example of Reducer Collision", "text": "<pre><code># With segment count = 5, the default reducer may cause collisions:\nconsumer_group :analytics do\n  parallel_segments(\n    count: 5,\n    partitioner: -&gt;(message) { message.headers['user_id'] }\n  )\n\n  topic :user_events do\n    consumer UserEventsConsumer\n  end\nend\n\n# User IDs and their segment assignments with default reducer:\n# user_id \"0\" -&gt; \"0\".sum = 48 -&gt; 48 % 5 = 3 (segment 3)\n# user_id \"5\" -&gt; \"5\".sum = 53 -&gt; 53 % 5 = 3 (segment 3) \u2190 collision!\n# user_id \"14\" -&gt; \"14\".sum = 101 -&gt; 101 % 5 = 1 (segment 1)\n# user_id \"23\" -&gt; \"23\".sum = 101 -&gt; 101 % 5 = 1 (segment 1) \u2190 collision!\n</code></pre> <p>This means that despite configuring 5 segments, the data will only utilize 2 segments, leaving the remaining 3 segments idle as long as no other user IDs are present.</p>"}, {"location": "Pro-Parallel-Segments/#custom-reducer-for-better-distribution", "title": "Custom Reducer for Better Distribution", "text": "<p>If the default reducer doesn't provide good distribution, implement a custom one:</p> <pre><code>consumer_group :analytics do\n  parallel_segments(\n    count: 5,\n    partitioner: -&gt;(message) { message.headers['user_id'] },\n    # Custom reducer using hash for better distribution\n    reducer: -&gt;(partition_key) { Digest::MD5.hexdigest(partition_key.to_s).to_i(16) % 5 }\n  )\n\n  topic :user_events do\n    consumer UserEventsConsumer\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#best-practices", "title": "Best Practices", "text": "<ol> <li>Prefer Headers Over Payload: Always use message headers or keys when possible to avoid forced deserialization</li> <li>Test Distribution: Validate that your partitioner and reducer combination provides even distribution</li> <li>Consider Data Relationships: Ensure related messages are routed to the same segment</li> <li>Monitor Segment Load: Use logging to verify segments are receiving balanced workloads</li> <li>Start Simple: Begin with straightforward partitioning strategies and optimize based on observed performance</li> </ol>"}, {"location": "Pro-Parallel-Segments/#partitioning-error-handling", "title": "Partitioning Error Handling", "text": "<p>Parallel Segments include error handling for partitioning and reduction operations:</p> <p>If your partitioner or reducer throws an exception, Karafka will:</p> <ol> <li>Emit an error event via <code>error.occurred</code> with type <code>parallel_segments.partitioner.error</code></li> <li>Assign the problematic message to a fallback segment (typically segment 0)</li> <li>Continue processing other messages</li> </ol> <p>If you do not want this automatic error recovery behavior, you need to catch and handle exceptions within your partitioner or reducer code. By handling errors yourself, you can control whether to fail fast, use alternative logic, or implement custom fallback strategies instead of relying on Karafka's default error handling.</p>"}, {"location": "Pro-Parallel-Segments/#consumer-group-naming", "title": "Consumer Group Naming", "text": "<p>When you define a consumer group with Parallel Segments, Karafka automatically creates multiple consumer groups with unique names. For example:</p> <pre><code>consumer_group :analytics do\n  parallel_segments(count: 3)\n  # ... topics\nend\n</code></pre> <p>This creates three consumer groups:</p> <ul> <li><code>analytics-parallel-0</code></li> <li><code>analytics-parallel-1</code></li> <li><code>analytics-parallel-2</code></li> </ul> <p>Each group processes only the messages assigned to its segment ID.</p>"}, {"location": "Pro-Parallel-Segments/#accessing-segment-information", "title": "Accessing Segment Information", "text": "<p>You can access segment information within your consumer:</p> <pre><code>class MyConsumer &lt; ApplicationConsumer\n  def consume\n    segment_id = consumer_group.segment_id\n    original_group = consumer_group.segment_origin\n\n    Karafka.logger.info(\n      \"Processing #{messages.count} messages in segment #{segment_id} \" \\\n      \"of group #{original_group}\"\n    )\n\n    messages.each do |message|\n      process_message(message)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#parallel-segments-vs-virtual-partitions", "title": "Parallel Segments vs Virtual Partitions", "text": "<p>While both features aim to increase parallelization, they operate at different layers and solve different problems:</p>"}, {"location": "Pro-Parallel-Segments/#operational-differences", "title": "Operational Differences", "text": "<ul> <li> <p>Virtual Partitions: Distribute messages from a single batch across multiple threads within the same consumer group. They excel at IO-bound workloads where each message or sub-batches can be processed independently</p> </li> <li> <p>Parallel Segments: Filter and distribute messages across multiple consumer groups before batch processing. They excel at CPU-bound workloads and scenarios where batches contain large groups of related messages</p> </li> </ul>"}, {"location": "Pro-Parallel-Segments/#when-parallel-segments-excel", "title": "When Parallel Segments Excel", "text": "<p>Parallel Segments are particularly effective when dealing with batches that contain large groups of messages with the same key (e.g., <code>user_id</code>, <code>session_id</code>, <code>order_id</code>). Virtual Partitions may struggle to distribute such batches effectively since they cannot split groups of related messages that need to be processed together.</p> <p>For example, if a batch contains 1000 messages where 800 belong to user_id <code>123</code> and 200 belong to user_id <code>456</code>, Virtual Partitions cannot effectively parallelize this work since the messages with the same <code>user_id</code> should be processed together. Even with <code>max_messages</code> set to 100, Virtual Partitions would still struggle because they cannot split the grouped messages across virtual partitions - each virtual partition would end up processing messages from the same user anyway, effectively removing any option for Virtual Partitions to parallelize the work. Parallel Segments solve this by filtering at the consumer group level - one segment processes all messages for user <code>123</code> while another processes messages for user <code>456</code>.</p>"}, {"location": "Pro-Parallel-Segments/#combining-both-features", "title": "Combining Both Features", "text": "<p>Since Parallel Segments and Virtual Partitions operate at different layers, they can work together effectively:</p> <ul> <li> <p>First Layer (Parallel Segments): Filter and distribute messages across consumer groups based on logical grouping</p> </li> <li> <p>Second Layer (Virtual Partitions): Further parallelize the filtered messages within each consumer group across multiple threads</p> </li> </ul> <p>This combination allows you to handle both grouped message scenarios and achieve fine-grained parallelization within each group.</p>"}, {"location": "Pro-Parallel-Segments/#cli", "title": "CLI", "text": "<p>Karafka provides CLI commands to help you manage Parallel Segments consumer groups, particularly when migrating from regular consumer groups or when you need to collapse segments back to a single group.</p>"}, {"location": "Pro-Parallel-Segments/#available-commands", "title": "Available Commands", "text": "<p>The Parallel Segments CLI provides three main commands:</p> Command Description Use Case <code>karafka parallel_segments distribute</code> Distribute offsets from original consumer group to parallel segments Initial setup to enable parallel processing\u2022 Moving from single consumer group to multiple segments\u2022 Starting parallel processing for the first time <code>karafka parallel_segments collapse</code> Collapse parallel segments back to original consumer group Shutting down parallel processing\u2022 Consolidating back to single consumer group\u2022 Maintenance or troubleshooting scenarios <code>karafka parallel_segments reset</code> Reset (collapse then distribute) parallel segments Restarting parallel processing from scratch\u2022 Fixing offset inconsistencies\u2022 Reconfiguring segment distribution"}, {"location": "Pro-Parallel-Segments/#command-options", "title": "Command Options", "text": "<p>All commands support the following options:</p> Option Description Example <code>--groups</code> or <code>--consumer_groups</code> Names of consumer groups to operate on. If not provided, operates on all parallel segments groups. <code>--groups analytics order_processing</code> <code>--force</code> Force the operation even when conflicts are detected. <code>--force</code>"}, {"location": "Pro-Parallel-Segments/#distribute-command", "title": "Distribute Command", "text": "<p>The <code>distribute</code> command helps you migrate from a regular consumer group to parallel segments by distributing the original consumer group's offsets across the parallel segment consumer groups.</p> <pre><code># Distribute offsets for all parallel segments consumer groups\nkarafka parallel_segments distribute\n\n# Distribute offsets for specific consumer groups only\nkarafka parallel_segments distribute --groups analytics order_processing\n\n# Force distribution even if parallel segments already have offsets\nkarafka parallel_segments distribute --force\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#how-distribution-works", "title": "How Distribution Works", "text": "<ol> <li>Offset Collection: Collects committed offsets from the original consumer group</li> <li>Validation: Checks that parallel segment groups don't already have offsets (unless <code>--force</code> is used)</li> <li>Distribution: Applies the original consumer group's offsets to all parallel segment groups</li> <li>Preservation: Keeps the original consumer group intact as a backup</li> </ol>"}, {"location": "Pro-Parallel-Segments/#example-migration-workflow", "title": "Example Migration Workflow", "text": "<pre><code># Original configuration\nconsumer_group :analytics do\n  topic :user_events do\n    consumer UserEventsConsumer\n  end\nend\n\n# New configuration with parallel segments\nconsumer_group :analytics do\n  parallel_segments(\n    count: 4,\n    partitioner: -&gt;(message) { message.headers['user_id'] }\n  )\n\n  topic :user_events do\n    consumer UserEventsConsumer\n  end\nend\n</code></pre> <p>After updating your configuration, run the distribution command:</p> <pre><code>karafka parallel_segments distribute --groups analytics\n</code></pre> <p>This will: - Create consumer groups: <code>analytics-parallel-0</code>, <code>analytics-parallel-1</code>, <code>analytics-parallel-2</code>, <code>analytics-parallel-3</code> - Set their offsets to match the original <code>analytics</code> consumer group - Allow seamless continuation of processing from where the original group left off</p>"}, {"location": "Pro-Parallel-Segments/#collapse-command", "title": "Collapse Command", "text": "<p>The <code>collapse</code> command consolidates parallel segments back to the original consumer group by taking the lowest committed offset from all segments.</p> <pre><code># Collapse all parallel segments consumer groups\nkarafka parallel_segments collapse\n\n# Collapse specific consumer groups only\nkarafka parallel_segments collapse --groups analytics\n\n# Force collapse even when offsets are inconsistent\nkarafka parallel_segments collapse --force\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#how-collapse-works", "title": "How Collapse Works", "text": "<ol> <li>Offset Collection: Gathers committed offsets from all parallel segment groups</li> <li>Validation: Ensures offsets are consistent across segments (unless <code>--force</code> is used)</li> <li>Lowest Offset Selection: Selects the lowest committed offset for each topic partition</li> <li>Application: Sets the original consumer group's offset to the lowest offset found</li> </ol>"}, {"location": "Pro-Parallel-Segments/#important-considerations", "title": "Important Considerations", "text": "<ul> <li>Potential Reprocessing: Using the lowest offset may cause some messages to be reprocessed</li> <li>Offset Consistency: Without <code>--force</code>, the command will fail if parallel segments have inconsistent offsets</li> <li>Group Preservation: Parallel segment groups are not automatically removed after collapse</li> </ul>"}, {"location": "Pro-Parallel-Segments/#reset-command", "title": "Reset Command", "text": "<p>The <code>reset</code> command performs a complete reset by first collapsing parallel segments and then redistributing offsets:</p> <pre><code># Reset parallel segments (collapse then distribute)\nkarafka parallel_segments reset --groups analytics\n</code></pre> <p>This is equivalent to running:</p> <pre><code>karafka parallel_segments collapse --groups analytics\nkarafka parallel_segments distribute --groups analytics\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#validation-and-safety", "title": "Validation and Safety", "text": "<p>The CLI commands include several safety mechanisms:</p>"}, {"location": "Pro-Parallel-Segments/#offset-validation", "title": "Offset Validation", "text": "<pre><code># This will fail if parallel segments already have offsets\nkarafka parallel_segments distribute --groups analytics\n\n# Error output:\n# Parallel segment analytics-parallel-0 already has offset 1000 set for user_events#0\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#consistency-checks", "title": "Consistency Checks", "text": "<pre><code># This will fail if parallel segments have inconsistent offsets\nkarafka parallel_segments collapse --groups analytics\n\n# Error output:\n# Inconclusive offsets for user_events#0: 1000, 1050, 1100\n# Parallel segments for analytics have inconclusive offsets\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#force-override", "title": "Force Override", "text": "<p>Use <code>--force</code> to bypass safety checks:</p> <pre><code># Force distribution even with existing offsets\nkarafka parallel_segments distribute --groups analytics --force\n\n# Force collapse with inconsistent offsets (uses lowest)\nkarafka parallel_segments collapse --groups analytics --force\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#best-practices-for-cli-usage", "title": "Best Practices for CLI Usage", "text": ""}, {"location": "Pro-Parallel-Segments/#migration-strategy", "title": "Migration Strategy", "text": "<ol> <li>Test First: Always test the migration process in a non-production environment</li> <li>Backup Offsets: Document current offsets before running any commands</li> <li>Monitor Progress: Watch consumer lag and processing rates after migration</li> <li>Gradual Rollout: Consider migrating one consumer group at a time</li> </ol>"}, {"location": "Pro-Parallel-Segments/#monitoring-after-migration", "title": "Monitoring After Migration", "text": "<pre><code>class MyConsumer &lt; ApplicationConsumer\n  def consume\n    # Log segment information for monitoring\n    Karafka.logger.info(\n      \"Segment #{consumer_group.segment_id} processing #{messages.count} messages\"\n    )\n\n    messages.each do |message|\n      process_message(message)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#cleanup-strategy", "title": "Cleanup Strategy", "text": "<p>After successful migration and operation, you may want to clean up:</p> <pre><code># Remove original consumer group (optional)\n# Note: This requires using Kafka's admin tools or Karafka's Admin API\n# The CLI does not automatically remove consumer groups\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#bandwidth-considerations", "title": "Bandwidth Considerations", "text": "<p>The primary trade-off with Parallel Segments is network bandwidth usage. Since each consumer group downloads all messages from the topic partition, network traffic is multiplied by the number of segments. Consider this when:</p> <ul> <li>Using cloud-based Kafka services where you pay for network transfer</li> <li>Processing high-volume topics with large messages</li> <li>Operating in bandwidth-constrained environments</li> </ul>"}, {"location": "Pro-Parallel-Segments/#combining-with-other-features", "title": "Combining with Other Features", "text": ""}, {"location": "Pro-Parallel-Segments/#parallel-segments-with-virtual-partitions", "title": "Parallel Segments with Virtual Partitions", "text": "<p>While not commonly needed, you can combine Parallel Segments with Virtual Partitions for extremely high-throughput scenarios:</p> <pre><code>consumer_group :high_throughput do\n  parallel_segments(\n    count: 2,\n    partitioner: -&gt;(message) { message.headers['category'] }\n  )\n\n  topic :high_volume_data do\n    consumer HighVolumeConsumer\n\n    virtual_partitions(\n      partitioner: -&gt;(message) { message.headers['user_id'] },\n      max_partitions: 4\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#parallel-segments-with-dead-letter-queue", "title": "Parallel Segments with Dead Letter Queue", "text": "<p>Parallel Segments work seamlessly with Dead Letter Queue:</p> <pre><code>consumer_group :resilient_processing do\n  parallel_segments(\n    count: 3,\n    partitioner: -&gt;(message) { message.headers['type'] }\n  )\n\n  topic :risky_data do\n    consumer RiskyDataConsumer\n\n    dead_letter_queue(\n      topic: 'failed_messages',\n      max_retries: 3\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Parallel-Segments/#conclusion", "title": "Conclusion", "text": "<p>Parallel Segments provide a way to scale CPU-intensive message processing in Karafka. By distributing work across multiple consumer groups, you can achieve horizontal scaling for computationally heavy workloads while maintaining message ordering guarantees within each segment.</p> <p>While the trade-off in network bandwidth usage is important to consider, the performance gains for certain workloads often justify this cost. Combined with Karafka's other features like Virtual Partitions, Dead Letter Queue, and monitoring, Parallel Segments offer a robust solution for high-throughput, CPU-intensive message processing scenarios.</p> <p>Remember to leverage the CLI commands for smooth migrations and ongoing management of your parallel segments deployment. The safety mechanisms built into these commands help prevent common pitfalls and ensure reliable operation of your parallel processing infrastructure.</p> <p>Last modified: 2025-07-08 18:18:53</p>"}, {"location": "Pro-Periodic-Jobs/", "title": "Periodic Jobs", "text": "<p>Periodic Jobs are a feature designed to allow consumers to perform operations at regular intervals, even without new data. This capability is particularly useful for applications that require consistent action, such as window-based operations or maintaining system readiness.</p>"}, {"location": "Pro-Periodic-Jobs/#using-periodic-jobs", "title": "Using Periodic Jobs", "text": "<p>To leverage this functionality, you must enable and configure it within your routing and implement the corresponding <code>#tick</code> method in your consumer. Here's how to get started:</p>"}, {"location": "Pro-Periodic-Jobs/#enabling-periodic-jobs-in-routing", "title": "Enabling Periodic Jobs in Routing", "text": "<p>To enable periodic jobs for a particular topic, specify it in the routing. This can be done in two ways, depending on your needs:</p> <ol> <li>Using Default: To use the default settings, enable periodic jobs for your topic as follows:</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      # Tick at most once every five seconds\n      periodic true\n    end\n  end\nend\n</code></pre> <ol> <li>Custom Arguments: If the default settings do not meet your requirements, you can always specify each of the options:</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      periodic(\n        # Tick at most once every 100 milliseconds\n        interval: 100,\n        # When paused (for any reason) run\n        during_pause: true,\n        # When consumption error occurred and we back-off and wait, do not run\n        during_retry: false\n      )\n    end\n  end\nend\n</code></pre> <p>The following options are available:</p> Option Type Default Description <code>:interval</code> Integer <code>5000</code> Minimum interval in milliseconds to run periodic jobs on the given topic. <code>:during_pause</code> Boolean, nil <ul> <li> <code>true</code> for regular jobs.         </li> <li> <code>false</code> for LRJ </li> </ul> Specifies whether periodic jobs should run when the partition is paused. <code>:during_retry</code> Boolean, nil <code>false</code> Indicates whether periodic jobs should run during a retry flow after an error. Note that <code>:during_pause</code> must also be <code>true</code> for this to function. The default is not to retry during retry flow unless explicitly set."}, {"location": "Pro-Periodic-Jobs/#implementing-the-tick-method", "title": "Implementing the <code>#tick</code> Method", "text": "<p>After enabling periodic jobs in the routing, you must implement a <code>#tick</code> method in your consumer. This method is where you define the tasks to be performed at each tick. Here's an example of a consumer with a <code>#tick</code> method:</p> <pre><code>class Consumer &lt; Karafka::BaseConsumer\n  def consume; end\n\n  def tick\n    puts \"Look, mom, I'm periodic!\"\n  end\nend\n</code></pre> <p>In this example, the <code>#tick</code> method prints a message, but it could perform any task in a real-world scenario, such as processing data, sending alerts, or updating a database. There are no additional actions required beyond implementing this method.</p>"}, {"location": "Pro-Periodic-Jobs/#compatibility-with-kafka-operations", "title": "Compatibility with Kafka Operations", "text": "<p>One of the significant advantages of periodic jobs is that they are fully compatible with regular Kafka-related operations. This means you can perform standard actions within your periodic jobs as you would in a stable consumption context. These operations include:</p> Operation Description Seeking Changing the consumer's position to a specific offset within a partition. Pausing and Resuming Temporarily halting the consumption of a topic and then resuming it. Marking Messages as Consumed Indicating that a message has been successfully processed and should not be re-consumed. Transactions Managing a group of producer and consumer actions as a single atomic operation. <p>This compatibility ensures that integrating periodic jobs into your application does not limit your ability to interact with Kafka in the usual ways. It provides a powerful combination of regular message consumption with the ability to perform scheduled tasks, making your applications more flexible and reliable.</p> <p>Below, you can find an example of a feature-flag / toggle-based processing that allows us to pause and resume processing depending on the toggle state:</p> <pre><code>class Consumer &lt; Karafka::BaseConsumer\n  # Week in milliseconds for pausing\n  WEEK_IN_MS = 604_800_000\n\n  def consume\n    # If flipper is off and we should not process this topic\n    unless Flipper.enabled?(:topics_processing, topic.name)\n      # We need to make sure we indicate that sleeping is happening because we explicitely\n      # wanted it and not because of some system events (LRJ, error, etc)\n      @paused_because_of_flipper = true\n      # Pause for a long time...\n      pause(messages.first.offset, WEEK_IN_MS)\n      return\n    end\n\n    messages.each do |message|\n      Events.persist!(message.payload)\n    end\n  end\n\n  # During tick we can just check if flipper is enabled and we should resume\n  def tick\n    # Do nothing if pausing did not happen because of flipper but for other reasons\n    return unless @paused_because_of_flipper\n    # Do nothing if we are still not supposed to process this topic\n    return unless Flipper.enabled?(:topics_processing, topic.name)\n\n    @paused_because_of_flipper = false\n    # Resume processing of this topic when it is time\n    resume \n  end\nend\n</code></pre>"}, {"location": "Pro-Periodic-Jobs/#polling-and-ticking-interdependency", "title": "Polling and Ticking Interdependency", "text": "<p>Understanding the interplay between polling and ticking is crucial for effectively managing both message consumption and the execution of periodic jobs. This interdependency ensures that the system not only fetches and processes messages efficiently but also executes regular, time-sensitive tasks even during low or no data activity periods.</p> <p>Timing of Periodic Jobs</p> <p>Remember that periodic jobs in Karafka may start immediately after the Kafka assignment is ready, potentially before the first batch of messages is received. Ensure your <code>#tick</code> method is designed to handle this scenario effectively.</p> <p>In such scenarios, using the <code>#used?</code> method is always recommended. This can be done using a conditional statement that checks if there was even a single batch consumed or scheduled for consumption.</p>"}, {"location": "Pro-Periodic-Jobs/#polling-mechanism-and-its-impact-on-ticking", "title": "Polling Mechanism and Its Impact on Ticking", "text": "<p>Polling in Karafka refers to the process of retrieving messages from Kafka. This occurs at intervals defined by the <code>max_wait_time</code>, which dictates how long the consumer should wait for messages. After each poll operation:</p> <ul> <li>Karafka stops polling momentarily and yields the fetched messages to the scheduler for distribution and execution.</li> <li>In cases where no data is fetched for a given topic partition and periodic jobs are enabled, a periodic job may be scheduled unless the interval is too short.</li> </ul>"}, {"location": "Pro-Periodic-Jobs/#ticking-and-periodic-jobs-scheduling", "title": "Ticking and Periodic Jobs Scheduling", "text": "<p>Ticking relates to the scheduling of periodic jobs at regular intervals. The nuances of how ticking works within the polling mechanism are crucial:</p> <ul> <li> <p>Interdependency: The polling intervals directly impact the ticking of periodic jobs. If the polling is infrequent or the message processing time is lengthy, the ticking for periodic jobs will be delayed correspondingly.</p> </li> <li> <p>Minimum Interval Setting: The interval parameter in periodic jobs denotes the minimum time between executions for a single topic partition. However, due to the interdependency with polling, the actual interval might be longer. Each poll can trigger only one periodic job execution per topic partition.</p> </li> <li> <p>Processing Time Impact: The duration of message processing can significantly affect ticking. For instance, if a subscription group's message processing blocks polling for an extended period, even a short-interval periodic job will be delayed, running less frequently than configured.</p> </li> </ul>"}, {"location": "Pro-Periodic-Jobs/#managing-overlap-in-periodic-and-long-running-jobs", "title": "Managing Overlap in Periodic and Long-Running Jobs", "text": "<p>Karafka doesn't start Periodic Jobs for a given topic partition when a Long-Running Job (LRJ) is active by default. However, this doesn't prevent an LRJ from initiating while a periodic job runs, as these are non-blocking and can overlap. While this overlapping can be advantageous for independent tasks, it might cause issues for tasks sharing resources or influencing each other.</p> <p>To prevent concurrent executions and potential conflicts, consider using the <code>#synchronize</code> method. This feature ensures that only one job instance runs simultaneously, safeguarding against overlapping. </p> <pre><code>class LongRunningConsumer &lt; ApplicationConsumer\n  def consume\n    # This will ensure that tick and consume won't run at the same time\n    synchronize do\n      compute_current_state\n      flush_to_db\n    end\n  end\n\n  def tick do\n    synchronize do\n      compute_current_state\n      flush_to_db\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Periodic-Jobs/#strategies-for-accurate-and-independent-ticking", "title": "Strategies for Accurate and Independent Ticking", "text": "<p>To maintain accurate and independent ticking, irrespective of polling intervals and message processing times, consider the following approaches:</p> <ul> <li> <p>Independent Subscription Groups: Utilize separate subscription groups for different topics or partitions. This isolates the periodic jobs, preventing the processing time of one group from affecting the ticking of another.</p> </li> <li> <p>Connection Multiplexing: Implement multiple connections to Kafka within the same application. This ensures that lengthy processing in one part of your application doesn't delay the execution of periodic jobs in another.</p> </li> <li> <p>Long-Running and Non-Blocking Jobs (LRJ/NBJ) Usage: For scenarios where you are dealing with long-running or non-blocking jobs, it's beneficial to design your tasks so they do not block the main thread. In these cases, the work being processed is non-blocking, meaning that polling and ticking can continue at their configured intervals without being delayed by the processing times of given messages. For topics associated with such jobs, periodic jobs will also become non-blocking and will execute at a consistent and steady frequency. This approach ensures that message processing and periodic tasks can occur seamlessly and independently, maintaining system responsiveness and reliability.</p> </li> </ul>"}, {"location": "Pro-Periodic-Jobs/#persistence-of-periodic-jobs-during-pauses", "title": "Persistence of Periodic Jobs During Pauses", "text": "<p>By default, Periodic Jobs in Karafka continue to run even when the consumption of a given topic partition is paused. This feature ensures that scheduled tasks maintain their rhythm and execute as configured, regardless of the consumer's state.</p> <p>Message consumption halts temporarily when a topic partition is paused, typically to manage system load or during maintenance. However, the periodic jobs associated with that partition aren't tied directly to the message flow. They operate on a time-based schedule, independent of whether messages are being consumed. As a result, even in the paused state, periodic jobs continue to tick and execute their tasks.</p> <p>This behavior is particularly beneficial for maintaining consistent operations like monitoring, reporting, or routine maintenance that need to continue irrespective of the consumer's state. It provides a layer of reliability and consistency, ensuring that vital tasks are not missed and that the system remains up-to-date and responsive.</p> <p>However, it's crucial to be aware of this persistence to manage system resources effectively and avoid unexpected behavior. Knowing that periodic jobs run continuously allows for better planning and utilization of system capabilities, ensuring that the tasks performed during pauses are necessary and optimized for efficiency.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      # Do not run when given topic partition is paused\n      periodic during_pause: false\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Periodic-Jobs/#behavior-of-periodic-jobs-during-consumption-retries", "title": "Behavior of Periodic Jobs During Consumption Retries", "text": "<p>By default, Karafka's Periodic Jobs are suspended during retry periods following an error in message consumption. This pause in periodic activities helps focus system resources on resolving the error. However, this behavior can be customized based on your application's needs.</p> <p>Adjust the <code>during_retry</code> parameter in the configuration to continue periodic tasks even during retries. Setting this to <code>true</code> allows Periodic Jobs to run, potentially aiding in error recovery or ensuring critical operations persist. However, consider the potential impact and complexity this might add to error handling.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      # Run when `#consume` failed and partition is paused and\n      # back-off has been applied\n      periodic during_retry: true\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Periodic-Jobs/#behaviour-on-errors", "title": "Behaviour on Errors", "text": "<p>In Karafka, handling errors occurring in periodic jobs within the <code>#tick</code> method is distinct from typical error-handling mechanisms like dead letter queues (DLQ) or retries applicable to the <code>#consume</code> method. The primary rationale for this approach is that the <code>#tick</code> method's functionality is not contingent on processing more data. Instead, it's designed to execute operations at predetermined intervals, irrespective of data presence.</p> <p>When an error occurs within the <code>#tick</code> method, it doesn't trigger the conventional DLQ or retry mechanisms. This is because periodic jobs are inherently different from standard message consumption; they are not associated with a batch of messages that can be retried. Instead, they are period-triggered actions meant to occur regularly. If an error happens during the execution of a <code>#tick</code> method, it doesn't prevent the method from being invoked again at the next scheduled interval. The system is designed to continue with the subsequent ticks, ensuring that periodic tasks maintain their rhythm.</p> <p>Despite not being subject to DLQ or retries, it's crucial to monitor and manage errors effectively. In Karafka, errors within the <code>#tick</code> method are published to the <code>error.occurred</code> notification channel. This allows for centralized monitoring and handling of errors. Each error notification event carries a <code>:type</code> set to <code>consumer.tick.error</code>, distinguishing it clearly as an error from the periodic job's tick operation. This explicit categorization aids in pinpointing the source of errors and facilitates more efficient debugging and error-handling strategies.</p> <p>By understanding and leveraging this behavior, developers can ensure that their periodic jobs in Karafka are robust and operate smoothly, even in the face of intermittent errors. It also underscores the importance of monitoring and responding to the error.occurred notifications to maintain the health and reliability of the system.</p> <pre><code>class Consumer &lt; Karafka::BaseConsumer\n  def consume; end\n\n  def tick\n    raise StandardError\n  end\nend\n\n# Subscribe to only monitor periodic jobs ticking errors\nKarafka.monitor.subscribe 'error.occurred' do |event|\n  type = event[:type]\n\n  if type == 'consumer.tick.error'\n    error = event[:error]\n    details = (error.backtrace || []).join(\"\\n\")\n\n    puts \"Oh no! An error: #{error} of type: #{type} occurred while ticking!\"\n    puts details\n  end\nend\n</code></pre>"}, {"location": "Pro-Periodic-Jobs/#conclusion", "title": "Conclusion", "text": "<p>Karafka's relationship between polling and ticking is vital for application design. While they are interconnected, with polling intervals impacting the timing of periodic jobs, understanding and leveraging this relationship can lead to more efficient and reliable applications. By acknowledging this interdependency and employing strategies like independent subscription groups or connection multiplexing, you can ensure that periodic tasks are executed as expected, even in complex systems with varying data flow and processing requirements.</p>"}, {"location": "Pro-Periodic-Jobs/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Toggle Switching for Pausing and Resuming Processing: Periodic jobs can incorporate a toggle mechanism for pausing and resuming data processing. This is useful for halting operations during maintenance, updates, or expected downtime. For example, a periodic job can automatically disable and later re-enable processing in high-load periods where real-time processing may hinder system performance. This method ensures flexibility and responsiveness, simplifying the management of processing activities without manual intervention or intricate scheduling.</p> </li> <li> <p>Regular Data Reporting: Generating reports at fixed intervals, even during periods of low or no data activity, ensuring that reports are delivered on schedule.</p> </li> <li> <p>Heartbeat Checks: Sending heartbeat messages or performing regular health checks to monitor system status and ensure all components function correctly, even during idle periods.</p> </li> <li> <p>Scheduled Maintenance Tasks: Performing routine database maintenance tasks, such as indexing or cleanup, regularly without depending on incoming data streams.</p> </li> <li> <p>Updating Static Datasets: Refreshing static or slowly changing datasets that are used in conjunction with real-time data, ensuring that all processing is done against the most current information.</p> </li> <li> <p>Micro-batch Processing: Instead of real-time processing, some systems might benefit from micro-batch processing, where messages are collected over time and then processed together. Periodic jobs can trigger these batches at defined intervals.</p> </li> <li> <p>Real-time Stock Market Analysis: Maintain a rolling window of the most recent 30 minutes of stock market data to analyze patterns and fluctuations. Whether or not new trades are coming in, periodically assess this window to provide traders with up-to-date insights and alerts on potential investment opportunities or risks.</p> </li> <li> <p>Social Media Sentiment Tracking: Keep a continuous window of the latest hour of social media posts related to a brand or product. Periodically analyze this data to gauge customer sentiment and brand perception, ensuring marketing teams have current feedback to inform strategies and respond to public sentiment trends, regardless of the volume of new posts.</p> </li> </ul>"}, {"location": "Pro-Periodic-Jobs/#summary", "title": "Summary", "text": "<p>Periodic Jobs in Karafka offer a versatile way to perform scheduled tasks at regular intervals, independent of message flow. This feature particularly benefits applications requiring consistent actions, like routine maintenance, data reporting, or heartbeat checks, even during low or no data activity periods.</p> <p>Periodic Jobs in Karafka provide a powerful tool for ensuring your application remains active and responsive, performing necessary tasks regularly. Whether you're maintaining system readiness, generating reports, or monitoring system status, periodic jobs can help keep your system efficient and reliable.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Piping/", "title": "Piping", "text": "<p>In complex systems where applications act as part of a larger data-processing ecosystem, efficiently passing data between processes is crucial. Karafka Pro includes a robust feature for message piping, allowing applications to forward processing results seamlessly to subsequent stages or other applications. This document explores the concept, use cases, and benefits of using message piping in Karafka Pro.</p>"}, {"location": "Pro-Piping/#what-is-message-piping", "title": "What is Message Piping?", "text": "<p>Message piping in Karafka refers to forwarding messages from one topic to another or to different applications within the same Kafka ecosystem. This is akin to Unix-like systems where commands can pipe output to other commands, facilitating complex data transformations and workflow orchestrations.</p>"}, {"location": "Pro-Piping/#why-use-message-piping", "title": "Why Use Message Piping?", "text": "<ul> <li> <p>Decoupling Components: Message piping helps decouple system components. Data producers do not need to know about the consumers, allowing for independent scaling and maintenance.</p> </li> <li> <p>Enhanced Data Flow Management: It provides a controlled way to manage data flow, ensuring data integrity and traceability through enhanced metadata headers similar to those used in enhanced Dead Letter Queues (DLQ).</p> </li> <li> <p>Efficiency: Message piping minimizes the overhead associated with messages piping by automatically applying a partitioning strategy and using raw payload and headers without any deserialization.</p> </li> </ul>"}, {"location": "Pro-Piping/#usage", "title": "Usage", "text": "<p>Karafka Pro provides built-in methods to facilitate message piping directly from within your consumers. These methods handle the complexities of Kafka message attributes, ensuring messages are forwarded with all necessary context intact.</p> <pre><code>class PaymentConsumer &lt; ApplicationConsumer\n  def consume\n    payment_process(messages.payloads)\n\n    # After processing, pipe messages to the next service\n    pipe_many_async(\n      topic: 'stock_check',\n      messages: messages\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Piping/#public-piping-methods", "title": "Public Piping Methods", "text": "<p>The following table describes the public methods available for message piping:</p> Method Description <code>pipe_async</code> Pipes a message to a specified topic asynchronously. <code>pipe_sync</code> Pipes a message to a specified topic synchronously. <code>pipe_many_async</code> Pipes multiple messages to a specified topic asynchronously. <code>pipe_many_sync</code> Pipes multiple messages to a specified topic synchronously."}, {"location": "Pro-Piping/#enhancing-messages-with-enhance_pipe_message", "title": "Enhancing Messages with <code>#enhance_pipe_message</code>", "text": "<p>You can define a <code>#enhance_pipe_message</code> method in your consumer to alter the message before it is piped. This method allows you to add or modify headers, change the payload, or apply any other transformations before forwarding the message.</p>"}, {"location": "Pro-Piping/#automatic-partition-key-selection", "title": "Automatic Partition Key Selection", "text": "<p>In Karafka, the message key selection for message piping is designed to maintain a high degree of ordering and integrity, especially when messages are forwarded to topics with differing partition counts. This ensures that correlated messages preserve their strong ordering, which is critical for processing sequences of interdependent events.</p> <p>The process of message key selection is handled as follows:</p> <ul> <li> <p>Key Available: If the source message includes a key, this key is reused when the message is piped. This ensures the message follows the same partitioning logic as before, maintaining its order and correlation with related messages.</p> </li> <li> <p>Key Not Available: When no key is present, Karafka automatically generates a partition key based on the partition number from which the message was originally consumed. This automatic key generation ensures that messages maintain their ordering by being routed to the same relative partition in the new topic.</p> </li> </ul> <p>This approach is particularly useful in scenarios where the number of partitions in the target topic differs from the source. It ensures that the message flow remains consistent and predictable, supporting scenarios like ordered processing and stateful computations where the order of messages is crucial.</p>"}, {"location": "Pro-Piping/#traces-in-headers", "title": "Traces in Headers", "text": "<p>To enhance debuggability, the following trace headers are automatically included when a message is piped:</p> <ul> <li><code>source_topic</code>: The topic from which the message originated.</li> <li><code>source_partition</code>: The partition within the original topic.</li> <li><code>source_offset</code>: The offset within the partition.</li> <li><code>source_consumer_group</code>: The consumer group ID that processed the message.</li> </ul> <p>These headers provide vital information for troubleshooting and understanding the message flow across different parts of your system.</p> <p>Trace Headers Replacement During Piping</p> <p>When piping data from a Dead Letter Queue (DLQ) topic, be aware that similar trace headers are already present due to the DLQ's mechanisms. During the piping operation, these headers will be overwritten with the new trace information generated by the piping process. This may impact traceability if the source DLQ headers are required for further analysis or debugging.</p>"}, {"location": "Pro-Piping/#exactly-once-semantics-and-transactions", "title": "Exactly-Once Semantics and Transactions", "text": "<p>Karafka Pro supports exactly-once semantics within its message piping feature, ensuring that messages are processed and forwarded precisely once, even in case of failures or retries. This is crucial in scenarios where message duplication or loss could lead to inconsistencies or erroneous behaviors in downstream systems.</p> <p>Here's how you can utilize transactions with message piping:</p> <pre><code>class Consumer &lt; ApplicationConsumer\n  def consume\n    transaction do\n      # Piping messages to the next topic as part of the transaction\n      pipe_many_async(topic: 'target', messages: messages)\n\n      # Marking the last message as consumed after successful piping\n      mark_as_consumed(messages.last)\n    end\n  end\nend\n</code></pre> <p>Transactional piping ensures that message forwarding completes successfully or not at all if any part of the transaction fails. This is particularly useful when your application logic involves multiple steps and needs to avoid partial execution. The <code>#mark_as_consumed</code> method acknowledges that the last message has been fully processed and its results confirmed, which is crucial for correct offset management in Karafka.</p>"}, {"location": "Pro-Piping/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Real-Time Data Processing: In a real-time analytics system, events can be piped from initial ingestion points directly to processing engines and then to dashboards or alert systems. For example, streaming IoT device data can be processed to detect anomalies and piped to different topics for immediate actions or longer-term analytics.</p> </li> <li> <p>Log Aggregation: Logs from various services can be piped into a centralized logging service, indexed and made searchable. This use case is crucial for debugging and monitoring large-scale distributed systems.</p> </li> <li> <p>Event-Driven Architecture: In an event-driven architecture, different services react to events as they occur. Message piping is essential for forwarding events to relevant services without creating tightly coupled integrations.</p> </li> </ul>"}, {"location": "Pro-Piping/#conclusion", "title": "Conclusion", "text": "<p>Karafka Pro's message piping feature significantly enhances the flexibility and efficiency of Kafka-based systems. Facilitating smooth data transfer between components without tight coupling enables the creation of scalable, maintainable, and robust distributed systems. Note, however, that this feature is part of Karafka Pro and requires a commercial license.</p> <p>Last modified: 2025-06-16 14:36:08</p>"}, {"location": "Pro-Rate-Limiting/", "title": "Rate Limiting", "text": "<p>Rate Limiting mechanism that allows controlling the speed at which messages are consumed from a Kafka topic. By limiting the consumption rate, Karafka can reduce the impact of high message throughput on the CPU and other resources, preventing a potential bottleneck in the processing pipeline.</p> <p>The Rate Limiting mechanism in Karafka is implemented using a window throttler that monitors the message consumption rate and pauses the consumption of a given topic partition when the configured limit is reached. The window throttler maintains a sliding window of the last processed messages and calculates the consumption rate by dividing the number of messages consumed in the window by the window size. If the consumption rate exceeds the configured limit, the throttler pauses the consumption of the topic for some time until the consumption rate falls below the limit.</p> <p>The Rate Limiting feature in Karafka can be configured on a per-topic basis. This means that different topics can have different consumption limits depending on the message volume and processing requirements. For example, a high-priority topic can have a lower limit than a low-priority topic to ensure that important messages are processed faster.</p>"}, {"location": "Pro-Rate-Limiting/#enabling-rate-limiting", "title": "Enabling Rate Limiting", "text": "<p>To enable the Rate Limiting feature in Karafka, you need to add the <code>throttle</code> option to your Karafka routing configuration. Here's an example of how to do that:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders do\n      consumer OrdersConsumer\n      # Allow for processing of at most 100 messages\n      # in a 60 second rolling window\n      throttle(limit: 100, interval: 60_000)\n    end\n  end\nend\n</code></pre> <p>Rate Limiting Impact on Internal Queues</p> <p>When using Rate Limiting, be aware it uses <code>#pause</code>, which will purge your internal message queue. This occurs because <code>#pause</code> is a fencing mechanism, invalidating all messages in the queue. To mitigate excessive network traffic due to re-fetching of messages, consider lowering the <code>queued.max.messages.kbytes</code> value. You can read more about this here.</p>"}, {"location": "Pro-Rate-Limiting/#behaviour-on-errors", "title": "Behaviour on errors", "text": "<p>When an error occurs during message processing in Karafka, the Rate Limiting feature behaves in a way that ensures that the same message is not re-processed immediately. Instead, Karafka waits for a configurable period (known as the \"backoff\") before retrying the message.</p> <p>During this retry interval, the message is not counted towards the rate limiting, so it does not contribute to the overall message processing rate. Once the retry interval has elapsed, Karafka will attempt to process the message again, and it will count towards the overall rate limiting as usual.</p>"}, {"location": "Pro-Rate-Limiting/#limitations", "title": "Limitations", "text": "<p>While a powerful tool, the Rate Limiting API in Karafka has a few limitations and nuances worth knowing:</p> <ul> <li> <p>Due to its reliance on the <code>#pause</code> method, especially for short durations, usage of the Rate Limiting API can inadvertently lead to significant network traffic. This arises because Karafka tries to replenish its internal buffer after resuming from a pause. When this behavior is frequent, it results in redundant fetching of the same data repeatedly, creating unnecessary network load. For a more detailed explanation of the impact of the <code>#pause</code> method on networking and its potential consequences, you can read more here.</p> </li> <li> <p>Rebalance in the Kafka cluster will reset the Rate Limiting mechanism in Karafka. This happens because rebalancing can cause a redistribution of partitions between the consumer group members, affecting the consumption rate. However, Karafka provides an advanced filtering API that allows extending the throttler to prevent reset on rebalancing as long as the same process will receive the same assignment back. This feature can be helpful in scenarios where the consumer group members are frequently rebalanced, and the Rate Limiting mechanism needs to maintain its state across rebalances.</p> </li> </ul>"}, {"location": "Pro-Rate-Limiting/#example-use-cases", "title": "Example Use Cases", "text": "<p>Here are some real-life examples of how Karafka's Rate Limiting feature can be useful across different industries:</p> <ul> <li> <p>General: Limiting the incoming message rate to match the external HTTP API limit that the system needs to call per each received message.</p> </li> <li> <p>Finance: Limiting the rate of incoming financial transactions to a trading platform to prevent resource exhaustion and ensure timely processing.</p> </li> <li> <p>Social Media: Controlling the rate of incoming messages from a social media platform's real-time feed to prevent overwhelming the processing pipeline.</p> </li> <li> <p>Retail: Throttling the pace of incoming orders in an e-commerce application to prevent inventory discrepancies and order processing delays.</p> </li> <li> <p>Advertising: Regulating the flow of ad impressions data from multiple ad exchanges to a centralized data analytics platform to prevent resource exhaustion and ensure timely analysis.</p> </li> <li> <p>Transportation: Controlling the rate of incoming vehicle telemetry data in a connected car platform to prevent resource exhaustion and ensure timely processing.</p> </li> </ul>"}, {"location": "Pro-Rate-Limiting/#summary", "title": "Summary", "text": "<p>In summary, the Rate Limiting mechanism in Karafka provides a flexible way to control the message consumption rate from Kafka topics, improving the scalability and resilience of message-based applications. By configuring the limits on a per-topic basis and using advanced filtering APIs, Karafka can adapt to different processing requirements and handle dynamic changes in the Kafka cluster topology.</p> <p>Last modified: 2023-12-26 18:25:56</p>"}, {"location": "Pro-Recurring-Tasks/", "title": "Recurring Tasks", "text": "<p>This feature provides a mechanism for scheduling and managing recurring tasks within Kafka-based applications. It allows you to define tasks that run at specified intervals, using cron-like syntax, ensuring that essential tasks are executed at the right time without manual intervention. This feature uses Kafka as the state store, so no extra database or third-party components are needed.</p> <pre><code># Example schedule for recurring tasks and events\nKarafka::Pro::RecurringTasks.define('1.0.0') do\n  schedule(id: 'daily_report', cron: '0 0 * * *') do\n    DailyReportJob.perform_async\n  end\n\n  schedule(id: 'cleanup', cron: '0 2 * * 7') do\n    CleanupJob.perform_async\n  end\n\n  # Executed tasks can also be events, they do not have to be jobs per-se\n  schedule(id: 'user_activity_summary', cron: '0 8 * * *') do\n    Karafka.producer.produce_sync(\n      topic: 'user_activities',\n      payload: { event: 'daily_summary', date: Date.today.to_s }.to_json,\n      key: 'user_activity_summary'\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#how-does-it-work", "title": "How Does It Work?", "text": "<p>Recurring Tasks use Kafka to manage scheduled tasks efficiently without needing third-party databases.</p> <p>Karafka, with its unique approach, stores the state of each recurring task, including the last and next execution times, directly in a Kafka topic. This use of Kafka ensures that even if your application crashes or restarts, the task state is preserved, allowing Karafka to resume tasks accurately. Kafka also handles commands like <code>enable</code>, <code>disable</code>, or <code>trigger</code>, which are sent to and processed by the managing consumer, allowing for dynamic task management at runtime.</p> <p>One key advantage of Recurring Tasks is its independence from external databases for managing task schedules. All necessary information is stored in Kafka, simplifying deployment and maintenance.</p> <p>Karafka ensures that only one process executes tasks by using Kafka's partition assignment. Only the process assigned to the relevant topic executes the tasks, guaranteeing that different processes don't run them multiple times. This provides strong execution warranties, ensuring each task runs only once at a scheduled time.</p> <p>In case of a process crash, Kafka automatically reassigns the partitions to another available consumer. The new process takes over task execution immediately, continuing from where the previous process left off. This automatic failover ensures high availability and seamless task execution continuity, even during unexpected failures.</p>"}, {"location": "Pro-Recurring-Tasks/#using-recurring-tasks", "title": "Using Recurring Tasks", "text": "<p>To start using Recurring Tasks, follow a few essential setup steps. These tasks are not automatically enabled and require specific configurations in your application. Here's what you need to do:</p> <ol> <li> <p>Add <code>fugit</code> To Your Gemfile: First, add the <code>fugit</code> gem to your Gemfile. Fugit is a cron parsing library that is not included in Karafka's dependencies by default but is required for defining and managing recurring tasks.</p> </li> <li> <p>Enable Recurring Tasks in Routing: Declare the <code>recurring_tasks</code> feature in your routing configuration.</p> </li> <li> <p>Create Required Kafka Topics: Once the routing is configured, you need to create the necessary Kafka topics that the recurring tasks will use for scheduling, logging, and task management.</p> </li> <li> <p>Define a Schedule: Finally, you must define the tasks and their schedules.</p> </li> </ol>"}, {"location": "Pro-Recurring-Tasks/#adding-fugit-to-your-gemfile", "title": "Adding <code>fugit</code> To Your Gemfile", "text": "<p>Fugit is a cron parsing library not included in Karafka's dependencies by default but is required for defining and managing recurring tasks. Add it to your Gemfile and run <code>bundle install</code>:</p> <pre><code># Other dependencies..\ngem 'fugit'\ngem 'karafka'\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#enabling-recurring-tasks-in-routing", "title": "Enabling Recurring Tasks in Routing", "text": "<p>To enable the recurring tasks feature in your application, you must declare it in your routing configuration:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    recurring_tasks(true) do |schedules_topic, logs_topic|\n      # Optional block allowing for reconfiguration of attributes when needed\n      # You can also reconfigure other things when needed\n      schedules_topic.config.replication_factor = 3\n      logs_topic.config.replication_factor = 2\n    end\n  end\nend\n</code></pre> <p>This will automatically create a special consumer group dedicated to the consumption of the schedules topic.</p>"}, {"location": "Pro-Recurring-Tasks/#creating-required-kafka-topics", "title": "Creating Required Kafka Topics", "text": "<p>Review Replication Factor Configuration</p> <p>Before deploying to production, it is crucial to read the Replication Factor Configuration for the Production Environment section. Ensuring the correct replication factor is set is vital for maintaining your Kafka topics' high availability and fault tolerance.</p> <p>When <code>recurring_tasks(true)</code> is invoked, this command will automatically create appropriate entries for Karafka Declarative Topics. This means that all you need to do is to run the:</p> <pre><code>bundle exec karafka topics migrate\n</code></pre> <p>Two appropriate topics with the needed configuration will be created.</p> <p>If you do not use Declarative Topics, please make sure to create those topics manually with below settings:</p> Topic name Settings karafka_recurring_tasks_schedules <ul> <li>             partitions: <code>1</code> </li> <li>             replication factor: aligned with your company policy           </li> <li> <code>'cleanup.policy': 'compact,delete'</code> </li> <li> <code>'retention.ms': 86400000 # 1 day</code> </li> </ul> karafka_recurring_tasks_logs <ul> <li>             partitions: <code>1</code> </li> <li>             replication factor: aligned with your company policy           </li> <li> <code>'cleanup.policy': 'delete'</code> </li> <li> <code>'retention.ms': 604800000 # 1 week</code> </li> </ul>"}, {"location": "Pro-Recurring-Tasks/#replication-factor-configuration-for-the-production-environment", "title": "Replication Factor Configuration for the Production Environment", "text": "<p>Setting the replication factor for Kafka topics used by the recurring tasks feature to more than 1 in production environments is crucial. The replication factor determines how many copies of the data are stored across different Kafka brokers. Having a replication factor greater than 1 ensures that the data is highly available and fault-tolerant, even in the case of broker failures.</p> <p>For example, if you set a replication factor of 3, Kafka will store the data on three different brokers. If one broker goes down, the data is still accessible from the other two brokers, ensuring that your recurring tasks continue to operate without interruption.</p> <p>Here's an example of how to reconfigure the Recurring Tasks topics so they have replication factor of 3:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    recurring_tasks(true) do |schedules_topic, logs_topic|\n      schedules_topic.config.replication_factor = 3\n      logs_topic.config.replication_factor = 3\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#defining-a-schedule", "title": "Defining a Schedule", "text": "<p>To define recurring tasks, use the <code>Karafka::Pro::RecurringTasks.define</code> method. This method allows you to specify the version of the schedule and define one or more tasks with their cron schedule and corresponding execution block.</p> <p>Task ID Naming Convention</p> <p>The <code>id</code> field used in defining tasks must comply with the following regular expression: <code>/\\A[a-zA-Z0-9_-]{1,}\\z/</code>. This means that the <code>id</code> must consist only of alphanumeric characters, underscores (<code>_</code>), or hyphens (<code>-</code>). No spaces or special characters are allowed; it must be at least one character long.</p> <pre><code>Karafka::Pro::RecurringTasks.define('1.0.0') do\n  schedule(id: 'daily_report', cron: '0 0 * * *') do\n    DailyReportJob.perform_async\n  end\n\n  schedule(id: 'cleanup', cron: '0 2 * * 7') do\n    CleanupJob.perform_async\n  end\n\n  # Executed tasks can also be events, they do not have to be jobs per-se\n  schedule(id: 'user_activity_summary', cron: '0 8 * * *') do\n    Karafka.producer.produce_sync(\n      topic: 'user_activities',\n      payload: { event: 'daily_summary', date: Date.today.to_s }.to_json,\n      key: 'user_activity_summary'\n    )\n  end\nend\n</code></pre> <p>In this example:</p> <ul> <li>The <code>daily_report</code> task is scheduled to run every day at midnight.</li> <li>The <code>cleanup</code> task is scheduled to run every Sunday at 2 AM.</li> <li>The <code>user_activity_summary</code> task publishes a daily summary event to the <code>user_activities</code> topic every day at 8 AM.</li> </ul>"}, {"location": "Pro-Recurring-Tasks/#configuration", "title": "Configuration", "text": "<p>The configuration for Recurring Tasks in Karafka is designed to be straightforward yet flexible. It allows you to fine-tune the behavior of your scheduled tasks to meet your application's specific needs.</p> <p>The configuration for Recurring Tasks is done in the Karafka application's setup block. Here's how you might configure some basic settings:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other configurations...\n\n    # Configuring Recurring Tasks\n    config.recurring_tasks.group_id = 'custom_group_id'\n    # Run the scheduler every 10 seconds\n    config.recurring_tasks.interval = 10_000\n    # Disable logging of task executions\n    config.recurring_tasks.logging = false\n\n    # You can also reconfigure used topics names\n    config.recurring_tasks.topics.schedules = 'recurring_schedules'\n    config.recurring_tasks.topics.logs = 'recurring_logs'\n  end\nend\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#key-configuration-options", "title": "Key Configuration Options", "text": "<ul> <li> <p><code>group_id</code>: Defines the consumer group ID used for the recurring tasks. This allows you to isolate these tasks within a specific group.</p> </li> <li> <p><code>producer</code>: Specifies the Kafka producer used for sending recurring tasks messages, like schedule updates and logs. Defaults to <code>Karafka.producer</code>.</p> </li> <li> <p><code>interval</code>: Determines the interval (in milliseconds) the scheduler will run to check for tasks that must be executed. The default is 15,000 milliseconds (15 seconds).</p> </li> <li> <p><code>logging</code>: Enables or disables the logging of task execution details. If set to true (the default), Karafka will log each task's execution, including both successful and failed attempts.</p> </li> <li> <p><code>topics.schedules</code>: Specifies the Kafka topic used for storing and managing the task schedules. By default, this is set to <code>karafka_recurring_tasks_schedules</code>, but you can customize it to any topic name that suits your application's needs.</p> </li> <li> <p><code>topics.logs</code>: Specifies the Kafka topic used for logging the execution of tasks. By default, this is set to <code>karafka_recurring_tasks_logs</code>. This topic stores logs of both successful and failed task executions, allowing you to monitor task performance and troubleshoot issues.</p> </li> </ul>"}, {"location": "Pro-Recurring-Tasks/#recurring-tasks-management", "title": "Recurring Tasks Management", "text": "<p>You can manage tasks dynamically using the following<code>Karafka::Pro::RecurringTasks</code> methods:</p> <ul> <li> <p>Enable a Task: <code>Karafka::Pro::RecurringTasks.enable(task_id)</code> enables a specific task based on its <code>task_id</code>, allowing it to resume execution according to its schedule.</p> </li> <li> <p>Disable a Task: <code>Karafka::Pro::RecurringTasks.disable(task_id)</code> disables the task, preventing it from running until re-enabled.</p> </li> <li> <p>Trigger a Task Manually: <code>Karafka::Pro::RecurringTasks.trigger(task_id)</code> immediately triggers the execution of a task, bypassing the schedule.</p> </li> </ul> <p>When you call these methods, a Kafka command event is produced, which is then processed by the consumer responsible for managing the recurring tasks. This mechanism ensures that only the designated consumer executes the commands, maintaining consistency across your application.</p> <p>If you use <code>'*'</code> as the <code>task_id</code>, the command will apply to all tasks available in the current schedule. This allows you to enable, disable, or trigger all tasks in one operation, giving you flexible control over task management.</p> <pre><code>Karafka::Pro::RecurringTasks.define('1.0.0') do\n  # Start it disabled by default\n  schedule(id: 'daily_report', cron: '0 0 * * *', enabled: false) do\n    puts \"Running daily report task...\"\n  end\n\n  schedule(id: 'cleanup', cron: '0 2 * * 7') do\n    puts \"Running cleanup task...\"\n  end\nend\n\n# Enable the daily report task\nKarafka::Pro::RecurringTasks.enable('daily_report')\n\n# Disable the cleanup task\nKarafka::Pro::RecurringTasks.disable('cleanup')\n\n# Trigger the daily report task immediately\nKarafka::Pro::RecurringTasks.trigger('daily_report')\n\n# Trigger all tasks immediately\nKarafka::Pro::RecurringTasks.trigger('*')\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#schedule-versioning", "title": "Schedule Versioning", "text": "<p>Versioning is an optional feature that adds a layer of safety and consistency during rolling deployments. It helps ensure that tasks are only executed when the assigned process has an appropriate schedule version, preventing older instances from accidentally running outdated schedules.</p> <p>When defining your recurring tasks schedule, you can specify a version number. This version is a safeguard during deployments, particularly in scenarios where multiple instances of your application might run simultaneously with both older and newer schedule definitions.</p> <p>During a rolling deployment, there might be a brief period when some instances of your application are still running an older code version with a different schedule. If an older instance of your application receives recurring task assignments, it will recognize that the schedule is no longer compatible with its in-memory definition. The older instance will halt the execution of the task and will raise an error, preventing any outdated schedules from being run. This ensures that only the new schedule version is executed, maintaining consistency and avoiding potential issues caused by version mismatches.</p> <p>If you don't specify a version, Karafka will operate without this safeguard. Regardless of its deployment stage, Karafka will attempt to execute the schedule it has in memory if it receives the schedules topic assignment. Versioning is recommended if you want to ensure that only the most up-to-date schedule is used during deployments.</p>"}, {"location": "Pro-Recurring-Tasks/#tasks-execution-logging", "title": "Tasks Execution Logging", "text": "<p>By default, after each task is executed, Karafka produces a log entry in the recurring tasks logs topic. This log entry is created regardless of whether the task execution was successful or failed. </p>"}, {"location": "Pro-Recurring-Tasks/#benefits-of-task-execution-logging", "title": "Benefits of Task Execution Logging", "text": "<ul> <li> <p>Auditability: Logging each task execution provides a comprehensive audit trail, allowing you to track when tasks were executed and their outcomes. This is crucial for maintaining transparency and accountability in your system.</p> </li> <li> <p>Monitoring and Debugging: The log entries enable effective monitoring and debugging of task executions. By analyzing these logs, you can identify patterns of failures or performance issues and address them proactively.</p> </li> <li> <p>Operational Insights: The logs offer valuable insights into the operational efficiency of your recurring tasks, helping you optimize schedules and improve overall system performance.</p> </li> </ul>"}, {"location": "Pro-Recurring-Tasks/#logging-details", "title": "Logging Details", "text": "<ul> <li> <p>Successful Executions: For tasks that are completed successfully, the log will record essential details such as task ID, execution time, and status.</p> </li> <li> <p>Failed Executions: In the event of a task failure, Karafka will record the failure status in the logs. However, it will not capture detailed error information, such as backtraces, in these logs. Instead, errors are published through Karafka's regular instrumentation pipeline, allowing detailed error tracking and handling through your existing monitoring tools.</p> </li> </ul> <p>Karafka's Web UI error tracking capabilities will automatically record and display failed task error details, similar to any other errors encountered.</p>"}, {"location": "Pro-Recurring-Tasks/#viewing-execution-logs", "title": "Viewing Execution Logs", "text": "<p>Execution logs for your recurring tasks can be accessed in two primary ways:</p> <ol> <li> <p>Web UI Explorer: The Karafka Web UI provides an intuitive interface for exploring execution logs. The Web UI Explorer lets you easily view and analyze task execution history, identify patterns, and monitor system health. This visual approach is beneficial for quickly assessing the status of your tasks and detecting any anomalies.</p> </li> <li> <p>Karafka Admin APIs: You can use the Karafka Admin APIs to read the logs topic directly for more programmatic access. This approach allows you to process the log messages customarily, such as integrating the data into your existing monitoring systems, triggering alerts based on specific log entries, or generating detailed reports on task execution performance.</p> </li> </ol> <pre><code>messages = Karafka::Admin.read_topic(\n  'karafka_recurring_tasks_logs',\n  0,\n  100\n)\n\nmessages.each do |message|\n  log_data = message.payload\n  puts \"Task ID: #{log_data[:task][:id]}, \" \\\n   \"Result: #{log_data[:task][:result]}, \" \\\n   \"Executed At: #{log_data[:dispatched_at]}\"\nend\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#web-ui-management", "title": "Web UI Management", "text": "<p>The Karafka Web UI provides comprehensive tools for managing and monitoring recurring tasks:</p> <ul> <li> <p>Inspect and Control Tasks: You can view, enable, disable, and trigger recurring tasks directly from the Web UI. This allows for real-time management and ensures tasks run as expected.</p> </li> <li> <p>Execution Logs: The Web UI enables you to explore the logs of task executions, offering insights into when tasks were executed, their outcomes, and whether any issues occurred.</p> </li> </ul> <p></p>"}, {"location": "Pro-Recurring-Tasks/#error-handling-and-retries", "title": "Error Handling and Retries", "text": "<p>An important aspect of the Recurring Tasks feature is how it handles errors during task execution:</p> <ul> <li> <p>No Automatic Retries: If a task fails (e.g., due to a transient error or an unexpected exception), it will not be automatically retried. The failure will be instrumented, but the scheduler will not attempt to rerun the task.</p> </li> <li> <p>Error Logging: All errors during task execution are instrumented. This can be used for monitoring and alerting purposes but does not provide any built-in retry mechanism.</p> </li> </ul> <p>It is recommended that tasks that require retry logic, error handling, or dead-letter queue (DLQ) management be queued into a background job.</p> <pre><code># example using  Karafka ActiveJob adapter\nKarafka::Pro::RecurringTasks.define('1.0.0') do\n  schedule(id: 'critical_operation', cron: '0 12 * * *') do\n    CriticalOperationJob.perform_async\n  end\nend\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#execution-modes", "title": "Execution Modes", "text": "<p>In larger setups, it's advisable to run a dedicated consumer process for executing recurring tasks and managing the Web UI (if used) to prevent potential saturation with other workloads.</p> <p>By isolating the recurring tasks execution in its own consumer process, you ensure that these tasks do not compete with other consumers for resources, which can be particularly important in high-throughput environments. This dedicated process will handle all scheduling and execution, leaving other consumers free to manage their specific workloads.</p> <p>Karafka provides CLI flag to facilitate running only dedicated consumer groups:</p> <pre><code>bundle exec karafka server --include-consumer-groups karafka_web,karafka_recurring_tasks\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#testing", "title": "Testing", "text": "<p>Karafka provides easy access to the current schedule and the individual tasks within that schedule to facilitate testing of your recurring tasks.</p> <p>You can access the current schedule via the <code>Karafka::Pro::RecurringTasks.schedule</code> method. This allows you to inspect the schedule, verify that tasks are correctly defined, and interact with the tasks programmatically.</p> <p>Each task within the schedule can be accessed using the <code>Karafka::Pro::RecurringTasks.schedule.tasks</code> method. This method returns a hash where the keys are the task IDs, and the values are the corresponding task objects. This structure makes finding and working with specific tasks by their IDs easy.</p> <p>For testing purposes, each task can be executed manually by invoking the <code>#execute</code> method on the task object. This method bypasses the cron schedule and any associated instrumentation, allowing you to directly test the task's functionality to ensure it works as expected without any side effects.</p> <p>Here's an example of how you might define a schedule and test it:</p> <pre><code>Karafka::Pro::RecurringTasks.define('1.0.0') do\n  schedule(id: 'daily_report', cron: '0 0 * * *') do\n    puts \"Running daily report task...\"\n  end\n\n  schedule(id: 'cleanup', cron: '0 2 * * 7') do\n    puts \"Running cleanup task...\"\n  end\nend\n\n# Access the current schedule\nschedule = Karafka::Pro::RecurringTasks.schedule\n\n# Access and manually execute the 'daily_report' task for testing\ndaily_task = schedule.tasks['daily_report']\n\n# RSpec syntax just as a demo\nexpect { daily_task.execute }.not_to raise_error\n\n# Access and manually execute the 'cleanup' task for testing\ncleanup_task = schedule.tasks['cleanup']\n# RSpec syntax just as a demo\nexpect { cleanup_task.execute }.not_to raise_error\n</code></pre>"}, {"location": "Pro-Recurring-Tasks/#warranties", "title": "Warranties", "text": "<p>Recurring Tasks provides strong execution warranties by leveraging Kafka\u2019s robust architecture. With Kafka as the backbone, tasks are guaranteed to execute only once at their scheduled time, managed by the process that holds the partition assignment for the relevant topic.</p> <ul> <li> <p>Single Process Execution: Karafka ensures that only one process can execute the scheduled tasks by assigning Kafka partitions to a single consumer. This prevents multiple processes from executing the same task simultaneously, offering a strong guarantee of task uniqueness and timing precision.</p> </li> <li> <p>Automatic Failover: In the event of a process crash, Kafka automatically reassigns the partitions to another available consumer. The new process immediately picks up from where the previous one left off, ensuring continuous task execution without losing the state or missing any scheduled runs.</p> </li> <li> <p>Consistency Across Deployments: With the optional versioning feature, Karafka further ensures that only the appropriate version of the schedule is executed, preventing older instances from running outdated tasks during rolling deployments.</p> </li> </ul>"}, {"location": "Pro-Recurring-Tasks/#limitations", "title": "Limitations", "text": "<ul> <li> <p>Interval Granularity: The smallest scheduling interval is one minute. Tasks can only be scheduled as frequently as this.</p> </li> <li> <p>Task Complexity: Tasks should be lightweight, as they run within the context of the consumer process. For heavy tasks, consider offloading the work to a background job processor.</p> </li> <li> <p>Cron Syntax: The cron syntax must be valid and recognized by the fugit gem, which powers the scheduling mechanism.</p> </li> <li> <p>Task Uniqueness: Task IDs must be unique within a schedule version to avoid conflicts.</p> </li> <li> <p>Precision and Frequency Drift: Karafka runs the scheduler every 15 seconds by default, with the next run starting 15 seconds after the previous one finishes. This can cause small timing drifts, especially if tasks take longer to complete. The interval is configurable.</p> </li> <li> <p>Oversaturation Lags: If the scheduler runs alongside other topic assignments and all workers are occupied, the scheduler may experience lags. By default, the scheduler is not prioritized over other consumers, which can lead to delays in task execution when system resources are constrained.</p> </li> <li> <p>Expected Lag Reporting: Due to how Recurring Tasks manage their offsets, their assignments always report a lag of at least 1. This is expected and normal but may initially seem surprising when viewed in the Web UI.</p> </li> </ul>"}, {"location": "Pro-Recurring-Tasks/#example-use-cases", "title": "Example Use-Cases", "text": "<ul> <li> <p>Regulatory Compliance and Auditing: Schedule regular tasks to automatically generate and store compliance reports, ensuring your organization meets audit requirements without manual intervention.</p> </li> <li> <p>Data Pipeline Orchestration: Coordinate complex data processing pipelines by triggering different stages of ETL (Extract, Transform, Load) processes at scheduled intervals, ensuring that data flows seamlessly from one stage to the next.</p> </li> <li> <p>SLA-Based Monitoring and Alerts: Monitor service-level agreements (SLAs) by scheduling checks that ensure critical metrics are within acceptable thresholds. Automatically trigger alerts or remediation actions if SLAs are breached.</p> </li> <li> <p>Automated Infrastructure Scaling: Dynamically adjust cloud resources by scheduling scaling operations based on expected traffic patterns. For example, scale up resources during peak hours and scale down during off-peak hours to optimize costs.</p> </li> <li> <p>End-of-Day Financial Reconciliation: Automate the end-of-day reconciliation of financial transactions, ensuring that all accounts are balanced and any discrepancies are flagged for manual review.</p> </li> <li> <p>Automated Backup and Disaster Recovery: Schedule regular backups of critical data and systems and automate disaster recovery drills to ensure that your organization is prepared for unexpected outages.</p> </li> <li> <p>User Engagement Events: Trigger user engagement events, such as sending personalized notifications or emails based on user behavior or inactivity, at specific times to increase user retention.</p> </li> <li> <p>Inventory Threshold Alerts: Publish alerts to a messaging system when inventory levels fall below a certain threshold, allowing other systems or teams to react in real time.</p> </li> <li> <p>Scheduled API Data Push: Automatically push data to external APIs at scheduled intervals, such as sending daily summaries or hourly updates to third-party services.</p> </li> <li> <p>Time-Based Feature Rollouts: Publish events that trigger the gradual rollout or rollback of features to users based on time, enabling time-sensitive feature management.</p> </li> <li> <p>Content Expiry Notifications: Trigger notifications or events when certain content (e.g., licenses, subscriptions, or promotions) is nearing its expiration, prompting renewal or other actions.</p> </li> </ul> <p>Last modified: 2025-05-21 16:48:20</p>"}, {"location": "Pro-Rotating-Credentials/", "title": "Rotating credentials", "text": "<p>If your credentials leak due to internal or external incidents, you should rotate them.</p> <p>To do so:</p> <ol> <li>Go to your license credentials page.</li> </ol> <p> </p> <ol> <li> <p>Press the \"Rotate credentials\" button.</p> </li> <li> <p>Read the disclaimers.</p> </li> </ol> <p> </p> <ol> <li> <p>Press the \"I understand. Send me an email with the rotation link.\" button.</p> </li> <li> <p>Click on the link received in the email.</p> </li> <li> <p>View your new credentials and replace them in your Gemfile and any other place where you use them.</p> </li> </ol> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Routing-Patterns/", "title": "Routing Patterns", "text": "<p>Karafka's Routing Patterns is a powerful feature that offers flexibility in routing messages from various topics, including topics created during the runtime of Karafka processes. This feature allows you to use regular expression (regexp) patterns in routes. When a matching Kafka topic is detected, Karafka will automatically expand the routing and start consumption without additional configuration. This feature greatly simplifies the management of dynamically created Kafka topics.</p>"}, {"location": "Pro-Routing-Patterns/#how-it-works", "title": "How It Works", "text": "<p>Routing Patterns is not just about using regex patterns. It's about the marriage of regexp and Karafka's topic routing system.</p> <p>When you define a route using a regexp pattern, Karafka monitors the Kafka topics. As soon as a topic matching the pattern emerges, Karafka takes the initiative. Without waiting for manual interventions or service restarts, it dynamically adds the topic to the routing tree, initiates a consumer for this topic, and starts processing data.</p> <p>Below, you can find a conceptual diagram of how the discovery process works:</p> <p> </p> <p> *This example illustrates how the detection process works. When Karafka detects new topics in Kafka, it will try to match them, expand routes, and process incoming data.    </p> <p>Upon detecting a new topic, Karafka seamlessly integrates its operations just as with pre-existing ones. Notably, regexp patterns identify topics even during application initialization, ensuring compatibility with topics established prior, provided they haven't been previously defined in the routes.</p> <p>Regexp Implementation Differences</p> <p>The underlying <code>librdkafka</code> library utilizes a regexp engine from a <code>libc</code> library. It's crucial to note that this engine supports a POSIX-compatible regular expression format, which is not fully aligned with the regexp engine used by Ruby. Given these differences, it's highly advisable to conduct thorough testing of your regexp patterns to confirm that the dynamic topics you intend to match are not only visible within Karafka and the Karafka Web UI but are also being consumed as expected. The discrepancies between the regexp engines could lead to unexpected behaviors. You can read more about this topic in this section.</p>"}, {"location": "Pro-Routing-Patterns/#representation-of-routing-patterns", "title": "Representation of Routing Patterns", "text": "<p>Defining a pattern within Karafka automatically translates this into an underlying topic representation with a regular expression (regexp) that matches the appropriate Kafka topics.</p> <p>To support this concept, from the Routing Patterns feature perspective, Karafka has three types of topics:</p> <ol> <li> <p>regular Represents the standard Kafka topics. These are directly matched based on their name without the need for any patterns. This type is used when a straightforward, one-to-one correlation with an existing topic and patterns are not used.</p> </li> <li> <p>matcher: This type signifies the representation of a regular expression used by <code>librdkafka</code>. It is the gateway for Karafka's dynamic topic discovery, laying the foundation for the <code>:discovered</code> type.</p> </li> <li> <p>discovered This type comes into play when there's a real, tangible topic that Karafka begins to listen to after it was matched with a regular expression.</p> </li> </ol> <p>The matcher topic holds a paramount position in the dynamic topic discovery mechanism. It embodies a regular expression subscription, acting as the initial point of discovery. When a new topic aligns with the matcher topic's criteria (whether during boot-up or at runtime), Karafka uses the matcher topic's configuration as the blueprint. This new topic inherits the settings and becomes part of the same consumer group and subscription group as its originating matcher topic.</p> <p>Subsequently, this newly registered topic is created as the <code>:discovered</code> type. To simplify its identification, especially in environments where multiple topics are at play, it's labeled as <code>:discovered</code> in the Web UI.</p> <p>Diagram below represents the relationship between topics of various types and how they operate within Karafka routing:</p> <p> </p> <p> *This example illustrates how a single consumer group and subscription group can incorporate multiple topics of various types. All `:discovered` topics always use the same settings as their base `:matcher` topic.    </p>"}, {"location": "Pro-Routing-Patterns/#enabling-routing-patterns", "title": "Enabling Routing Patterns", "text": "<p>Creating patterns for routing in Karafka is done by using the routing <code>#pattern</code> method. Think of it as the twin of the <code>#topic</code> method but with an added flair of pattern matching:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    pattern(/.*_dlq/) do\n      consumer DlqConsumer\n    end\n\n    # patterns accept same settings as `#topic`\n    # so they can use all the Karafka features\n    pattern(/.*_customers/) do\n      consumer CustomersDataConsumer\n      long_running_job true\n      manual_offset_management true\n      delay_by(60_000)\n    end\n  end\nend\n</code></pre> <p>The same usage contexts apply since this method is a twin to the <code>#topic</code>. You can define your patterns in the following places:</p> <ul> <li> <p>Routing root Level: Place it directly in your routing.</p> </li> <li> <p>Consumer Group: Use it within a <code>#consumer_group</code> block.</p> </li> <li> <p>Subscription Group: Insert it inside the <code>#subscription_group</code> block.</p> </li> </ul> <p>Regardless of where you use it, it works similarly to the <code>#topic</code> method, but searches for topics based on patterns.</p> <p>Patterns crafted in such a way are called \"anonymous patterns\". This terminology highlights that these patterns don't have a predefined name. Instead, Karafka generates a name prefixed with \"karafka-pattern-\" based on the regular expression content. This approach ensures unique and distinguishable matcher topics but, at the same time, makes it much harder to exclude pattern routes from the CLI. Anonymous patterns are easy to start with and great for development. However, we do recommend assigning them names in the later stages before shipping to production.</p>"}, {"location": "Pro-Routing-Patterns/#named-patterns", "title": "Named Patterns", "text": "<p>Named and anonymous patterns in Karafka work the same way when setting up routing. The key difference is that named patterns have a specific name you choose, while anonymous patterns don't. This name is handy when picking or skipping certain routes in Karafka using the CLI. It's good to start with anonymous patterns when testing things out. But, as you finalize how you use them, switching to named patterns can make things more transparent and consistent.</p> <p>You define named patterns similarly to anonymous by using the <code>#pattern</code> method but instead of providing only the regular expression, the expectation is that you provide both the name and the regexp:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    pattern(:dlqs_pattern, /.*_dlq/) do\n      consumer DlqConsumer\n    end\n\n    pattern(:customers_pattern, /.*_customers/) do\n      consumer CustomersDataConsumer\n      long_running_job true\n      manual_offset_management true\n      delay_by(60_000)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Routing-Patterns/#activejob-routing-patterns", "title": "ActiveJob Routing Patterns", "text": "<p>In the case of ActiveJob, a new method is available called <code>#active_job_pattern</code> that allows you to define pattern matchings for ActiveJob jobs. Its API is similar to the <code>#pattern</code> one and works the same way:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # Anonymous active job pattern\n    active_job_pattern(/.*_fast_jobs/)\n\n    # or a named on with some extra options\n    active_job_pattern(:active_jobs, /.*_late_jobs/) do\n      delay_by(60_000)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Routing-Patterns/#negative-matching-and-exclusion-patterns", "title": "Negative Matching and Exclusion Patterns", "text": "<p>Sometimes, you may need to route messages from topics that match a pattern but with specific exclusions. A common use case is when you want to handle one specific topic differently from others that follow a similar naming pattern.</p>"}, {"location": "Pro-Routing-Patterns/#excluding-specific-topic-prefixes", "title": "Excluding Specific Topic Prefixes", "text": "<p>When working with topic naming conventions like <code>[app_name].data_results</code>, you might want to process most topics matching this pattern with one consumer while directing a specific topic (e.g., <code>activities.data_results</code>) to a different consumer.</p> <p>Since POSIX regular expressions used by <code>librdkafka</code> don't support negative lookahead assertions (<code>?!</code>) available in Ruby's regex engine, you'll need to use alternative approaches for exclusion patterns.</p> <p>One effective method is to use character-by-character negative matching to exclude specific prefixes:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    # Special consumer for activities.data_results\n    topic 'activities.data_results' do\n      consumer ActivitiesConsumer\n    end\n\n    # It has to be one-line to work with librdkafka\n    negative_matching = &lt;&lt;~PATTERN.gsub(/\\s+/, '')\n      ^(\n        [^a]|\n        a[^c]|\n        ac[^t]|\n        act[^i]|\n        acti[^v]|\n        activ[^i]|\n        activi[^t]|\n        activit[^i]|\n        activiti[^e]|\n        activitie[^s]|\n        activities[^.]\n      )\n    PATTERN\n\n    exclusion_regex = Regexp.new(negative_matching + \".*data_results$\")\n\n    # All other app_name.data_results topics\n    pattern(exclusion_regex) do\n      consumer GenericConsumer\n    end\n  end\nend\n</code></pre> <p>While this pattern looks complex, it works by explicitly matching any string that doesn't start with \"activities\" followed by a period. The regex checks each character's position and ensures it either doesn't match the expected character in \"activities\" or, if it does match that far, it diverges afterward.</p>"}, {"location": "Pro-Routing-Patterns/#testing-exclusion-patterns", "title": "Testing Exclusion Patterns", "text": "<p>Given the complexity of these patterns and the differences between Ruby and POSIX regex engines, it's crucial to test your exclusion patterns thoroughly:</p> <pre><code># The regex pattern that excludes \"activities.data_results\"\nnegative_matching = &lt;&lt;~PATTERN.gsub(/\\s+/, '')\n  ^(\n    [^a]|\n    a[^c]|\n    ac[^t]|\n    act[^i]|\n    acti[^v]|\n    activ[^i]|\n    activi[^t]|\n    activit[^i]|\n    activiti[^e]|\n    activitie[^s]|\n    activities[^.]\n  )\nPATTERN\n\nexclusion_regex = Regexp.new(negative_matching + \".*data_results$\")\n\n# Topics that should match (be processed by GenericConsumer)\nshould_match = [\n  'users.data_results',\n  'orders.data_results',\n  'metrics.data_results',\n  'active.data_results'  # Note: this is not \"activities\"\n]\n\n# Topics that should NOT match (be processed by ActivitiesConsumer)\nshould_not_match = [\n  'activities.data_results'\n]\n\nshould_match.each do |topic|\n  assert ruby_posix_regexp_same?(topic, exclusion_regex), \n         \"Expected '#{topic}' to match in both Ruby and POSIX\"\nend\n\nshould_not_match.each do |topic|\n  assert !ruby_posix_regexp_same?(topic, exclusion_regex), \n         \"Expected '#{topic}' to NOT match in both Ruby and POSIX\"\nend\n</code></pre>"}, {"location": "Pro-Routing-Patterns/#when-to-use-exclusion-patterns", "title": "When to Use Exclusion Patterns", "text": "<p>While exclusion patterns provide a powerful way to route messages, they should be used carefully:</p> <ul> <li>They're ideal when you need different Karafka-level configurations (like virtual partitions or specific error handling) for different topics</li> <li>They simplify routing logic by keeping it at the configuration level instead of embedding it in consumer code</li> <li>They're most maintainable when the exclusion list is small and stable</li> </ul>"}, {"location": "Pro-Routing-Patterns/#limiting-patterns-used-per-process", "title": "Limiting Patterns used per process", "text": "<p>Karafka's named and anonymous patterns are associated with a specific matcher topic name. This uniform naming system allows more flexibility when using the Karafka Command Line Interface (CLI). As with standard topics, you can include or exclude matcher topics when running Karafka processes by referencing their name.</p> <p>Giving the below routing setup:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    pattern(:dlqs_pattern, /.*_dlq/) do\n      consumer DlqConsumer\n    end\n\n    pattern(:customers_pattern, /.*_customers/) do\n      consumer CustomersDataConsumer\n    end\n  end\nend\n</code></pre> <p>You can limit the process operations to only specific patterns:</p> <pre><code># This code will not run topics and patterns other than `dlqs_pattern`\n#\n# You can use space to provide more patterns\nbundle exec karafka server --include-topics dlqs_pattern\n</code></pre> <p>as well as you can exclude patterns from being part of the operations:</p> <pre><code># This code will run ony topics and patterns other than `dlqs_pattern`\n#\n# You can use space to provide more patterns\nbundle exec karafka server --exclude-topics dlqs_pattern\n</code></pre>"}, {"location": "Pro-Routing-Patterns/#limitations", "title": "Limitations", "text": "<p>There are key aspects to consider to ensure efficient and consistent behavior:</p> <ol> <li> <p>Changing Regexp in Anonymous Patterns: If the regular expression for an anonymous pattern is changed, its name will change too.</p> </li> <li> <p>Avoid Overlapping Regexps: It's crucial to avoid defining multiple regular expressions within the same consumer group that might match the same topics. This can lead to unexpected behavior because of possible reassignments and rebalances.</p> </li> <li> <p>Thoroughly Test Patterns: Always ensure your patterns don't overlap within a single consumer group. Regular testing can prevent unwanted behavior.</p> </li> <li> <p>Potential Regexp Differences: The regular expressions in Ruby and <code>librdkafka</code> in C might not work identically. Always test the matching behaviors before deploying to production.</p> </li> <li> <p>Runtime Topic Detection Isn't Immediate: When a new topic emerges, its detection isn't real-time. It's influenced by the cache TTL, governed by the <code>topic.metadata.refresh.interval.ms</code> setting. The default is 5 seconds in development and 5 minutes in production. For most production scenarios, sticking to the 5-minute default is advised as it strikes a good balance between operational responsiveness and system load.</p> </li> <li> <p>Internal Regular Expression Requirements of <code>librdkafka</code>: The library requires regular expression strings to start with <code>^</code>. Karafka's Routing Patterns adapt Ruby's regular expressions to fit this format internally. Remembering this transformation and thoroughly testing your patterns before deploying is important. You can find the adjusted regular expression in the Web UI under the routing page topic details view if you wish to review the adjusted regular expression.</p> </li> <li> <p>Too Broad Regular Expressions: Broad regular expressions may unintentionally match a wide range of topics, leading to the over-consumption of topics that were not intended to be included. This can result in excessive resource utilization, unexpected data processing, and potential bottlenecks in the system.</p> </li> </ol> <p>Please ensure you're familiar with these considerations to harness the full power of Routing Patterns without encountering unexpected issues.</p>"}, {"location": "Pro-Routing-Patterns/#dlq-accidental-auto-consumption", "title": "DLQ Accidental Auto-Consumption", "text": "<p>This feature requires careful attention when used alongside Dead Letter Queues (DLQs) to avoid unintended behaviors. A common pitfall arises from using regular expressions that are not precise enough, leading to scenarios where the base topic consumer consumes the DLQ topics themselves.</p> <p>The issue occurs when a regular expression, designed to match topics for consumption, also inadvertently matches the DLQ topics. This mistake can initiate an endless cycle of consuming and failing messages from the DLQ, creating a loop that hampers the error-handling process.</p> <p>Such a case can lead to situations where messages destined for the DLQ due to processing errors are re-consumed as regular messages, only to fail and be sent back to the DLQ, perpetuating a cycle.</p> <p>Below, you can see an example routing setup that will cause the DLQ topic to be consumed by the <code>EventsConsumer</code> despite it not being explicitly declared.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    pattern /.*\\.events/ do\n      consumer EventsConsumer\n\n      # The above regexp will also match the below DLQ topic\n      dead_letter_queue(\n        topic: 'global.events.dlq'\n      )\n    end\n  end\nend\n</code></pre> <p>To prevent such loops, it's essential to:</p> <ul> <li> <p>Craft regular expressions precisely, ensuring they match only the intended topics and not the DLQs unless explicitly desired.</p> </li> <li> <p>Adopt clear naming conventions for DLQ topics that can easily be excluded in regex patterns.</p> </li> <li> <p>Test regular expressions thoroughly against various topic names as described here.</p> </li> </ul> <p>While routing patterns offer a dynamic and powerful method to manage topic consumption in Karafka, they demand careful consideration and testing, especially when integrating DLQ mechanisms.</p>"}, {"location": "Pro-Routing-Patterns/#differences-in-regexp-evaluation-between-ruby-and-libc", "title": "Differences in Regexp Evaluation Between Ruby and libc", "text": "<p>When dealing with regular expressions in Karafka, it's important to understand the underlying differences between the regex engines used by Ruby and the one used by <code>librdkafka</code>. <code>librdkafka</code>, which is a core component of Karafka for interacting with Kafka, defaults to the POSIX regex engine provided by <code>libc</code>. On the other hand, Ruby employs a different format for regular expressions, specifically the Oniguruma engine, known for its extensive feature set and flexibility.</p>"}, {"location": "Pro-Routing-Patterns/#key-differences", "title": "Key Differences", "text": "<ol> <li> <p>Syntax and Features: The Oniguruma engine (Ruby) supports a broader range of syntaxes and features than the POSIX regex engine. These include look-ahead and look-behind assertions, non-greedy matching, and named groups, among others.</p> </li> <li> <p>Character Classes: Ruby's regex engine supports POSIX bracket expressions but also includes additional character class shorthands (like <code>\\d</code>, <code>\\w</code>, <code>\\s</code>, and their uppercase counterparts), which are not part of the POSIX standard.</p> </li> <li> <p>Grouping and Capturing: Ruby supports non-capturing groups with <code>?:</code>, named groups with <code>?&lt;name&gt;</code>, and atomic groups, which are unavailable in the POSIX regex engine.</p> </li> </ol>"}, {"location": "Pro-Routing-Patterns/#testing-regular-expressions", "title": "Testing Regular Expressions", "text": "<p>Given the differences between Ruby's and libc's regular expression engines, it is recommended to thoroughly test your regular expressions to ensure compatibility, especially when using them with Karafka for dynamic topic routing. Testing becomes crucial because a regular expression in Ruby might behave differently or not work with librdkafka.</p> <p>To bridge the gap and verify that your regular expressions work as expected in both environments, you can use the POSIX regex engine directly within a Ruby context through command-line tools like <code>grep</code>. This approach allows you to test regular expressions against the POSIX standard and ensure they're compatible with librdkafka.</p> <p>Here's how you can test a POSIX regular expression in a Unix-like terminal:</p> <pre><code>echo 'sample_string' | grep -E 'posix_regex'\n</code></pre> <p>We also recommend creating a test helper class or method to ensure consistency in matching behaviors similar to the one below:</p> <pre><code>def ruby_posix_regexp_same?(test_string, ruby_regex)\n  # Prepare POSIX regex from Ruby regex\n  posix_regex = ruby_regex.source\n\n  # Evaluate regex in Ruby\n  ruby_match = !!(test_string =~ ruby_regex)\n\n  # Prepare command for bash execution\n  grep_command = \"echo '#{test_string}' | grep -E '#{posix_regex}' &gt; /dev/null\"\n\n  # Evaluate regex in bash (POSIX)\n  posix_match = system(grep_command)\n\n  # Compare results\n  comparison_result = ruby_match == posix_match\n\n  # Remove printing for automated specs\n  puts \"Ruby match: #{ruby_match}\"\n  puts \"POSIX match: #{posix_match}\"\n  puts \"Comparison: #{comparison_result}\"\n\n  comparison_result\nend\n</code></pre> <p>Below, you can see how certain regular expressions differ between Ruby and <code>libc</code>:</p> <pre><code>ruby_posix_regexp_same?('test12.production', /\\d\\d/)\n# Ruby match: true\n# POSIX match: false\n# Comparison: false\n\nruby_posix_regexp_same?('test12.production', /[0-9]{2}/)\n# Ruby match: true\n# POSIX match: true\n# Comparison: true\n\nruby_posix_regexp_same?('test12.production', /[0-9]{10}/)\n# Ruby match: false\n# POSIX match: false\n# Comparison: true\n</code></pre> <p>This comparative testing method offers a straightforward way to ensure your regular expressions behave as expected across different environments, particularly when working with librdkafka in Karafka.</p>"}, {"location": "Pro-Routing-Patterns/#example-use-cases", "title": "Example Use Cases", "text": "<ol> <li> <p>Tenant-specific Topics: Modern SaaS applications often cater to multiple tenants, each requiring its own data isolation. You can ensure data segregation by having a Kafka topic for each tenant, like <code>tenantA_events</code>, <code>tenantB_logs</code>. Routing Patterns can simplify the consumption from these dynamically created topics.</p> </li> <li> <p>Environment-based Topics: Development environments like staging, production, or QA might generate events. Using routing patterns can streamline the consumption process if these are categorized into topics like <code>staging_logs</code> and <code>prod_errors</code>.</p> </li> <li> <p>Versioned Topics: As systems evolve, data formats and structures change. Regexp patterns can handle these variations smoothly if you've chosen to version your topics, like <code>data_v1</code>, <code>data_v2</code>.</p> </li> <li> <p>Date-based Topics: The feature becomes invaluable for systems that rotate topics based on timeframes, like <code>logs_202301</code> and <code>logs_202302</code>, ensuring no topic goes unnoticed.</p> </li> <li> <p>Special-event Topics: Seasonal events, promotions, or sales like <code>blackfriday_deals</code>, <code>holiday_discounts</code> often have dedicated topics. This feature ensures that such transient topics are efficiently catered to.</p> </li> <li> <p>Automated Testing Topics: In CI/CD pipelines, where automated tests might create on-the-fly topics like <code>test_run_001</code>, <code>test_run_002</code>, regexp routing can prove to be a boon.</p> </li> <li> <p>Backup and Archive Topics: Systems that create backup topics, like <code>archive_2023Q1</code>, <code>backup_2023Q2</code>, can benefit by dynamically routing these for monitoring or analysis.</p> </li> <li> <p>Error and Debug Topics: Special topics created for debugging or error tracking, like <code>errors_critical</code>, <code>debug_minor</code>, can be consumed automatically using patterns.</p> </li> <li> <p>Dedicated DLQ (Dead Letter Queue) Topics: Handling erroneous or unprocessable messages becomes crucial as applications scale. Systems can isolate and address these problematic messages by employing dedicated Kafka topics for DLQ, such as <code>dlq_orders</code>, <code>dlq_notifications</code>. Karafka's Routing Patterns can be leveraged to automatically detect and route messages to these DLQ topics, ensuring efficient monitoring and subsequent troubleshooting.</p> </li> </ol>"}, {"location": "Pro-Routing-Patterns/#summary", "title": "Summary", "text": "<p>Karafka's Routing Patterns offers a dynamic solution for message routing, utilizing the power of regular expressions. By defining regexp patterns within routes, this feature allows automatic detection and consumption of Kafka topics that match the specified patterns. This functionality ensures agile integration of new and pre-existing topics that have yet to be defined in routes, simplifying the management process and eliminating the need for manual configuration. Whether handling tenant-specific or dedicated Dead Letter Queue topics, Karafka's Routing Patterns enhance flexibility and efficiency in data flow management.</p> <p>Last modified: 2025-05-21 16:48:20</p>"}, {"location": "Pro-Scheduled-Messages/", "title": "Scheduled Messages", "text": "<p>Karafka's Scheduled Messages feature allows users to designate specific times for messages to be sent to particular Kafka topics. This capability ensures that messages are delivered at predetermined times, optimizing workflows and allowing for precise processing and message handling timing in case a message should not arrive immediately.</p> <p>Conceptually, it works in a similar way to the ETF1 Kafka Message Scheduler.</p>"}, {"location": "Pro-Scheduled-Messages/#how-does-it-work", "title": "How Does It Work?", "text": "<p>Karafka's Scheduled Messages feature provides a straightforward approach to scheduling Kafka messages for future delivery, allowing users to set precise timings for when messages should reach the desired topics. Using a specialized API, Karafka allows users to easily specify the delivery time for messages without having to handle the complex scheduling details directly. This is achieved by wrapping the messages in an envelope that includes all necessary scheduling information and dispatching them to a special \"in-the-middle\" topic, which is then managed automatically by Karafka.</p> <p>Each day at midnight, Karafka's scheduler, a dedicated consumer, reloads and scans a specific Kafka topic designated for storing scheduled messages. This routine includes loading new messages meant for the day, ensuring they are ready for dispatch at specified times. To maintain a seamless and continuous operation, the scheduler dispatches messages at regular intervals, typically every 15 seconds, which is configurable. This ensures that all scheduled messages are delivered when needed.</p> <p>One key aspect of Karafka's handling of scheduled messages is its treatment of message payloads. It ensures the payload and user headers remain untouched by simply proxy-passing them with added trace headers.</p> <p> </p> <p> *Illustration presenting how Scheduled Messages work.    </p> <p>When Karafka dispatches a scheduled message, it also produces a special tombstone message that goes back to the scheduled messages topic. This tombstone message serves as a marker to indicate that a particular message has already been dispatched. In case of a system failure or a consumer group rebalance, this tombstone prevents the same message from being sent multiple times, ensuring that messages are processed exactly once.</p> <p>The tombstone message effectively sets the message payload to null using the same key as the scheduled message. This is recognized by Kafka, which then uses this marker to eventually remove the message from the schedules topic. Kafka's log compaction feature looks for these null payloads and purges them during the cleanup, preventing duplicate processing and optimizing the storage by removing obsolete data. Additionally, this compaction process has the benefit of streamlining Karafka's daily schedule loading. After midnight, when the scheduler re-reads the topic to load the day's schedule, compacted messages marked with tombstone messages and subsequently removed are not even sent from Kafka. This reduces the amount of data the scheduler needs to process, allowing for faster and more efficient loading of the daily schedule, thus enhancing the overall performance and responsiveness of the scheduling system.</p>"}, {"location": "Pro-Scheduled-Messages/#enabling-scheduled-messages", "title": "Enabling Scheduled Messages", "text": "<p>Karafka provides a convenient API to facilitate the scheduling of messages directly within your routing configuration. By invoking the <code>#scheduled_messages</code> method, you can easily set up a consumer group and topics dedicated to handling your scheduled messages. This setup involves minimal configuration from the user's end and leverages Karafka's built-in declarative topics API for needed topics management.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    scheduled_messages('scheduled_messages_topic_name')\n  end\nend\n</code></pre> <p>Karafka will automatically set up the necessary configurations for this topic and the states topic, including:</p> <ul> <li>Enabling specific serializers and deserializers tailored for handling scheduled message formats.</li> <li>Configuring Kafka consumer settings that are optimized for message scheduling, such as enabling manual offset management and log compaction policies.</li> <li>Establishing periodic execution checks to ensure messages are dispatched according to their scheduled times.</li> </ul> <p>This approach simplifies setting up scheduled messages, making them accessible without requiring deep dives into Kafka's configuration details.</p>"}, {"location": "Pro-Scheduled-Messages/#creating-required-kafka-topics", "title": "Creating Required Kafka Topics", "text": "<p>When <code>scheduled_messages</code> is invoked, this command will automatically create appropriate entries for Karafka Declarative Topics. This means that all you need to do is to run the:</p> <pre><code>bundle exec karafka topics migrate\n</code></pre> <p>Two appropriate topics with the needed configuration will be created.</p> <p>If you do not use Declarative Topics, please make sure to create those topics manually with below settings:</p> Topic name Settings YOUR_SCHEDULED_TOPIC_NAME <ul> <li>             partitions: <code>5</code> </li> <li>             replication factor: aligned with your company policy           </li> <li> <code>'cleanup.policy': 'compact'</code> </li> <li> <code>'min.cleanable.dirty.ratio': 0.1</code> </li> <li> <code>'segment.ms': 3600000 # 1 hour</code> </li> <li> <code>'delete.retention.ms': 3600000 # 1 hour</code> </li> <li> <code>'segment.bytes': 52428800 # 50 MB</code> </li> </ul> YOUR_SCHEDULED_TOPIC_NAME_states <ul> <li>             partitions: <code>5</code> </li> <li>             replication factor: aligned with your company policy           </li> <li> <code>'cleanup.policy': 'compact'</code> </li> <li> <code>'min.cleanable.dirty.ratio': 0.1</code> </li> <li> <code>'segment.ms': 3600000 # 1 hour</code> </li> <li> <code>'delete.retention.ms': 3600000 # 1 hour</code> </li> <li> <code>'segment.bytes': 52428800 # 50 MB</code> </li> </ul> <p>Topic Partition Consistency Required</p> <p>When manually creating or reconfiguring topics for scheduled messages, ensure that both topics have the same number of partitions. This consistency is crucial for maintaining the integrity and reliability of the message scheduling and state tracking processes.</p>"}, {"location": "Pro-Scheduled-Messages/#replication-factor-configuration-for-the-production-environment", "title": "Replication Factor Configuration for the Production Environment", "text": "<p>Setting the replication factor for Kafka topics used by the scheduled messages feature to more than 1 in production environments is crucial. The replication factor determines how many copies of the data are stored across different Kafka brokers. Having a replication factor greater than 1 ensures that the data is highly available and fault-tolerant, even in the case of broker failures.</p> <p>For example, if you set a replication factor of 3, Kafka will store the data on three different brokers. If one broker goes down, the data is still accessible from the other two brokers, ensuring that your recurring tasks continue to operate without interruption.</p> <p>Here's an example of how to reconfigure the scheduled messages topics so they have replication factor of 3:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    scheduled_messages(:scheduled_messages) do |schedules_topic, states_topic|\n      schedules_topic.config.replication_factor = 2\n      states_topic.config.replication_factor = 2\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Scheduled-Messages/#advanced-configuration", "title": "Advanced Configuration", "text": "<p>Karafka\u2019s Scheduled Messages feature is designed to be highly configurable to meet diverse application requirements and optimize operational performance. The configuration phase allows you to tailor various aspects of handling scheduled messages, including dispatch frequency, message batching, and producer specifications.</p> <p>During the initial setup of the Karafka application, you have the opportunity to adjust settings that affect the behavior of scheduled messages, such as:</p> <ul> <li> <p>Dispatch Interval: The frequency with which Karafka checks for and dispatches messages.</p> </li> <li> <p>Batch Size: The number of messages dispatched in a single batch. This is crucial for systems with high message volumes or limited resources, where managing load is essential for maintaining performance.</p> </li> <li> <p>Producer Customization: You can specify which producer Karafka should use for scheduled messages. This allows the integration of custom logic, such as specific partitioning strategies or the usage of a transactional producer.</p> </li> <li> <p>Topic Naming: The naming of topics used for message states can be customized to fit naming conventions or organizational policies, providing clarity and consistency in resource management.</p> </li> </ul> <pre><code>Karafka.setup do |config|\n  # Dispatch checks every 5 seconds\n  config.scheduled_messages.interval = 5_000\n  # Dispatch up to 200 messages per batch\n  config.scheduled_messages.flush_batch_size = 200\nend\n</code></pre> <p>Please refer to the code sources for more details.</p>"}, {"location": "Pro-Scheduled-Messages/#multi-application-deployments", "title": "Multi-Application Deployments", "text": "<p>When deploying Karafka's Scheduled Messages feature across multiple applications within the same Kafka cluster, it's essential to ensure proper isolation and prevent conflicts between applications. This section covers configuration for managing scheduled messages in multi-application environments.</p>"}, {"location": "Pro-Scheduled-Messages/#consumer-group-isolation", "title": "Consumer Group Isolation", "text": "<p>The most critical aspect of running scheduled messages across multiple applications is ensuring that each application uses a unique consumer group ID and dedicated schedule topics. This prevents applications from interfering with each other's message processing and ensures that scheduled messages are handled correctly by their intended applications.</p> <pre><code># Application 1 - \"orders_app\"\nclass OrdersApp &lt; Karafka::App\n setup do |config|\n   config.client_id = 'orders_app'\n   # Unique group ID for this application\n   config.consumer_group_id = 'orders_app_schedules_group'\n   # Other config here...\n end\n\n routes.draw do\n   scheduled_messages('scheduled_messages_orders')\n end\nend\n\n# Application 2 - \"billing_app\"\nclass BillingApp &lt; Karafka::App\n setup do |config|\n   config.client_id = 'billing_app'\n   # Different group ID for this application\n   config.consumer_group_id = 'billing_app_schedules_group'\n   # Other config here...\n end\n\n routes.draw do\n   scheduled_messages('scheduled_messages_billing')\n end\nend\n</code></pre>"}, {"location": "Pro-Scheduled-Messages/#dedicated-topic-strategy", "title": "Dedicated Topic Strategy", "text": "<p>Each application needs to use its own dedicated topics for scheduled messages:</p> Application Scheduled Messages Topic States Topic Orders App <code>scheduled_messages_orders</code> <code>scheduled_messages_orders_states</code> Billing App <code>scheduled_messages_billing</code> <code>scheduled_messages_billing_states</code> Inventory App <code>scheduled_messages_inventory</code> <code>scheduled_messages_inventory_states</code>"}, {"location": "Pro-Scheduled-Messages/#scheduling-messages", "title": "Scheduling Messages", "text": "<p>To schedule a message, you must wrap your regular message content and additional scheduling information. This is done using the <code>Karafka::Pro::ScheduledMessages.schedule</code> method. This method requires you to prepare the message just as you would for a standard Kafka message, but instead of sending it directly to a producer, you wrap it to include scheduling details.</p> <p>Below, you can find exact details on how to do this:</p> <ol> <li>Prepare Your Message: Construct your message payload and any associated headers as usual for a Kafka message.</li> </ol> <pre><code>message = {\n  topic: 'events',\n  payload: { type: 'inserted', count: 123 }.to_json\n}\n</code></pre> <ol> <li>Define the Delivery Time: Determine when the message should be dispatched. This time is specified as a Unix epoch timestamp and passed as the epoch parameter.</li> </ol> <pre><code># Make sure to use a Unix timestamp format\nfuture_unix_epoch_time = 15.minutes.from_now.to_i\n</code></pre> <ol> <li>Wrap the Message: Use the schedule method to wrap your message with the scheduling details:</li> </ol> <pre><code>enveloped = Karafka::Pro::ScheduledMessages.schedule(\n  message: message,\n  epoch: future_unix_epoch_time,\n  envelope: {\n    # The Kafka topic designated for scheduled messages\n    # Please read other sections on the envelope details available\n    topic: 'scheduled_messages_topic'\n  }\n)\n</code></pre> <ol> <li>Dispatch the Wrapped Message: Once the message is wrapped with all required scheduling details, the resulting hash represents a fully prepared scheduled message. This message can then be dispatched like any other message in Karafka:</li> </ol> <pre><code>Karafka.producer.produce_async(enveloped)\n</code></pre> <p>This call places the wrapped message into the designated scheduling topic, which remains until the specified epoch time is reached. At that time, Karafka's internal scheduling mechanism automatically processes and dispatches the message to its intended destination.</p>"}, {"location": "Pro-Scheduled-Messages/#message-key-uniqueness-and-ordering-management", "title": "Message Key Uniqueness and Ordering Management", "text": "<p>When using this feature, it is crucial to understand dispatched messages keys and their uniqueness operations. Each message sent to the scheduler must have a distinct key within its partition to ensure accurate scheduling and prevent any potential overlaps or errors during the dispatch process.</p> <p>By default, Karafka generates a unique key for each scheduled message envelope. This key is a combination of the topic to which the message will eventually be dispatched and a UUID. The formula used is something akin to: \"{target_topic}-{UUID}\". This key is then assigned to the <code>:key</code> field in the envelope, ensuring that each message is uniquely identified within the scheduling system.</p> <p>This generated key serves multiple purposes:</p> <ul> <li> <p>Uniqueness: Guarantees that each message can be uniquely identified and retrieved.</p> </li> <li> <p>Management: Facilitates updating or canceling the scheduled dispatch of the message.</p> </li> </ul> <p>Suppose the original message contains <code>partition_key</code> or a direct <code>partition</code> reference. In that case, Karafka will also attempt to use any of those values as the envelope <code>partition_key</code> to ensure that consecutive messages with the same details are always dispatched to the same partition and later sent to a target topic in order.</p> <p>If you prefer to use your key instead of relying on the auto-generated key, you can provide your chosen key directly in the envelope. When you specify your key this way, Karafka will use it without generating a new one. This approach allows you to maintain consistency or alignment with other systems or data structures you might be using:</p> <pre><code>enveloped = Karafka::Pro::ScheduledMessages.schedule(\n  message: message,\n  epoch: future_unix_epoch_time,\n  envelope: {\n    topic: 'scheduled_messages_topic',\n    # You can set key\n    key: 'anything you want',\n    # As well as the partition key\n    partition_key: 'anything'\n  }\n)\n</code></pre> <p>Ensure Unique Custom Keys</p> <p>The custom key you use must remain unique within the scheduler topic's partition context. Reusing the same key for multiple messages in the same partition is not advisable, as each subsequent message will overwrite the previous one in the scheduling system, regardless of their scheduled times.</p> <p>If you must ensure that particular messages are dispatched in a specific order, you can utilize the <code>partition_key</code> field in the envelope. While the key may still be automatically generated to maintain uniqueness, setting the <code>partition_key</code> to a value that matches the underlying message key ensures that:</p> <ul> <li>All related messages (sharing the same raw message key) are dispatched to the same partition.</li> <li>The order of messages is preserved as per the sequence of their scheduling.</li> </ul> <p>This strategy leverages Kafka's partitioning mechanism, providing a predictable processing order. Messages with the same <code>partition_key</code> are guaranteed to be sent to the same partition and thus processed in the order they were sent:</p>"}, {"location": "Pro-Scheduled-Messages/#cancelling-scheduled-messages", "title": "Cancelling Scheduled Messages", "text": "<p>Karafka provides a straightforward mechanism to cancel scheduled messages before they are dispatched. This capability is essential for scenarios where conditions change after a message has been scheduled and the message is no longer required or needs to be replaced.</p> <p>To cancel a scheduled message, you will utilize the <code>#cancel</code> method provided by Karafka's Scheduled Messages feature. This method lets you specify the unique key of the message you want to cancel. This key should be the same as the one used in the message envelope when the message was originally scheduled.</p> <p>To cancel a scheduled message:</p> <ol> <li> <p>Identify the Message Key: Locate the unique key of the message you intend to cancel. This is the key that was specified or generated when the message was initially scheduled.</p> </li> <li> <p>Specify the Topic: Provide the topic where the scheduled message resides. This is essential because Karafka can handle multiple scheduling topics, and specifying the correct topic ensures the message is accurately identified and targeted for cancellation.</p> </li> <li> <p>Invoke the <code>cancel</code> Method: Use the <code>cancel</code> method to create a cancellation request for the scheduled message. This method requires the unique key and the topic to formulate the cancellation command properly.</p> </li> </ol> <pre><code>cancellation_message = Karafka::Pro::ScheduledMessages.cancel(\n  key: 'your_message_key',\n  envelope: { topic: 'your_scheduled_messages_topic' }\n)\n</code></pre> <ol> <li>Dispatch the Cancellation Message: After generating the cancellation message, it must be dispatched using Karafka's producer to ensure the cancellation is processed. This step finalizes the cancellation by publishing the tombstone message to the scheduling topic.</li> </ol> <pre><code>Karafka.producer.produce_sync(cancellation_message)\n</code></pre> <p>The unique key is critical in the cancellation process for several reasons:</p> <ul> <li> <p>Targeting the Correct Message: It ensures that the exact message intended for cancellation is correctly identified.</p> </li> <li> <p>Avoiding Conflicts: By maintaining unique keys for each message, you prevent potential issues where multiple messages might be inadvertently affected by a single cancellation command.</p> </li> </ul> <p>Custom Keys and Automatic Key Generation</p> <p>When scheduling a message, if you choose to use a custom key or rely on the automatically generated key by Karafka, it's important to use the same key consistently for both scheduling and canceling the message. This consistency helps prevent errors and ensures that the correct message is targeted for cancellation.</p> <p>Handling Cancellations with <code>partition_key</code></p> <p>If the message was scheduled with a <code>partition_key</code> to maintain a specific order or partition consistency, it is advisable to use the same <code>partition_key</code> during cancellation. This practice helps manage related messages within the same partition effectively and maintains the order and integrity of operations within that partition.</p>"}, {"location": "Pro-Scheduled-Messages/#updating-scheduled-messages", "title": "Updating Scheduled Messages", "text": "<p>Karafka's Scheduled Messages feature allows updating messages that have already been scheduled but not yet dispatched. This functionality is crucial when a scheduled message's content, timing, or conditions need to be modified before its dispatch.</p> <p>Updating a scheduled message in Karafka involves a straightforward process similar to scheduling a new message, with a key distinction: you must use the same unique envelope key used for the original scheduling. This ensures that the new message parameters overwrite the old ones in the scheduling system.</p> <p>To update the scheduled message:</p> <ol> <li> <p>Identify the Original Envelope Key: Use the same envelope key that was assigned during the initial scheduling of the message. This key is crucial for targeting the correct message for updates.</p> </li> <li> <p>Prepare the Updated Message: Create the new message payload as you would for any Kafka message. Include any changes to the message content or any additional headers needed.</p> </li> <li> <p>Set the New Dispatch Time: If the timing needs to be changed, determine the new dispatch time. This time should be specified as a Unix epoch timestamp.</p> </li> <li> <p>Use the <code>schedule</code> Method: Employ the <code>schedule</code> method to wrap your updated message with the scheduling details, using the same key and specifying the new or unchanged dispatch time:</p> </li> </ol> <pre><code>updated_message = Karafka::Pro::ScheduledMessages.schedule(\n  message: new_message_to_replace_old,\n  epoch: new_future_unix_epoch_time,\n  envelope: {\n    topic: 'scheduled_messages_topic',\n    # Same key as the original message\n    key: original_envelope_key\n  }\n)\n</code></pre> <ol> <li>Dispatch the Updated Message: Send the updated message using Karafka\u2019s producer. This step replaces the original scheduled message with the new content and timing in the scheduling system:</li> </ol> <pre><code>Karafka.producer.produce_async(updated_message)\n</code></pre>"}, {"location": "Pro-Scheduled-Messages/#monitoring-and-metrics", "title": "Monitoring and Metrics", "text": "<p>Karafka's Scheduled Messages feature integrates with standard monitoring tools provided by Karafka and its Web UI, allowing for visibility into the system's performance and behavior. It is, however, important to understand how this feature consumer may deviate from the standard consumer behaviors.</p> <p>Karafka treats scheduled messages like any other messages within its ecosystem:</p> <ul> <li> <p>Message Consumption Tracking: Karafka marks messages as consumed in the scheduling topics as they arrive. This standard practice allows monitoring systems to track the progress and detect if there are delays or backlogs in message processing.</p> </li> <li> <p>Visibility in Karafka Web UI: The Karafka Web UI provides a straightforward way to view message consumption status, including any system lag. This is particularly useful for quickly diagnosing issues or confirming that scheduled messages are being processed as expected.</p> </li> </ul> <p>Due to the nature of how scheduled messages are managed:</p> <ul> <li> <p>Daily Reloads: Around midnight, when Karafka reloads the daily schedules, there are expected spikes in activity within the scheduled messages topics. This behavior is normal and should be accounted for in performance metrics and monitoring setups.</p> </li> <li> <p>Message Throughput: Monitoring systems should be configured to differentiate between regular message throughput and the spikes caused by these reloads. Understanding this distinction will help in accurately assessing system performance without raising false alarms during expected spikes.</p> </li> </ul> <p>Karafka enhances monitoring capabilities by publishing regular messages to a states topic, which contains crucial data about scheduled operations:</p> <ul> <li> <p>Daily Schedules: Messages sent to the states topic include data on the number of messages scheduled for dispatch on a given day. This provides a clear metric to gauge the volume of upcoming message dispatches and prepare for potential load increases.</p> </li> <li> <p>Consumer State Reporting: Each message in the state topic also reports the current state of the consumer handling a given partition. </p> </li> </ul> <p>The states can be:</p> <ul> <li> <p><code>loading</code>: Indicates that the consumer is currently refreshing the data during the daily reload or after a rebalance.</p> </li> <li> <p><code>loaded</code>: Signifies that all scheduled data is in memory and that dispatches proceed uninterrupted.</p> </li> </ul> <p>These state updates are vital for understanding the message scheduling system's real-time status, providing insights into both routine operations and any issues that may arise.</p>"}, {"location": "Pro-Scheduled-Messages/#consumer-data-and-error-reporting", "title": "Consumer Data and Error Reporting", "text": "<p>Aside from the periodic state updates sent to the states topic, the error reporting and data handling for consumers of scheduled messages do not differ from other Karafka consumers:</p> <ul> <li> <p>Error Handling: Any errors encountered by the scheduled message consumers are handled and reported like other consumers within the Karafka system.</p> </li> <li> <p>Metrics Collection: Metrics for error rates, processing times, and other key performance indicators are collected and can be viewed in the Karafka Web UI or through integrated monitoring tools.</p> </li> </ul> <p>This consistent monitoring and error reporting approach ensures that administrators and developers can use familiar tools and processes to manage and troubleshoot scheduled message consumers, simplifying system maintenance and oversight.</p>"}, {"location": "Pro-Scheduled-Messages/#web-ui-management", "title": "Web UI Management", "text": "<p>Karafka's Web UI displays daily dispatch estimates and the current loading state of partitions involved in scheduled messaging. This visibility is crucial after midnight reloads, allowing administrators to detect operational delays or issues quickly.</p> <p>Additionally, the Web UI offers a detailed exploration of scheduled messages, showing specific information about each message queued for future dispatch. Users can view scheduled times, payload content, and other relevant metadata, aiding in monitoring and verifying the scheduling system's effectiveness.</p> <p> </p> <p>Future Day Cancellation Reporting Limitations</p> <p>The Web UI scheduled messages reporting will not reflect cancellations of scheduled messages for days beyond the current day. This limitation is a performance optimization that allows the system to store statistics for upcoming days without having to maintain complete data for each future day.</p> <p>While cancellation statistics for the current day are 100% accurate, cancellations for future days will only be reflected in the reporting once that day becomes the current day. This approach significantly reduces memory usage and processing overhead, as the system only needs to maintain detailed schedule data for the current day rather than for all future scheduled messages.</p> <p>When you cancel a scheduled message for a future day, the cancellation is properly recorded in Kafka, but the Web UI statistics will not be updated until that day arrives and the full schedule for that day is loaded into memory.</p>"}, {"location": "Pro-Scheduled-Messages/#error-handling-and-retries", "title": "Error Handling and Retries", "text": "<p>This feature is designed to ensure robust error handling and efficient retry mechanisms, minimizing the potential impact of errors on scheduled message dispatches. Here's an overview of how error handling and retries are managed:</p> <p>Key Points on Error Handling:</p> <ul> <li> <p>No Deserialization of Payload: Karafka does not deserialize the payload of scheduled messages; it passes the raw data through. This approach eliminates serialization errors, common sources of faults in message processing systems.</p> </li> <li> <p>Raw Values for Keys and Headers: Similarly to the payload, Karafka uses raw values for keys and headers. This ensures that the data integrity is maintained without introducing serialization or format errors during the handling process.</p> </li> <li> <p>No Default DLQ (Dead Letter Queue) Strategies: By default, Karafka does not implement any Dead Letter Queue strategies for intermediate message topics used in scheduling. Any blocking errors may require manual intervention.</p> </li> </ul> <p>Retry Policies:</p> <ul> <li> <p>Default User Configuration: Karafka relies on the default retry policies specified in the user's configuration settings. This means that delivery errors and other operational errors will trigger retries according to the predefined rules in the configuration.</p> </li> <li> <p>Handling Dispatch Errors: While dispatch errors and other issues may cause messages to be dispatched more than once, the system is designed to ensure that no messages are skipped.</p> </li> </ul> <p>Schema Compatibility and System Upgrades:</p> <ul> <li> <p>Schema Incompatibilities: When changes to the Karafka schema affect the scheduled messages feature, the system is built to detect these incompatibilities and halt dispatching. This behavior is crucial to prevent issues arising from processing messages with an outdated schema version.</p> </li> <li> <p>Halting Dispatch During Upgrades: When schema changes are detected, the dispatching process will stop, and it will not resume until the affected components of the system are upgraded to the compatible version. This approach mirrors how critical updates are managed in systems like Karafka's Web UI, ensuring stability and consistency during upgrades.</p> </li> <li> <p>Rolling Deployments: The halt-on-incompatibility feature supports rolling deployments by allowing parts of a system to be updated incrementally. During this process, message dispatching is safely paused until all components are confirmed to be compatible, minimizing downtime and disruption.</p> </li> </ul> <p>This feature's error handling and retry strategies ensure that the system remains resilient and reliable, even in the face of operational challenges. Its approach to managing dispatch errors, retries, and schema changes ensures that messages are handled appropriately under all circumstances, supporting continuous and dependable operations.</p>"}, {"location": "Pro-Scheduled-Messages/#warranties", "title": "Warranties", "text": "<p>Karafka's Scheduled Messages feature offers a set of warranties designed to ensure robust functionality and reliability within production environments. Here's a summary of the key guarantees and limitations:</p> <ul> <li> <p>Message Delivery Guarantee: Karafka ensures that all scheduled messages will be delivered at least once, minimizing the risk of message loss. However, messages might be delivered multiple times under certain circumstances like network or Kafka cluster disruptions.</p> </li> <li> <p>Timeliness of Message Dispatch: While Karafka aims to dispatch messages close to their scheduled times, there is a flexibility window influenced by the dispatch frequency (default is every 15 seconds). This means that exact second-by-second scheduling might see slight deviations.</p> </li> <li> <p>System Stability: Karafka is built to maintain Stability under varying loads and manage daily schedule reloads without significant disruptions. Stability can be affected by external factors such as network performance and server capacity.</p> </li> <li> <p>Application-Level Failures: Karafka does not cover failures due to issues in consumer application logic or other external application errors.</p> </li> <li> <p>Configuration Dependence: The warranties assume optimal and correct configuration settings. Misconfigurations or inappropriate settings that impact performance and reliability are not covered.</p> </li> <li> <p>External Dependencies: Issues caused by external factors, including Kafka cluster performance, network latency, and hardware failures, are outside the scope of these warranties.</p> </li> </ul>"}, {"location": "Pro-Scheduled-Messages/#transactional-support", "title": "Transactional Support", "text": "<p>Karafka's Scheduled Messages feature provides transactional support to ensure exactly-once delivery of messages when used with a transactional WaterDrop producer. This support hinges on the proper configuration and execution of message batching and transaction handling within the Karafka ecosystem.</p> <p>When using a transactional WaterDrop producer, the Scheduled Messages feature can achieve exactly once delivery by including the dispatched message to the target topic and the corresponding tombstone message in a single transaction batch. This approach ensures that both sending the message and marking it as dispatched (via the tombstone) are atomically committed to Kafka, thus preventing any duplication or message loss during processing.</p>"}, {"location": "Pro-Scheduled-Messages/#limitations", "title": "Limitations", "text": "<p>While Scheduled Messages offer powerful message scheduling and dispatch capabilities, users should be aware of inherent limitations when integrating this feature into their systems. Understanding these limitations can help you plan and optimize scheduled messages usage within your applications.</p> <ol> <li> <p>Single Delivery Point: Scheduled messages are designed to be delivered to a specific topic at a predetermined time. Once a message is scheduled, redirecting it to a different topic at runtime is not supported. Message dispatch can be canceled, though.</p> </li> <li> <p>Lack of Built-In Dead Letter Queue: Karafka does not provide a built-in Dead Letter Queue (DLQ) mechanism for messages that fail to be processed after all retry attempts. Users must implement their own DLQ handling to manage undeliverable messages systematically.</p> </li> <li> <p>Dependency on System Clock: The scheduling mechanism depends on the accuracy and synchronization of system clocks across the Kafka cluster. Time discrepancies between servers can lead to delays in message delivery.</p> </li> <li> <p>Storage Overhead: Scheduled messages are stored until their dispatch time arrives. This can lead to increased storage usage on Kafka brokers, especially if large volumes of messages are scheduled far in advance or message payloads are large.</p> </li> <li> <p>No Real-Time Cancellation Feedback: When a scheduled message is canceled, there is no immediate feedback or confirmation that the message has been successfully removed from the schedule.</p> </li> <li> <p>Message Duplication Risks: Due to the nature of Kafka and the possibility of retries in the event of critical delivery errors, message duplication is a risk unless a transactional producer is used. While Karafka ensures that messages are not lost, duplicate messages may be delivered under certain failure scenarios.</p> </li> <li> <p>No Priority Queueing: Karafka does not support priority queueing for scheduled messages. Messages are dispatched in the order they are due, without any mechanism to prioritize specific messages over others, regardless of urgency.</p> </li> <li> <p>Memory Overhead: Karafka manages daily schedules by loading them into memory, which means the Karafka process handling these schedules may consume more memory than a regular Karafka consumer. This increase in memory usage can be significant depending on the volume and complexity of the scheduled tasks, requiring careful resource management.</p> </li> <li> <p>Dispatch Frequency: The dispatch frequency for scheduled messages is set by default to every 15 seconds, which can introduce inaccuracy to the exact moment of message delivery. While this interval is configurable, the nature of batch processing may not meet the needs of applications requiring precise second-by-second scheduling.</p> </li> <li> <p>Daily Schedule Management: Karafka manages schedules daily, which means that after midnight, it refetches all the data to build a new daily schedule. Messages scheduled right after midnight each day until the loading phase is done may be scheduled a bit later due to this data loading. This could introduce delays in message dispatch that are critical for time-sensitive tasks.</p> </li> <li> <p>High Egress Network Traffic: Because Karafka rescans all partitions of the scheduling topic after midnight to build daily schedules, this can cause increased egress network traffic. This might result in higher costs when using a Kafka provider that charges for network traffic.</p> </li> <li> <p>Independent Deployment and Management of Scheduler Consumers: Due to how Karafka reloads schedules daily and during restarts or rebalances, it is advisable to deploy and manage Karafka consumer processes that handle scheduled messages independently from regular consumer processes. Frequent redeployments or rebalancing of these scheduler consumers can lead to excessive schedule loading from Kafka, increasing network traffic and introducing delays in message scheduling and potential increases in operational costs.</p> </li> <li> <p>Limited Accuracy of Future State Reporting: While the state reporting for the current day's scheduled messages is accurate, the accuracy of state reporting for future days is more limited and primarily based on estimates. This estimation can lead to discrepancies, particularly regarding message cancellations:</p> <ul> <li> <p>Estimate Nature: The state information for future days includes all messages scheduled up to the last system check. However, it may not accurately reflect cancellations or other modifications to these messages until the schedules are fully reloaded the following day.</p> </li> <li> <p>Reliance on Daily Reloads: This limitation is due to the system's design, which relies on daily reloads to update and confirm the accuracy of the scheduled messages. Until this reload occurs, any cancellations made after the last update won't be reflected in the state reports.</p> </li> </ul> </li> </ol>"}, {"location": "Pro-Scheduled-Messages/#production-deployment", "title": "Production Deployment", "text": "<p>Given this feature's unique operational characteristics, particularly its data-loading patterns, it is advisable to deploy it in a manner that ensures long-running stability and isolates it from other processes.</p> <ul> <li> <p>Long-Running Stability: Scheduled messages are designed to operate in a continuous, long-running manner. This setup is crucial because the feature involves daily reloading of schedules based on predefined times (midnight). Ensuring these processes are not interrupted by deployments or other operational changes is key to maintaining their reliability.</p> </li> <li> <p>Isolation from Other Processes: Due to the significant resource consumption during data loading phases\u2014both in terms of memory and network bandwidth\u2014it's beneficial to isolate the scheduled messages handling from other processes. This separation helps prevent any potential performance degradation in different parts of your system due to the intensive data-loading activities in scheduled message processing.</p> </li> </ul>"}, {"location": "Pro-Scheduled-Messages/#best-practices", "title": "Best Practices", "text": "<ul> <li> <p>Dedicated Instances: Consider deploying the scheduled messages feature on dedicated instances or containers. This approach allows for tailored resource allocation, such as CPU, memory, and network bandwidth, which can be adjusted based on the specific demands of the scheduling tasks without affecting other services.</p> </li> <li> <p>Monitoring and Scaling: Implement robust monitoring to track the performance and resource utilization of the scheduled messages feature. This monitoring should focus on memory usage, CPU load, and network traffic metrics. Based on these metrics, apply auto-scaling policies where feasible to handle variations in load, especially those that are predictable based on the scheduling patterns.</p> </li> <li> <p>Avoid Frequent Redeploys: Minimize the frequency of redeployments for the components handling scheduled messages. Frequent restarts can disrupt the scheduling process, especially if they occur during daily data reloads.</p> </li> <li> <p>Update and Maintenance Windows: If possible, schedule maintenance and updates during off-peak hours. Choose times when the impact on scheduled message processing would be minimal, ideally when there are fewer critical tasks scheduled.</p> </li> <li> <p>Disaster Recovery and High Availability: Ensure that your deployment strategy includes provisions for disaster recovery and high availability. This could involve replicating the scheduling components across multiple data centers or using cloud services that provide built-in redundancy and failover capabilities.</p> </li> </ul>"}, {"location": "Pro-Scheduled-Messages/#external-producers-support", "title": "External Producers Support", "text": "<p>Karafka's Scheduled Messages feature is designed with flexibility, allowing integration with external producers beyond Karafka and WaterDrop. This enables scheduled messages to be dispatched from various technologies, including Go, JavaScript, or any other language and framework that supports Kafka producers.</p> <p>When using external producers to schedule messages, it's crucial to follow a specific header format to ensure Karafka's scheduled message consumer can recognize and correctly process these messages. The required headers are as follows:</p> <ul> <li> <p><code>schedule_schema_version</code>: Indicates the version of the scheduling schema used, which helps manage compatibility. For current implementations, this should be set to 1.0.0.</p> </li> <li> <p><code>schedule_target_epoch</code>: Specifies the Unix epoch time when the message should be dispatched. This timestamp determines when the message will be processed and sent to its target topic.</p> </li> <li> <p><code>schedule_source_type</code>: This should always be set to <code>schedule</code> to indicate that the message is a scheduled type.</p> </li> <li> <p><code>schedule_target_topic</code>: Defines the Kafka topic to which the message should be dispatched when its scheduled time arrives.</p> </li> <li> <p><code>schedule_target_key</code>: A unique identifier for the message within its target topic. This key is crucial for ensuring that messages are uniquely identified and managed within the scheduling system.</p> </li> </ul> <p>Additional optional headers can include:</p> <ul> <li> <p><code>schedule_target_partition</code>: If specified, directs the message to a particular partition within the target topic. This can be crucial for maintaining order or handling specific partitioning strategies.</p> </li> <li> <p><code>schedule_target_partition_key</code>: Used to determine the partition to which the message will be routed within the target topic based on Kafka's partitioning algorithm.</p> </li> </ul> <p>When implementing an external producer to schedule messages for Karafka, ensure that the message conforms to the required header format and includes all necessary scheduling information.</p>"}, {"location": "Pro-Scheduled-Messages/#alternatives", "title": "Alternatives", "text": "<p>When considering alternatives to Scheduled Messages feature, two other options provide distinct approaches to delaying or controlling the timing of message processing: Delayed Topics and Piping. Each method has its own advantages and use cases, depending on the specific requirements of your application, so they may also be a viable option worth considering.</p>"}, {"location": "Pro-Scheduled-Messages/#delayed-topics", "title": "Delayed Topics", "text": "<p>Delayed Topics in Karafka provide a mechanism to delay message consumption from specific topics for a set period of time. This is useful when you need to delay message processing globally for a topic without scheduling each message individually.</p> <ul> <li> <p>How It Works: By setting a delay on a topic, Karafka pauses consumption for a specified time before processing the messages. Other topics and partitions are unaffected, allowing for efficient resource utilization.</p> </li> <li> <p>Use Cases: Delayed Processing is ideal for scenarios like retry mechanisms, buffer periods for additional processing or validation, or temporary postponement during peak traffic.</p> </li> </ul> <p>Pros:</p> <ul> <li>Simple to set up using the <code>delay_by</code> option on topics.</li> <li>Causes no global lag since only the designated topic is delayed.</li> <li>Particularly useful when messages need consistent delays across the entire topic.</li> </ul> <p>Cons:</p> <ul> <li>Delay applies to all messages in the topic uniformly.</li> <li>Lack of fine-grained control for scheduling individual messages at specific times.</li> </ul>"}, {"location": "Pro-Scheduled-Messages/#delayed-piping", "title": "Delayed Piping", "text": "<p>Piping with Delayed Topics offers an advanced alternative to Scheduled Messages by leveraging both Delayed Processing and Piping to create \"time buckets.\" With this approach, you can create multiple delayed topics (e.g., <code>messages_5m</code>, <code>messages_30m</code>, <code>messages_1h</code>) that act as buffers for different delay durations. Once the delay period expires, messages are piped from these delayed topics to their final destination for processing.</p> <p>Use Cases:</p> <ul> <li> <p>Batch Processing: When processing time-sensitive data that needs to be delayed by different amounts of time (e.g., 5 minutes, 30 minutes, 1 hour) before final processing, using time buckets is an efficient approach.</p> </li> <li> <p>Time-Windowed Actions: Triggering actions based on elapsed time (e.g., after 30 minutes or 1 hour) can be easily orchestrated with delayed topics and piping.</p> </li> <li> <p>Throttling or Rate Limiting: You can effectively manage load or rate-limit processes by distributing messages into time buckets over different periods.</p> </li> </ul> <p>Pros:</p> <ul> <li>Flexibility to create multiple time delays for different types of messages.</li> <li>Allows you to group delayed messages into \"buckets\" based on their time requirements.</li> <li>Seamless integration of delayed processing with piping ensures that messages are forwarded to their final destination once their delay expires.</li> <li>Simplifies complex workflows that require messages to be processed after various time intervals.</li> </ul> <p>Cons:</p> <ul> <li>More configuration is required, as you need to manage multiple topics for different time delays.</li> <li>Timing is still dependent on topic-level delays, so more sophisticated scheduling might be required to fine-tune control over individual messages (beyond the bucket-level delay).</li> </ul>"}, {"location": "Pro-Scheduled-Messages/#choosing-between-alternatives", "title": "Choosing Between Alternatives", "text": "<ul> <li> <p>Scheduled Messages: Best for cases where you need precise control over the exact dispatch time for individual messages.</p> </li> <li> <p>Delayed Processing: The right choice when you need to delay the processing of all messages in a topic by a uniform time period, providing a sense of reassurance and organization.</p> </li> <li> <p>Piping with Delayed Topics: Ideal when you need time-bucketed delays for messages (e.g., 5 minutes, 30 minutes, 1 hour) and want messages to be forwarded to a final destination after their respective delays.</p> </li> </ul>"}, {"location": "Pro-Scheduled-Messages/#example-use-cases", "title": "Example Use-Cases", "text": "<ul> <li> <p>Regulatory Compliance Reports: Schedule a message to deliver a critical compliance report right before the audit deadline, ensuring all data is fresh and the submission is timely without last-minute rushes.</p> </li> <li> <p>Data Processing Kickoff: Send a scheduled message to initiate a complex ETL process at the start of a financial quarter, ensuring data is processed at the right moment for analysis and reporting.</p> </li> <li> <p>SLA Compliance Check: Set up a message to trigger an SLA compliance verification exactly when a new service level agreement goes into effect, automating the monitoring process without manual setup each time.</p> </li> <li> <p>Resource Scaling for Event Peaks: Automate infrastructure scaling by scheduling messages to increase cloud resources just hours before anticipated high-traffic events like online sales or product launches.</p> </li> <li> <p>Personalized User Engagement: Schedule a message to send a personalized offer to a user on their anniversary of joining your service, enhancing personal connection and loyalty.</p> </li> <li> <p>Feature Activation: At the start of a promotional period, use a scheduled message to activate a new application feature for all users, ensuring synchronized access for all users.</p> </li> <li> <p>Cache Expiration and Refresh: Schedule a message to trigger the refresh of a specific cache segment right before its set expiration time. This ensures data remains current without manual monitoring and maintains system performance and reliability.</p> </li> </ul> <p>Last modified: 2025-05-20 19:59:54</p>"}, {"location": "Pro-Scheduling-API/", "title": "Scheduling API", "text": "<p>Karafka Pro provides an advanced scheduling API that improves how consumption jobs are managed within a process. This introduction provides an overview of Karafka's advanced scheduling capabilities, emphasizing its control and flexibility in job execution and resource distribution.</p> <p>The Scheduling API was designed to control when specific consumption jobs are placed on the processing queue. This API is not just a simple timer-based mechanism but a sophisticated controller that allows for precise and intelligent scheduling of tasks. The scheduling API provides:</p> <ol> <li> <p>Granular Control Over Job Execution: With this API, developers can dictate when a job enters the processing queue, enabling previously unattainable precision. This control is essential for scenarios where the timing of job execution is critical to the overall system performance.</p> </li> <li> <p>Efficient Resource Distribution: By controlling when jobs are queued, the scheduling API facilitates granular resource allocation. This feature is handy in environments where resources are limited or must be distributed effectively among multiple tasks.</p> </li> <li> <p>Strategic Job Sequencing: The API allows for the sequencing of jobs so that some can be withheld until others are completed. This capability is crucial for maintaining dependencies between tasks and ensuring that high-priority jobs are processed promptly.</p> </li> <li> <p>Adaptive Scheduling Based on System State: Beyond static scheduling, the API can adapt job queuing based on the current system state. This means the scheduling decisions can respond to real-time conditions, such as workload changes, resource availability, or custom-defined metrics.</p> </li> </ol>"}, {"location": "Pro-Scheduling-API/#execution-model", "title": "Execution Model", "text": "<p>Understanding the job execution model in Karafka is crucial for effectively utilizing the advanced scheduling API. This section explains how Karafka manages and executes jobs, focusing on the relationship between subscription groups, the scheduler, workers, and the jobs queue.</p> <p>Karafka's interaction with Kafka and subsequent job processing are fundamentally based on two types of threads: the Listener and Worker thread(s). These threads serve distinct yet complementary data consumption and processing roles. Understanding the functions and interplay of these threads is critical to grasping how Karafka operates efficiently and effectively.</p>"}, {"location": "Pro-Scheduling-API/#listener-threads", "title": "Listener Threads", "text": "<p>The Listener thread serves two critical functions:</p> <ol> <li> <p>Fetching Data from Kafka: The Listener thread constantly polls Kafka to retrieve messages. This continuous polling is essential to ensure that new data is fetched as it becomes available.</p> </li> <li> <p>Triggering Work Scheduling: Once the Listener thread has successfully fetched messages from Kafka, its next responsibility is to trigger the scheduling of this work. This step is crucial as it involves handing the fetched messages to the scheduler. The scheduler then determines the appropriate time and order for these jobs to be processed based on the current system state and any custom logic defined within the scheduler.</p> </li> </ol> <p>The Listener thread, therefore, acts as the bridge between Kafka and Karafka's internal processing mechanisms, ensuring a steady flow of data into the system.</p> <p>It is important to note that the Listener thread in Karafka is designed to recognize when not to poll more data from Kafka. This mechanism ensures that it refrains from fetching additional messages if the system is currently processing a workload or if certain conditions necessitating a data ingestion pause are met. Essentially, the Listener thread only polls for more data when the system is ready, thereby maintaining a balanced and efficient processing environment.</p>"}, {"location": "Pro-Scheduling-API/#worker-threads", "title": "Worker Threads", "text": "<p>The Worker threads are where the actual processing of messages occurs. These threads are responsible for:</p> <ol> <li> <p>Picking Up Jobs: Worker threads monitor the job queue, where messages scheduled for processing are placed by the scheduler. When a job is available in the queue, a Worker thread picks it up for execution.</p> </li> <li> <p>Executing Jobs: The primary function of a Worker thread is to process the jobs it picks from the queue.</p> </li> </ol> <p>In environments with high volumes of data or complex processing requirements, multiple Worker threads may be employed to handle the workload efficiently. This multi-threaded approach allows Karafka to process many messages concurrently, significantly enhancing throughput and reducing latency.</p>"}, {"location": "Pro-Scheduling-API/#conclusion", "title": "Conclusion", "text": "<p>The Listener and Worker threads in Karafka are central to its architecture, working in tandem to ensure efficient data flow and processing. The Listener thread's role in fetching data and triggering scheduling, combined with the Worker thread's job execution responsibilities, creates a robust and scalable system capable of handling the demands of real-time data processing with Kafka.</p> <p>Below, you can find a simplified illustration of the cooperation of Listener and Worker threads and their connection via the jobs queue.</p> <p> </p> <p> *This example illustrates how Listener and Worker threads cooperate via the jobs queue.    </p>"}, {"location": "Pro-Scheduling-API/#job-locking-and-polling-synchronization", "title": "Job Locking and Polling Synchronization", "text": "<p>To effectively use Scheduling API, it is recommended to understand the Karafka polling mechanism and its relationship with the jobs execution layer. Understanding the interplay between them is crucial when writing custom schedulers.</p> <ul> <li> <p>Blocking Standard Non-Long Running Jobs: Karafka's handling of standard, non-long-running jobs is inherently blocking in nature. This means that while a job from a particular subscription group is running, Karafka will not poll more data from Kafka. The rationale behind this design is rooted in Kafka's <code>max.poll.interval.ms</code> setting, which functions as a heartbeat for the polling process.</p> </li> <li> <p>Automatic Blocking with Job Queues: Jobs enqueued in Karafka's job queue automatically trigger a block on polling. This feature simplifies the management of job execution, as there is no need for explicit locking mechanisms for these jobs. By automatically blocking polling when a job is queued, Karafka reduces the complexity of job management. Developers don't have to implement lock mechanisms for standard job queueing operations manually.</p> </li> <li> <p>Explicit Locking for Delayed Jobs: Your scheduler may wait to place jobs on the jobs queue. In cases where a job needs to be delayed or withheld from immediate queuing, explicit locking is required. This is achieved using the jobs queue's <code>#lock</code> method. A locked job must be explicitly unlocked before it can be enqueued using the <code>#unlock</code> method. This lock-unlock mechanism allows developers to control the timing of job enqueuing while still adhering to Kafka's polling expectations.</p> </li> <li> <p>Ensuring All Jobs are Enqueued: A fundamental principle in Karafka's job management is that every job, regardless of its perceived importance or urgency, must eventually be enqueued unless given subscription group is in a recovery state. Discarding jobs, even if they appear unnecessary, can disrupt the delicate balance of the polling mechanism and lead to data consistency or lost messages. Ensure that all jobs are eventually enqueued. This applies to scenarios that seem irrelevant, for example, for topics revoked after a rebalance. Karafka's internal logic will detect if the job is no longer needed. In such cases, the worker will bypass the user-defined logic, preventing unnecessary processing and saving system resources. However, the scheduler still needs to schedule the work.</p> </li> </ul>"}, {"location": "Pro-Scheduling-API/#implementing-a-scheduler", "title": "Implementing a Scheduler", "text": "<p>This section provides a guide on implementing and using a custom scheduler. It will cover the initialization of a scheduler, choosing the appropriate type (stateful or stateless), and using non-blocking methods.</p> <p>Make Sure All Jobs Are Scheduled</p> <p>Please ensure that every job provided to the scheduler gets scheduled except for the subscription group recovery case. It's okay if job scheduling is delayed, but all jobs must end up in the jobs queue. Not doing this can cause problems with how the system works.</p>"}, {"location": "Pro-Scheduling-API/#types-of-schedulers", "title": "Types of Schedulers", "text": "<p>You can build two primary types of schedulers: stateful and stateless. Each scheduling method in this API has a non-blocking counterpart, which is vital for specific use cases.</p>"}, {"location": "Pro-Scheduling-API/#stateful-schedulers", "title": "Stateful Schedulers", "text": "<p>A stateful scheduler maintains state information across its scheduling runs. This means that the scheduler can retain the knowledge you want, including buffers for jobs, which can influence the execution of current or future tasks. Stateful schedulers are particularly useful in scenarios where the order and outcome of jobs are interdependent. Since the default API for building schedulers is fundamentally thread-safe, you do not have to use any locking mechanisms.</p>"}, {"location": "Pro-Scheduling-API/#stateless-schedulers", "title": "Stateless Schedulers", "text": "<p>In contrast, a stateless scheduler does not retain state information from one task to the next. Each scheduling task is treated as an independent event without knowledge of past or future tasks. Stateless schedulers are more straightforward and may be preferable in scenarios where tasks are entirely separate.</p>"}, {"location": "Pro-Scheduling-API/#api-methods", "title": "API Methods", "text": "<p>Please note that each method described in the following section has a non-blocking counterpart. These are easily identifiable by their <code>on_</code> prefix. For instance, for the method named <code>manage</code>, its non-blocking equivalent is <code>on_manage</code>.</p> <p>The custom scheduler should inherit from the <code>Karafka::Pro::Processing::Schedulers::Base</code> class and, depending on needs, should define following methods:</p> Method Parameters Required Description <code>#schedule_consumption</code> Array with consumption jobs Yes Executed when new consumption jobs are available that should be handled. <code>#schedule_revocation</code> Array with revocation jobs No Executed when new revocation jobs are available that should be handled. Implementation of this method is optional, as there is a default FIFO implementation done. <code>#schedule_shutdown</code> Array with shutdown jobs No Executed when new shutdown jobs are available that should be handled. Implementation of this method is optional, as there is a default FIFO implementation done. <code>#schedule_idle</code> Array with idle jobs No Executed when new idle jobs are available that should be handled. Implementation of this method is optional, as a default FIFO implementation is done. Idle jobs are internal and should only be played with if understood well. <code>#schedule_periodic</code> Array with periodic jobs No Executed when new periodic jobs are available that should be handled. Implementation of this method is optional, as a default FIFO implementation is done. <code>#schedule_eofed</code> Array with eofed jobs No Executed when new eofed jobs are available that should be handled. Implementation of this method is optional, as a default FIFO implementation is done. <code>#manage</code> None No Executed each time any job is finished and on each tick, which by default is every 5 seconds. This method allows for dynamic, state change-based scheduling. The default implementation of this method assumes a stateless scheduler and does nothing. <code>#clear</code> Id of the subscription group for which the underlying client is being reset. No Executed on critical crashes when Karafka needs to reset the underlying client connecting with Kafka. It should be used for the removal of jobs that are no longer associated with a client. The default implementation of this method assumes a stateless scheduler and does nothing. <p>Alongside the scheduler methods, you can find the job queue scheduler-related API methods below:</p> Method Parameters Description <code>#&lt;&lt;</code> Job Adds the job to the jobs queue from which workers pick jobs for execution. <code>#lock</code> Job Locks the polling process for a subscription group associated with this job. If this job is a regular blocking job and is not unlocked, the related group polling will not happen. Always use this method to withhold jobs from processing based on the scheduler's internal logic. Jobs should always be added to the jobs queue or locked to block the polling. <code>#unlock</code> Job Unlocks previously locked job so polling can resume once scheduling is done and there are no other subscription group locks. <p>Additionally, jobs themselves provide the following methods that can be useful when creating schedulers:</p> Method Parameters Description <code>#group_id</code> None Returns a given job subscription ID. Useful with <code>#clear</code> for obsolete jobs eviction upon recovery client resets. <code>#finished?</code> None Returns information if the given job has been finished. Useful when jobs are dependent on each other."}, {"location": "Pro-Scheduling-API/#using-a-custom-scheduler", "title": "Using a Custom Scheduler", "text": "<p>The only thing you need to do to use a custom scheduler is to assign it during your Karafka setup process:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Make sure to assign the class and not the instance as it is dynamically built\n    config.internal.processing.scheduler_class = MyCustomScheduler\n  end\nend\n</code></pre>"}, {"location": "Pro-Scheduling-API/#example-custom-scheduler", "title": "Example Custom Scheduler", "text": "<p>Besides viewing the example scheduler below, we encourage you to check Karafka's default schedulers in the Karafka sources for more real-life examples.</p> <p>Below is an example scheduler that locks all jobs and ensures that only one job can run at a time. While this particular scheduler is probably not what you aim for, it illustrates healthy usage of locking, scheduling jobs, and storing jobs in an intermediate buffer before they are scheduled for execution.</p> <pre><code># This scheduler withholds processing of jobs all except one.\n# It makes sure only one job can run, despite of the concurrency level\n# Such scheduler instance operates on all subscription groups but this API is thread-safe so each\n# of the methods is automatically wrapped with a mutex.\n# This means, that no concurrency-safe primitives are needed here\nclass OneThreadScheduler &lt; ::Karafka::Pro::Processing::Schedulers::Base\n  def initialize(queue)\n    super\n    # Intermediate buffer to hold jobs that we do not want to immediately schedule\n    @jobs_buffer = []\n  end\n\n  # Locks each job, so polling won't run and runs the manager that will schedule\n  # one job for execution if no jobs running\n  #\n  # @param jobs_array [Array] array with jobs to schedule\n  def schedule_consumption(jobs_array)\n    jobs_array.each do |job|\n      @jobs_buffer &lt;&lt; job\n      @queue.lock(job)\n    end\n\n    internal_manage\n  end\n\n  # This method runs each time any job is finished and every 5 seconds if no\n  # jobs are being finished. This allows to create schedulers that can operate\n  # based on changing external conditions\n  def manage\n    internal_manage\n  end\n\n  # Removes jobs that should not run due to a recovery reset.\n  #\n  # @param group_id [String] subscription group id needed to remove jobs that would be in the\n  #   scheduler in case of a recovery reset\n  def clear(group_id)\n    @jobs_buffer.delete_if { |job| job.group_id == group_id }\n  end\n\n  private\n\n  # Checks if there is at least one job running and if so, will do nothing.\n  # If no work is being done and there is anything in the buffer, we take it, unlock and schedule\n  # for execution\n  def internal_manage\n    @jobs_buffer.delete_if do |job|\n      next unless @queue.statistics[:busy].zero?\n\n      @queue &lt;&lt; job\n      @queue.unlock(job)\n\n      true\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Scheduling-API/#concurrency-and-ticking-frequency-management", "title": "Concurrency And Ticking Frequency Management", "text": "<p>Karafka's scheduling system is designed for efficient concurrency and ticking frequency management. A single scheduler operates in a multi-threaded environment across all subscription groups. This centralized approach helps manage tasks coherently. The system employs a mutex under the hood to prevent race conditions during scheduling, ensuring that multiple threads can operate without conflicting. Significantly, this scheduling lock does not block the worker threads from polling data, allowing for parallel processing of jobs.</p> <p>The scheduler's ticking frequency, set by default to five seconds, can be adjusted in the settings to meet specific application needs. This ticking, crucial for job initiation, occurs independently in each subscription group listener thread. However, the frequency of these ticks is consistent with the configured value, ensuring a uniform approach to job processing.</p> <p>Finally, Karafka emphasizes the need for the scheduler to be fast and efficient. Since the scheduler can block data polling, its performance is vital for maintaining the system's efficiency.</p> <p>Below is an example of how to change the ticking interval to 2.5 seconds.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.internal.tick_interval = 2_500\n  end\nend\n</code></pre>"}, {"location": "Pro-Scheduling-API/#revocation-and-shutdown-jobs-scheduling", "title": "Revocation and Shutdown Jobs Scheduling", "text": "<p>In Karafka, revocation and shutdown jobs, much like consumption jobs, can technically be scheduled using custom logic. However, it is generally recommended to stick with the default scheduling behavior provided by the <code>Base</code> scheduler for these jobs. The primary reason for this recommendation lies in the nature of these jobs as lifecycle events.</p> <p>Revocation and shutdown jobs are not frequent occurrences in the lifecycle of a Kafka application. They represent specific moments in the application's operation, such as when a consumer leaves a group (revocation), or the application shuts down (shutdown). Given their infrequent nature, these events rarely require the kind of complex scheduling logic that might be necessary for regular consumption jobs. Another essential aspect is the interaction between these lifecycle jobs and the ongoing consumption jobs. When a consumption job for the same topic partition is withheld, one might wonder if it's appropriate to proceed with revocation or shutdown jobs. The answer is yes; running these jobs is reasonable and recommended. Karafka's design allows it to detect when a scheduled consumption job is no longer necessary \u2013 for instance if the consumer has already been revoked or shut down. In such cases, Karafka will not execute the redundant consumption job but will run the required housekeeping internal logic.</p> <p>This approach ensures that the system remains efficient and responsive to its operational state without the need for complex custom scheduling logic for revocation and shutdown events. By default, the <code>Base</code> scheduler is well-equipped to handle these events effectively, making it advisable to rely on these built-in mechanisms for most use cases.</p>"}, {"location": "Pro-Scheduling-API/#expired-jobs-scheduling", "title": "Expired Jobs Scheduling", "text": "<p>In Karafka, all jobs given to the scheduler must be scheduled, even if they seem redundant. This is essential for maintaining system integrity and efficiency. The only exception is for accumulated jobs of a subscription group under recovery.</p> <p>In the event of a lost assignment in Karafka, it is crucial to understand the handling of all jobs, including those that may seem expired or no longer valid. Despite their apparent irrelevance, there is a necessity to schedule these jobs. This approach is not just about executing user-defined tasks; it also encompasses the execution of essential housekeeping and management routines within Karafka.</p> <p>When a partition assignment is lost, it seems logical to discard any related jobs. However, Karafka places a significant emphasis on scheduling these jobs for several reasons:</p> <ol> <li> <p>Running Housekeeping and Management Code: Beyond user-defined tasks, Karafka performs various internal operations critical for the system's stability and efficiency. These operations include cleanup tasks, state updates, and other management activities vital for maintaining the integrity and performance of the system.</p> </li> <li> <p>Recognition of Lost Partitions: Karafka is designed with the intelligence to recognize when a partition has been lost. In such cases, even though a job related to that partition is scheduled, the system can determine whether the execution of user code is still relevant or necessary.</p> </li> <li> <p>Selective Execution: Upon scheduling, Karafka evaluates the context of each job. If the system identifies a specific job associated with a lost partition, it will refrain from executing the user code linked to that job. This selective execution ensures that resources are well-spent on tasks that are no longer applicable or necessary due to the changed state of the assignment.</p> </li> <li> <p>Maintaining System Coherence: This scheduling process and then selectively executing or skipping jobs ensures that Karafka maintains a coherent state. It avoids scenarios where ignoring the scheduling of these jobs might lead to inconsistencies or missed execution of critical housekeeping tasks.</p> </li> </ol> <p>In summary, scheduling all jobs, including those that may initially appear expired or invalid due to a lost assignment, is a fundamental aspect of Karafka's design. This approach ensures that all necessary housekeeping and management routines are executed, maintaining the system's stability and integrity. Karafka's intelligent job evaluation mechanism is crucial in this process, ensuring that resources are used efficiently and that user code is only executed when relevant.</p>"}, {"location": "Pro-Scheduling-API/#rejecting-jobs-of-a-subscription-group-under-recovery", "title": "Rejecting Jobs of a Subscription Group Under Recovery", "text": "<p>In Karafka, during recovery scenarios, there's a specific exception to the usual job scheduling process:</p> <ol> <li> <p>Recovery Process: When Karafka encounters a critical error, it may need to reset the given subscription group connection to Kafka. This is part of its recovery mechanism.</p> </li> <li> <p>Clearing Jobs with <code>#clear</code> Method: Karafka invokes the <code>#clear</code> scheduler method to facilitate this recovery, providing the ID of the subscription group being reset.</p> </li> <li> <p>Matching Jobs with <code>#group_id</code>: Jobs in the scheduler that haven't been scheduled yet should be matched against this group ID using their <code>#group_id</code> method.</p> </li> <li> <p>Rejecting Specific Jobs: Any unscheduled job associated with the group under-recovery should be removed instead of scheduled. This is done to prevent conflicts and ensure a smooth recovery process.</p> </li> </ol> <p>In essence, during recovery, your scheduler should selectively reject and remove unscheduled jobs related to the group in recovery, rather than scheduling them. This is the only exception to the general rule of scheduling all jobs.</p>"}, {"location": "Pro-Scheduling-API/#assignments-aware-scheduling", "title": "Assignments Aware Scheduling", "text": "<p>Karafka includes a feature known as the \"assignments tracker.\" Its primary function is to keep track of active assignments, materializing them by returning the routing topics and the appropriate partitions assigned at any given moment. This feature is automatically subscribed as part of Karafka, and it's designed to be lightweight from a computational standpoint, mainly operating during rebalances.</p> <p>To understand the significance of the assignments tracker, let's draw a comparison with tools like Sidekiq. In Sidekiq, assignments are typically fixed, meaning that once a worker is assigned a particular queue, it remains static. However, Kafka's approach to work distribution is inherently dynamic due to its rebalancing protocol. This dynamism implies that the assignments for a given process, including topics and partitions, can change over time.</p> <p>Adhering to a fixed workload distribution in a Kafka environment can be inefficient and lead to resource wastage. For instance, consider a scenario where a custom scheduler allocates 50% of workers to one topic and 50% to another. If the Kafka assignment only assigns one of these topics to a particular consumer group, 50% of the workers will remain idle, not performing any work. This example highlights the potential inefficiencies in a static workload distribution model within a Karafka setup.</p> <p>Karafka addresses this issue by recommending the use of its assignments monitoring API when building complex schedulers. Through <code>Karafka::App.assignments</code>, users can access current assignments, allowing the scheduler to base its decisions on the actual assigned topics and partitions. This approach ensures the system can dynamically react to assignment changes, optimizing overall resource utilization. By constantly monitoring and adapting to the current state of topic and partition assignments, your scheduler can ensure that all workers are engaged efficiently, contributing to a more balanced and effective processing environment.</p> <p>Below is an example of a custom scheduler dedicating an even number of workers based on the assigned topic count.</p> <pre><code>class FairScheduler &lt; ::Karafka::Pro::Processing::Schedulers::Base\n  def initialize(queue)\n    super\n    @buffer = []\n    @scheduled = []\n  end\n\n  def schedule_consumption(jobs_array)\n    # Always lock for the sake of code simplicity\n    jobs_array.each do |job|\n      @buffer &lt;&lt; job\n      queue.lock(job)\n    end\n\n    manage\n  end\n\n  def manage\n    # Clear previously scheduled job that have finished\n    # We use it to track topics work that is already running\n    @scheduled.delete_if(&amp;:finished?)\n\n    # If all threads are already running there is no point in more assignments\n    # This could be skipped ofc as more would just go to the queue but it demonstrates that\n    # we can also use queue statistics in schedulers\n    return if queue.statistics[:busy] &gt;= concurrency\n\n    @buffer.delete_if do |job|\n      # If we already have enough work of this topic, we do nothing\n      next if active_per_topic(job) &gt;= workers_per_topic\n\n      # If we have space for it, we allow it to operate\n      @scheduled &lt;&lt; job\n      queue.unlock(job)\n      queue &lt;&lt; job\n\n      true\n    end\n  end\n\n  def clear(group_id)\n    @buffer.delete_if { |job| job.group_id == group_id }\n  end\n\n  private\n\n  def concurrency\n    Karafka::App.config.concurrency\n  end\n\n  # Count already scheduled and running jobs for topic of the job we may schedule\n  def active_per_topic(job)\n    @scheduled.count { |s_job| s_job.executor.topic == job.executor.topic }\n  end\n\n  # Get number of topics assigned\n  # If there are more topics than workers, we assume 1\n  def workers_per_topic\n    (concurrency / Karafka::App.assignments.size.to_f).ceil\n  end\nend\n</code></pre>"}, {"location": "Pro-Scheduling-API/#example-use-cases", "title": "Example Use Cases", "text": "<p>Here is a list of use cases where the Scheduling API can be useful:</p> <ul> <li> <p>Long-Running Jobs Management: Scheduling API can effectively manage long-running jobs to prevent them from monopolizing resources. This ensures these jobs don't disrupt regular tasks or cause system imbalances, maintaining overall system efficiency and reliability.</p> </li> <li> <p>Dynamic Resource Allocation Based on Traffic Volume: Automatically adjust resource allocation in real-time based on the volume of incoming messages from Kafka. This helps in scaling up resources during peak times and scaling down during low-traffic periods.</p> </li> <li> <p>Prioritization of Critical Jobs: Implement prioritization within the job queue to ensure that critical or time-sensitive messages from Kafka are processed first, optimizing response times for high-priority tasks.</p> </li> <li> <p>Resource Allocation Based on Job Complexity: Schedule jobs to the queue based on their complexity, allocating more resources to complex jobs and fewer to simpler tasks, thus optimizing processing times and resource usage.</p> </li> <li> <p>Resource Intensive Job Throttling: Implement throttling for resource-intensive jobs. Schedule these jobs to balance the load on the system, preventing any single job type from monopolizing CPU, memory, or network bandwidth and ensuring stable system performance.</p> </li> <li> <p>Job Prioritization Based on Data Urgency: Prioritize and schedule jobs based on the urgency of the data they process. For instance, real-time analytics jobs can be given higher priority over batch processing jobs.</p> </li> <li> <p>Adaptive Job Scheduling Based on Pattern Recognition: Implement adaptive scheduling where the system learns from job execution patterns. For instance, if certain jobs take longer at specific times, the scheduler can adapt and allocate more time or resources accordingly.</p> </li> </ul>"}, {"location": "Pro-Scheduling-API/#summary", "title": "Summary", "text": "<p>The Karafka Scheduling API provides advanced capabilities for managing consumption jobs within a process, emphasizing precise control and efficient resource distribution. It enables granular scheduling of tasks, sequencing of jobs, and adaptive scheduling based on the system's current state.</p> <p>It supports custom scheduler implementation, with stateful and stateless options, and includes a variety of methods for job management. The API's design ensures concurrency and frequency management. Its flexibility and efficiency make it ideal for diverse scenarios, including long-running job management, dynamic resource allocation, and adaptive scheduling based on traffic patterns or job complexity.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Security/", "title": "Karafka Pro Security", "text": "<p>Using Karafka Pro means we are part of your Open Source Supply Chain. We take this exceptionally seriously.</p> <p>Each Pro customer is encouraged to implement our license gem integrity verification flow that ensures license consistency provided via our Gem server.</p> <p>On top of that, to ensure the stability and security of the system:</p> <ul> <li>Karafka GitHub organization participates in the private vulnerability reporting program.</li> <li>No Karafka ecosystem components collect and send out any data from your systems.</li> <li>We do not hold your credit card information. All subscription management is done via Stripe.</li> <li>We do not collect any PII when a gem license is fetched. The only things logged are IP and the last request time.</li> <li>All of Karafka ecosystem code is available publicly in our GitHub organization.</li> <li>Karafka gem server is monitored with uptime tools.</li> <li>We monitor server logs for suspicious activities and requests.</li> <li>Every single SSH connection is logged.</li> <li>Karafka gem server allows for credentials rotation in case of an external leak.</li> <li>We monitor the integrity of all the licenses using automated tools to ensure they are not compromised.</li> <li>Our license packages do not include any code beyond the code needed to read the version and the license files.</li> <li>Unless explicitly contacted by us, your license id should never change and should be locked in the Gemfile.</li> <li>All Karafka components released to RubyGems are either digitally signed and published from an account with 2FA enabled or published using the RubyGems Trusted Publishing feature.</li> <li>Karafka Pro provides script you can include in your CI/CD pipeline to ensure license integrity.</li> <li>Every release of every Karafka component is announced on our Slack.</li> </ul> <p>If your organization policy prevents using any external dependency sources, a Karafka Pro license can be bundled into your application. This, however, requires a separate Enterprise agreement with us. Please contact us for more details.</p> <p>Last modified: 2025-05-23 13:01:11</p>"}, {"location": "Pro-Support/", "title": "Karafka Pro Support", "text": "<p>Karafka offers community support via Github issues and a Slack channel.</p> <p>Karafka Pro offers priority support via a dedicated per-organization private Slack channel and via email.</p>"}, {"location": "Pro-Support/#version-support", "title": "Version Support", "text": "<p>Please check out the Versions Lifecycle and EOL for information about supported versions.</p>"}, {"location": "Pro-Support/#priority-support", "title": "Priority Support", "text": "<p>Thinking about introducing Kafka to your Ruby and Rails stack?</p> <p>With Karafka, you can get priority support with a max response time of 2 working days. For support, email contact@karafka.io or send us a message using one of the dedicated private channels using our Slack.</p> <p>Please email using the same domain as the original license email or explain your connection to the licensed company.</p>"}, {"location": "Pro-Support/#onboarding", "title": "Onboarding", "text": "<p>Yearly Pro customers may request a two-hour video chat session with Maciej Mensfeld to discuss their application(s), requirements, and how best to leverage the various Karafka features. Contact us at contact@karafka.io to set up your session.</p> <p>Please email using the same domain as the original license email or explain your connection to the licensed company.</p>"}, {"location": "Pro-Support/#upgrade-support", "title": "Upgrade Support", "text": "<p>While we provide comprehensive upgrade guides for users to migrate to new versions, it may not be enough, especially when upgrading from unsupported old versions. The upgrade guides offer step-by-step instructions on how to upgrade Karafka, including changes in configuration, code, and dependencies.</p> <p>However, in some cases, users may be upgrading from versions that are no longer supported or have significant differences in functionality, making the upgrade process more complex. Pro yearly subscribed users may reach out for additional support in such cases.</p> <p>Upgrading Karafka can be a critical task for maintaining the health and performance of an application, so it's essential to approach the process carefully and with sufficient resources.</p>"}, {"location": "Pro-Support/#prioritized-bug-fixes", "title": "Prioritized Bug Fixes", "text": "<p>We recognize the significance of prompt and effective bug resolution for your ongoing projects and application stability. Hence, with the Karafka Pro version, we assure a superior level of attention to any bugs or issues you might encounter during your application development. Here's how we do it:</p> <ol> <li> <p>Priority in the Queue: Karafka Pro users' bug reports get precedence over standard reports. Any reported bugs are placed at the top of the triage process, which allows us to initiate the debugging and resolution process sooner. This priority queue ensures that we address your issues quickly and keep your projects moving.</p> </li> <li> <p>Faster Release Cycles for Bug Fixes: In line with our commitment to serving our Pro customers better, bug fixes for issues reported by Pro users get released faster. We ensure that these fixes are incorporated into our release cycle promptly, significantly reducing the wait time for bug resolution.</p> </li> <li> <p>Proactive Communication: We believe in maintaining clear and proactive communication with our users. We will keep you updated on the status of your reported bugs. You will be notified about the progress from the moment we begin working on the bug until its final resolution. This helps keep you in the loop and ensures transparency.</p> </li> <li> <p>Enhanced Testing: We understand the unique requirements and high standards of our Karafka Pro users. We perform additional layers of testing for bug fixes reported by Pro users to ensure the stability and reliability of the solution. </p> </li> <li> <p>Consideration for Backporting Features or Fixes: We understand that certain features or fixes can significantly enhance the functionality and stability of your current projects. As a Karafka Pro user, you can request a backport of a particular feature or fix to support your specific needs. Please note that the possibility of backporting will be assessed on a case-by-case basis, taking into account factors like the feasibility of the backport and its impact on system stability.</p> </li> </ol> <p>By opting for Karafka Pro, you are not just purchasing a product but partnering with people committed to ensuring the smooth operation of your applications.</p> <p>Please note that our software is provided \"as is.\" We recommend utilizing the trial period to thoroughly test it, as we cannot guarantee it will be entirely bug-free or that all issues will be resolved. That said, we always strive to deliver the best, and historically, there have been no unresolved bugs. However, given Kafka's complexity, situations can vary.</p> <p>Last modified: 2023-10-25 17:28:02</p>"}, {"location": "Pro-Transactions/", "title": "Transactions", "text": "<p>Transactions in Karafka provide a mechanism to ensure that a sequence of actions is treated as a single atomic unit. In the context of distributed systems and message processing, a transaction ensures that a series of produce and consume operations are either all successfully executed or none are, maintaining data integrity even in the face of system failures or crashes.</p> <p>Karafka's and Kafka's transactional support extends across multiple partitions and topics. This capability is crucial for applications that require strong consistency guarantees when consuming from and producing various topics. A classic use case is the read-process-write pattern, where a consumer reads messages from a source topic, processes them, and produces the results to a sink topic. Using transactions, you can ensure that messages' consumption and subsequent production are atomic, preventing potential data loss or duplication.</p> <p>Karafka supports Kafka's Exactly-Once Semantics, the gold standard for message processing systems. It ensures that each message is processed exactly once, eliminating data duplication or loss risks. In simpler terms, despite failures, retries, or other anomalies, each message will affect the system state only once.</p> <p>In Kafka, achieving Exactly-Once Semantics involves ensuring that:</p> <ul> <li> <p>Producers do not write duplicate records. Kafka achieves this by handling idempotence at the producer level. An idempotent producer assigns a sequence number to each message, and the broker ensures that each sequence is written only once.</p> </li> <li> <p>Consumers process messages only once. This is trickier and involves ensuring that the commit of the consumer's offset (which marks where the consumer is in a topic partition) is part of the same transaction as the message processing. If the consumer fails after processing the message but before committing the offset, the message might be processed again, leading to duplicates.</p> </li> </ul> <p>Karafka transactions provide Exactly-Once Semantics by ensuring that producing to a topic and committing the consumer offset are part of the same atomic transaction. When a transactional producer publishes messages, they are not immediately visible to consumers. They become visible only after the producer commits the transaction. If the producer fails before committing, the consumers do not read the messages, and the state remains consistent.</p>"}, {"location": "Pro-Transactions/#using-transactions", "title": "Using Transactions", "text": "<p>Scope of WaterDrop Transactions</p> <p>Please note that this document concentrates solely on the consumer-related aspects of Karafka's transactions. For a comprehensive understanding of transactions and to ensure a well-rounded mastery of Karafka's transactional capabilities, delving into the WaterDrop transactions documentation is imperative.</p> <p>Avoid Mixing Transactional and Non-Transactional Offset Committing</p> <p>Mixing transactional offset committing with non-transactional offset committing is strongly discouraged. When these two modes are combined, it can lead to unpredictable behavior and compromise the integrity of your data processing.</p> <p>In such scenarios, Kafka will issue warnings in its logs, which may look like this:</p> <p><code>WARN [GroupMetadataManager brokerId=1]   group: ID with leader: LEADER-ID has received offset commits from consumers as well as transactional producers.   Mixing both types of offset commits will generally result in surprises and should be avoided.   (kafka.coordinator.group.GroupMetadataManager)</code></p> <p>To ensure reliable and predictable processing, always commit offsets using the mode within your consumer. If transactional processing is enabled, all offset commits should be part of a transaction. If you're processing offsets non-transactionally, do not mix this with transactional operations.</p> <p>Before using transactions, you need to configure your producer to be transactional. This is done by setting <code>transactional.id</code> in the kafka settings scope:</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'transactional.id': 'unique-id'\n  }\nend\n</code></pre>"}, {"location": "Pro-Transactions/#simple-usage", "title": "Simple Usage", "text": "<p>The only thing you need to do to start using transactions is to wrap your code with a <code>#transaction</code> block:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    sum = 0\n\n    messages.each do |message|\n      sum += message.payload['count']\n    end\n\n    transaction do\n      produce_async(topic: :sums, payload: sum.to_s)\n\n      mark_as_consumed(messages.last)\n    end\n  end\nend\n</code></pre> <p>Karafka will automatically start a <code>#producer</code> with support for committing the message offset inside. That way, your message production can be interconnected with marking the offset. Either both will be successful or none.</p> <p>In case the transaction fails (for any reason), neither of the messages will be produced nor the offset will be marked.</p>"}, {"location": "Pro-Transactions/#aborting-transactions", "title": "Aborting Transactions", "text": "<p>Any exception or error raised within a transaction block will automatically result in the transaction being aborted. This ensures that if there are unexpected behaviors or issues during message production or processing, the entire batch of messages within that transaction won't be committed, preserving data consistency.</p> <p>Below, you can find an example that ensures that all the messages are successfully processed and only in such cases all produced messages are being sent to Kafka together with marking of last message as consumed:</p> <pre><code>def consume\n  transaction do\n    messages.each do |message|\n      payload = message.payload\n\n      next unless message.payload.fetch(:type) == 'update'\n\n      # If this exception is raised, none of the messages will be dispatched\n      raise UnexpectedResource if message.payload.fetch(:resource) != 'user'\n\n      # Pipe the data to users specific topic\n      produce_async(topic: 'users', payload: message.raw_payload)\n    end\n\n    mark_as_consumed(messages.last)\n  end\nend\n</code></pre> <p>Karafka also provides a manual way to abort a transaction without raising an error. By using <code>raise(WaterDrop::AbortTransaction)</code>, you can signal the transaction to abort. This method is advantageous when you want to abort a transaction based on some business logic or condition without throwing an actual error.</p> <pre><code>def consume\n  transaction do\n    messages.each do |message|\n      # Pipe all events\n      producer.produce_async(topic: 'events', payload: message.raw_payload)\n    end\n\n    mark_as_consumed(messages.last)\n\n    # And abort if more events are no longer needed\n    raise(WaterDrop::AbortTransaction) if KnowledgeBase.more_events_needed?\n  end\nend\n</code></pre>"}, {"location": "Pro-Transactions/#automatic-offset-management-in-transactions", "title": "Automatic Offset Management in Transactions", "text": "<p>Karafka operates with a default mode that employs automatic offset management. This efficient approach commits offsets after the successful processing of each batch, streamlining the message-handling process:</p> <pre><code># Note: There's no need to mark messages as consumed manually;\n# it occurs automatically post-batch processing.\ndef consume\n  messages.each do |message|\n    EventStore.save!(message.payload)\n  end\nend\n</code></pre> <p>Karafka recognizes and harmonizes the two flows when integrating transactions with this automatic offset management. This allows for the seamless use of transactions and the explicit marking of messages as consumed within these transactions without interfering with Karafka's implicit offset management behavior.</p> <pre><code>def consume\n  # Take at most 100 events and aggregate them, pipe them into a\n  # separate topic and mark all as consumed in one go.\n  messages.each_slice(100).each do |batch|\n    transaction do\n      result = Aggregator.merge(batch.map(&amp;:payload))\n\n      produce_async(topic: :aggregations_stream, payload: result.to_json)\n\n      mark_as_consumed(batch.last)\n    end\n  end\nend\n</code></pre> <p>Karafka's design capitalizes on Kafka's transactional mechanisms, intricately linking the consumption of messages with the output of subsequent messages within a singular, indivisible operation. </p> <p>Consequently, if a transaction is prematurely aborted or encounters a failure, the offsets of the consumed messages remain uncommitted. As a result, the same batch of messages is queued for reprocessing, effectively nullifying data loss and strictly adhering to the principles of exactly-once processing semantics. Thus, the integration of automatic offset management with transactions is seamless and devoid of risk, ensuring the integrity and consistency of message processing.</p>"}, {"location": "Pro-Transactions/#manual-offset-management-in-transactions", "title": "Manual Offset Management in Transactions", "text": "<p>Direct Transactional Producer Usage Is Not Recommended</p> <p>Using a transactional producer directly in a <code>#consume</code> method, bypassing the <code>#wrap</code> mechanism, is strongly discouraged unless you're fully managing the offsets in your consumer. While this approach may seem straightforward for basic use cases, it fails to accommodate advanced offset management scenarios, such as handling messages that need to be redirected to a Dead Letter Queue (DLQ). </p> <p>When using Manual Offset Management, you can provide a custom producer to the <code>#transaction</code> method as its first argument. This allows for greater flexibility in managing transactions, particularly in scenarios requiring advanced control over producers and offsets.</p> <p>In such cases, you do not have to use the <code>#wrap</code> API and you can embed selection of the consumer into your transactional flow directly:</p> <pre><code>def consume\n  PRODUCERS.with do |transactional_producer|\n    # Use this only when fully managing offsets yourself\n    transaction(transactional_producer) do\n      messages.each do |message|\n        # Custom processing logic\n        process_message(message)\n\n        mark_as_consumed(message)\n      end\n    end\n  end\nend\n</code></pre> <p>Using <code>#wrap</code> with Manual Offset Management and Custom Producers</p> <p>When providing a custom producer directly to the <code>#transaction</code> method while using manual offset management, you must ensure that no Karafka features that automatically manage and store offsets are used in your consumer. Any inadvertent offset management by Karafka could interfere with the integrity of your manual offset strategy.</p> <p>In many cases, it may be a better idea to leverage the <code>#wrap</code> API, even when using manual offset management. The <code>#wrap</code> API allows you to handle custom producer assignment and lifecycle management seamlessly, while still ensuring that the overall Karafka consumption flow - including synchronization and framework-level operations\u2014executes correctly. This approach reduces the risk of inconsistencies and simplifies producer handling in complex scenarios.</p>"}, {"location": "Pro-Transactions/#using-a-dedicated-transactional-producer", "title": "Using a Dedicated Transactional Producer", "text": "<p>Karafka's <code>#transaction</code> method is designed to handle complex message processing scenarios efficiently, especially in highly-traffic systems. It allows using a custom producer instance instead of the default <code>Karafka.producer</code>. This functionality is crucial in two primary contexts:</p> <ul> <li>Dedicated Transactional Producer: You can use a separate, dedicated producer for transactions when you need stronger guarantees about the atomicity and consistency of operations. This specialized producer works in tandem with the standard, faster, non-transactional producer, and it's beneficial for operations where maintaining transaction integrity is critical.</li> </ul> <pre><code>def consume\n  # Use a dedicated transactional producer instead of\n  # default faster one for this type of operations\n  transaction do\n    result = Aggregator.merge(messages.payloads)\n\n    produce_async(topic: :aggregations_stream, payload: result.to_json)\n\n    mark_as_consumed(batch.last)\n  end\nend\n\n# @param _action_name [Symbol] name of action like :consume, :revoked, etc\ndef wrap(_action_name)\n  default = self.producer\n  self.producer = TRANSACTIONAL_PRODUCER\n\n  yield\n\n  self.producer = default\nend\n</code></pre> <ul> <li>Connection Pool of Producers: In high-traffic systems where throughput is vital, efficiently managing producer instances becomes essential. Using different producer instances with the <code>#transaction</code> method enables you to set up a pool of producers. This pool allows your system to handle multiple transactions simultaneously by providing an available producer for each transaction, optimizing resource use, and maintaining system performance.</li> </ul> <pre><code>def consume\n  # If there were issues during wrapping (producer selection, etc), re-raise it so a backff\n  # policy can be applied\n  raise @wrap_error if @wrap_error\n\n  transaction do\n    result = Aggregator.merge(messages.payloads)\n\n    produce_async(topic: :aggregations_stream, payload: result.to_json)\n\n    mark_as_consumed(batch.last)\n  end\nend\n\n# The `#wrap` allows you to wrap the actions with a custom block that can be used to assign the\n# selected producer from the pool\n#\n# @param _action_name [Symbol] name of action like :consume, :revoked, etc\ndef wrap(_action_name)\n  # Store the original producer\n  default = producer\n\n  PRODUCERS.with do |selected_producer|\n    # Assign the transactional producer\n    self.producer = selected_producer\n\n    yield\n\n    # Restore the original producer so the one from the pool does not leak out\n    self.producer = default\n  end\n# Ensure yield is called even in case of any errors\n# This error is just an example error\nrescue NoProducerAvailableError =&gt; e\n  @wrap_error = e\n\n  yield\nensure\n  @wrap_error = false\nend\n</code></pre> <p>The importance of having a pool of producers is highlighted by how transactions lock producers in Karafka. When a transaction starts, it locks its producer to the current thread, making the producer unavailable for other operations until the transaction is completed or rolled back. In a high-traffic system, this could lead to performance issues if multiple transactions are waiting for the same producer.</p> <p>With a producers' connection pool, this challenge is mitigated. When a transaction begins, it picks the assigned producer from the pool, allowing other operations to proceed in parallel with their producers. After the transaction ends, the producer is released back to the pool, ready to be used for new transactions.</p> <p>In essence, with support for dedicated transactional producers, Karafka's <code>#transaction</code> method offers a structured and efficient way to manage message transactions in highly-traffic systems.</p> <p>Direct Transactional Producer Usage Is Not Recommended</p> <p>Using a transactional producer directly in a <code>#consume</code> method, bypassing the <code>#wrap</code> mechanism, is strongly discouraged unless you're fully managing the offsets in your consumer. While this approach may seem straightforward for basic use cases, it fails to accommodate advanced offset management scenarios, such as handling messages that need to be redirected to a Dead Letter Queue (DLQ). </p> <p>In these scenarios, the Karafka framework might invoke the transactional producer after the main consumption logic completes. This implicit usage could lead to unintended transactional behavior or conflicts, undermining the integrity of your processing logic.</p> <p>The recommended approach is to utilize the <code>#wrap</code> method when customizing the producer. This ensures that the transactional producer is seamlessly managed across the entire lifecycle of the action, including any framework-level operations that occur after your custom logic. By adhering to this practice, you maintain consistency, avoid unexpected issues, and fully leverage the robustness of Karafka's transactional processing capabilities.</p> <p>Ensure <code>#wrap</code> Always Calls <code>yield</code></p> <p>It is critical to ensure that the <code>#wrap</code> method always calls <code>yield</code>, even if operations like selecting a producer from a pool fail. The <code>yield</code> statement in <code>#wrap</code> executes the entire operational flow within Karafka, including not only your custom logic but also essential framework-level synchronization and processing code.</p> <p>You can read more about this requirement here.</p>"}, {"location": "Pro-Transactions/#risks-of-early-exiting-transactional-block", "title": "Risks of Early Exiting Transactional Block", "text": "<p>In all versions of Karafka, using <code>return</code>, <code>break</code>, or <code>throw</code> to exit a transactional block early is not allowed.</p> <p>However, the behavior differs between versions of WaterDrop:</p> <ul> <li>pre 2.8.0: Exiting a transaction using <code>return</code>, <code>break</code>, or <code>throw</code> would cause the transaction to rollback.</li> <li>2.8.0 and Newer: Exiting a transaction using these methods will raise an error.</li> </ul> <p>It is not recommended to use early exiting methods. To ensure that transactions are handled correctly, refactor your code to avoid using <code>return</code>, <code>break</code>, or <code>throw</code> directly inside transactional blocks. Instead, manage flow control outside the transaction block.</p> <p>BAD:</p> <pre><code>MAX = 10\n\ndef consume\n  count = 0\n\n  transaction do\n    messages.each do |message|\n      count += 1\n\n      producer.produce_async(topic: 'events', payload: message.raw_payload)\n\n      # This will cause either abort or error\n      return if count &gt;= MAX\n    end\n  end\nend\n</code></pre> <p>GOOD:</p> <pre><code>MAX = 10\n\ndef consume\n  transaction do\n    # This will be ok, since it is not directly in the transaction block\n    produce_with_limits(messages)\n  end\nend\n\ndef produce_with_limits(messages)\n  count = 0\n\n  messages.each do |message|\n    count += 1\n\n    producer.produce_async(topic: 'events', payload: message.raw_payload)\n\n    return if count &gt;= MAX\n  end\nend\n</code></pre>"}, {"location": "Pro-Transactions/#balancing-transactions-and-long-running-jobs", "title": "Balancing Transactions and Long-Running Jobs", "text": "<p>Providing a custom producer to the <code>#transaction</code> method temporarily overwrites the default producer for that specific consumer instance. This behavior is relevant in scenarios involving Long-Running Jobs that execute alongside the message consumption process, such as handling <code>#revoked</code> under Long Running Jobs (LRJ).</p> <p>It's crucial to understand the implications of this producer reassignment:</p> <ul> <li> <p>Temporary Producer Reassignment: During the transaction's execution, the custom producer you provide becomes the active producer for the consumer. Any operations within the transaction's scope will use this custom producer instead of the default one.</p> </li> <li> <p>Implications for Long-Running Jobs: For long-running jobs actions <code>#revoked</code> that might run parallel with the consumption process, the transactional producer (the custom producer provided to the transaction) may be used for these operations. This could be a concern if the transactional producer is locked for an extended period due to a lengthy transaction, potentially affecting parallel processing.</p> </li> <li> <p>Recommendation for Systems with Parallel Processing Needs: If your system frequently handles long-running jobs alongside message consumption, especially if these jobs are expected to run in parallel with transactions, it's advisable to use a pool of producers consistently. Doing so ensures that a locked producer in a lengthy transaction doesn't hinder the performance or progress of other parallel operations. Instead of relying on the default <code>#producer</code> consumer reference, managing and allocating producers from a dedicated pool can significantly enhance system robustness and concurrency, allowing each transaction or job to operate with its dedicated producer resource.</p> </li> </ul> <pre><code># Example consumer that due to usage of LRJ in the routing can have the `#consume`\n# and `#revoked` run in parallel. Due to this, consumers pool is used to make sure\n# that ongoing transaction and revocation get their respective dedicated producers\nclass LrjOperableConsumer\n  def consume\n    # Re-raise wrap error (if any)\n    raise @wrap_error if @wrap_error\n\n    # Uses the default transactional producer taken from a pool assigned via `#wrap`\n    transaction do\n      result = Aggregator.merge(messages.payloads)\n\n      produce_async(topic: :aggregations_stream, payload: result.to_json)\n\n      mark_as_consumed(batch.last)\n    end\n  end\n\n  def revoked\n    # If would run in parallel to `#consume` when LRJ is in use, will receive a\n    # different producer instance\n    PRODUCERS.with do |producer|\n      producer.produce_async(topic: :revokactions, payload: @state.to_json)\n    end\n  end\n\n  def wrap(action_name)\n    # since for LRJ revoked can run in parallel with `#consume` we do not reassign the consumer but\n    # rather we opt for explicit producer selection\n    return yield if action_name == :revoked\n\n    # Store the original producer\n    default = producer\n\n    PRODUCERS.with do |selected_producer|\n      # Assign the transactional producer\n      self.producer = selected_producer\n\n      yield\n\n      # Restore the original producer so the one from the pool does not leak out\n      self.producer = default\n    end\n  # yield needs to be called always, even in case of wrap errors\n  rescue NoProducerAvailableError =&gt; e\n    @wrap_error = e\n    yield\n  end\nend\n</code></pre>"}, {"location": "Pro-Transactions/#offset-metadata-storage", "title": "Offset Metadata Storage", "text": "<p>The Offset Metadata Storage feature allows you to attach custom metadata to message offsets, enhancing the traceability and context of message processing. Crucially, this metadata can be included when you use the <code>#mark_as_consumed</code> method within transactions, ensuring the metadata is committed alongside the successful transaction. This feature is fully functional within and outside of transactional contexts, providing a consistent and flexible approach to enriching your message with valuable contextual data.</p> <pre><code>def consume\n  transaction do\n    result = Aggregator.merge(messages.payloads)\n\n    produce_async(topic: :aggregations_stream, payload: result.to_json)\n\n    # Providing offset metadata will fully work from inside of transactions\n    mark_as_consumed(\n      batch.last,\n      # Make sure that this argument is a string and in case of a JSON, do not\n      # forget to define a custom deserializer\n      {\n        process_id: Process.uid,\n        aggregated_state: @aggregator.to_h, \n      }.to_json\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Transactions/#transactions-after-revocation", "title": "Transactions After Revocation", "text": "<p>Specific scenarios, like partition revocation, can introduce complexities that must be handled gracefully. Here's how Karafka transactions behave after revocation:</p> <ol> <li> <p>Transactions Post-Revocation: Transactions in Karafka that solely focus on producing messages, without marking any message as consumed, continue to function normally even after revocation. This feature is handy in specific scenarios, such as executing transactions within the <code>#revoked</code> method, where you might want to continue producing messages based on some internal state or logic.</p> </li> <li> <p>Consumption Marking and Assignment Loss: If a transaction attempts to mark a message as consumed after partition revocation, Karafka raises a <code>Karafka::Errors::AssignmentLostError</code>. This behavior is intentional and caters to the system's consistency guarantees. Since the consumer no longer owns the partition, marking messages, as consumed, could lead to inconsistencies and is therefore prevented.</p> </li> <li> <p>Handling Assignment Loss During Transactions: If the assignment is lost while a transaction is in progress, the transaction is automatically rolled back, and an error is raised. This rollback is crucial to maintaining the atomicity and integrity of transactions, ensuring that partial or inconsistent states do not persist in the system.</p> </li> </ol> <p>In summary, Karafka's transaction handling after revocation is designed to maintain the integrity and consistency of message processing. By allowing message production to continue post-revocation and ensuring that consumption marking is tightly controlled, Karafka provides a robust framework for managing transactions, even in the face of complex distributed system behaviors like partition revocation.</p>"}, {"location": "Pro-Transactions/#transactions-in-the-dead-letter-queue", "title": "Transactions in the Dead-Letter Queue", "text": "<p>This section explains how transactions interact with the DLQ and the implications for message processing.</p> <ol> <li> <p>Consistent Transaction Behavior: From the user's perspective, transactions in Karafka behave consistently, whether or not the DLQ is utilized. This means the practice of wrapping your message processing code within transactions remains unchanged, providing a consistent development experience.</p> </li> <li> <p>Transactional Dead-Letter Queue Operations: In scenarios involving persistent errors - where messages need to be moved to the DLQ - Karafka, by default, uses transactions to perform two critical operations atomically: moving the message to the DLQ and committing the offset (when necessary). This ensures that the message relocation to the DLQ and the acknowledgment of message processing (offset commit) are treated as a single atomic operation, maintaining consistency.</p> </li> </ol> <p>Disabling Transactions During DLQ Dispatches</p> <p>It's worth noting that this behavior can be adjusted. If the transactional mode in the DLQ configuration is turned off, Karafka won't use transactions to move messages to the DLQ. You can read more about this here.</p> <ol> <li> <p>Error Handling and Retries: If an error occurs during the DLQ operation, such as partition revocation or networking issues, Karafka's default behavior is to retry processing the same batch. This retry mechanism ensures that transient failures don't lead to message loss or unacknowledged message consumption. The system attempts to process the batch again, allowing the operation to succeed.</p> </li> <li> <p>Considerations for DLQ Dispatching: In certain situations, particularly under specific configurations or system constraints, DLQ dispatches might not be possible. For instance, if network issues prevent communication with the DLQ topic or transactional integrity can't be maintained due to partition revocations, the DLQ operations might not proceed as expected.  In such cases, it's important to understand that the DLQ might not operate, meaning that messages that fail processing persistently might not be moved to the DLQ. This situation underscores the importance of monitoring and potentially adjusting the system configuration or handling mechanisms to ensure that messages are either processed successfully or reliably moved to the DLQ.</p> </li> </ol> <p>In conclusion, while transactions in Karafka provide a robust mechanism for processing messages consistently and atomically, their interaction with the DLQ introduces specific behaviors and considerations.</p>"}, {"location": "Pro-Transactions/#delivery-warranties", "title": "Delivery Warranties", "text": "<p>Karafka, leveraging Kafka's Transactional Producer, offers solid delivery warranties that ensure data integrity and reliable message processing in distributed systems.</p> <p>Here's how Karafka's delivery warranties manifest in transactions:</p> <ul> <li> <p>Atomicity Across Partitions and Topics: Karafka transactions maintain atomicity within a single partition or topic and across multiple ones. This feature is invaluable when a transaction spans producing and consuming from multiple topics or partitions, ensuring that all these operations succeed or fail as a single unit.</p> </li> <li> <p>Exactly-Once Semantics (EOS): Karafka supports Kafka's exactly-once semantics within its transactional framework. This ensures that each message processed in a transaction is affected exactly once in the system, nullifying the risks associated with data duplication or loss, even in scenarios involving retries or system failures.</p> </li> <li> <p>Idempotent Writes: Through the use of Kafka's Transactional Producer, Karafka ensures idempotent writes within transactions. Even if the transactional producer attempts to send a message multiple times, each message is written only once, preventing data duplication and contributing to the EOS guarantee.</p> </li> <li> <p>Consistent State in Failure Scenarios: Karafka's transactional processing is designed to maintain system consistency, even when failures occur. If a transaction doesn't complete successfully due to issues like system crashes or network failures, it's aborted. This rollback mechanism ensures that incomplete or partial transactions don't corrupt the system state.</p> </li> <li> <p>Isolation and Concurrency Control: Transactions in Karafka are well-isolated, ensuring that the operations of an ongoing transaction aren't visible to others until the transaction is committed. This level of isolation is crucial in maintaining data consistency, particularly in environments where transactions are highly concurrent.</p> </li> <li> <p>Robust Failure Recovery: Karafka is built to handle failures gracefully. If a transaction is interrupted (e.g., due to a producer crash), Karafka ensures that the system can recover consistently, aligning with the last committed transaction. This resilience is key to maintaining continuous and reliable operations.</p> </li> </ul>"}, {"location": "Pro-Transactions/#instrumentation", "title": "Instrumentation", "text": "<p>Transactions Instrumentation is directly tied to the producer handling the transaction. To effectively monitor transaction behavior, it's essential to integrate your instrumentation with the transactional producers. This ensures accurate tracking and analysis of transactional activities, enhancing system monitoring and reliability. Refer to the WaterDrop Transactions Instrumentation section for a comprehensive approach to transaction instrumentation.</p> <p>However, it's important to know that consumer lag monitoring for transactional consumers behaves differently. Since offsets are committed as part of the transaction by the producer rather than by the consumer, the usual consumer metrics (like <code>consumer_lag_stored</code>) will not be published or will show <code>-1</code> (or remain at whatever initial offset they had when subscribing). In other words, consumer lag is not directly visible at the consumer level because it's bypassing the consumer's offset manager.</p> <p>Consumer Lag Monitoring</p> <p>It's essential to be aware that consumer lag monitoring for transactional consumers behaves differently. Since offsets are committed as part of the transaction by the producer rather than by the consumer, the usual consumer metrics (like <code>consumer_lag_stored</code>) will not be published or will show <code>-1</code> (or remain at whatever initial offset they had when subscribing). In other words, consumer lag is not directly visible at the consumer level because it's bypassing the consumer's offset manager.</p>"}, {"location": "Pro-Transactions/#karafka-web-ui", "title": "Karafka Web UI", "text": "<p>Karafka Web UI compensates for the lack of lag reporting in the <code>statistics.emitted</code> and tracks offsets that are updated post-transaction. In the Web UI, you'll see lag that is \"more or less\" accurate - though you might notice small <code>+1/-1</code> discrepancies due to offset reporting nuances.</p>"}, {"location": "Pro-Transactions/#custom-instrumentation", "title": "Custom Instrumentation", "text": "<p>If you've built custom instrumentation around consumer lag or offsets, you'll need to compensate manually. Because the consumer doesn't store the offsets, you can't rely on the usual consumer statistics to get the lag. You'll need to base your metrics on post-transaction offsets or handle offset tracking in your own way.</p>"}, {"location": "Pro-Transactions/#transaction-event-notification", "title": "Transaction Event Notification", "text": "<p>For advanced monitoring or custom integrations, remember that the Karafka consumer publishes a <code>consumer.consuming.transaction</code> notification event after each successful transaction. This event can be used to hook into transaction completions and incorporate transaction-aware logic in your instrumentation or metrics gathering.</p> <p><code>consumer.consuming.transaction</code> Instrumentation Event</p> <p>The <code>consumer.consuming.transaction</code> instrumentation event is triggered after each successful transaction. Importantly, if you raise <code>WaterDrop::AbortTransaction</code> to abort a transaction, this event will still be triggered. Similar to how ActiveRecord transactions handle rollbacks internally, the abort, in this case, does not bubble up as an exception. Instead, the transaction is cleanly rolled back, and the event is published, allowing you to track transaction completions even in cases where they were aborted.</p>"}, {"location": "Pro-Transactions/#using-transactions-in-swarm-mode", "title": "Using Transactions in Swarm Mode", "text": "<p>For detailed information about working with transactional producers in Swarm Mode, including configuration inheritance, transactional ID management, and best practices, please refer to our dedicated documentation section on Transactional Producer Handling in Swarm Mode.</p>"}, {"location": "Pro-Transactions/#performance-implications", "title": "Performance Implications", "text": "<p>While Kafka transactions in Karafka provide strong consistency guarantees and data integrity, they have specific performance implications crucial to understanding system architecture and design. Here are the key considerations:</p> <ol> <li> <p>Increased Latency: Transactions introduce a particular overhead due to the additional coordination and state management required to ensure atomicity and consistency. This can lead to increased latency in message processing, as the system needs to ensure that all parts of the transaction are completed before moving forward.</p> </li> <li> <p>Resource Utilization: Transactional operations typically consume more resources compared to non-transactional ones. This is because the system must maintain additional state information and handle the coordination and rollback mechanisms in case of failures. As a result, there might be an increased load on the brokers and a higher consumption of network resources.</p> </li> <li> <p>Throughput Considerations: The use of transactions can impact the system's overall throughput. The need to ensure atomicity and exactly-once semantics means that messages within a transaction need to be processed more controlled, which can reduce the rate at which messages are processed compared to non-transactional workflows.</p> </li> <li> <p>Producer Locking in Karafka (Waterdrop): In Karafka, when a transaction is initiated, the Waterdrop producer is locked to the thread handling the transaction. This means any other thread can only use the producer once the transaction is completed. This locking mechanism is crucial for ensuring the integrity of the transaction but can lead to contention and reduced parallelism, especially in high-throughput scenarios where multiple threads need to produce messages concurrently.</p> </li> <li> <p>Handling Failures and Retries: Transactions necessitate a more complex handling of failures and retries. If a part of the transaction fails, the whole transaction needs to be rolled back and potentially retried. This complexity can add to the processing time and requires careful design to avoid issues such as deadlocks or repeated failures.</p> </li> <li> <p>Risk of Hanging Transactions: Hanging transactions pose a significant risk, often resulting from inconsistencies between the replicas and the transaction coordinator. Historically, analyzing these situations has been challenging due to limited visibility into the producers' and transaction coordinators' states. However, Kafka provides tools to detect, analyze, and recover from hanging transactions, enhancing system stability and performance. You can read more about this issue here.</p> </li> </ol> <p>In summary, while Kafka transactions in Karafka provide significant data consistency and reliability benefits, they also introduce specific performance implications. It's essential to weigh these factors carefully when designing your system and implement monitoring and performance tuning to ensure that the system can handle the required load while maintaining the integrity of the transactions. Understanding and managing these implications can help balance consistency guarantees and system performance.</p>"}, {"location": "Pro-Transactions/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Financial Transactions Processing: Ensuring that each financial transaction, whether a money transfer, payment, or stock trade, is processed exactly once, avoiding any financial discrepancies or double transaction issues.</p> </li> <li> <p>Inventory Management: Managing inventory updates precisely after sales or restocking, using Kafka to synchronize the updates across multiple systems and prevent scenarios like overselling or mismatches in stock levels.</p> </li> <li> <p>Order Processing Systems: Handling the lifecycle of an e-commerce order, from placement to delivery, by ensuring that each stage of the order is processed exactly once, thereby avoiding duplicate processing or lost orders.</p> </li> <li> <p>Data Pipeline Deduplication: Cleaning up data streams by removing duplicate data points is particularly crucial in data analytics and processing pipelines to ensure that downstream consumers work with unique, clean datasets.</p> </li> <li> <p>User Activity Tracking: Accurately recording user actions such as clicks, views, and interactions without duplication, thereby enabling precise analytics and the delivery of personalized content based on accurate user behavior data.</p> </li> <li> <p>IoT Device State Synchronization: Ensuring that messages from IoT devices are processed exactly once to maintain a consistent and accurate view of the device states is critical in environments where real-time monitoring and control are essential.</p> </li> <li> <p>Distributed System Command Processing: Processing commands issued to distributed systems like microservices architectures exactly once to prevent state corruption and ensure that the system's state remains consistent and reliable.</p> </li> <li> <p>Real-time Analytics and Monitoring: Providing accurate real-time analytics and monitoring by aggregating and processing logs or metrics data from various sources without duplication, ensuring that the insights derived are based on reliable data.</p> </li> <li> <p>Multi-Database Synchronization: Syncing data across different databases or data stores while ensuring that each update is applied exactly once, thereby preventing data drift and maintaining consistency across distributed data systems.</p> </li> </ul> <p>These scenarios illustrate the pivotal role of Karafka transactions in ensuring data integrity and consistency across various domains, leveraging Kafka's exactly-once processing semantics.</p>"}, {"location": "Pro-Transactions/#summary", "title": "Summary", "text": "<p>Karafka's Kafka transactions provide a robust mechanism for ensuring atomicity and consistency in distributed message processing. Handling produce and consume operations as a single unit prevents data loss or duplication, which is crucial for applications demanding strong consistency across partitions and topics.</p> <p>The framework supports Kafka's Exactly-Once Semantics, ensuring each message impacts the system state precisely once. This is crucial for operations like financial transactions or real-time analytics. However, performance implications like increased latency and resource demands must be considered.</p> <p>Karafka ensures comprehensive transaction management, including automatic offset management, dedicated transactional producers, and effective handling in revocation and Dead-Letter queue scenarios. </p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Virtual-Partitions/", "title": "Virtual Partitions", "text": "<p>Virtual Partitions allow you to parallelize the processing of data from a single partition. This can drastically increase throughput when IO operations are involved.</p> <p>While the default scaling strategy for Kafka consumers is to increase partitions count and number of consumers, in many cases, this will not provide you with desired effects. In the end, you cannot go with this strategy beyond assigning one process per single topic partition. That means that without a way to parallelize the work further, IO may become your biggest bottleneck.</p> <p>Virtual Partitions solve this problem by providing you with the means to further parallelize work by creating \"virtual\" partitions that will operate independently but will, as a collective processing unit, obey all the Kafka warranties.</p> <p> </p> <p> *This example illustrates the throughput difference for IO intense work, where the IO cost of processing a single message is 1ms.    </p> <p>Alternative Scaling Approach</p> <p>For CPU-intensive workloads or scenarios where data clustering makes Virtual Partitions less effective, consider Parallel Segments, which operate at the consumer group level.</p>"}, {"location": "Pro-Virtual-Partitions/#using-virtual-partitions", "title": "Using Virtual Partitions", "text": "<p>The only thing you need to add to your setup is the <code>virtual_partitions</code> definition for topics for which you want to enable it:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      # Distribute work to virtual partitions per order id\n      virtual_partitions(\n        partitioner: -&gt;(message) { message.headers['order_id'] },\n        # Defines how many concurrent virtual partitions will be created for this\n        # topic partition. When not specified, Karafka global concurrency setting\n        # will be used to make sure to accommodate as many worker threads as possible.\n        max_partitions: 5\n      )\n    end\n  end\nend\n</code></pre> <p>No other changes are needed.</p> <p>The virtual <code>partitioner</code> requires to respond to a <code>#call</code> method, and it accepts a single Karafka message as an argument.</p> <p>The return value of this partitioner needs to classify messages that should be grouped uniquely. We recommend using simple types like strings or integers.</p> <p>User-Handled Errors in Partitioner</p> <p>Handling errors within the <code>partitioner</code> is primarily the user's responsibility. However, Karafka will catch partitioning errors. Suppose an error occurs even once for a given message in a batch. In that case, Karafka will emit an error via <code>error.occurred</code> and will proceed by assigning all messages from that batch to a single virtual partition. Despite this safeguard, users must manage and mitigate any exceptions or errors in their custom partitioning logic to ensure their application's smooth operation and prevent unexpected behavior and potential data processing issues.</p>"}, {"location": "Pro-Virtual-Partitions/#available-options", "title": "Available Options", "text": "<p>Below is a list of arguments the <code>#virtual_partitions</code> topic method accepts.</p> Parameter Type Description <code>max_partitions</code> Integer Max number of virtual partitions that can come from the single distribution flow. When set to more than the Karafka threading, it will create more work than workers. When less, we can ensure we have spare resources to process other things in parallel. <code>partitioner</code> <code>#call</code> Virtual Partitioner we want to use to distribute the work. <code>offset_metadata_strategy</code> Symbol How we should match the metadata for the offset. <code>:exact</code> will match the offset matching metadata and <code>:current</code> will select the most recently reported metadata. <code>reducer</code> <code>#call</code> Reducer for VPs key. It allows for a custom reducer to achieve enhanced parallelization when the default reducer is insufficient. <code>distribution</code> Symbol          Strategy used to distribute messages across virtual partitions:         <ul> <li> <code>:consistent</code> (default) ensures messages with the same key always go to the same virtual partition, maintaining consistency across batches.           </li> <li> <code>:balanced</code> distributes work evenly across workers while preserving message order within key groups, improving utilization by up to 50% for uneven workloads.           </li> </ul>"}, {"location": "Pro-Virtual-Partitions/#messages-distribution", "title": "Messages Distribution", "text": "<p>Message distribution is based on the outcome of the <code>virtual_partitions</code> settings. Karafka will make sure to distribute work into jobs with a similar number of messages in them (as long as possible). It will also take into consideration the current <code>concurrency</code> setting and the <code>max_partitions</code> setting defined within the <code>virtual_partitions</code> method and will take into consideration appropriate <code>:strategy</code>.</p> <p>Below is a diagram illustrating an example partitioning flow of a single partition data. Each job will be picked by a separate worker and executed in parallel (or concurrently when IO is involved).</p> <p> </p>"}, {"location": "Pro-Virtual-Partitions/#partitioning-based-on-the-message-key", "title": "Partitioning Based on the Message Key", "text": "<p>Suppose you already use message keys to direct messages to partitions automatically. In that case, you can use those keys to distribute work to virtual partitions without any risks of distributing data incorrectly (splitting dependent data to different virtual partitions):</p> <pre><code>routes.draw do\n  topic :orders_states do\n    consumer OrdersStatesConsumer\n\n    # Distribute work to virtual partitions based on the message key\n    virtual_partitions(\n      partitioner: -&gt;(message) { message.key }\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Virtual-Partitions/#partitioning-based-on-the-message-payload", "title": "Partitioning Based on the Message Payload", "text": "<p>Since the virtual partitioner accepts the message as the argument, you can use both <code>#raw_payload</code> as well as <code>#payload</code> to compute your distribution key:</p> <pre><code>routes.draw do\n  topic :orders_states do\n    consumer OrdersStatesConsumer\n\n    # Distribute work to virtual partitions based on the user id, ensuring,\n    # that per user, everything is in order\n    virtual_partitions(\n      partitioner: -&gt;(message) { message.payload.fetch('user_id') }\n    )\n  end\nend\n</code></pre> <p>Lazy Deserialization Advisory</p> <p>Keep in mind that Karafka provides lazy deserialization. If you decide to use payload data, deserialization will happen in the main thread before the processing. That is why, unless needed, it is not recommended.</p>"}, {"location": "Pro-Virtual-Partitions/#partitioning-randomly", "title": "Partitioning Randomly", "text": "<p>If your messages are independent, you can distribute them randomly by running <code>rand(Karafka::App.config.concurrency)</code> for even work distribution:</p> <pre><code>routes.draw do\n  topic :orders_states do\n    consumer OrdersStatesConsumer\n\n    # Distribute work to virtual partitions based on the user id, ensuring,\n    # that per user, everything is in order\n    virtual_partitions(\n      partitioner: -&gt;(_) { rand(Karafka::App.config.concurrency) }\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Virtual-Partitions/#round-robin-partitioning", "title": "Round-Robin Partitioning", "text": "<p>If your messages are independent, you can also distribute them round-robin, ensuring their even distribution even during periods of lower traffic.</p> <pre><code># Create your partitioner\nclass RoundRobinPartitioner\n  def initialize\n    # You can replace the general concurrency with a VPs limit\n    @cycle = (0...Karafka::App.config.concurrency).cycle\n  end\n\n  # @param _message [Karafka::Messages::Message] ignored as partitioner not message based\n  # @return [Integer] VP assignment partition\n  # @note This always runs in the partitioner within the listener thread,\n  #   so standard Ruby iterator is ok as no thread-safety issues are expected\n  def call(_message)\n    @cycle.next\n  end\nend\n\n# Assign it to the topics you want\nroutes.draw do\n  topic :orders_states do\n    consumer OrdersStatesConsumer\n\n    # Distribute work to virtual partitions based on the user id, ensuring,\n    # that per user, everything is in order\n    virtual_partitions(\n      partitioner: RoundRobinPartitioner.new\n    )\n  end\nend\n</code></pre>"}, {"location": "Pro-Virtual-Partitions/#distribution-strategies", "title": "Distribution Strategies", "text": "<p>Karafka's Virtual Partitions feature provides two distribution strategies to determine how messages are allocated across consumer instances:</p> <ul> <li><code>:consistent</code> (default)</li> <li><code>:balanced</code>.</li> </ul> <p>These strategies give you flexibility in optimizing message distribution based on your specific workload characteristics and processing approach.</p>"}, {"location": "Pro-Virtual-Partitions/#consistent-distribution-default", "title": "Consistent Distribution (Default)", "text": "<p>By default, Karafka uses a consistent distribution strategy that ensures messages with the same partitioner result are always assigned to the same virtual partition consumer. This provides predictable and stable message routing, particularly important for stateful processing or when message order within a key group must be preserved across multiple batches.</p> <pre><code>routes.draw do\n topic :orders_states do\n   consumer OrdersStatesConsumer\n\n   virtual_partitions(\n     partitioner: -&gt;(message) { message.headers['order_id'] },\n     # Default - each key always gets routed to the same virtual partition\n     # This provides consistent multi-batch distribution\n     distribution: :consistent\n   )\n end\nend\n</code></pre> <p>The consistent distribution strategy ensures that:</p> <ol> <li>The same virtual partition always processes messages with the same partitioner outcome</li> <li>Distribution remains stable between batches</li> <li>Per-key ordering is strictly maintained</li> </ol> <p>However, consistent distribution can sometimes lead to suboptimal resource utilization when certain keys contain significantly more messages than others, potentially leaving some worker threads idle while others are overloaded.</p>"}, {"location": "Pro-Virtual-Partitions/#balanced-distribution", "title": "Balanced Distribution", "text": "<p>Karafka also supports a balanced distribution strategy that dynamically distributes workloads across available workers, potentially improving resource utilization by up to 50%. This strategy prioritizes even work distribution while maintaining message order within each key group.</p> <pre><code>routes.draw do\n topic :orders_states do\n   consumer OrdersStatesConsumer\n\n   virtual_partitions(\n     partitioner: -&gt;(message) { message.headers['order_id'] },\n     # Balanced distribution for more even workload distribution\n     distribution: :balanced\n   )\n end\nend\n</code></pre> <p>The balanced distribution strategy operates as follows:</p> <ol> <li>Messages are grouped by their partition key (as determined by the partitioner)</li> <li>Key groups are sorted by size (number of messages) in descending order</li> <li>Each key group is assigned to the worker with the least current workload</li> <li>Messages within each group maintain their offset order</li> </ol> <p>This approach ensures that:</p> <ul> <li>Larger message groups are processed first</li> <li>Work is distributed more evenly across available workers</li> <li>Message order within each key group is preserved within a single batch</li> <li>All available worker threads are utilized effectively</li> </ul>"}, {"location": "Pro-Virtual-Partitions/#important-considerations-for-balanced-distribution", "title": "Important Considerations for Balanced Distribution", "text": "<p>When using the balanced distribution strategy, keep in mind:</p> <ul> <li>Cross-batch assignment is not guaranteed - Unlike consistent distribution, the same key may be assigned to different virtual partitions across different batches</li> <li>Stateful processing considerations - If your consumer maintains state for specific keys across multiple batches, consistent distribution may still be more appropriate</li> <li>Messages with the same key are never split - While keys may be assigned to different virtual partitions in different batches, all messages with the same key in a single batch will be processed together</li> </ul>"}, {"location": "Pro-Virtual-Partitions/#choosing-the-right-distribution-strategy", "title": "Choosing the Right Distribution Strategy", "text": "<p>Consider these factors when selecting a distribution strategy:</p> Use <code>:consistent</code> when: Use <code>:balanced</code> when: Processing requires stable assignment of keys to workers across batches Processing is stateless or state is managed externally You're implementing window-based aggregations spanning multiple polls Maximizing worker thread utilization is a priority Predictable routing is more important than even utilization Message keys have highly variable message counts Keys have relatively similar message counts You want to optimize for throughput with uneven workloads"}, {"location": "Pro-Virtual-Partitions/#performance-comparison", "title": "Performance Comparison", "text": "<p>The balanced distribution strategy can significantly improve resource utilization in high-throughput scenarios with uneven message distribution. Internal benchmarks show improvements of up to 50% in throughput for workloads where:</p> <ul> <li>Message keys have highly variable message counts</li> <li>Processing is IO-bound (such as database operations)</li> <li>Worker threads would otherwise be underutilized with consistent distribution</li> </ul> <p>The performance gains are most significant when:</p> <ol> <li>Some keys contain many more messages than others</li> <li>The total number of keys is greater than the number of available worker threads</li> <li>Message processing involves IO operations that can benefit from concurrent execution</li> </ol>"}, {"location": "Pro-Virtual-Partitions/#managing-number-of-virtual-partitions", "title": "Managing Number of Virtual Partitions", "text": "<p>By default, Karafka will create at most <code>Karafka::App.config.concurrency</code> concurrent Virtual Partitions. This approach allows Karafka to occupy all the threads under optimal conditions.</p>"}, {"location": "Pro-Virtual-Partitions/#limiting-number-of-virtual-partitions", "title": "Limiting Number of Virtual Partitions", "text": "<p>However, it also means that other topics may not get their fair share of resources. To mitigate this, you may dedicate only 80% of the available threads to Virtual Partitions.</p> <pre><code>setup do |config|\n  config.concurrency = 10\nend\n\nroutes.draw do\n  topic :orders_states do\n    consumer OrdersStatesConsumer\n\n    virtual_partitions(\n      partitioner: -&gt;(message) { message.payload.fetch('user_id') },\n      # Leave two threads for other work of other topics partitions\n      # (non VP or VP of other partitions)\n      max_partitions: 8\n    )\n  end\nend\n</code></pre> <p>Virtual Partitions Workload Distribution</p> <p>Virtual Partitions <code>max_partitions</code> setting applies per topic partition. In the case of processing multiple partitions, there may be a case where all the work happens on behalf of Virtual Partitions.</p>"}, {"location": "Pro-Virtual-Partitions/#increasing-number-of-virtual-partitions", "title": "Increasing Number of Virtual Partitions", "text": "<p>There are specific scenarios where you may be interested in having more Virtual Partitions than threads. One example would be to create one Virtual Partition for the data of each user. If you set the <code>max_partitions</code> to match the <code>max_messages</code>, Karafka will create each Virtual Partition based on your grouping without reducing it to match number of worker threads.</p> <pre><code>setup do |config|\n  config.concurrency = 10\n  config.max_messages = 200\nend\n\nroutes.draw do\n  topic :orders_states do\n    consumer OrdersStatesConsumer\n\n    virtual_partitions(\n      partitioner: -&gt;(message) { message.payload.fetch('user_id') },\n      # Make sure, that each virtual partition always contains data of only a single user\n      max_partitions: 200\n    )\n  end\nend\n</code></pre> <p>Virtual Partitions Lifespan</p> <p>Please remember that Virtual Partitions are long-lived and will stay in the memory for as long as the Karafka process owns the given partition.</p>"}, {"location": "Pro-Virtual-Partitions/#virtual-offset-management", "title": "Virtual Offset Management", "text": "<p>When Karafka consumes messages with Virtual Partitions, it uses Virtual Offset Management, which is built on top of the regular offset management mechanism. This innovative approach enables a significant reduction in potentially double-processed messages. By employing Virtual Offset Management, Karafka intelligently tracks the offsets of messages consumed in all the virtual partitions, ensuring that each message is consumed only once, regardless of errors. This powerful feature enhances the reliability and efficiency of message processing, eliminating the risk of duplicate processing and minimizing any associated complications, thereby enabling seamless and streamlined data flow within your system.</p> <p>This feature operates on a few layers to provide as good warranties as possible while ensuring that each virtual partition can work independently. Below you can find a detailed explanation of each component making Virtual Offset Management.</p>"}, {"location": "Pro-Virtual-Partitions/#collective-state-materialization", "title": "Collective State Materialization", "text": "<p>While each of the Virtual Partitions operates independently, they are bound together to a single Kafka Partition. Collective State Materialization transforms the knowledge of messages marked as consumed in each virtual partition into a Kafka offset that can be committed. This process involves computing the highest possible offset by considering all the messages marked as consumed from all the virtual partitions. By analyzing the offsets across virtual partitions, Karafka can determine the maximum offset reached, allowing for an accurate and reliable offset commit to Kafka. This ensures that the state of consumption is properly synchronized and maintained.</p> <p>Whenever you <code>mark_as_consumed</code> when using Virtual Partitions, Karafka will ensure that Kafka receives the highest possible continuous offset matching the underlying partition.</p> <p>Below you can find a few examples of how Karafka transforms messages marked as consumed in virtual partitions into an appropriate offset that can be committed to Kafka.</p> <p> </p>"}, {"location": "Pro-Virtual-Partitions/#virtual-partition-collective-marking", "title": "Virtual Partition Collective Marking", "text": "<p>When a message is marked as consumed, Karafka recognizes that all previous messages in that virtual partition have been processed as well, even if they were not explicitly marked as consumed.</p> <p>This functionality seamlessly integrates with collective marking to materialize the highest possible Kafka offset. With collective marking, Karafka can efficiently track the progress of message consumption across different virtual consumers.</p> <p>Below you can find an example illustrating which of the messages will be virtually marked as consumed when given message in a virtual partition is marked.</p> <p> </p>"}, {"location": "Pro-Virtual-Partitions/#reprocessing-exclusions", "title": "Reprocessing Exclusions", "text": "<p>When using Virtual Partitions, Karafka automatically skips previously consumed messages upon retries. This feature ensures that in the event of an error during message processing in one virtual partition, Karafka will not attempt to reprocess the messages that have already been marked as consumed in any of the virtual partitions.</p> <p>By automatically skipping already consumed messages upon encountering an error, Karafka helps ensure the reliability and consistency of message processing. It prevents the application from reprocessing messages unnecessarily and avoids potential data duplication issues during error recovery scenarios.</p> <p>This behavior is advantageous in scenarios where message processing involves external systems or operations that are not idempotent. Skipping previously consumed messages reduces the risk of executing duplicate actions and helps maintain the integrity of the overall system.</p> <p> </p>"}, {"location": "Pro-Virtual-Partitions/#behaviour-on-errors", "title": "Behaviour on Errors", "text": "<p>For a single partition-based Virtual Partitions group, offset management and retries policies are entangled. They behave on errors precisely the same way as regular partitions with one difference: back-offs and retries are applied to the underlying regular partition. This means that if an error occurs in one of the virtual partitions, Karafka will pause based on the highest possible Virtual Offset computed using the Virtual Offset Management feature and will exclude all the messages that were marked as consumed in any of the virtual partitions.</p> <p> </p> <p>If processing in all virtual partitions ends up successfully, Karafka will mark the last message from the underlying partition as consumed.</p> <p>Impact of Pausing on Message Count</p> <p>Since pausing happens in Kafka, the re-fetched data may contain more or fewer messages. This means that after retry, the number of messages and their partition distribution may differ. Despite that, all ordering warranties will be maintained.</p>"}, {"location": "Pro-Virtual-Partitions/#collapsing", "title": "Collapsing", "text": "<p>When an error occurs in virtual partitions, pause, retry and collapse will occur. Collapsing allows virtual partitions to temporarily restore all the Kafka ordering warranties allowing for the usage of things like offset marking and Dead-Letter Queue.</p> <p>You can detect that your Virtual Partitions consumers are operating in the collapsed mode by invoking the <code>#collapsed?</code> method:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      Event.store!(message.payload)\n\n      puts 'We operate in a collapsed mode' if collapsed?\n    end\n  end\nend\n</code></pre> <p> </p> <p> *This example illustrates the retry and collapse of two virtual partitions into one upon errors.    </p>"}, {"location": "Pro-Virtual-Partitions/#usage-with-dead-letter-queue", "title": "Usage with Dead Letter Queue", "text": "<p>Virtual Partitions can be used together with the Dead Letter Queue. This can be done due to Virtual Partitions' ability to collapse upon errors.</p> <p>The only limitation when combining Virtual Partitions with the Dead Letter Queue is the minimum number of retries. It needs to be set to at least <code>1</code>:</p> <pre><code>routes.draw do\n  topic :orders_states do\n    consumer OrdersStatesConsumer\n    virtual_partitions(\n      partitioner: -&gt;(message) { message.headers['order_id'] }\n    )\n    dead_letter_queue(\n      topic: 'dead_messages',\n      # Minimum one retry because VPs needs to switch to the collapsed mode\n      max_retries: 1\n    )\n  end\nend\n</code></pre> <p>DLQ independent mode usage with Virtual Partitions</p> <p>The <code>independent</code> DLQ flag in Karafka can be used with the Virtual Partitions. When an error occurs in virtual partitions, pause, retry, and collapse will occur. Collapsing allows virtual partitions to temporarily restore all the Kafka ordering warranties, meaning that the <code>independent</code> flag can operate in the same fashion as if virtual partitions were not used.</p>"}, {"location": "Pro-Virtual-Partitions/#manual-collapsing", "title": "Manual Collapsing", "text": "<p>When working with Virtual Partitions in Karafka, users can manually invoke a collapsing operation. This provides flexibility and control, especially when non-linear message processing is required.</p> <p>Just as an error in a Virtual Partition will trigger a collapse, ensuring adherence to Kafka's ordering warranties upon retries, users can also initiate this collapse process manually. By doing so, the Virtual Partitions temporarily collapse and restore the natural message processing order established by Kafka.</p> <p>To manually collapse your Virtual Partitions, you need to invoke the <code>#collapse_until!</code> method as follows:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    imported, failed = import(messages)\n\n    return if failed.empty?\n\n    # Collapse until last broken message\n    collapse_until!(failed.last.offset)\n\n    # Note, that pausing is collective for all VPs, hence we need to make sure, that when we pause\n    # we select the lowest offset out of all VPs for a given topic partition\n    synchronize do\n      lowest_offset = PauseManager.pause(topic, partition, failed.first.offset)\n      pause(lowest_offset, 5_000)\n    end\n  end\nend\n</code></pre> <p>When multiple Virtual Partitions invoke the <code>#collapse_until!</code> method concurrently, Karafka ensures consistency by considering all requested offsets. If different partitions request different offsets, the system will prioritize and collapse until the highest requested offset. This ensures that no messages before that offset are processed out of order, maintaining the integrity of your message stream even in complex processing scenarios. So, if multiple collapses are requested simultaneously, the most conservative (highest offset) collapse request takes precedence.</p>"}, {"location": "Pro-Virtual-Partitions/#ordering-warranties", "title": "Ordering Warranties", "text": "<p>Virtual Partitions provide three types of warranties in regards to order:</p> <ul> <li>Standard warranties per virtual partitions group - that is, from the \"outside\" of the virtual partitions group Kafka ordering warranties are preserved.</li> <li>Inside each virtual partition - the partitioner order is always preserved. That is, offsets may not be continuous (1, 2, 3, 4), but lower offsets will always precede larger (1, 2, 4, 9). This depends on the <code>virtual_partitions</code> <code>partitioner</code> used for partitioning a given topic.</li> <li>Strong Kafka ordering warranties when operating in the <code>collapsed</code> mode with automatic exclusion of messages virtually marked as consumed.</li> </ul> <p> </p> <p> *Example distribution of messages in between two virtual partitions.    </p>"}, {"location": "Pro-Virtual-Partitions/#consumers-synchronization", "title": "Consumers Synchronization", "text": "<p>When using Virtual Partitions in Karafka, multiple consumers will concurrently operate on data from the same topic partition. This can lead to potential data races and inconsistencies if not properly managed. To help with this, Karafka provides a mechanism to synchronize access among these consumers using the <code>#synchronize</code> method.</p> <p>Karafka's <code>#synchronize</code> method uses a mutex to guarantee that the code inside its block will not face race conditions with other consumers. This ensures that only one consumer can execute the synchronized block of code at a time, thus providing a way to perform operations that should be atomic safely.</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    sum = messages.payloads.map { |data| data.fetch(:count) }.sum\n\n    # Make sure that the counter is not in a race-condition with other\n    # consumers from same partition\n    synchronize do\n      counts = Cache.fetch(topic, partition) || 0\n      Cache.set(topic, partition, counts + sum)\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Virtual-Partitions/#thread-management-and-consumers-assignment", "title": "Thread Management and Consumers Assignment", "text": "<p>Karafka assigns each virtual partition a dedicated, long-lived consumer instance. This design ensures that messages within a virtual partition are processed consistently and independently from other partitions and that those consumers can implement things like accumulators and buffers. However, there is no fixed relationship between threads and consumer instances, allowing for flexible and efficient use of resources.</p> <p>One of Karafka's key strengths is the adaptability of its worker threads. Any available thread can run any consumer instance, a dynamic allocation that ensures efficient utilization of all available resources. This flexibility prevents idle threads, maximizing throughput. The dynamic nature of thread assignment also means that different threads can seamlessly pick up the work of a particular virtual partition between batches, giving users a sense of control. </p> <p>The assignment of messages to virtual partitions is consistent and based on a partitioner key. This key ensures that messages with the same key are consistently routed to the same virtual partition. Consequently, the same consumer instance processes these messages, maintaining the order and integrity required for reliable multi-batch message handling.</p> <p>Karafka's architecture involves maintaining a map of consumer instances mapped to virtual partitions. These instances are managed dynamically and persist as long as the partition assignment is active. This persistence ensures stability and consistency in message processing, even as threads are reassigned between batches.</p> <p>By decoupling thread assignment from consumer instances and ensuring dedicated, long-lived consumer instances per virtual partition, Karafka achieves a balance between flexibility and consistency. This design allows for efficient resource utilization, consistent message processing, and the ability to handle high-throughput scenarios effectively.</p> <p> </p> <p> *This example illustrates the flow of message distribution through virtualization and scheduling until the worker threads jobs pickup for processing.    </p>"}, {"location": "Pro-Virtual-Partitions/#reducer-replacement", "title": "Reducer Replacement", "text": "<p>In Karafka, the default reducer for Virtual Partitions is a method designed to distribute messages across virtual partitions. It does this by using a simple mathematical operation on the sum of the stringified version of the virtual key. While this method is generally effective, it may not be fully optimal under certain configurations. For example, it could consistently use only 60% or less of the available threads, leading to inefficiencies and underutilization of resources.</p> <p>Karafka allows you to replace the default reducer with a custom one to address this. This can be particularly useful when implementing a more sophisticated partitioning strategy to enhance parallelization and balance the load more effectively. By customizing the reducer, you can ensure that all available threads are optimally utilized, leading to better performance and throughput for your application.</p>"}, {"location": "Pro-Virtual-Partitions/#implementing-a-custom-reducer", "title": "Implementing a Custom Reducer", "text": "<p>A custom reducer must respond to the <code>#call</code> method, accepting a virtual key and returning an integer representing the assigned virtual partition. Below is an example of how to implement and configure a custom reducer:</p> <pre><code># Custom reducer that uses a different strategy for distributing messages\nclass CustomReducer\n  def call(virtual_key)\n    concurrency = Karafka::App.config.concurrency\n\n    # Implement your custom logic here\n    # For example, you could use a hash function or any other distribution logic\n    Digest::MD5.hexdigest(virtual_key.to_s).to_i(16) % concurrency\n  end\nend\n\n# Configure the custom reducer in your Karafka application\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.concurrency = 10\n  end\n\n  routes.draw do\n    topic :orders_states do\n      consumer OrdersStatesConsumer\n\n      # Use the custom reducer for virtual partitions\n      virtual_partitions(\n        partitioner: -&gt;(message) { message.headers['order_id'] },\n        reducer: CustomReducer.new\n      )\n    end\n  end\nend\n</code></pre> <p>This class defines a custom reducer with a <code>#call</code> method. The method uses an MD5 hash function to compute an integer based on the virtual key, ensuring a more distributed and potentially collision-resistant assignment of messages to virtual partitions.</p>"}, {"location": "Pro-Virtual-Partitions/#benefits-of-using-a-custom-reducer", "title": "Benefits of Using a Custom Reducer", "text": "<ul> <li> <p>Enhanced Distribution: Custom reducers allow for more complex and nuanced distribution strategies, which can better balance the load and reduce hotspots.</p> </li> <li> <p>Adaptability: Different applications have different needs. A custom reducer can be tailored to specific requirements, whether that's handling unique data distributions, optimizing for specific performance characteristics, or integrating with other systems.</p> </li> <li> <p>Scalability: By refining how messages are distributed across virtual partitions, custom reducers can help ensure that processing scales efficiently as the volume of messages increases.</p> </li> </ul>"}, {"location": "Pro-Virtual-Partitions/#when-to-use-a-custom-reducer", "title": "When to Use a Custom Reducer", "text": "<ul> <li> <p>Complex Workflows: If your application has complex workflows that require fine-tuned distribution of messages.</p> </li> <li> <p>Performance Optimization: When you need to optimize the performance and efficiency of message processing.</p> </li> <li> <p>Specialized Requirements: If your data and processing requirements are unique and cannot be effectively managed by the default reducer.</p> </li> </ul>"}, {"location": "Pro-Virtual-Partitions/#monitoring", "title": "Monitoring", "text": "<p>Karafka default monitor and the Web UI dashboard work with virtual partitions out of the box. No changes are needed. Virtual batches are reported as they would be regular batches.</p>"}, {"location": "Pro-Virtual-Partitions/#shutdown-and-revocation-handlers", "title": "Shutdown and Revocation Handlers", "text": "<p>Both <code>#shutdown</code> and <code>#revoked</code> handlers work the same as within regular consumers.</p> <p>For each virtual consumer instance, both are executed when shutdown or revocation occurs. Please keep in mind that those are executed for each instance. That is, upon shutdown, if you used ten threads and they were all used with virtual partitions, the <code>#shutdown</code> method will be called ten times. Once per each virtual consumer instance that was in use.</p> <p> </p>"}, {"location": "Pro-Virtual-Partitions/#virtual-partitions-vs-increasing-number-of-partitions-vs-parallel-segments", "title": "Virtual Partitions vs. Increasing Number of Partitions vs. Parallel Segments", "text": "<p>When building a scalable Kafka consumer application with Karafka, you'll likely be faced with a decision about parallelization strategies: Should you increase the number of physical partitions, leverage Virtual Partitions, or use Parallel Segments? All three strategies aim to parallelize work to boost processing speed but offer different advantages and considerations based on your workload characteristics. This section delves into the differences between these methods and provides insights to help you make an informed decision.</p>"}, {"location": "Pro-Virtual-Partitions/#conceptual-differences", "title": "Conceptual Differences", "text": "<ul> <li> <p>Physical Partitions: These are actual divisions of a Kafka topic. Messages within a topic are divided amongst these partitions. Consumers can read from multiple partitions concurrently, but a single partition's data can only be read by one consumer at a time.</p> </li> <li> <p>Virtual Partitions: VPs are a feature provided by Karafka. They allow for further parallelization of work within a physical partition. Using VPs, multiple workers can simultaneously process different chunks of data from the same physical partition.</p> </li> <li> <p>Parallel Segments: Parallel Segments create multiple independent consumer groups that each process filtered subsets of messages from the same topic partition, optimized for CPU-intensive workloads and scenarios where Virtual Partitions may be inefficient.</p> </li> </ul>"}, {"location": "Pro-Virtual-Partitions/#work-distribution", "title": "Work Distribution", "text": "<ul> <li> <p>Physical Partitions: Increasing the number of partitions allows more consumer processes to read data concurrently. However, you're constrained by the fact that only one consumer can read from a partition at a given time.</p> </li> <li> <p>Virtual Partitions: VPs enable multiple workers to process data from a single partition. This is useful when data within a partition can be divided into independent logical chunks. The result is better work distribution and utilization of worker threads, especially in heavy-load scenarios.</p> </li> <li> <p>Parallel Segments: PSs create multiple independent consumer groups that each download all messages from the same partition but filter and process only their assigned subset. This approach excels when batches contain large groups of related messages (e.g., many messages with the same <code>user_id</code>) that cannot be effectively split by Virtual Partitions, making it particularly effective for CPU-intensive workloads.</p> </li> </ul>"}, {"location": "Pro-Virtual-Partitions/#polling-characteristics", "title": "Polling Characteristics", "text": "<ul> <li> <p>Physical Partitions: Efficient polling requires a consistent distribution of messages across all partitions. Challenges arise when there's uneven message distribution or when polled batches do not include data from multiple partitions, leading to some consumers being under-utilized.</p> </li> <li> <p>Virtual Partitions: VPs compensate for uneven polling. Even if polling fetches data mainly from one partition, VPs ensure that multiple workers distribute and process the data. This mitigates the impact of uneven distribution.</p> </li> <li> <p>Parallel Segments: PSs handle uneven polling differently - each consumer group polls and downloads all messages from the partition, then filters to process only their assigned subset. This approach provides consistent workload distribution across segments regardless of polling patterns, but at the cost of increased network bandwidth usage since each segment downloads the full message set.</p> </li> </ul> <p>Virtual Partitions and Parallel Segments both serve as mechanisms to enhance parallel processing without directly influencing the underlying partition-polling mechanism. When consumers poll data from Kafka, they rely on the actual partitions. The introduction of these features keeps this fundamental process the same. Instead, the true benefit arises from the ability to distribute and parallelize work across multiple consumer threads (Virtual Partitions) or multiple consumer groups (Parallel Segments), allowing for improved scalability and performance without necessitating changes to the core Kafka infrastructure or the way data is polled.</p>"}, {"location": "Pro-Virtual-Partitions/#comparative-scenario", "title": "Comparative Scenario", "text": "<p>Consider a single-topic scenario where IO is involved, and data can be further partitioned:</p> <ul> <li> <p>Scenario #1: 200 partitions with 20 Karafka consumer processes and a concurrency of 10 results in 200 total worker threads.</p> </li> <li> <p>Scenario #2: 100 partitions, 20 Karafka consumer processes with a concurrency of 10, and Virtual Partitions enabled, also results in 200 total worker threads.</p> </li> <li> <p>Scenario #3: 10 partitions with 5 parallel segments each, 20 Karafka consumer processes with a concurrency of 10, resulting in 200 total worker threads across 50 consumer groups.</p> </li> </ul> <p>In all scenarios, the number of worker threads remains the same. However, with VPs (Scenario #2), Karafka will perform better than Scenario #1 due to a more effective distribution of work among worker threads. This enhanced performance is especially pronounced when messages are uneven across topic partitions or batches polled from Kafka contain data from a few or even one topic partition.</p> <p>Parallel Segments (Scenario #3) excel when batches contain large groups of related messages that cannot be effectively distributed by Virtual Partitions, such as when most messages in a batch share the same <code>user_id</code>. The difference is in the work distribution as sub-parts of each topic partition will be processed independently across multiple consumer groups. The trade-off is increased network bandwidth usage, as each segment downloads all messages before filtering to process only their assigned subset.</p>"}, {"location": "Pro-Virtual-Partitions/#conclusion", "title": "Conclusion", "text": "<p>While increasing the number of Kafka partitions offers a more native way to parallelize data processing, both Virtual Partitions and Parallel Segments provide more efficient, flexible, and Karafka-optimized approaches. Your choice should be based on your application's specific needs, the nature of your data, and the load you anticipate. </p> <p>Virtual Partitions are ideal for IO-bound workloads and scenarios where messages can be evenly distributed within a single consumer group. Parallel Segments excel for CPU-intensive processing and situations where batches contain large groups of related messages that need to be processed together but can be filtered at the consumer group level.</p> <p>When aiming for maximum throughput and efficient resource utilization, especially under heavy load, Virtual Partitions or Parallel Segments together with well-partitioned topics are more favorable choices than simply increasing partition count.</p>"}, {"location": "Pro-Virtual-Partitions/#scalability-constraints", "title": "Scalability Constraints", "text": "<ul> <li> <p>Physical Partitions: There's a limit to how many physical partitions you can have, and re-partitioning a topic can be a complex task. Also, there's overhead associated with managing more partitions.</p> </li> <li> <p>Virtual Partitions: VPs provide scalability within the confines of existing physical partitions. They don't add to the management overhead of Kafka and offer a more flexible way to scale processing power without altering the topic's physical partitioning.</p> </li> </ul>"}, {"location": "Pro-Virtual-Partitions/#customizing-the-partitioning-engine-load-aware-partitioning", "title": "Customizing the partitioning engine / Load aware partitioning", "text": "<p>There are scenarios upon which you can differentiate your partitioning strategy based on the number of received messages per topic partition. It is impossible to set it easily using the default partitioning API, as this partitioner accepts single messages. However, Pro users can use the <code>Karafka::Pro::Processing::Partitioner</code> as a base for a custom partitioner that can achieve something like this.</p> <p>One great example of this is a scenario where you may want to partition messages in such a way as to always end up with at most <code>5 000</code> messages in a single Virtual Partition.</p> <pre><code># This is a whole process partitioner, not a per topic one\nclass CustomPartitioner &lt; Karafka::Pro::Processing::Partitioner\n  def call(topic_name, messages, coordinator, &amp;block)\n    # Apply the \"special\" strategy for this special topic unless VPs were collapsed\n    # In the case of collapse you want to process with the default flow.\n    if topic_name == 'balanced_topic' &amp;&amp; !coordinator.collapsed?\n      balanced_strategy(messages, &amp;block)\n    else\n      # Apply standard behaviours to other topics\n      super\n    end\n  end\n\n  private\n\n  # Make sure you end up with virtual partitions that always have at most 5 000 messages and create\n  # as few partitions as possible\n  def balanced_strategy(messages)\n    messages.each_slice(5_000).with_index do |slice, index|\n      yield(index, slice)\n    end\n  end\nend\n</code></pre> <p>Once you create your custom partitioner, you need to overwrite the default one in your configuration:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.internal.processing.partitioner_class = CustomPartitioner\n  end\nend\n</code></pre> <p>When used that way, your <code>balanced_topic</code> will not use the per topic <code>partitioner</code> nor <code>max_partitions</code>. This topic data distribution will solely rely on your <code>balanced_strategy</code> logic.</p>"}, {"location": "Pro-Virtual-Partitions/#example-use-cases", "title": "Example Use Cases", "text": "<p>Here are some use cases from various industries where Karafka's virtual partition feature can be beneficial:</p> <ul> <li> <p>Adtech: An ad-tech company may need to process a large number of ad impressions or clicks coming in from a single Kafka topic partition. By using virtual partitions to parallelize the processing of these events, they can improve the efficiency and speed of their ad-serving system, which often involves database operations.</p> </li> <li> <p>E-commerce: In the e-commerce industry, processing many product orders or inventory updates can be IO bound. By using virtual partitions to parallelize the processing of these events, e-commerce companies can improve the efficiency and speed of their systems, enabling them to serve more customers and update inventory more quickly.</p> </li> <li> <p>Logistics: A logistics company may need to process a large volume of shipment or tracking data coming in from a single Kafka topic partition. By using virtual partitions to parallelize the processing of this data, they can improve the efficiency of their logistics operations and reduce delivery times.</p> </li> <li> <p>Healthcare: In healthcare, processing a large volume of patient data can be IO bound, particularly when interacting with electronic health records (EHR) or other databases. By using virtual partitions to parallelize the processing of patient data coming from a single Kafka topic partition, healthcare organizations can improve the efficiency of their data analysis and provide more timely care to patients.</p> </li> <li> <p>Social Media: Social media platforms often need to process many user interactions, such as likes, comments, and shares, coming in from a single Kafka topic partition. By using virtual partitions to parallelize the processing of these events, they can improve the responsiveness of their platform and enhance the user experience.</p> </li> </ul> <p>Overall, virtual partitions can be beneficial in any industry where large volumes of data need to be processed quickly and efficiently, particularly when processing is IO bound. By parallelizing the processing of data from a single Kafka topic partition, organizations can improve the performance and scalability of their systems, enabling them to make more informed decisions and deliver better results.</p> <p>Last modified: 2025-07-08 17:55:56</p>"}, {"location": "Pro-Web-UI-Branding/", "title": "Pro Web UI Branding", "text": "<p>The Karafka Web UI branding feature allows you to customize the UI to reflect its running environment (e.g., development, staging, production). This configuration helps prevent mistakes by clarifying which environment you are working in. The branding options include setting a label, displaying a notice, and defining the branding style.</p> <p>The branding configuration is done through the <code>config.ui.branding</code>. You can adjust the following settings:</p> <ul> <li> <p><code>type</code>: Defines the styling for the branding notice. It aligns with our UI styling options and can be set to one of the following: <code>:info</code>, <code>:error</code>, <code>:warning</code>, <code>:success</code>, <code>:primary</code>. The default value is <code>:info</code>.</p> </li> <li> <p><code>label</code>: A string that serves as the environment label (e.g., \"Production\" or \"Staging\"). This label is displayed below the logo in the Web UI. To disable the label, set this to <code>false</code>. The default value is <code>false</code>.</p> </li> <li> <p><code>notice</code>: An additional wide alert notice highlighting extra environmental details. This is a string that can be used for a custom message or set to <code>false</code> to disable it. The default value is <code>false</code>.</p> </li> </ul> <pre><code>Karafka::Web.setup do |config|\n  config.ui.branding.type = :warning\n  config.ui.branding.notice = 'You are working in the production environment \u2013 proceed with caution!'\n  config.ui.branding.label = 'Production'\nend\n</code></pre> <p></p>"}, {"location": "Pro-Web-UI-Branding/#best-practices", "title": "Best Practices", "text": "<ul> <li>Always set a unique label for each environment (e.g., \"Production\", \"Development\") to avoid any accidental confusion.</li> <li>Use the notice field to display critical environment-specific information, especially in production environments.</li> <li>Choose an appropriate type to convey the importance or caution level of the environment visually.</li> </ul> <p>Last modified: 2024-09-17 10:03:38</p>"}, {"location": "Pro-Web-UI-Commanding/", "title": "Consumers Control", "text": "<p>Karafka offers capabilities for controlling and managing consumers at both process and partition levels. These management features, accessible via the Web UI, allow administrators to interact with consumer processes in real-time to maintain optimal performance and address issues.</p>"}, {"location": "Pro-Web-UI-Commanding/#process-level-control", "title": "Process-Level Control", "text": "<p>Process-level commanding includes tracing consumers for detailed state information, quieting consumers to reduce activity, and stopping them when necessary.</p> <p>Quiet and Stop Commands Limitation</p> <p>Quiet and Stop commands do not function with Karafka processes operating in Embedded or Swarm mode. This limitation arises because, in these modes, the Karafka process itself is not directly responsible for its state management.</p>"}, {"location": "Pro-Web-UI-Commanding/#configuration", "title": "Configuration", "text": "<p>Commanding is turned on by default. During each consumer process startup, it initiates a special \"invisible\" connection to Kafka. This connection is used exclusively for administrative commands that can be executed from the Web UI, such as stopping, quieting, and tracing consumers for backtraces.</p> <p>To turn off this feature, you can set the <code>config.commanding.active</code> configuration option to <code>false</code>. Disabling commanding removes the extra Kafka connection dedicated to these administrative tasks. Consequently, it also disables the ability to execute these commands from the Web UI.</p> <pre><code># Completely disable commanding from Web UI\nKarafka::Web.setup do |config|\n  # Other config options...\n\n  # Set this to false to disable commanding completely\n  config.commanding.active = false\nend\n\nKarafka::Web.enable!\n</code></pre> <p>Opting out of commanding is recommended for environments where direct consumer manipulation via the Web UI is either unnecessary or could pose a risk to the stability or security of Kafka operations. This configuration adjustment is particularly relevant in tightly controlled environments where changes to consumer behavior should be managed through more rigorous operational workflows.</p>"}, {"location": "Pro-Web-UI-Commanding/#controls-and-commands", "title": "Controls and Commands", "text": "<p>The commanding functionality within the Web UI is organized into two distinct tabs: \"Controls\" and \"Commands.\" These tabs provide a centralized interface for managing consumer processes, making it easier for administrators to perform and track administrative actions directly from the Web UI.</p> <p>Commands issued through the Web UI are retained in Kafka for a default period of 7 days. After this duration, these commands are automatically removed from the system. This automatic cleanup helps manage the storage and maintain efficiency within Kafka without manual intervention.</p>"}, {"location": "Pro-Web-UI-Commanding/#controls-tab", "title": "Controls Tab", "text": "<p>The \"Controls\" tab is primarily focused on direct consumer process management. It presents a list of all consumer processes currently active within the system. Each listed process is accompanied by detailed information, including its current state, performance metrics, and any other relevant operational data that can help assess the process's health and activity.</p> <p>Alongside these details, the \"Controls\" tab also provides actionable commands that can be issued for each consumer process. These commands include options such as:</p> <ul> <li>Tracing: Requesting backtraces and other runtime information from the consumer to diagnose issues or assess performance.</li> <li>Quieting: Reducing the consumer's activity level to manage load or prepare for maintenance without stopping the process entirely.</li> <li>Stopping: Completely halting a consumer process for scenarios like system upgrades, bug fixes, or decommissioning.</li> </ul> <p>This tab serves as the operational center for managing consumers, allowing administrators to apply commands individually or in bulk, depending on the situation.</p> <p> </p>"}, {"location": "Pro-Web-UI-Commanding/#commands-tab", "title": "Commands Tab", "text": "<p>The \"Commands\" tab offers a historical view of the commands issued through the Web UI and results received from the consumer processes. This tab lists recent commands along with detailed outcomes, such as the success or failure of each command and the diagnostic data returned from tracing actions.</p> <p>The functionality of the \"Commands\" tab is crucial for several reasons:</p> <ul> <li>Tracking and Auditing: It provides a log of all commands issued, which is essential for auditing and tracking the administrative actions taken over time.</li> <li>Result Verification: Administrators can verify the outcomes of issued commands, particularly the backtraces and diagnostic information returned by the tracing command. This helps confirm diagnostic efforts' effectiveness and understand consumer processes' current state.</li> <li>Historical Analysis: This tab records past commands and their results, aiding in identifying trends or recurring issues across consumer processes. This information can inform future improvements or adjustments in system configuration.</li> </ul> <p>Each command in the Commands tab progresses through multiple states in its lifecycle:</p> <ol> <li>Request: Initial command issued by the Web UI</li> <li>Acceptance: Confirmation that the consumer process has received the command, but it is one that may not be immediately executed</li> <li>Result: Final outcome of the command execution</li> </ol> <p> </p>"}, {"location": "Pro-Web-UI-Commanding/#tracing", "title": "Tracing", "text": "<p>Tracing is a diagnostic feature that allows administrators to request backtraces from active consumer processes. This command is useful for identifying the internal state and tracing the execution path of a consumer at any given moment without stopping or disrupting its operations. By invoking the tracing command via the Web UI, administrators can gain insights into the call stack and other runtime details, which are crucial for debugging complex issues in consumer behavior.</p> <p>A special command message is dispatched to the targeted consumer when the \"Trace\" button is pressed in the Web UI. Upon receiving this message, the consumer iterates over all Ruby threads currently running within the process and records the backtrace of each thread. These backtraces provide a snapshot of the execution stack for each thread, which can be invaluable for debugging and understanding the consumer's behavior at a particular moment. After collecting the backtraces, the consumer compiles these into a special result message that is published back to Kafka. This result message is accessible for inspection, providing real-time diagnostic information about the consumer's state.</p> <p> </p>"}, {"location": "Pro-Web-UI-Commanding/#use-cases", "title": "Use Cases", "text": "<ol> <li>Debugging Deadlocks: Identifying and resolving thread deadlocks, where two or more threads are waiting on each other to release resources.</li> <li>Performance Bottlenecks: Diagnosing performance issues by understanding which threads are consuming most resources or are stuck in long-running operations.</li> <li>Unexpected Delays: Investigating unexplained delays in message processing, potentially caused by external API calls or resource locking.</li> <li>Error Reproduction: Capturing the exact state of a consumer when an intermittent or hard-to-reproduce error occurs, aiding in debugging.</li> <li>Optimization: Analyzing thread states for potential optimizations in how resources are utilized, or operations are conducted.</li> <li>Training and Learning: Educating new developers or operators about the internal workings of a consumer by showing real-time thread activities and states.</li> </ol>"}, {"location": "Pro-Web-UI-Commanding/#quieting", "title": "Quieting", "text": "<p>Quieting is a command designed to gracefully reduce the activity of a consumer process. When a quiet command is issued, the consumer stops accepting new jobs or batches but continues to process any currently active tasks. This feature is particularly useful during deployments or when a controlled slowdown in consumer activities is required. Quieting ensures that ongoing processing completes successfully while preventing new work from being started, aiding in smooth transitions and system maintenance without abrupt interruptions.</p> <p>Stability Through Quiet State</p> <p>Moving to and staying in a quiet state means that no rebalance will be triggered. Although the consumer will not process new messages, it will not relinquish its assignments, maintaining its position within the consumer group.</p>"}, {"location": "Pro-Web-UI-Commanding/#use-cases_1", "title": "Use Cases", "text": "<ol> <li>Deployment Updates: Reducing consumer activity before deploying updates or patches ensures a smooth transition and reduces the risk of data loss or errors during application updates.</li> <li>System Maintenance: Temporarily reduce the load on the system during maintenance activities like hardware upgrades or network changes to maintain overall system stability.</li> <li>Performance Diagnostics: Isolating performance issues without stopping the consumer entirely, allowing for live monitoring and troubleshooting while minimizing impact on overall operations.</li> <li>Error Containment: In case of an identified error affecting a consumer's tasks, quieting allows the consumer to finish current tasks without accepting new potentially compromised work containing the error.</li> <li>Incremental Upgrades: Gradually upgrading consumers in a system without causing a full rebalance or downtime, by quieting certain consumers at a time while others take over the processing load.</li> <li>Controlled Shutdowns: Preparing consumers for a controlled shutdown by quieting them to complete the processing of current messages while not picking up new ones, ensuring data integrity and smooth restarts.</li> </ol>"}, {"location": "Pro-Web-UI-Commanding/#stopping", "title": "Stopping", "text": "<p>The stopping command is used to halt a consumer process entirely. This command should be used with caution as it stops all processing activities after <code>shutdown_timeout</code> is reached. Stopping is typically employed when a consumer needs to be taken offline for upgrades, troubleshooting, or when decommissioning is required. Once stopped, a consumer process will need to be manually restarted, and it will resume from the last committed offset in Kafka, ensuring no loss of data but requiring careful management to avoid processing delays or other operational impacts.</p> <p>Topics and Partitions Reassignment</p> <p>When a consumer is stopped, its assignments are redistributed among the remaining active consumers in the group, ensuring that message processing continues seamlessly without interruption.</p>"}, {"location": "Pro-Web-UI-Commanding/#use-cases_2", "title": "Use Cases", "text": "<ol> <li>Emergency Shutdown: Quickly shutting down a consumer that is causing severe problems, such as data corruption or excessive resource consumption, to prevent further damage to the system.</li> <li>System Overhaul: Stopping consumers completely to allow for major system upgrades or reconfigurations that require a complete halt of data processing activities.</li> <li>Decommissioning Nodes: Stopping consumers on specific nodes that are being decommissioned or replaced, ensuring that these nodes no longer participate in processing.</li> <li>Bug Fixes: Halting a consumer to apply critical bug fixes that cannot be addressed while the consumer is running, ensuring the integrity of the fix deployment.</li> <li>Resource Reallocation: Stopping a consumer to reallocate resources such as memory and CPU to other critical applications, especially in resource-constrained environments.</li> <li>Testing Failover: Stopping consumers to test the resilience and failover capabilities of the system, ensuring that other consumers or nodes can take over smoothly.</li> <li>Performance Benchmarking: Temporarily stopping consumers from performing clean-slate performance benchmarking without background noise from ongoing data processing.</li> </ol>"}, {"location": "Pro-Web-UI-Commanding/#partition-level-processing-control", "title": "Partition-Level Processing Control", "text": "<p>In addition to process-level control, Karafka offers granular control over individual partition processing. These features allow you to pause/resume processing and adjust offset positions for specific partitions without affecting the entire consumer process or need for any code changes.</p>"}, {"location": "Pro-Web-UI-Commanding/#access-points", "title": "Access Points", "text": "<p>Partition-level controls are accessible from two main locations:</p> <ol> <li>Health Overview: The Health dashboard displays all active consumer group partitions with action buttons for offset adjustment and pause control</li> <li>Consumer Subscriptions: Navigating to a specific consumer process shows its subscriptions with partition management options</li> </ol> <p> </p>"}, {"location": "Pro-Web-UI-Commanding/#pause-and-resume-partitions", "title": "Pause and Resume Partitions", "text": "<p>The partition pause feature allows you to temporarily stop message processing for a specific partition without stopping the entire consumer process.</p>"}, {"location": "Pro-Web-UI-Commanding/#pausing-a-partition", "title": "Pausing a Partition", "text": "<p>To pause a partition:</p> <ol> <li>Navigate to Health \u2192 Overview or to a specific consumer's subscriptions</li> <li>Locate the partition you want to pause</li> <li>Click the Pause button (pause icon)</li> <li>Enter pause configuration:</li> </ol> <ul> <li>Pause Duration: Enter time in seconds (0 for indefinite pause)</li> <li>Safety Check: Option to prevent override of existing pauses</li> </ul> <ol> <li>Click Set or Update Pause</li> </ol> <p> </p> <p>Running Consumer Process Operation</p> <p>Pause operations apply to actively running consumers. The operation:</p> <ul> <li>Takes effect during the next poll operation (after current message processing completes)</li> <li>May affect message processing</li> <li>Cannot be undone</li> <li>Will take effect only if the current process still owns the assignment</li> </ul> <p>Lag Reporting During Pauses</p> <p>During long pauses, lag reporting on paused topic partitions may stop as librdkafka freezes the last known high watermark. Real-time, state-independent lag metrics can always be checked in the Cluster Lags tab of the Health section.</p>"}, {"location": "Pro-Web-UI-Commanding/#resuming-a-partition", "title": "Resuming a Partition", "text": "<p>To resume a paused partition:</p> <ol> <li>Navigate to Health \u2192 Overview or to a specific consumer's subscriptions</li> <li>Locate the paused partition (indicated by \"paused\" status)</li> <li>Click the Resume button (play icon)</li> <li>In the Resume dialog, optionally select:</li> </ol> <ul> <li>Reset Counter: Reset the processing attempts counter when resuming</li> </ul> <ol> <li>Click Resume Processing</li> </ol> <p> </p> <p>Resuming a partition restores normal message processing operations. The resumption takes effect after the current processing cycle completes and before the next polling operation.</p>"}, {"location": "Pro-Web-UI-Commanding/#offset-management", "title": "Offset Management", "text": "<p>The offset management feature allows you to adjust the position from which a consumer reads messages within a partition.</p>"}, {"location": "Pro-Web-UI-Commanding/#editing-partition-offsets", "title": "Editing Partition Offsets", "text": "<p>To adjust a partition offset:</p> <ol> <li>Navigate to Health \u2192 Overview or to a specific consumer's subscriptions</li> <li>Locate the partition to modify</li> <li>Click the Edit Offset button (pencil icon)</li> <li>Configure offset adjustment:</li> </ol> <ul> <li>New Offset: Enter the desired offset position (limited by the partition's low and high watermarks)</li> <li>Prevent Overtaking: Option to only adjust if consumer hasn't moved beyond requested offset</li> <li>Resume Immediately: If partition is paused, resume processing immediately</li> </ul> <ol> <li>Click Adjust Offset</li> </ol> <p> </p> <p>Running Consumer Process Operation</p> <p>Offset adjustments apply to actively running consumers. The operation:</p> <ul> <li>Takes effect during the next poll operation (after current message processing completes)</li> <li>May affect message processing</li> <li>Cannot be undone</li> <li>Will take effect only if the current process still owns the assignment</li> </ul>"}, {"location": "Pro-Web-UI-Commanding/#use-cases_3", "title": "Use Cases", "text": ""}, {"location": "Pro-Web-UI-Commanding/#offset-management-use-cases", "title": "Offset Management Use Cases", "text": "<ul> <li>Error Recovery: Skip past problematic messages that cause repeated failures</li> <li>Replay Processing: Move backward to reprocess messages after fixing a bug</li> <li>Debugging: Examine specific messages by positioning the consumer at exact offsets</li> <li>Testing: Validate message handling by processing specific message ranges</li> <li>Catch-up: Skip ahead to reduce processing lag when historical data isn't required</li> </ul>"}, {"location": "Pro-Web-UI-Commanding/#pauseresume-use-cases", "title": "Pause/Resume Use Cases", "text": "<ul> <li>Maintenance Windows: Pause specific partitions during maintenance periods</li> <li>Resource Management: Temporarily halt non-critical partitions under high load</li> <li>Rate Limiting: Implement basic rate limiting by pausing and resuming processing</li> <li>Troubleshooting: Isolate problems by pausing specific partitions for investigation</li> <li>Coordinated Deployments: Pause processing before deployments to prevent message loss</li> </ul>"}, {"location": "Pro-Web-UI-Commanding/#limitations-and-considerations", "title": "Limitations and Considerations", "text": "<ul> <li>Processing Cycle: All operations take effect after the current processing cycle completes and before the next polling operation</li> <li>Assignment Ownership: Commands only affect partitions if the targeted consumer still owns the assignment</li> <li>Pause Visibility: Lag metrics for paused partitions may become stale as high watermark updates freeze</li> <li>Offset Boundaries: Offset adjustments must be within the partition's low and high watermarks</li> <li>Consumer Group Coordination: Changes to one consumer may trigger rebalancing across the consumer group</li> <li>Command Persistence: Commands are sent through Kafka and will persist even through consumer restarts</li> <li>Process vs. Partition: These features control individual partitions, not entire consumer processes</li> </ul>"}, {"location": "Pro-Web-UI-Commanding/#connection-management", "title": "Connection Management", "text": "<p>The commanding feature in Karafka Pro utilizes the Pro Iterator to establish a pub-sub-like connection for managing consumer processes. This connection is distinct from standard subscriptions and is not visible in the Web UI. This design choice helps prevent unnecessary noise in the UI and ensures that the connection remains responsive as long as the entire Ruby process is operational.</p> <p>Unlike standard data flows, this special connection is built to avoid saturation and flow any potential instabilities, where messages pass from listeners through queues to consumers. Standard flows could be overwhelmed during critical moments, significantly reducing responsiveness when needed most. By bypassing the typical data flow path, the commanding feature maintains a high level of responsiveness, even under heavy system load.</p> <p>This dedicated subscription, while not \"mission-critical\", is designed to be reliable, incorporating recovery procedures and automatic reconnections. It does not publish statistics or other metrics, focusing on efficient management and swift responses to administrative commands. This approach, in turn, ensures robust and continuous operation, maintaining system stability and operational efficiency.</p> <p>Key points include:</p> <ul> <li>Invisible Connection: The pub-sub connection used by the commanding feature is not shown in the Web UI, avoiding unnecessary noise.</li> <li>Responsiveness: Ensures high responsiveness by bypassing the standard data flow, crucial during debugging.</li> <li>Reliability: Incorporates recovery and reconnection mechanisms for continuous operation.</li> <li>Error Reporting: Publishes errors if they occur but does not track or publish statistics or other metrics.</li> </ul>"}, {"location": "Pro-Web-UI-Commanding/#summary", "title": "Summary", "text": "<p>Karafka Pro's consumer control capabilities are essential for any organization looking to leverage Kafka for real-time data processing and streaming. They provide the necessary controls to manage consumer behavior effectively at both process and partition levels, ensuring that Kafka clusters are performant and resilient under various operating conditions.</p> <p>The combination of process-level commands (trace, quiet, stop) and partition-level controls (pause, resume, offset adjustment) provides a comprehensive toolkit for administrators to implement precise control strategies and resolve issues with minimal disruption to the overall message processing workflow.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Web-UI-Explorer/", "title": "Pro Web UI Explorer", "text": "<p>Karafka Data Explorer is an essential tool for users seeking to navigate and comprehend the data produced to Kafka. Offering an intuitive interface and a deep understanding of the routing table, the explorer ensures that users can access deserialized data effortlessly for seamless viewing.</p> <p>Below you can find the primary features of the Karafka Data Explorer.</p> <p>Large Payloads and Performance Considerations</p> <p>Explorer is not suited for viewing and managing messages with large payloads (e.g., tens of MB in size). This is because it downloads all payloads for each page load, leading to significant network and memory usage. While it does not deserialize the data, the sheer volume of data transferred can cause performance bottlenecks.</p>"}, {"location": "Pro-Web-UI-Explorer/#topics-list-view", "title": "Topics List View", "text": "<p>Before diving deep into the data, having a bird's eye view of all available topics is essential. This feature offers a list of all the topics, allowing users to select and focus on specific ones. It serves as a starting point for data exploration, giving a clear overview of the topics landscape.</p> <p></p>"}, {"location": "Pro-Web-UI-Explorer/#per-topic-view", "title": "Per Topic View", "text": "<p>This feature provides an overview of the most recent data across all partitions for a specific topic, ensuring you don't miss out on the latest insights.</p>"}, {"location": "Pro-Web-UI-Explorer/#limitations", "title": "Limitations", "text": "<ol> <li> <p>Data Merging from Multiple Partitions: When using the Per Topic View, the system merges data from several partitions. However, there's a limitation. If the number of partitions exceeds the number of elements allowed to be displayed on a single page, not all data might be visible immediately. For example, if there are 30 partitions but the page only shows data from 25, you would only see the combined data from those 25 partitions on the first page. The remaining data from the other partitions would be visible on the subsequent pages (Page 2, Page 3, and so on).</p> </li> <li> <p>Displaying Sparse Data from Many Partitions: Another limitation might arise in situations where topics have a small number of messages. If a topic has many partitions (say more than 25) but only a few messages, not all may be visible in the Per Topic View. This is due to the merging process, where data from many partitions might overshadow those few messages.</p> </li> <li> <p>Low Watermark Offset Viewing: It's worth noting that the limitations mentioned mainly apply when observing data close to the low watermark offset, typically when there's not much data present (less than 100 messages). In such cases, the view might provide a partial picture due to the abovementioned constraints.</p> </li> </ol> <p>However, in most other scenarios, where there's a substantial amount of data, the Per Topic View should function seamlessly, providing a holistic and accurate representation of the most recent data across all partitions.</p>"}, {"location": "Pro-Web-UI-Explorer/#per-partition-view-with-offset-based-pagination", "title": "Per Partition View with Offset-Based Pagination", "text": "<p>Dive deeper into each partition and scroll through the data using offset-based pagination. This offers a granular view, ensuring detailed exploration of data within partitions.</p>"}, {"location": "Pro-Web-UI-Explorer/#time-based-offset-lookups", "title": "Time-Based Offset Lookups", "text": "<p>Ever wanted to see a message generated at a particular moment? This feature facilitates precisely that. You can navigate to messages produced at specific times by performing time-based offset lookups. This functionality is particularly advantageous for debugging, allowing for precise tracking and resolution.</p>"}, {"location": "Pro-Web-UI-Explorer/#timestamp-based-offset-lookups", "title": "Timestamp-Based Offset Lookups", "text": "<p>Need to investigate messages from a specific moment in time? Karafka Web UI provides Kafka messages timestamp-based offset lookups that enable you to jump directly to messages produced at exact Unix timestamps. This feature is invaluable for incident analysis, allowing you to correlate message production with system events or errors. Enter the timestamp, and the Explorer will navigate to the corresponding offset, making historical data exploration efficient and precise.</p> <p></p>"}, {"location": "Pro-Web-UI-Explorer/#real-time-display-of-the-recent-message", "title": "Real-Time Display of the Recent Message", "text": "<p>Stay updated with the latest information. The explorer can display the most recent message from a given topic partition in real time and even supports an auto-refresh feature, ensuring you always have the current data at your fingertips.</p>"}, {"location": "Pro-Web-UI-Explorer/#detailed-message-view", "title": "Detailed Message View", "text": "<p>Every message is more than just its content. With the Karafka Data Explorer, you can access the complete details of any message. This includes its payload, headers, and associated metadata, ensuring a comprehensive understanding of the data.</p> <p></p>"}, {"location": "Pro-Web-UI-Explorer/#message-republishing", "title": "Message Republishing", "text": "<p>Occasionally, there might be a need to republish a message for various reasons. This feature empowers users to seamlessly republish any message to the same topic partition. It retains the original payload and all the headers, ensuring data consistency and integrity during republishing.</p>"}, {"location": "Pro-Web-UI-Explorer/#surroundings-lookup", "title": "Surroundings Lookup", "text": "<p>\"Surroundings Lookup\" enhances Karafka Web UI's debugging capabilities by allowing users to navigate directly to a specific message and instantly view its preceding and subsequent messages. This is essential for understanding the context, especially during batch processing.</p>"}, {"location": "Pro-Web-UI-Explorer/#summary", "title": "Summary", "text": "<p>The Karafka Data Explorer is your go-to solution for an in-depth exploration of data produced to Kafka. It's not just about viewing the data; it's about understanding it.</p> <p>Last modified: 2025-05-01 16:51:26</p>"}, {"location": "Pro-Web-UI-Health/", "title": "Pro Web UI Health", "text": "<p>The health views of the Web UI display the current status of all the running Karafka instances aggregated on a per-consumer-group basis. Those views allow users to monitor the health of their messages consumption and troubleshoot any issues that may arise including issues related to hanging transactions (LSO issues). It also allows quick identification of performance bottlenecks and can help with capacity planning.</p> <p></p>"}, {"location": "Pro-Web-UI-Health/#lso-freezes-awareness", "title": "LSO Freezes Awareness", "text": "<p>In the world of Kafka, the Last Stable Offset (LSO) is pivotal in ensuring message integrity and order, especially for idempotent producers. However, at times, the LSO may hang, affecting the consumption of messages and potentially bringing to a standstill any consumers operating at a <code>read_committed</code> isolation level. This documentation will shed light on the concept, the problems it may cause, and how the Karafka Web UI can be a lifesaver during such situations.</p>"}, {"location": "Pro-Web-UI-Health/#understanding-the-lso-last-stable-offset", "title": "Understanding the LSO - Last Stable Offset", "text": "<p>The Last Stable Offset (LSO) is a checkpoint marking the last point at which records were successfully committed. It is a significant reference point because any records beyond this point are not considered stable and may not be safely consumed by clients requiring transactional consistency.</p>"}, {"location": "Pro-Web-UI-Health/#the-risk-of-lso-freezes", "title": "The Risk of LSO Freezes", "text": "<p>If the LSO hangs or is stuck, it signifies that new records have yet to be committed beyond this point. When such a scenario happens, all consumers with a <code>read_committed</code> isolation level will be unable to proceed. Essentially, they will have to wait until the LSO issue is resolved, which can be a significant challenge for real-time data processing systems.</p>"}, {"location": "Pro-Web-UI-Health/#a-beacon-in-lso-freezes", "title": "A Beacon in LSO Freezes", "text": "<p>The Karafka Web UI is equipped with robust health views that swiftly identify cases where consumers cannot progress due to a stuck LSO.</p> <p>Karafka's Web UI has visual cues to indicate potential problems concerning the LSO:</p> <ol> <li>At Risk (Yellow Highlight)<ul> <li>Scenario: Consumption is at risk but is still moving forward. This happens when there is still data before reaching the LSO, so the consumer is progressing.</li> <li>Web UI Indication: The partition will be highlighted in yellow.</li> <li>LSO State: \"At risk\"</li> </ul> </li> </ol> <p></p> <ol> <li>Stopped (Red Highlight)<ul> <li>Scenario: Consumption is halted and cannot move forward. This situation arises when more data is available on the topic, but it lies beyond the LSO, and the consumer has already reached it.</li> <li>Web UI Indication: The partition will be highlighted in red, emphasizing that it is stopped.</li> <li>LSO State: \"Stopped\".</li> </ul> </li> </ol> <p></p> <p>These visual indicators allow immediate awareness of potential problems, ensuring quick identification and action.</p>"}, {"location": "Pro-Web-UI-Health/#conclusion", "title": "Conclusion", "text": "<p>Awareness of LSO freezes, and its implications is vital for any Kafka-based system. The Karafka Web UI provides a proactive approach to detect and visually indicate such issues, ensuring administrators and users can take quick corrective actions. Regularly monitoring the health view and being aware of the LSO states can be crucial for the seamless functioning of your Kafka-based data processing system.</p>"}, {"location": "Pro-Web-UI-Health/#cluster-lags", "title": "Cluster Lags", "text": "<p>Unlike many other Web UI features centered around insights from consumer processes and producers, the Cluster Lags functionality offers a direct view into Kafka's perception of lag. This is critical for comprehensive monitoring and management.</p> <p>In Kafka terms, lag refers to the difference between the last message produced to a topic and the last message consumed. These metrics become invaluable in scenarios where consumer processes are not active or experiencing issues. The Cluster Lags are derived from the lag information stored directly in Kafka, providing an independent and accurate measure of message processing delays.</p> <p>This feature is handy in environments where:</p> <ul> <li>Consumer processes are temporarily down or inactive.</li> <li>Consumers are misbehaving or not processing messages as expected.</li> </ul> <p>Focusing on the lag data directly from Kafka lets you gain insights into system performance and potential bottlenecks without relying solely on consumer process metrics.</p> <p></p> <p>Last modified: 2024-08-02 10:38:59</p>"}, {"location": "Pro-Web-UI-Policies/", "title": "Pro Web UI Policies", "text": "<p>Karafka's Web UI includes a comprehensive policies engine that provides granular control over user actions across all UI components.</p> <p>This engine allows administrators to define and enforce policies on what specific users can view and do within the Web UI, ensuring compliance with data protection and privacy standards.</p> <p>Data sanitization and filtering are integral to this engine, enabling the sanitization or exclusion of sensitive portions of payloads to prevent accidental exposure of sensitive information. When encryption is enabled, no data is displayed by default as a safeguard. However, partial sanitization can be applied to display non-sensitive parts of the payload, ensuring that only secure information is presented while sensitive elements remain protected.</p>"}, {"location": "Pro-Web-UI-Policies/#usage", "title": "Usage", "text": "<p>Karafka's Web UI was designed to keep data privacy and security at its core. Ensuring selective visibility becomes paramount as we navigate the vast expanse of information stored in Kafka topics. Karafka achieves this via a three-tiered approach:</p> <ul> <li> <p>Requests Policies: The first level of control involves the ability to open particular pages within the Web UI. Configured via <code>ui.policies.requests</code>, a per-request policy engine can be used to both track and control access to specific URLs. This ensures that only authorized users can access certain parts of the interface, thereby providing a foundational layer of security and access management.</p> </li> <li> <p>Messages Policies: At its basic level, the decision to display or mask fundamental components of a message is made. By using the <code>ui.policies.messages</code> setting, users can dictate whether they want the entire payload, all headers, and the key (if provided) to be visible or hidden. This form of filtering provides an overarching control, allowing users, for instance, to completely obscure the payload while continuing to show headers and message key.</p> </li> <li> <p>Partial Payload Sanitization: For those seeking a more nuanced approach, Karafka's partial payload sanitization is the answer. This method enables granular control over the data's visibility. Instead of blanketing an entire message, it allows specific attributes within a deserialized message, such as an address or other sensitive information, to be masked. While ensuring a higher level of data security, this process necessitates additional effort and precision in its implementation.</p> </li> </ul> <p>In essence, Karafka offers both a broad-stroke and a fine-tuned approach to data visibility, ensuring that while essential information remains accessible, sensitive data is securely tucked away.</p>"}, {"location": "Pro-Web-UI-Policies/#requests-policies", "title": "Requests Policies", "text": "<p>The Requests Policies feature in Karafka's Web UI provides a mechanism for controlling access to specific pages and functionalities within the Web UI on a per-request basis. Configured via <code>ui.policies.requests</code>, this policy engine allows the definition and enforcement of rules that determine which users can access particular URLs, ensuring a foundational layer of security and access management.</p> <p>To utilize the Requests Policies, you must create a custom policy class that defines the logic for allowing or denying access to specific requests. This custom policy must implement the <code>allow?</code> method, which evaluates the request details and returns a boolean indicating whether the request should be permitted.</p> <p>Below is an example of how to define and configure a custom Requests Policy:</p> <pre><code>class MyCustomRequestsPolicy\n  # @param env [Hash] rack env object that we can use to get request details\n  # @return [Boolean] should this request be allowed or not\n  def allow?(env)\n    # Example logic: Allow access only if the user is an admin\n    user = env['rack.session'][:user]\n    user &amp;&amp; user.admin?\n  end\nend\n</code></pre> <p>Once your policy is ready, you need to replace the default one in the configuration as follows:</p> <pre><code>Karafka::Web.setup do |config|\n  config.ui.policies.requests = MyCustomRequestsPolicy.new\nend\n</code></pre>"}, {"location": "Pro-Web-UI-Policies/#messages-policies", "title": "Messages Policies", "text": "<p>Two steps are needed to use your custom messages policies:</p> <ol> <li>A custom messages policy needs to be created</li> <li>The defined messages policy must replace the default one via the reconfiguration.</li> </ol> <p>Each messages policy requires six methods to be present:</p> <ol> <li><code>#key?</code> - should the message key be presented</li> <li><code>#headers?</code> - should the headers be visible</li> <li><code>#payload?</code> - should the payload be visible</li> <li><code>#download?</code> - should it be allowed to download this message raw payload</li> <li><code>#export?</code> - should it be allowed to download the deserialized and sanitized payload as JSON</li> <li><code>#republish?</code> - should it be allowed to republish the message back to Kafka</li> </ol> <p>Each method receives a message (of type <code>::Karafka::Messages::Message</code>) as a parameter and returns a boolean indicating whether the corresponding part of the message (key, headers, or payload) should be visible.</p> <p>Below, you can see an example of a custom messages policy that hides all the information:</p> <pre><code>class MyCustomMessagesPolicy\n  def key?(_message)\n    false\n  end\n\n  def headers?(_message)\n    false\n  end\n\n  def payload?(_message)\n    false\n  end\n\n  def download?(message)\n    false\n  end\n\n  def export?(message)\n    false\n  end\n\n  def republish?(message)\n    false\n  end\nend\n</code></pre> <p>Once your policy is ready, you need to replace the default one in the configuration as follows:</p> <pre><code>Karafka::Web.setup do |config|\n  config.ui.policies.messages = MyCustomMessagesPolicy.new\nend\n</code></pre>"}, {"location": "Pro-Web-UI-Policies/#partial-payload-sanitization", "title": "Partial Payload Sanitization", "text": "<p>To filter or sanitize part of the data to be presented in the Karafka Web-UI, it is necessary to accomplish three key things:</p> <ol> <li> <p>Wrapping Deserializers with a Sanitizer Layer: The deserializers, which are responsible for converting the raw Kafka payloads into a format your application understands, need to be wrapped with a sanitizer layer. However, this sanitization should only occur in the context of the Web server. In other words, the raw data is being transformed twice: first, when it's deserialized, and again when the sanitizer filters out sensitive information before it is displayed on the Web UI.</p> </li> <li> <p>Context-Aware Wrapper: The wrapper used to sanitize the data should be able to understand its operating context. It should be aware of whether it is operating in a Web server context (in which case it should sanitize the data) or in a Karafka server context (in which case it should leave the data untouched). This ensures that sensitive information is only filtered when data is being presented on the Web UI and not during backend processing or other non-UI-related tasks.</p> </li> <li> <p>Routing Wrapper Injection: The final step for sanitizing data displayed in the Karafka Web UI is Wrapper Routing Injection, where the sanitizing wrapper is incorporated into the Karafka routing. This ensures the data is filtered for sensitive content after deserialization but before being displayed on the UI.</p> </li> </ol> <p>It is crucial to ensure that the deserializer wrappers are only used in the context of a Web server displaying the Web UI. The reason for this is that Karafka may otherwise accidentally use sanitized data when it is performing business logic operations. This could lead to unintended side effects, such as inaccurate data processing or potentially even data loss. The sanitization process is specifically intended to prevent sensitive data from being displayed on the Web UI. It is not meant to impact the data used by the backend system for processing or decision-making tasks.</p> <p>Remember that the sanitization process should be implemented carefully to ensure that it doesn't interfere with the regular operation of your Karafka application. Always test your sanitization process thoroughly to ensure it behaves as expected and does not inadvertently impact your application's functionality.</p> <p>Below you can find an example implementation of a wrapper that removes the replaces the <code>:address</code> key from the deserializers hash with a <code>[FILTERED]</code> string.</p> <pre><code># Define your sanitizer that will wrap the payload deserializer\nclass AddressSanitizer\n  def initialize(deserializer)\n    @deserializer = deserializer\n  end\n\n  def call(message)\n    payload = @deserializer.call(message)\n\n    # You need to set it yourself, it is NOT set by Karafka\n    # return full payload unless we're in Puma (indicating Web-UI)\n    return payload unless ENV.key?('PUMA')\n\n    # Replace the address field with indicator, that it was filtered\n    payload[:address] = '[FILTERED]' if payload.key?(:address)\n\n    # Return the result payload\n    payload\n  end\nend\n\n# And mount it inside the karafka.rb routing\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :orders do\n      consumer ExampleConsumer\n      # Make sure, that the OrdersDeserializer is wrapped with an address sanitizer\n      # so the address is not visible in the Web-UI\n      deserializers(\n        payload: AddressSanitizer.new(OrdersDeserializer.new)\n      )\n    end\n  end\nend\n</code></pre> <p>Below you can find an example of the effect of the usage of a similar sanitizer that removes the <code>visitor_id</code> from the displayed data:</p> <p> </p>"}, {"location": "Pro-Web-UI-Policies/#example-use-cases", "title": "Example Use Cases", "text": "<p>The filtering and sanitization feature can be handy in various scenarios, such as:</p> <ul> <li> <p>Privacy Compliance: If your application processes personal data covered by regulations such as GDPR or CCPA, you can use filtering to prevent this data from being displayed, thus helping you stay compliant with data privacy laws.</p> </li> <li> <p>Secure Debugging: During debugging, developers may need to inspect data flows without being exposed to sensitive information. In this case, filtering can allow them to see necessary data while hiding sensitive details.</p> </li> <li> <p>Customer Support: In a customer support scenario, agents might need access to specific non-sensitive data to help diagnose or resolve issues. Filtering can show only the data required to address the customer's concern without exposing sensitive customer information.</p> </li> <li> <p>Audit and Compliance: In industries like finance or healthcare, compliance officers or auditors may need to inspect data flow while ensuring sensitive data like financial transactions or patient health data remains secure. Filtering can help present the necessary information while maintaining data security and regulatory compliance.</p> </li> <li> <p>Data Analysis: For data analysis or machine learning purposes, often raw data is used that may contain sensitive elements. A data analyst can utilize the filtering feature to see the data they need while still preserving the privacy of sensitive information.</p> </li> </ul>"}, {"location": "Pro-Web-UI-Policies/#summary", "title": "Summary", "text": "<p>This ability to filter and sanitize data provides a powerful tool to ensure data privacy and security while still giving the necessary visibility into the data flow within your Kafka topics.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Web-UI-Search/", "title": "Pro Web UI Search", "text": "<p>The Search feature is a tool that enables users to search and filter messages efficiently. This feature allows users to search within one or multiple partitions, start from a specific time or offset, apply custom matchers to payloads, keys, or headers, and use custom deserializers for data.</p>"}, {"location": "Pro-Web-UI-Search/#usage", "title": "Usage", "text": "<p>To access the search functionality in Web UI, you need to navigate to the Explorer and look for the loop icon. This icon is available when browsing through the data of a particular topic or partition. Clicking on the loop icon opens the search modal, allowing you to configure your search parameters and perform a detailed search.</p> <p>The search modal includes several fields and options to refine your search:</p> <ul> <li> <p>Matcher: Select the type of matcher you want to use for your search. Matchers define the criteria for searching within messages, such as searching within payloads, keys, or headers.</p> </li> <li> <p>Phrase: Enter the phrase you want to search for within the selected messages. This is the string that the search will look for according to the matcher criteria.</p> </li> <li> <p>Partitions: Choose which partitions to include in the search. You can select all partitions or specify individual ones. This allows you to narrow down your search to specific parts of your Kafka topic.</p> </li> <li> <p>Offset: Define the starting point for the search. You have three options:</p> <ul> <li>Latest: Start searching from the most recent messages.</li> <li>Offset: Specify an exact offset to start from.</li> <li>Timestamp: Provide a timestamp (in milliseconds) to start the search from a specific point in time.</li> </ul> </li> <li> <p>Messages: Select the limit on the number of messages to scan. The available options ensure the search operation remains efficient and does not overload the system.</p> </li> </ul> <p>Once you have configured your search parameters, click the \"Search\" button to initiate the search. The search results and detailed metadata will be displayed, helping you analyze and understand the data based on your specified criteria.</p> <p></p>"}, {"location": "Pro-Web-UI-Search/#reconfiguration", "title": "Reconfiguration", "text": "<p>Search functionality can be reconfigured to better fit your specific needs and requirements. This is particularly useful if you need to adjust the search parameters to optimize performance or customize the behavior of the search operation.</p> <p>Below are the key settings you can modify.</p>"}, {"location": "Pro-Web-UI-Search/#matchers", "title": "Matchers", "text": "<p>The <code>matchers</code> determine the criteria used for searching within messages. By default, the following matchers are available:</p> <ul> <li> <p><code>Matchers::RawPayloadIncludes</code>: This matcher searches for the specified phrase within the raw payload of the message. It is case-sensitive and ignores encoding issues.</p> </li> <li> <p><code>Matchers::RawKeyIncludes</code>: This matcher checks if the specified phrase exists within the raw key of the message. It is also case-sensitive and ignores encoding issues.</p> </li> <li> <p><code>Matchers::RawHeaderIncludes</code>: This matcher searches the message's raw headers. It is case-sensitive and checks if any header key or value contains the specified phrase.</p> </li> </ul> <p>You can customize this list to include your own matchers or remove existing ones.</p> <pre><code># Use only raw payload and a custom matcher\nKarafka::Web.setup do |config|\n  config.ui.search.matchers = [\n    MyCustomMatcher,\n    Matchers::RawPayloadIncludes\n  ]\nend\n</code></pre>"}, {"location": "Pro-Web-UI-Search/#timeout", "title": "Timeout", "text": "<p>The <code>timeout</code> setting defines the maximum duration (in milliseconds) a search operation can run before it is automatically stopped. This prevents long-running searches that could hang the browser or degrade performance.</p> <pre><code>Karafka::Web.setup do |config|\n  config.ui.search.timeout = 60_000 # 60 seconds\nend\n</code></pre>"}, {"location": "Pro-Web-UI-Search/#limits", "title": "Limits", "text": "<p>The <code>limits</code> setting specifies the maximum number of messages that can be scanned in a search operation. You can adjust these limits based on your data size and performance considerations. The default values are <code>1,000</code>, <code>10,000</code>, and <code>100,000</code>.</p> <pre><code>Karafka::Web.setup do |config|\n  config.ui.search.limits = [\n    500,\n    5_000,\n    50_000,\n    200_000\n  ]\nend\n</code></pre>"}, {"location": "Pro-Web-UI-Search/#limitations", "title": "Limitations", "text": "<p>Below, you can find a detailed list of search capabilities' limitations from an end-user perspective. Understanding these limitations is important for effectively using the search functionality and managing expectations regarding its performance and scope.</p> <p>Possibility Of Reconfiguration</p> <p>Certain limitations of the search capabilities can be changed by reconfiguring the search defaults during the Web UI configuration. This includes adjusting the search timeout, modifying the limits on the number of messages, and customizing matchers to better fit specific use cases.</p> <ol> <li> <p>Search Timeout: The search operation has a maximum duration (timeout) of 30,000 milliseconds (30 seconds). If a search takes longer, it will be stopped automatically, potentially leaving some messages unchecked.</p> </li> <li> <p>Limit on Number of Messages: The search can only handle a certain number of messages (limits). This constraint ensures performance but may not be sufficient for huge data sets.</p> </li> <li> <p>Case Sensitivity: The default search matchers (<code>RawPayloadIncludes</code>, <code>RawKeyIncludes</code>, <code>RawHeaderIncludes</code>) are case-sensitive, which might not meet the needs of users who require case-insensitive searches.</p> </li> <li> <p>Partition Scanning: The search distributes the scanning limit evenly across partitions. If you specify a limit of 100,000 messages for a topic with five partitions, each partition will scan up to 20,000 messages. If messages are unevenly distributed, this might result in some partitions not being fully searched.</p> </li> <li> <p>Real-Time Message Influx Handling: When searching from the latest messages, if new messages continue to come in, the search stops at the time it started to avoid endless searching. This can prevent capturing the most recent messages.</p> </li> <li> <p>Matcher Limitations: Matchers must be predefined and active. Users cannot create ad-hoc matchers during the search operation.</p> </li> <li> <p>Complex Data Types and Encodings: The search matchers ignore encoding issues. This could be problematic when dealing with complex data types or non-standard encodings.</p> </li> </ol> <p>These limitations highlight the trade-offs made to balance performance, simplicity, and functionality within the Karafka Web UI search capabilities.</p>"}, {"location": "Pro-Web-UI-Search/#metadata-details", "title": "Metadata Details", "text": "<p>The Search Metadata Details section provides detailed insights into the performance and outcomes of your search query. This type of information may be crucial for understanding the efficiency and effectiveness of your search and can help diagnose potential issues.</p> <p>When you initiate a search, the results page contains a hidden detailed summary and partition-specific information, offering valuable insights into the search process. You can display it by clicking the stats icon above the search results.</p> <p></p> <p>The summary table at the top of the Search Metadata Details section highlights three key metrics:</p> <ol> <li> <p>Total Messages Checked: This figure represents the total number of messages that the search operation scanned. Understanding how many messages were checked can help you gauge the scope and thoroughness of the search. For instance, if you specified a limit for the search, this metric will show how close the operation came to reaching that limit.</p> </li> <li> <p>Matches: This count shows the number of messages that matched the search criteria. By looking at the number of matches, you can quickly determine how relevant their search query was. A higher number of matches indicates that the search criteria were effective, while fewer matches might suggest that the criteria need adjustment.</p> </li> <li> <p>Search Time: The duration of the search operation is also displayed and measured in seconds. This metric is crucial for evaluating the performance of the search. If the search time is too long, you might consider refining their search criteria or limiting the partitions you search in to improve efficiency.</p> </li> </ol> <p>Beneath the summary table, the partition details table provides more granular insights into how the search performed across different partitions:</p> <ol> <li> <p>Partition: Each partition involved in the search is listed here. This helps users see which parts of their data were included in the search operation.</p> </li> <li> <p>Messages Checked: This number indicates how many messages were scanned for each partition. This detail helps users understand how the search workload was distributed across the partitions. If some partitions have significantly fewer messages checked, it might be due to the specified search limits or partition-specific data distribution.</p> </li> <li> <p>Offsets: This column shows the range of message offsets checked in each partition, from the first to the last message. The offsets provide a sense of where in the data stream the search operation focused its efforts.</p> </li> <li> <p>Lookup Range: The time range for the messages checked in each partition is displayed here. This indicates the temporal scope of the search, showing how far back the search looked in terms of message timestamps. This is useful for understanding whether the search covered recent data, historical data, or a mix of both.</p> </li> <li> <p>Matches: This column displays the number of matches found in each partition. Seeing the distribution of matches across partitions can help users identify which partitions contain the most relevant data according to the search criteria.</p> </li> </ol> <p>The Search Metadata Details are highly useful for several reasons:</p> <ol> <li> <p>Performance Monitoring: By reviewing the search time and the number of messages checked, users can gauge the performance of their search queries and make adjustments to improve efficiency.</p> </li> <li> <p>Debugging and Optimization: Detailed partition statistics can help identify any partitions requiring further investigation or optimization.</p> </li> <li> <p>Relevance Assessment: By examining the number of matches, you can quickly assess the relevance of their search criteria and refine them if necessary.</p> </li> <li> <p>Temporal Analysis: The lookup range provides insights into the temporal scope of the search, which helps analyze data trends over time.</p> </li> </ol>"}, {"location": "Pro-Web-UI-Search/#custom-matchers", "title": "Custom Matchers", "text": "<p>Creating custom matchers allows you to tailor the search functionality to meet your needs. Matchers define how messages are searched and can be customized to handle different data types or search criteria. Additionally, matchers can be configured to be available only for the topics that are relevant to them, ensuring that they are applied appropriately and efficiently.</p>"}, {"location": "Pro-Web-UI-Search/#why-create-custom-matchers", "title": "Why Create Custom Matchers?", "text": "<ul> <li> <p>Specific Search Requirements: Custom matchers enable you to implement search logic that fits unique requirements, such as searching within nested JSON fields or applying custom deserialization logic.</p> </li> <li> <p>Enhanced Search Precision: By creating matchers that understand your specific data structure, you can improve the precision and relevance of search results.</p> </li> <li> <p>Flexibility: Custom matchers provide the flexibility to extend the default search capabilities of Karafka Web UI, making it adaptable to a wide range of use cases.</p> </li> <li> <p>Security: Custom matchers allow you to limit or expand search capabilities based on specific topics. This ensures that sensitive data is only accessible through appropriate matchers, enhancing overall security by restricting search operations to relevant topics.</p> </li> </ul>"}, {"location": "Pro-Web-UI-Search/#how-to-build-custom-matchers", "title": "How to Build Custom Matchers", "text": "<p>To create a custom matcher, you need to define a class that inherits from the <code>Karafka::Web::Pro::Ui::Lib::Search::Matchers::Base</code> class provided by Karafka. Your custom matcher must implement the call method, which takes a phrase and a message as arguments and returns a boolean indicating whether the phrase is found in the message.</p> <p>Here's an example of a custom matcher that searches within a specific JSON field:</p> <pre><code>class JsonFieldIncludes &lt; Karafka::Web::Pro::Ui::Lib::Search::Matchers::Base\n  def call(phrase, message)\n    # Referencing `#payload` will deserialize it using the routing deserializer\n    json_payload = message.payload\n    json_payload['specific_field'].to_s.include?(phrase)\n  rescue Encoding::CompatibilityError\n    false\n  end\n\n  class &lt;&lt; self\n    def name\n      'JSON Field Includes'\n    end\n\n    # Make it work for all the topics\n    def active?(_topic_name)\n      true\n    end\n  end\nend\n</code></pre> <p>Once you have defined your custom matcher, you must configure Karafka Web UI to use it. Add your custom matcher to the list of matchers in the configuration:</p> <pre><code>Karafka::Web.setup do |config|\n  config.ui.search.matchers = [\n    JsonFieldIncludes,\n    Karafka::Web::Pro::Ui::Lib::Search::Matchers::RawPayloadIncludes,\n    Karafka::Web::Pro::Ui::Lib::Search::Matchers::RawKeyIncludes,\n    Karafka::Web::Pro::Ui::Lib::Search::Matchers::RawHeaderIncludes\n  ]\nend\n</code></pre> <p></p> <p>The <code>.active?</code> method enables or disables matchers for specific topics. This can be useful if certain matchers are only relevant to particular types of data or topics. By default, matchers are always active.</p> <p>Here\u2019s an example of how to implement the <code>.active?</code> method to activate a matcher conditionally:</p> <pre><code>class ConditionalMatcher &lt; Karafka::Web::Pro::Ui::Lib::Search::Matchers::Base\n  def call(phrase, message)\n    # Custom search logic\n    message.raw_payload.include?(phrase)\n  rescue Encoding::CompatibilityError\n    false\n  end\n\n  class &lt;&lt; self\n    def name\n      'Conditional Matcher'\n    end\n\n    def active?(topic_name)\n      # Only activate this matcher for a specific topic\n      topic_name == 'important_topic'\n    end\n  end\nend\n</code></pre>"}, {"location": "Pro-Web-UI-Search/#important-considerations", "title": "Important Considerations", "text": "<ul> <li> <p>Performance: If your custom matcher involves deserialization or complex processing, be mindful of the impact of the performance. Deserialization can be resource-intensive, so ensure your matcher is optimized for performance.</p> </li> <li> <p>Error Handling: Ensure any errors within your custom matcher are properly handled. Unhandled exceptions will bubble up and cause a 500 error in the Web UI. To prevent this, it\u2019s crucial to catch and manage potential errors within the matcher.</p> </li> <li> <p>Network Traffic: Searches retrieve all messages within the specified range, which can increase network traffic, especially with Kafka vendors that charge based on usage. If live polling is enabled, it will re-trigger searches every 5 seconds by default, further increasing data transfer and potentially raising costs. Consider disabling or adjusting live-polling intervals to manage these costs effectively if you use a Kafka vendor that charges for the network traffic.</p> </li> </ul>"}, {"location": "Pro-Web-UI-Search/#summary", "title": "Summary", "text": "<p>By creating and configuring custom matchers, you can extend the functionality of Karafka Web UI to suit your needs better, providing more precise and relevant search capabilities. The <code>.active?</code> method allows you to conditionally activate matchers, ensuring that only the necessary matchers are applied to each topic, optimizing performance and relevance.</p> <p>Last modified: 2024-08-23 19:16:33</p>"}, {"location": "Pro-Web-UI-Topics-Insights/", "title": "Pro Web UI Topics Insights", "text": "<p>The \"Topics Insights\" feature in Karafka Pro Web UI is a comprehensive suite designed to provide users with detailed information and analytics about their Kafka topics. This feature is crucial for developers who must ensure optimal configuration and performance of their Kafka topics.</p>"}, {"location": "Pro-Web-UI-Topics-Insights/#configuration-explorer", "title": "Configuration Explorer", "text": "<p>The first tab under Topics Insights is the Configuration Explorer, where users can delve into the specific settings of each topic. This view helps users verify that the topic configurations are aligned with their operational requirements and best practices. The configuration attributes visible here are as follows:</p> <ul> <li>Name: Displays the name of the Kafka configuration parameter.</li> <li>Value: Shows the current parameter setting.</li> <li>Default: Informs if the setting is the cluster default or if it was changed.</li> <li>Sensitive: Indicates whether the parameter holds sensitive information (e.g., passwords).</li> <li>Read only: Shows whether the parameter is modifiable or read-only.</li> </ul> <p>This detailed breakdown helps quickly understand how each topic is configured, making it easier to manage Kafka's behavior and ensure compliance with security and operational standards.</p> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Insights/#replication", "title": "Replication", "text": "<p>The Replication tab provides insights into the replication dynamics of each topic. It's crucial for ensuring data durability and high availability in Kafka. The attributes included in this tab are:</p> <ul> <li>Partition: The specific partition of a topic.</li> <li>Leader: The leader's broker ID for each partition.</li> <li>Replica count: The total number of replicas per partition.</li> <li>In sync brokers: The count of brokers currently in sync with the leader.</li> </ul> <p>This tab is essential for monitoring the health and integrity of topic replication. It helps users identify potential issues like under-replicated partitions or uneven distribution of leader roles.</p> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Insights/#distribution", "title": "Distribution", "text": "<p>The Distribution tab offers a visual and analytical perspective on the message distribution across all topic partitions. This analysis is vital for identifying \"hot\" (overly active) or \"cold\" (less active) partitions, which can affect the performance and scalability of your Kafka setup. The attributes displayed in this tab include:</p> <ul> <li>Partition: Identifies the specific partition.</li> <li>Estimated count: Provides an estimation of the number of messages in the partition.</li> <li>Total share: Shows the percentage of total messages across all partitions this partition holds.</li> <li>Diff: Highlights the difference in message count between this partition and others, helping identify imbalance.</li> </ul> <p>Moreover, the Distribution tab features a graph that visually represents the distribution of messages across partitions. This graphical insight can be instrumental in quickly identifying disparities in message load, guiding administrators in practical tasks such as rebalancing partitions or adjusting producer configurations to achieve a more even distribution.</p> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Insights/#benefits-of-distribution-insights", "title": "Benefits of Distribution Insights", "text": "<ul> <li>Performance Optimization: Understanding message distribution helps in tuning the system for better performance by reallocating resources or adjusting partitioning.</li> <li>Scalability Planning: Identifying hot partitions allows for proactive scalability efforts, ensuring that the system can handle increases in load without performance degradation.</li> <li>Troubleshooting and Maintenance: Uneven distribution can be a symptom of broader issues, such as configuration errors or network problems. Early detection enables quicker resolution.</li> </ul>"}, {"location": "Pro-Web-UI-Topics-Insights/#offsets", "title": "Offsets", "text": "<p>The Offsets tab allows you to inspect all topic partitions' high and low watermark offsets. This feature is crucial for understanding your message flow in Kafka. By examining these offsets, you can determine the current position of consumers about the total available messages in each partition.</p> <p>Understanding offsets is useful in several scenarios:</p> <ul> <li>Lag Monitoring: Helps identify how far behind consumers are in processing messages.</li> <li>Performance Analysis: Assists in diagnosing performance issues by revealing partitions that may have unusually high or low offsets.</li> <li>Troubleshooting: Aids in detecting potential problems in message processing and consumption.</li> <li>Distribution Monitoring: Provides insights into how messages are distributed across partitions, helping identify imbalances and optimize resource allocation.</li> </ul> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Insights/#summary", "title": "Summary", "text": "<p>In summary, the Distribution Insights feature of the Karafka Pro Web UI is a powerful tool that empowers users with the necessary insights to manage, optimize, and troubleshoot Kafka topics effectively. By providing detailed configurations, replication status, and distribution analytics, it enables administrators to maintain a robust and efficient Kafka environment, thereby enhancing performance, scalability, and troubleshooting capabilities.</p> <p>Last modified: 2024-08-02 14:47:01</p>"}, {"location": "Pro-Web-UI-Topics-Management/", "title": "Pro Web UI Topics Management", "text": "<p>Karafka Web UI Pro provides topic management capabilities, allowing you to create, configure, delete, and manage topics and partitions directly from the web interface. These features eliminate the need for command-line tools or separate admin interfaces, streamlining your Kafka administration workflow.</p> <p>Changes Visibility in Web UI</p> <p>When administrative operations (creating/deleting topics, modifying configurations, adding partitions) are submitted through the Web UI, they are immediately accepted by Kafka. However, due to the refresh interval and cluster synchronization timing, the Web UI display may take several seconds to reflect these changes. Additionally, while changes become effective immediately in Kafka, they may take time to propagate across all brokers in the cluster fully. If changes don't appear immediately in the UI, wait at least 15-30 seconds, refresh the page, or navigate to another section and back to see the updated state.</p>"}, {"location": "Pro-Web-UI-Topics-Management/#creating-topics", "title": "Creating Topics", "text": "<p>The topic creation feature allows you to easily create new Kafka topics with customizable configurations directly from the Web UI.</p>"}, {"location": "Pro-Web-UI-Topics-Management/#topic-creation-process", "title": "Topic Creation Process", "text": "<p>To create a new topic:</p> <ol> <li>Navigate to Home \u2192 Topics</li> <li>Click on Create Topic</li> <li>You'll see the \"Topic Creation Settings\" screen with important notices:</li> </ol> <p>Topic Creation Information</p> <ul> <li>Topic name cannot be changed after creation</li> <li>Number of partitions can only be increased, never decreased</li> <li>Additional settings can be configured from the topic configuration page</li> <li>It may take Kafka up to a few minutes to fully synchronize the new topic</li> <li>Consumers may require additional time to discover the topic, depending on their metadata refresh frequency</li> </ul> <ol> <li>Fill in the required fields:</li> </ol> <ul> <li>Topic Name: Only alphanumeric characters, dots, underscores, and hyphens are allowed</li> <li>Number of Partitions: Minimum 1 partition (cannot be decreased after creation)</li> <li>Replication Factor: Number of replicas for each partition (minimum 1, recommended 3 for production)</li> </ul> <ol> <li>Click the submit button to create your topic</li> </ol> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Management/#deleting-topics", "title": "Deleting Topics", "text": "<p>When a topic is no longer needed, you can safely remove it through the Web UI.</p>"}, {"location": "Pro-Web-UI-Topics-Management/#topic-deletion-process", "title": "Topic Deletion Process", "text": "<p>To delete a topic:</p> <ol> <li>Navigate to Home \u2192 Topics \u2192 [Your Topic Name]</li> <li>Click on Delete Topic at the page bottom</li> <li>You'll see a comprehensive warning screen:</li> </ol> <p>Topic Removal Warning</p> <ul> <li>All data in this topic will be permanently deleted and cannot be recovered</li> <li>All consumers and producers for this topic will stop functioning</li> <li>Applications dependent on this topic may experience errors or disruptions</li> <li>Consumer group offsets associated with this topic will be lost</li> </ul> <p>Before proceeding, ensure that</p> <ul> <li>All applications consuming from this topic have been properly shut down</li> <li>All producers to this topic have been stopped</li> <li>You have backed up any critical data if needed</li> <li>You have notified relevant team members about this deletion</li> </ul> <ol> <li>Review the final warning showing the topic name and partition count</li> <li>Confirm the deletion</li> </ol> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Management/#managing-topic-configuration", "title": "Managing Topic Configuration", "text": "<p>The topic configuration management feature allows you to view and modify the configuration settings of existing Kafka topics with a user-friendly interface.</p>"}, {"location": "Pro-Web-UI-Topics-Management/#viewing-topic-configuration", "title": "Viewing Topic Configuration", "text": "<p>To view a topic's configuration:</p> <ol> <li>Navigate to Home \u2192 Topics \u2192 [Your Topic Name] \u2192 Configuration</li> <li>You'll see a tabular view with all configuration parameters:</li> </ol> <ul> <li>Name: The configuration parameter name</li> <li>Value: Current setting value</li> <li>Default: Whether this is the default value</li> <li>Sensitive: Whether the parameter contains sensitive information</li> <li>Read Only: Whether the parameter can be modified</li> <li>Options: Additional details about the parameter</li> </ul> <p>The configuration table includes all standard Kafka topic settings, such as:</p> <ul> <li><code>cleanup.policy</code></li> <li><code>compression.type</code></li> <li><code>delete.retention.ms</code></li> <li><code>file.delete.delay.ms</code></li> <li><code>retention.bytes</code></li> <li><code>retention.ms</code></li> <li><code>segment.bytes</code></li> <li>And many more</li> </ul> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Management/#modifying-topic-configuration", "title": "Modifying Topic Configuration", "text": "<p>To modify a specific configuration parameter:</p> <ol> <li>Navigate to Home \u2192 Topics \u2192 [Your Topic Name] \u2192 Configuration</li> <li>Click on the specific parameter you want to modify</li> <li>Click Edit</li> <li>You'll see the \"Configuration Update Warning\":</li> </ol> <p>Configuration Update Warning</p> <ul> <li>Changing topic configurations may affect topic behavior and performance</li> <li>Some changes may take time to propagate across the cluster</li> <li>Applications consuming from this topic may be impacted</li> </ul> <p>Before updating this configuration</p> <ul> <li>Ensure you understand the impact of changing this value</li> <li>Consider testing the change in a non-production environment first</li> <li>Monitor the topic after the change to ensure expected behavior</li> </ul> <ol> <li>You'll see:</li> </ol> <ul> <li>The property name (e.g., <code>cleanup.policy</code>)</li> <li>Current value (e.g., <code>delete</code>)</li> <li>A field to enter the new value</li> </ul> <ol> <li>Enter the new value and submit the change</li> </ol> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Management/#managing-topic-partitions", "title": "Managing Topic Partitions", "text": "<p>Karafka Web UI Pro allows you to increase the number of partitions for existing topics to scale throughput.</p>"}, {"location": "Pro-Web-UI-Topics-Management/#increasing-partition-count", "title": "Increasing Partition Count", "text": "<p>To increase the number of partitions:</p> <ol> <li>Navigate to Home \u2192 Topics \u2192 [Your Topic Name] \u2192 Distribution</li> <li>Click on Increase Partitions</li> <li>You'll see the \"Partition Update Warning\":</li> </ol> <p>Partition Update Warning</p> <ul> <li>Increasing partitions is a one-way operation - partition count cannot be decreased later</li> <li>Adding partitions affects message ordering and consistent hashing</li> <li>Consumers will need to detect the partition count change and rebalance</li> <li>Message distribution across partitions may become uneven until data rotates</li> <li>Changes may take several minutes to be visible in the UI but will be applied immediately</li> </ul> <p>Before increasing partitions</p> <ul> <li>Ensure all consumers support dynamic partition detection</li> <li>Consider the impact on message ordering in your applications</li> <li>Plan for temporary rebalancing as consumers detect the change</li> <li>Monitor consumer lag during and after the operation</li> <li>Consider increasing partitions during low-traffic periods</li> </ul> <ol> <li>You'll see:</li> </ol> <ul> <li>Current partition count (e.g., <code>1</code>)</li> <li>A field to enter the new partition count (must be greater than the current count)</li> </ul> <ol> <li>Enter the new partition count and submit the change</li> </ol> <p> </p>"}, {"location": "Pro-Web-UI-Topics-Management/#best-practices", "title": "Best Practices", "text": "<p>When managing topics through the Karafka Web UI Pro, consider these best practices:</p> <ol> <li>Naming Conventions: Establish and follow a consistent topic naming convention</li> <li>Right-sizing Partitions: Start with a reasonable number of partitions based on expected throughput</li> <li>Replication Factor: Use a replication factor of at least 3 for production topics</li> <li>Configuration Tuning: Adjust retention and segment settings based on data volume and access patterns</li> <li>Caution with Deletion: Always double-check before deleting topics, as this action cannot be undone</li> <li>Permission Control: Use the Policies feature to control who can manage topics</li> <li>Change Documentation: Document significant topic changes for team awareness</li> <li>Testing: Test configuration changes in non-production environments when possible</li> <li>Monitoring: Monitor topic metrics after making changes to ensure desired behavior</li> </ol>"}, {"location": "Pro-Web-UI-Topics-Management/#limitations-and-considerations", "title": "Limitations and Considerations", "text": "<ul> <li>Some operations may be restricted by your Kafka cluster configuration</li> <li>Adding partitions is possible, but reducing the number of partitions is not supported by Kafka</li> <li>Topic creation and deletion operations require appropriate ACL permissions</li> <li>Configuration changes may take time to propagate across the cluster</li> <li>The broker must have <code>delete.topic.enable=true</code> to support topic deletion</li> <li>UI updates may take several minutes to reflect changes, even though they are applied immediately</li> </ul>"}, {"location": "Pro-Web-UI-Topics-Management/#summary", "title": "Summary", "text": "<p>The topic management features in Karafka Web UI Pro provide a comprehensive solution for administering your Kafka topics without leaving the web interface. With intuitive interfaces for creating topics, modifying configurations, and managing partitions, these tools streamline common administrative tasks and provide a user-friendly alternative to command-line tools.</p> <p>Combined with other Karafka Pro features like Data Explorer, Topics Insights, and Search, topic management completes the toolset needed for efficient Kafka operations and maintenance.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Pro-Web-UI/", "title": "Enhanced Web UI", "text": "<p>The Enhanced Web UI, aside from all the features from the OSS version, also offers additional features and capabilities not available in the free version, making it a better option for those looking for more robust monitoring and management capabilities for their Karafka applications. Some of the key benefits of the Enhanced Web UI version include the following:</p> <ul> <li>Enhanced consumers utilization metrics providing much better insights into processes resources utilization.</li> <li>Consumer process inspection to quickly analyze the state of a given consuming process.</li> <li>Consumer jobs inspection to view currently running jobs on a per-process basis.</li> <li>Health dashboard containing general consumption overview information</li> <li>Data Explorer allowing for viewing and exploring the data produced to Kafka topics. It understands the routing table and can deserialize data before it is displayed.</li> <li>Enhanced error reporting allowing for backtrace inspection and providing multi-partition support.</li> <li>DLQ / Dead insights allowing to navigate through DLQ topics and messages that were dispatched to them.</li> <li>Consumer Processes Commanding from the Web interface.</li> </ul>"}, {"location": "Pro-Web-UI/#getting-started", "title": "Getting Started", "text": "<p>Karafka Web UI will automatically switch to the Pro mode when Karafka Pro is in use.</p> <p>There are no extra steps needed unless you want to completely disable consumers processes management.</p>"}, {"location": "Pro-Web-UI/#dashboard", "title": "Dashboard", "text": "<p>The dashboard provides an all-encompassing insight into your Karafka operations. It\u2019s an indispensable tool for anyone looking to monitor, optimize, and troubleshoot their Karafka processes. With its user-friendly interface and detailed metrics, you have everything you need to ensure the smooth running of your Kafka operations.</p> <p></p>"}, {"location": "Pro-Web-UI/#consumers", "title": "Consumers", "text": "<p>Enhanced consumer view reports all of the metrics available in the OSS version but also reports:</p> <ul> <li>Machine memory usage</li> <li>Machine memory available</li> <li>Average CPU load from the last minute, 5 minutes, and 15 minutes</li> <li>Threads utilization from the last 60 seconds</li> </ul> <p>Those metrics can allow you to identify bottlenecks (CPU vs. IO) in your Karafka consumers.</p>"}, {"location": "Pro-Web-UI/#consumer-process-inspection", "title": "Consumer Process Inspection", "text": "<p>Consumer process inspection view provides real-time visibility into the performance and behavior of a given consumer process and its Kafka subscriptions.</p> <p></p>"}, {"location": "Pro-Web-UI/#consumer-jobs-inspection", "title": "Consumer Jobs Inspection", "text": "<p>The consumer jobs inspection view provides real-time visibility into the jobs running at the current moment on a given consumer instance.</p> <p></p>"}, {"location": "Pro-Web-UI/#consumer-details-inspection", "title": "Consumer Details Inspection", "text": "<p>This feature offers users a detailed look into each process's current state report. It's a valuable tool for thorough debugging and precise per-process inspection.</p> <p></p>"}, {"location": "Pro-Web-UI/#commanding", "title": "Commanding", "text": "<p>This feature has its own dedicated documentation that you can access here.</p>"}, {"location": "Pro-Web-UI/#health", "title": "Health", "text": "<p>This dashboard views show Karafka consumers' groups' health states with their lag aggregated information and basic trends.</p> <p>Here you can learn more about the health information available in this dashboard view.</p>"}, {"location": "Pro-Web-UI/#topics-insights", "title": "Topics Insights", "text": "<p>The \"Topics Insights\" feature in Karafka Pro Web UI is a comprehensive suite designed to provide users with detailed information and analytics about their Kafka topics. This feature is crucial for developers who must ensure optimal configuration and performance of their Kafka topics. You can learn more about this feature here.</p>"}, {"location": "Pro-Web-UI/#explorer", "title": "Explorer", "text": "<p>The \"Web UI Explorer\" in Karafka Pro Web UI provides detailed insights and analytics about Kafka messages. Essential for developers, it helps monitor, debug, and optimize Kafka applications. Learn more here</p>"}, {"location": "Pro-Web-UI/#search", "title": "Search", "text": "<p>The Search feature is part of the Karafka Web UI Explorer, but due to its complexity, it has its dedicated section that can be found here.</p>"}, {"location": "Pro-Web-UI/#recurring-tasks", "title": "Recurring Tasks", "text": "<p>In the Karafka Pro Web UI, you can manage and monitor recurring tasks with ease:</p> <ul> <li>Inspect Tasks: View detailed information on each task, including status, next execution, and last execution result.</li> <li>Control Tasks: Enable, disable, or trigger tasks directly from the UI.</li> <li>View Logs: Access and explore execution logs to track task performance and troubleshoot issues.</li> </ul> <p></p>"}, {"location": "Pro-Web-UI/#scheduled-messages", "title": "Scheduled Messages", "text": "<p>Karafka Pro's Enhanced Web UI provides detailed insights into scheduled messages, enabling efficient management and monitoring:</p> <ul> <li>Daily Dispatch Estimates: View estimates of messages scheduled for dispatch across partitions, aiding in operational planning.</li> <li>Loading State Monitoring: Monitor the loading state of partitions to ensure readiness, especially after nightly reloads.</li> <li>Message Exploration: Access detailed information about messages, queued for future dispatch, including scheduled times and payload details.</li> </ul> <p> </p>"}, {"location": "Pro-Web-UI/#errors", "title": "Errors", "text": "<p>Enhanced Web UI errors provide a few enhancements:</p> <ul> <li>Supports error tracking on a high-scale due to the support of multiple partitions for the error topic.</li> <li>Supports errors backtrace reporting in the dashboard.</li> </ul> <p>It allows for easier debuggability and error exploration, enabling users to perform real-time data analysis and troubleshoot issues faster.</p> <p>Errors list:</p> <p></p> <p>Error details:</p> <p></p>"}, {"location": "Pro-Web-UI/#dlq-dead", "title": "DLQ / Dead", "text": "<p>Dead insights allowing to navigate through DLQ topics and messages that were dispatched to them.</p> <p>Automatic Qualification of DLQ Topics</p> <p>Web UI will automatically classify any topics that contain <code>dlq</code> or <code>dead_letter</code> in their names - irrespective of case - as Dead Letter Queue (DLQ) topics. This means topics labeled with variations such as <code>DLQ</code>, <code>dlq</code>, <code>Dead_Letter</code>, or <code>DEAD_LETTER</code> will be viewed and managed under the DLQ view. </p> <p>DLQ dispatched messages view:</p> <p></p> <p>DLQ dispatched per message view:</p> <p></p>"}, {"location": "Pro-Web-UI/#branding", "title": "Branding", "text": "<p>This feature allows you to set an environment-specific notice and a menu label to distinguish between different environments like production, development, and staging. This helps prevent confusion and mistakes when managing various environments.</p> <p>This feature has its own dedicated documentation that you can access here.</p> <p></p>"}, {"location": "Pro-Web-UI/#policies", "title": "Policies", "text": "<p>Karafka's Web UI includes a comprehensive policies engine that provides granular control over user actions across all UI components. This engine allows administrators to define and enforce policies on what specific users can view and do within the Web UI, ensuring compliance with data protection and privacy standards.</p> <p>This feature has its own dedicated documentation that you can access here.</p>"}, {"location": "Pro-Web-UI/#custom-styling", "title": "Custom Styling", "text": "<p>Web UI supports the ability to customize the appearance and behavior of the interface through custom CSS and JavaScript. This feature allows you to adapt the Web UI to match your organization's branding, enhance usability for specific use cases, or add visual indicators for different environments.</p> <p>Custom styling can be configured using two settings in the Web UI configuration:</p> <pre><code>Karafka::Web.setup do |config|\n  # Custom CSS configuration\n  config.ui.custom_css = '/path/to/your/custom.css'\n  # or inline CSS\n  config.ui.custom_css = '.dashboard { background-color: #f5f5f5; }'\n\n  # Custom JavaScript configuration\n  config.ui.custom_js = '/path/to/your/custom.js'\n  # or inline JavaScript\n  config.ui.custom_js = 'document.addEventListener(\"DOMContentLoaded\", () =&gt; { console.log(\"JS loaded\"); });'\nend\n</code></pre> <p>Both <code>custom_css</code> and <code>custom_js</code> settings accept either:</p> <ul> <li>A file path to a CSS or JavaScript file</li> <li>A string containing inline CSS or JavaScript code</li> <li><code>false</code> to disable custom styling (default)</li> </ul>"}, {"location": "Pro-Web-UI/#css-targeting-by-controller-and-action", "title": "CSS Targeting by Controller and Action", "text": "<p>To facilitate more granular styling, the Web UI automatically adds special CSS classes to the <code>&lt;body&gt;</code> element:</p> <ul> <li><code>.controller-NAME</code> - Contains the current controller name (e.g., <code>controller-dashboard</code>)</li> <li><code>.action-NAME</code> - Contains the current action name (e.g., <code>action-index</code>)</li> </ul> <p>These classes allow you to target specific pages or views with your custom CSS:</p> <pre><code>/* Style only the dashboard page */\nbody.controller-dashboard {\n  background-color: #f8f9fa;\n}\n\n/* Style a specific action within a controller */\nbody.controller-consumers.action-show .stats-card {\n  border-left: 4px solid #28a745;\n}\n\n/* Style error pages differently */\nbody.controller-errors {\n  background-color: #fff8f8;\n}\n</code></pre>"}, {"location": "Pro-Web-UI/#compatibility-notes", "title": "Compatibility Notes", "text": "<ul> <li>Custom CSS and JavaScript are loaded after the default styles and scripts</li> <li>The custom styling feature respects the Web UI's dark mode settings</li> <li>Custom assets are properly cached by the browser for optimal performance</li> <li>Custom styling is applied to all pages, including error pages</li> </ul> <p>When used in combination with the Branding feature, custom styling provides a comprehensive way to tailor the Karafka Web UI to your organization's needs.</p>"}, {"location": "Pro-Web-UI/#topics-management", "title": "Topics Management", "text": "<p>The \"Topics Management\" feature in Karafka Web UI allows you to create, delete, configure, and manage Kafka topics directly from the web interface without command-line tools. You can create topics with custom settings, safely remove unused topics, adjust configurations, and scale throughput by increasing partition counts.</p> <p>This feature has dedicated documentation that you can access here.</p> <p></p> <p>Last modified: 2025-05-01 16:51:26</p>"}, {"location": "Problems-and-Troubleshooting/", "title": "Help!", "text": "<p>Read below for tips.  If you still need help, you can:</p> <ul> <li>Ask your question in The Karafka official Slack channel</li> <li>Open a GitHub issue.  (Don't be afraid to open an issue, even if it's not a Karafka bug.  An issue is just a conversation, not an accusation!)</li> <li>Check our FAQ and the Pro FAQ</li> </ul> <p>You should not email any Karafka committer privately.</p> <p>Please respect our time and efforts by sticking to one of the options above.</p> <p>Please consider buying the Pro subscription for additional priority Pro support and extra features.</p>"}, {"location": "Problems-and-Troubleshooting/#oss-support-policy", "title": "OSS Support Policy", "text": "<p>Karafka Official Support Policy can be found here.</p>"}, {"location": "Problems-and-Troubleshooting/#reporting-problems", "title": "Reporting problems", "text": "<p>When you encounter issues with Karafka, there are several things you can do:</p> <ul> <li>Feel free to open a Github issue</li> <li>Feel free to ask on our Slack channel</li> <li>Use our integration specs and example apps to create a reproduction code that you can then share with us.</li> </ul>"}, {"location": "Problems-and-Troubleshooting/#debugging", "title": "Debugging", "text": "<p>Karafka now has a dedicated and comprehensive debugging guide that covers troubleshooting message duplication, offset handling, crashes, and other consumer-related issues. To keep things focused, debugging content has been moved out of this document and can be found in the new Debugging document.</p> <p>Last modified: 2025-04-07 14:56:33</p>"}, {"location": "Producing-Messages/", "title": "Producing Messages", "text": "<p>It's quite common when using Kafka to treat applications as parts of a bigger pipeline (similarly to Bash pipeline) and forward processing results to other applications. Karafka provides a way of dealing with that by allowing you to use the WaterDrop messages producer from any place within your application.</p> <p>You can access the pre-initialized WaterDrop producer instance using the <code>Karafka.producer</code> method from any place within your codebase.</p> <pre><code>Karafka.producer.produce_async(\n  topic: 'events',\n  payload: Events.last.to_json\n)\n</code></pre> <p>WaterDrop is thread-safe and operates well at scale.</p> <p>If you're looking to produce messages within Karafka consumers, you have several convenient alias methods at your disposal, including <code>#producer</code>, <code>#produce_sync</code>, <code>#produce_async</code>, <code>#produce_many_sync</code>, and <code>#produce_many_async</code>. Here's how you might use them:</p> <pre><code>class VisitsConsumer &lt; ApplicationConsumer\n  def consume\n    ::Visit.insert_all(messages.payloads)\n\n    producer.produce_async(\n      topic: 'events',\n      payload: { type: 'inserted', count: messages.count }.to_json\n    )\n\n    # Or you can use the listed methods directly, bypassing `#producer` reference:\n    produce_async(\n      topic: 'events',\n      payload: { type: 'inserted', count: messages.count }.to_json\n    )\n  end\nend\n</code></pre> <p>Please follow the WaterDrop documentation for more details on how to use it.</p>"}, {"location": "Producing-Messages/#messages-piping", "title": "Messages Piping", "text": "<p>If you are looking for seamless message piping in Kafka-based systems, we recommend checking out the message piping feature in Karafka Pro. Exclusive to Karafka Pro, this feature offers synchronous and asynchronous forwarding capabilities with enhanced traceability, which is perfect for streamlining data workflows.</p>"}, {"location": "Producing-Messages/#producer-shutdown", "title": "Producer Shutdown", "text": "<p>When using the Karafka producer in processes like Puma, Sidekiq, or rake tasks, it is always recommended to call the <code>#close</code> method on the producer before shutting it down.</p> <p>This is because the <code>#close</code> method ensures that any pending messages in the producer's buffer are flushed to the Kafka broker before shutting down the producer. If you do not call <code>#close</code>, there is a risk that some messages may not be sent to the Kafka broker, resulting in lost or incomplete data.</p> <p>In addition, calling <code>#close</code> also releases any resources held by the producer, such as network connections, file handles, and memory buffers. Failing to release these resources can lead to memory leaks, socket exhaustion, or other system-level issues that can impact the stability and performance of your application.</p> <p>Overall, calling <code>#close</code> on the Karafka producer is a best practice that helps ensure reliable and efficient message delivery to Kafka while promoting your application's stability and scalability.</p> <p>Below you can find an example of how to <code>#close</code> the producer used in various Ruby processes. Please note, that you should not close the producer manually if you are using the Embedding API in the same process.</p>"}, {"location": "Producing-Messages/#closing-producer-used-in-karafka", "title": "Closing Producer Used in Karafka", "text": "<p>When you shut down the Karafka consumer, the <code>Karafka.producer</code> automatically closes. There's no need to close it yourself. If you're using multiple producers or a more advanced setup, you can use the <code>app.stopped</code> event during shutdown to handle them.</p>"}, {"location": "Producing-Messages/#closing-producer-used-in-puma-single-mode", "title": "Closing Producer Used in Puma (Single Mode)", "text": "<pre><code># config/puma.rb \n\n# There is no `on_worker_shutdown` equivalent for single mode\n@config.options[:events].on_stopped do\n  Karafka.producer.close\nend\n</code></pre>"}, {"location": "Producing-Messages/#closing-producer-used-in-puma-cluster-mode", "title": "Closing Producer Used in Puma (Cluster Mode)", "text": "<pre><code># config/puma.rb \n\non_worker_shutdown do\n  ::Karafka.producer.close\nend\n</code></pre>"}, {"location": "Producing-Messages/#closing-producer-used-in-sidekiq", "title": "Closing Producer Used in Sidekiq", "text": "<pre><code># config/initializers/sidekiq.rb\n\nSidekiq.configure_server do |config|\n  # You can use :shutdown for older Sidekiq versions if\n  # :exit is not available\n  config.on(:exit) do\n    ::Karafka.producer.close\n  end\nend\n</code></pre>"}, {"location": "Producing-Messages/#closing-producer-used-in-solid-queue", "title": "Closing Producer Used in Solid Queue", "text": "<pre><code># config/initializers/solid_queue.rb\n\n# This code will close the producer in each worker process\nSolidQueue.on_worker_exit do\n  ::Karafka.producer.close\nend\n\n# Below is optional - useful only when publishing events to Kafka\n# from the supervisor process\nSolidQueue.on_exit do\n  ::Karafka.producer.close\nend\n</code></pre>"}, {"location": "Producing-Messages/#closing-producer-used-in-passenger", "title": "Closing Producer Used in Passenger", "text": "<pre><code>PhusionPassenger.on_event(:stopping_worker_process) do\n  ::Karafka.producer.close\nend\n</code></pre>"}, {"location": "Producing-Messages/#closing-producer-used-in-a-rake-task", "title": "Closing Producer Used in a Rake Task", "text": "<p>In case of rake tasks, just invoke <code>::Karafka.producer.close</code> at the end of your rake task:</p> <pre><code>desc 'My example rake task that sends all users data to Kafka'\ntask send_users: :environment do\n  User.find_each do |user|\n    ::Karafka.producer.produce_async(\n      topic: 'users',\n      payload: user.to_json,\n      key: user.id\n    )\n  end\n\n  # Make sure that the producer is always closed before finishing\n  # any rake task\n  ::Karafka.producer.close\nend\n</code></pre>"}, {"location": "Producing-Messages/#closing-producer-in-any-ruby-process", "title": "Closing Producer in any Ruby Process", "text": "<p>While integrating Karafka producers into your Ruby applications, it's essential to ensure that resources are managed correctly, especially when terminating processes. We generally recommend utilizing hooks specific to the environment or framework within which the producer operates. These hooks ensure graceful shutdowns and resource cleanup tailored to the application's lifecycle.</p> <p>However, there might be scenarios where such specific hooks are not available or suitable. In these cases, Ruby's <code>at_exit</code> hook can be employed as a universal fallback to close the producer before the Ruby process exits. Here's a basic example of using at_exit with a Karafka producer:</p> <pre><code>at_exit do\n  Karafka.producer.close\nend\n</code></pre>"}, {"location": "Producing-Messages/#producing-to-multiple-clusters", "title": "Producing to Multiple Clusters", "text": "<p>Karafka, by default, provides a producer that sends messages to a specified Kafka cluster. If you don't configure it otherwise, this producer will always produce messages to the default cluster that you've configured Karafka to work with. If you only specify one Kafka cluster in your configuration, all produced messages will be sent to this cluster. This is the out-of-the-box behavior and works well for many setups with a single cluster.</p> <p>However, if you have a more complex setup where you'd like to produce messages to different Kafka clusters based on certain logic or conditions, you need a more customized setup. In such cases, you must configure a producer for each cluster to which you want to produce. This means you'll have separate producer configurations tailored to each cluster, allowing you to produce to any of them as required.</p> <p>In scenarios where you want to decide which cluster to produce to based on the consumer logic or the consumed message, you can override the <code>#producer</code> method in your consumer. By overriding this method, you can specify a dedicated cluster-aware producer instance depending on your application's logic.</p> <pre><code># Define your producers for each of the clusters\nPRODUCERS_FOR_CLUSTERS = {\n  primary: Karafka.producer,\n  secondary: ::WaterDrop::Producer.new do |p_config|\n    p_config.kafka = {\n      'bootstrap.servers': 'localhost:9095',\n      'request.required.acks': 1\n    }\n  end\n}\n\n\n# And overwrite the default producer in any consumer you need\nclass MyConsumer &lt; ApplicationConsumer\n  def consume\n    messages.each do |message|\n      # Pipe messages to the secondary cluster\n      producer.produce_async(topic: message.topic, payload: message.raw_payload)\n    end\n  end\n\n  private\n\n  def producer\n    PRODUCERS_FOR_CLUSTERS.fetch(:secondary)\n  end\nend\n</code></pre> <p>The Web UI relies on per-producer listeners to monitor asynchronous errors. If you're crafting your consumers and utilizing the Web UI, please ensure you configure this integration appropriately.</p> <p>By leveraging this flexibility in Karafka, you can effectively manage and direct the flow of messages in multi-cluster Kafka environments, ensuring that data gets to the right place based on your application's unique requirements.</p> <p>Last modified: 2025-05-16 21:07:08</p>"}, {"location": "Purchase-Karafka-Pro/", "title": "Purchase Karafka Pro", "text": "<p>Purchase or trial Karafka Pro</p> <p>Karafka Pro includes a commercial-friendly license, priority support, architecture consultations, high throughput data processing-related features, and more.</p> <p>A portion of the income is distributed back to other OSS projects that Karafka relies on under the hood.</p> <p>Help us provide high-quality open-source software.</p> <p>If you want to purchase or trial Karafka Pro (there is a 14-day trial without any commitment requirements), please go to our website and follow the instructions there.</p> <p>Purchase or trial Karafka Enterprise</p> <p>From a feature perspective, Karafka Pro and Enterprise deliver the same value, ensuring you get consistent functionality regardless of your choice. To fully grasp Karafka's capabilities, we encourage you to utilize the Pro trial, allowing you firsthand experience of its functionalities. If, after your trial, you find that the Enterprise license aligns more with your organizational needs, especially in terms of licensing and support features, please get in touch with us via email.</p> <p>Last modified: 2025-05-16 21:07:08</p>"}, {"location": "Resources-Management/", "title": "Resources Management", "text": "<p>This document provides a detailed examination of threading, TCP connection management, memory usage, and CPU utilization in Karafka. It delves into how Karafka manages multiple tasks simultaneously, handles TCP connections efficiently, and optimizes resource usage for Kafka consumers and producers.</p>"}, {"location": "Resources-Management/#threading", "title": "Threading", "text": "<p>Karafka's multithreaded nature is one of its strengths, allowing it to manage numerous tasks simultaneously. To understand how it achieves this, it's essential to realize that Karafka's threading model isn't just about worker poll threads. It also extends to other aspects of Karafka's functionality.</p>"}, {"location": "Resources-Management/#consumer", "title": "Consumer", "text": "<p>Aside from worker threads, each subscription group within a consumer group uses a background thread. These threads handle the polling and scheduling of messages for the assigned topics in the group.</p> <p>The C <code>librdkafka</code> client library, which Karafka uses under the hood, uses <code>2</code> to <code>4</code> threads per subscription group. This is crucial to remember, as each consumer group added to your application will introduce an additional <code>3</code> to <code>4</code> threads in total.</p> <p>The Kafka cluster size can also affect the number of threads since Karafka maintains connections with multiple brokers in a cluster. Therefore, a larger cluster size may result in more threads. A single consumer group Karafka server process in a Ruby on Rails application on a small cluster will have approximately <code>25</code> to <code>30</code> threads.</p> <p>This may sound like a lot, but for comparison, Puma, a popular Ruby web server in a similar app, will have around <code>21</code> to <code>25</code> threads. It's important to note that having a higher thread count in Karafka is perfectly normal. Karafka is designed to handle the complexity of multiple brokers and consumer groups in a Kafka cluster, which inherently requires more threads.</p> <p>Karafka Pro Web-UI gives you enhanced visibility into your Karafka server processes. This includes the ability to inspect the thread count on a per-process basis. This detailed view can provide invaluable insights, helping you understand how your Karafka server is performing and where any potential bottlenecks might occur.</p> <p>This visibility and a sound understanding of how Karafka utilizes threads can be a great asset when troubleshooting performance issues or planning for future scalability.</p> <p>If you are interested in the total number of threads your Karafka servers use, Karafka Pro Web-UI gives you visibility into this value.</p> <p>This detailed view can provide invaluable insights, helping you understand how your Karafka server is performing and where any potential bottlenecks might occur.</p>"}, {"location": "Resources-Management/#producer", "title": "Producer", "text": "<p>In the current implementation, each Karafka producer employs a relatively simple threading model to efficiently handle asynchronous message delivery to Kafka. A vital characteristic of this model is that each producer instantiates at least two additional threads. Here's how these threads function:</p> <ul> <li> <p>Ruby Thread: The first thread operates within the Ruby environment. Its primary role is to manage communication with librdkafka, ensuring that messages are queued and sent to the Kafka cluster efficiently. This thread also handles various events and callbacks that arise during the message delivery process.</p> </li> <li> <p>librdkafka Thread: The second thread is managed by librdkafka itself, the native library Karafka leverages for interacting with Kafka. This thread is crucial for performing network I/O operations and managing internal events of the Kafka protocol.</p> </li> <li> <p>Broker-Specific Thread: librdkafka also creates an additional thread for each broker it connects to. This thread is solely responsible for managing communication with that particular broker and handling tasks such as message transfers, acknowledgments, and network events.</p> </li> </ul> <p>Despite adding these threads, the overall impact on system resources is minimal. The threads mostly wait for events or data, meaning their CPU usage is generally low, making this model highly efficient in resource consumption.</p>"}, {"location": "Resources-Management/#kafka-tcp-connections", "title": "Kafka TCP Connections", "text": "<p>Karafka's efficient management of TCP connections is substantially powered by librdkafka. This native library implements a smart connection strategy to optimize network interactions with Kafka brokers, ensuring robustness and efficiency. Below, you can find a general description of how librdkafka deals with both consumer and producer connections. Please refer to the appropriate sub-section for context-specific details.</p> <ul> <li> <p>Selective Connections: librdkafka only attempts to establish TCP connections with brokers necessary for its operation. This includes one of the brokers listed in <code>bootstrap.servers</code>, partition leaders, and specific coordinators (group and transaction). This targeted approach helps minimize unnecessary network traffic and optimizes connection management.</p> </li> <li> <p>Bootstrap and Failover Mechanism: The connection process begins with the <code>bootstrap.servers</code>. If the first attempted bootstrap server is unavailable, librdkafka will try the next server in a randomized order. This failover mechanism ensures the client can connect to the cluster even if some brokers are down.</p> </li> <li> <p>Metadata Requests: Upon establishing the first connection with any broker, librdkafka sends a Metadata request. This request is crucial as it retrieves a complete list of all brokers within the cluster, along with their roles and capabilities.</p> </li> <li> <p>Broker Cache: After receiving the Metadata response, librdkafka maintains an internal cache of all broker information. This cache includes details like broker node IDs and their roles, enabling librdkafka to intelligently connect directly to the appropriate broker for specific operations like producing or consuming messages.</p> </li> <li> <p>Initial Use of Bootstrap Brokers: Initially, connections to bootstrap brokers are utilized primarily for fetching Metadata. However, these connections might be repurposed for regular broker interactions under certain conditions.</p> </li> <li> <p>Advertised Listeners: If the hostname and port of a bootstrap broker match those of a broker from the Metadata response (as specified by the <code>advertised.listeners</code> configuration), the connection to this bootstrap broker is then associated with that broker's ID. Subsequently, it is used for full protocol operations, such as message production or consumption.</p> </li> <li> <p>Automatic Reconnection: librdkafka is designed to automatically attempt reconnecting to brokers if their connections drop but are still required for ongoing operations. This feature is vital for maintaining uninterrupted service even during network issues or broker restarts.</p> </li> <li> <p>Connection Lifecycle Management: If a broker is no longer needed (e.g., due to a change in partition leadership), librdkafka will not attempt to reconnect, thus optimizing resource usage and connection management.</p> </li> </ul>"}, {"location": "Resources-Management/#consumer_1", "title": "Consumer", "text": "<p>Karafka uses one librdkafka client per subscription group.</p> <p>To calculate the estimated TCP usage for a single Karafka consumer process, excluding producer and other TCP connections, use the formula:</p> <pre><code># Number of effective subscription groups\nsub_groups_count = 5\n\n# Number of brokers\n\nbrokers_count = 3\n\n# Extra one is for a metadata connection\ntotal = sub_groups_count * (brokers_count + 1)\n\nputs total #=&gt; 20 \n</code></pre> <p>This accounts for each subscription group's connections to all brokers, plus one additional connection for metadata queries.</p>"}, {"location": "Resources-Management/#producer_1", "title": "Producer", "text": "<p>In Karafka, each producer internally uses a single librdkafka client, which is designed to establish one TCP connection per broker in the Kafka cluster. This architecture implies that a single Karafka producer can potentially open as many TCP connections as brokers in the cluster. This fact becomes particularly important in larger systems or in setups where producers are dynamically created for different topics.</p> <p>For instance, consider a scenario where a Ruby process is configured to spawn a separate producer for each of <code>10</code> different topics. If your Kafka cluster consists of <code>5</code> brokers, this configuration would result in each producer maintaining <code>5</code> TCP connections \u2014 one for each broker. Consequently, the total number of TCP connections in just one Ruby process would be <code>50</code> (<code>10</code> topics x <code>5</code> brokers). When scaled up to <code>100</code> processes, this architecture would lead to a staggering total of <code>5000</code> TCP connections, which can impact network performance and resource utilization.</p> <p>Awareness of this potential multiplication of TCP connections is crucial. Systems architects and developers need to consider the implications of such a setup, including the increased overhead on network resources and the complexity of managing a larger number of connections, which can introduce more points of failure and complicate troubleshooting.</p> <p>To optimize the management of TCP connections and enhance overall system performance, it is advisable to leverage WaterDrop, Karafka's thread-safe producer library. WaterDrop allows for the use of a single producer instance to dispatch messages across multiple topics efficiently. This method reduces the number of TCP connections needed and simplifies the producer management by minimizing the number of producer instances in the system. Adopting this approach is recommended in most scenarios as it provides a more scalable and maintainable architecture, especially in systems where topics and brokers are numerous.</p> <p>Below, you can find the formula to estimate the TCP usage of your processes cluster in regard to WaterDrop usage:</p> <pre><code>number_of_producers_per_process = 10\nnumber_of_processes = 500\nnumber_of_brokers = 5\n\ntotal = number_of_producers_per_process * number_of_processes * number_of_brokers\n\nputs total #=&gt; 25 000\n</code></pre>"}, {"location": "Resources-Management/#database-connections-usage", "title": "Database Connections Usage", "text": "<p>Karafka, by itself, does not manage PostgreSQL or any other database connections directly. When using frameworks like Ruby on Rails, database connections are typically managed by the ActiveRecord Connection Pool.</p> <p>Under normal circumstances, Karafka will use the <code>concurrency</code> number of database connections at most. This is because, at any given time, that's the maximum number of workers that can run in parallel.</p> <p>However, the number of potential concurrent database connections might increase when leveraging advanced Karafka APIs, such as the Filtering API, or making alterations to the scheduler and invoking DB requests from it. This is because these APIs operate from the listeners threads. In such advanced scenarios, the maximum number of concurrent DB connections would be the sum of the number of workers (<code>concurrency</code>) and the total number of subscription groups.</p> <p>It's important to note that a situation where all these threads would execute database operations simultaneously is highly unlikely. Therefore, in most use cases, the simplified assumption that only the <code>concurrency</code> parameter determines potential DB connections should suffice.</p>"}, {"location": "Resources-Management/#memory-usage", "title": "Memory Usage", "text": "<p>Karafka demonstrates robustness and efficiency in managing memory resources, particularly in high-throughput environments. Here are some key aspects of how memory usage is handled within Karafka:</p> <ul> <li> <p>Baseline Memory Consumption: A freshly started Karafka-Rails application has a baseline memory footprint of around <code>80MB</code>. This initial consumption provides a starting point for understanding the memory requirements in typical deployment scenarios.</p> </li> <li> <p>No Known Memory Leaks: Karafka has no known memory leaks within its components as of the latest updates.</p> </li> <li> <p>Batch Processing and Memory Release: By default, Karafka retains the memory occupied by messages and their payload until an entire batch is processed. Karafka makes no assumptions about the nature of the processing. While this ensures flexibility in handling complex workflows, it can also increase memory usage during high-throughput operations. For those looking to optimize memory management and release message memory more proactively, Karafka Pro offers a Cleaner API.</p> </li> <li> <p>Ruby Version Considerations: It's important to note that external factors such as Ruby versions can affect memory usage. For instance, Ruby <code>3.3.0</code> has been observed to have memory leak issues due to bugs introduced in that version.</p> </li> <li> <p>Impact of High Throughput: Karafka's design to handle tens of thousands of messages per second means that any memory leaks in other gems or the application code can be more problematic than in traditional web applications. The high message throughput can quickly escalate minor leaks into significant issues, affecting system stability and performance.</p> </li> <li> <p>Multi-threaded Environment: Karafka's use of multiple threads to process tasks in parallel can lead to a larger memory footprint than single-threaded applications. Each thread consumes memory for its stack and may duplicate particular objects, leading to higher overall memory usage.</p> </li> <li> <p>Reporting Memory Issues: No software is entirely free of issues, and memory leaks can occur for various reasons, including interactions with other software components. If you suspect Karafka has a memory leak, you should report this. Such issues are treated with high urgency to ensure that they are resolved promptly, maintaining the high reliability of Karafka for all users.</p> </li> </ul>"}, {"location": "Resources-Management/#cpu-usage", "title": "CPU Usage", "text": "<p>Karafka is designed to efficiently handle high-throughput message processing, leveraging modern CPU architectures to optimize performance. Here are the key aspects of CPU usage in Karafka:</p> <ul> <li> <p>Asynchronous Operations: Karafka employs asynchronous operations wherever possible, using Global VM Lock (GVL) releasing locks rather than standard sleep operations. This approach reduces idle CPU time and maximizes resource usage efficiency, allowing Karafka to perform more operations concurrently without unnecessary delays.</p> </li> <li> <p>Multi-threaded Nature: The multi-threaded design of Karafka, despite the constraints imposed by the Ruby GVL, enables efficient parallel data processing. This architecture is particularly effective in environments where quick handling of large volumes of data is crucial. By distributing tasks across multiple threads, Karafka can leverage the CPU more effectively.</p> </li> <li> <p>Swarm Mode for Intensive Workloads: Karafka offers a Swarm mode for CPU-intensive tasks, which spawns multiple processes under a supervisor. This model is designed to fully utilize multiple CPUs or cores, effectively scaling the processing capabilities across the available hardware resources. Swarm mode is especially beneficial for applications requiring significant computational power, as it helps to distribute the load and prevent any single process from becoming a bottleneck.</p> </li> <li> <p>Underlying librdkafka Multithreading: The librdkafka library, which underpins Karafka's interaction with Kafka, is multi-threaded. It can efficiently utilize multiple cores available on modern machines, enhancing the capability to manage multiple connections and perform various network and I/O operations concurrently.</p> </li> <li> <p>Optimization Opportunities: Given its multi-threaded nature and efficient asynchronous techniques, Karafka allows for significant optimization opportunities regarding CPU usage. Developers can fine-tune the number of threads and the operational parameters of Karafka to match the specific performance and resource requirements.</p> </li> </ul> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "Routing/", "title": "Routing", "text": "<p>The routing engine provides an interface to describe how messages from all the topics should be received and consumed.</p> <p>Due to the dynamic nature of Kafka, you can use multiple configuration options; however, only a few are required.</p>"}, {"location": "Routing/#routing-dsl-organization", "title": "Routing DSL Organization", "text": "<p>Karafka uses consumer groups to subscribe to topics. Each consumer group needs to be subscribed to at least one topic (but you can subscribe with it to as many topics as you want). To replicate this concept in our routing DSL, Karafka allows you to configure settings on two levels:</p> <ul> <li>settings level - root settings that will be used everywhere</li> <li>topic level - options that need to be set on a per topic level or overrides to options set on a root level</li> </ul> <p>Most of the settings (apart from the <code>consumer</code>) are optional and if not configured, will use defaults provided during the configuration of the app itself.</p> <p>Karafka provides two ways of defining topics on which you want to listen:</p>"}, {"location": "Routing/#single-consumer-group-with-multiple-topics-mode", "title": "Single Consumer Group with Multiple Topics Mode", "text": "<p>In this mode, Karafka will create a single consumer group to which all the topics will belong.</p> <p>It is recommended for most of the use-cases and can be changed later.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :example do\n      consumer ExampleConsumer\n    end\n\n    topic :example2 do\n      consumer Example2Consumer\n    end\n  end\nend\n</code></pre>"}, {"location": "Routing/#multiple-consumer-groups-mode", "title": "Multiple Consumer Groups Mode", "text": "<p>In this mode, Karafka will use a single consumer group per each of the topics defined within a single <code>#consumer_group</code> block.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    consumer_group :group_name do\n      topic :example do\n        consumer ExampleConsumer\n      end\n\n      topic :example2 do\n        consumer ExampleConsumer2\n      end\n    end\n\n    consumer_group :group_name2 do\n      topic :example3 do\n        consumer Example2Consumer3\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Routing/#multiple-subscription-groups-mode", "title": "Multiple Subscription Groups Mode", "text": "<p>Karafka uses a concept called <code>subscription groups</code> to organize topics into groups that can be subscribed to Kafka together. This aims to preserve resources to achieve as few connections to Kafka as possible.</p> <p>Each subscription group connection operates independently in a separate background thread. They do, however, share the workers pool for processing.</p> <p>All the subscription groups define within a single consumer group will operate within the same consumer group.</p> <p>Pro Subscription Group Multiplexing</p> <p>Karafka's Open Source version supports one connection per subscription group without ability to subscribe to the same topic multiple times from the same process. If you want to establish many connections to the same topic from a single process, upgrade to Karafka Pro. Multiplexing allows multiple connections to the same topic within a single subscription group, enhancing performance and parallel processing.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    subscription_group 'a' do\n      topic :A do\n        consumer ConsumerA\n      end\n\n      topic :B do\n        consumer ConsumerB\n      end\n\n      topic :D do\n        consumer ConsumerD\n      end\n    end\n\n    subscription_group 'b' do\n      topic :C do\n        consumer ConsumerC\n      end\n    end\n  end\nend\n</code></pre> <p>You can read more about the concurrency implications of using subscription groups here.</p>"}, {"location": "Routing/#subscription-group-multiplexing", "title": "Subscription Group Multiplexing", "text": "<p>For those using the advanced options in Karafka Pro, we have a special page dedicated to the Multiplexing feature. Multiplexing allows you to establish multiple independent connections to Kafka to subscribe to one topic from a single process. This detailed resource covers everything you need to know about how Multiplexing works, how to set it up, and tips for using it effectively. To learn all about this feature and make the most of it, please check out the Multiplexing documentation.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    # Always establish two independent connections to this topic from every\n    # single process. They will be able to poll and process data independently\n    subscription_group 'a', multiplex: 2 do\n      topic :A do\n        consumer ConsumerA\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Routing/#routing-patterns", "title": "Routing Patterns", "text": "<p>For users leveraging the advanced capabilities of Karafka Pro, the Routing Patterns feature has its dedicated documentation page. This page delves deep into the behavior, configuration, and best practices surrounding Routing Patterns. Please refer to the Routing Patterns documentation to explore this feature in detail and gain comprehensive insights.</p>"}, {"location": "Routing/#overriding-defaults", "title": "Overriding Defaults", "text": "<p>Almost all the default settings configured can be changed on either on the <code>topic</code> level. This means that you can provide each topic with some details in case you need a non-standard way of doing things (for example, you need batch consuming only for a single topic).</p>"}, {"location": "Routing/#shared-defaults", "title": "Shared Defaults", "text": "<p>This option allows you to define default settings that apply to all the topics defined in your routing unless those are defined explicitely when describing the appropriate topic. This not only simplifies configurations but also ensures consistency throughout your application.</p> <p>Here's how you can set up routing defaults and then define a topic that overrides one of those defaults:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    defaults do\n      config(\n        # Ensure there are always 5 partitions by default\n        partitions: 5,\n        # Make sure that topic is replicated in production\n        replication_factor: Rails.env.production? ? 2 : 1\n      )\n    end\n\n    topic :A do\n      consumer ConsumerA\n      # When overwriting defaults, all settings need to be\n      # redefined for a given method. Partial redefinition\n      # is not allowed and will not work\n      config(\n        partitions: 2,\n        replication_factor: Rails.env.production? ? 2 : 1\n      )\n    end\n\n    topic :B do\n      consumer ConsumerB\n    end\n  end\nend\n</code></pre> <p>When you decide to override any default option for a topic within the <code>#topic</code> block, it's crucial to understand that you must set all the arguments for that particular option. Partial updating of arguments is not supported.</p> <p>Karafka will not use the user-specified defaults you've defined in the defaults block if you attempt to update the arguments for an option partially. Instead, it will revert to the framework's internal defaults for the missing arguments. This could lead to unexpected behavior in your application if not considered.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    defaults do\n      config(\n        # Ensure there are always 5 partitions by default\n        partitions: 5,\n        # Make sure that topic is replicated in production\n        replication_factor: Rails.env.production? ? 2 : 1\n      )\n    end\n\n    topic :A do\n      consumer ConsumerA\n\n      # BAD idea because `replication_factor` is going to be set\n      # to `1` as it is the framework default\n      config(partitions: 2)\n    end\n  end\nend\n</code></pre>"}, {"location": "Routing/#topic-level-options", "title": "Topic Level Options", "text": "<p>There are several options you can set inside of the <code>topic</code> block. All of them except <code>consumer</code> are optional. Here are the most important ones:</p> Option Value type Description active Boolean Set to <code>false</code> if you want to have the given topic defined but not consumed. Helpful when working with topics via admin API consumer Class Name of a consumer class that we want to use to consume messages from a given topic deserializers Hash Names of deserializers that we want to use to deserializes the incoming data (payload, key and headers) manual_offset_management Boolean Should Karafka automatically mark messages as consumed or not long_running_job Boolean Converts this topic consumer into a job that can run longer than <code>max.poll.interval.ms</code> virtual_partitions Hash Allows you to parallelize the processing of data from a single partition. dead_letter_queue Hash Provides a systematic way of dealing with persistent consumption errors. delay_by Integer Feature that enables delaying message processing from specific topics for a specified time. expire_in Integer Feature that allows messages to be excluded from processing automatically in case they are too old. filter <code>#call</code> Feature that allows users to filter messages based on specific criteria. config Hash Allows for specifying each of the topic settings and their creation via the CLI commands kafka Hash Allows you to configure alternative cluster on a per-topic basis for a multi-cluster setup <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    consumer_group :videos_consumer do\n      topic :binary_video_details do\n        config(partitions: 2)\n        consumer Videos::DetailsConsumer\n        deserializers(\n          payload: Serialization::Binary::Deserializer.new\n        )\n      end\n\n      topic :new_videos do\n        config(partitions: 5, replication_factor: 4)\n        consumer Videos::NewVideosConsumer\n      end\n    end\n\n    topic :events do\n      config(partitions: 1, 'cleanup.policy': 'compact')\n      # Set to false because not for consumption.\n      # Only inspection via admin API.\n      active false\n      deserializers(\n        payload: EventsDeserializer.new\n      )\n    end\n  end\nend\n</code></pre>"}, {"location": "Routing/#kafka-scope-configuration-reuse", "title": "Kafka Scope Configuration Reuse", "text": "<p>Karafka uses the <code>inherit</code> flag to support partial Kafka routing reconfiguration at the topic level. This allows you to maintain a consistent base configuration while applying specific alterations to individual topics. When the inherit flag is <code>true</code>, the topic's Kafka settings will merge with the root-level defaults, enabling more granular and flexible configurations without redefining all settings.</p> <p>This feature is handy in scenarios where most settings remain consistent across topics, but a few need to be customized. By leveraging the inherit option, you can streamline your configurations, reduce redundancy, and ensure that only the necessary changes are applied on a per-topic basis.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      'auto.offset.reset': 'earliest'\n    }\n  end\n\n  routes.draw do\n    # Topic that inherits base kafka settings and adds a specific one\n    topic :example1 do\n      consumer ExampleConsumer1\n      kafka(\n        'enable.partition.eof': true,\n        inherit: true\n      )\n    end\n\n    # Topic with its own kafka settings without inheritance\n    topic :example2 do\n      consumer ExampleConsumer2\n      kafka(\n        'bootstrap.servers': '127.0.0.1:9092',\n        'enable.partition.eof': true\n      )\n    end\n\n    # Another topic that inherits base kafka settings and adds a specific one\n    topic :example3 do\n      consumer ExampleConsumer3\n      kafka(\n        'fetch.message.max.bytes': 10_000_000,\n        inherit: true\n      )\n    end\n  end\nend\n</code></pre> <p>Kafka Scope Config Reuse Without <code>inherit</code></p> <p>When using Kafka scope configuration at the topic level, be aware that without the <code>inherit</code> setting, there is no automatic inheritance of Kafka settings from the root-level configuration. This means you must duplicate all Kafka scope configurations for each topic. Failure to do so can result in default settings being applied instead of the intended configuration, which might lead to unexpected behavior or inconsistent setup across your topics.</p>"}, {"location": "Routing/#modular-monolith-approach", "title": "Modular Monolith Approach", "text": "<p>A Modular Monolith architecture focuses on separating a monolith application into well-defined, loosely coupled modules. These modules can evolve and scale independently but still operate as part of a single unit. With Karafka, embracing this architecture becomes efficient due to its flexible routing mechanism.</p> <p>One of Karafka's routing beauties is the ability to call the #draw method multiple times. In a Modular Monolith architecture context, each of your application's modules can define its own set of topic routes.</p> <ul> <li> <p>Decoupling: Each module can define and manage its message routing without interfering with others.</p> </li> <li> <p>Scalability: As modules grow, they can independently evolve their messaging strategies.</p> </li> <li> <p>Maintainability: Changes to routing in one module won't impact others, making it easier to manage and refactor.</p> </li> </ul> <p>Within each module, you can define a Karafka routing block using the #draw method:</p> <pre><code># app/modules/orders/karafka_routes.rb\nKarafka.routing.draw do\n  topic :order_created do\n    consumer Orders::OrderCreatedConsumer\n  end\n\n  topic :order_updated do\n    consumer Orders::OrderUpdatedConsumer\n  end\nend\n</code></pre> <pre><code># app/modules/users/karafka_routes.rb\nKarafka.routing.draw do\n  topic :user_registered do\n    consumer Users::UserRegisteredConsumer\n  end\nend\n</code></pre> <p>By leveraging the ability to draw routes multiple times, Karafka seamlessly fits into a Modular Monolith architecture. This allows for improved code organization, easier maintenance, and the flexibility to evolve each module independently.</p> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "SBOM/", "title": "Software Bill of Materials (SBOM)", "text": "<p>This page presents the Software Bill of Materials (SBOM) for Karafka and its runtime dependencies. An SBOM is a comprehensive inventory that details the components, libraries, and software packages utilized in a software product. It plays a crucial role in understanding the software's composition, enhancing transparency, and bolstering security by identifying potential vulnerabilities.</p> <p>This page exists because of our commitment to security, compliance, and transparency. It serves as a resource for users and developers to understand the external dependencies that Karafka relies on during operation.</p> <p>Runtime Dependencies in the Karafka SBOM</p> <p>This SBOM explicitly contains only the Karafka ecosystem's runtime dependencies. This document does not include development and test dependencies, which are crucial during the build and testing phases but are not required for the software's operation.</p> <p>Version-Specific SBOM Details</p> <p>This SBOM reflects the components used in the most recent versions of all ecosystem components within Karafka. It is important to note that older versions may have different dependencies.</p> <p>License Variability in OSS Dependencies</p> <p>Please be aware that the license status of the dependencies within the Karafka ecosystem may change over time due to the dynamic nature of open-source software (OSS) and dependency management. While we strive to keep this SBOM as accurate and up-to-date as possible, it represents a best-effort snapshot. For those seeking to construct a comprehensive and current SBOM for their projects, incorporating all dependencies accurately, we recommend utilizing tools such as Mend.io. Mend.io can help automate the creation of a detailed SBOM, ensuring it reflects the complete state of your target software, including any license changes in its dependencies.</p>"}, {"location": "SBOM/#karafka-subcomponents", "title": "karafka + subcomponents", "text": "Software Version License Copyrights        karafka             2.0+ (without Pro enhancements)      LGPL-3.0-only       or       Commercial Maciej Mensfeld        karafka pro             2.0+ (Pro enhancements)      Commercial Maciej Mensfeld        karafka-core             2.0+      MIT Maciej Mensfeld        waterdrop             All      LGPL-3.0-only       or       Commercial Maciej Mensfeld        zeitwerk             All      MIT Xavier Noria        karafka-web             All (without Pro enhancements)      LGPL-3.0-only Maciej Mensfeld        karafka-web pro             All (Pro enhancements)      Commercial Maciej Mensfeld        e-ruby             All      MIT Jeremy Evans        roda             All      MIT Jeremy Evans        tilt             All      MIT Jeremy Evans        fugit             All      MIT John Mettraux        et-orbi             All      MIT John Mettraux        raabro             All      MIT John Mettraux        tailwindcss             3.4.7      MIT Tailwind Labs, Inc.        heroicons             N/A      MIT Tailwind Labs, Inc.        daisyUI             4.12.10      MIT Pouya Saadeghi        turbo             3.4.7      MIT 37signals LLC        air datepicker             3.4.0      MIT Timofey Marochkin        highlight.js + embedded themes             11.7.0      BSD-3-Clause Ivan Sagalaev        chart.js             4.1.1      MIT Chart.js Contributors        color             0.3.0      MIT Jukka Kurkela        timeago.js             4.0.2      MIT Hust.cc"}, {"location": "SBOM/#rdkafka-ruby-subcomponents", "title": "rdkafka-ruby + subcomponents", "text": "Software Version License Copyrights        rdkafka / rdkafka-ruby             All      MIT Maciej Mensfeld + project contributors        ffi             All      BSD-3-Clause Ruby FFI project contributors        mini_portile2             All      MIT        Luis Lavena and Mike Dalessio             rake             All      MIT Jim Weirich"}, {"location": "SBOM/#librdkafka-subcomponents", "title": "librdkafka + subcomponents", "text": "Software Version License Copyrights        librdkafka      2.8.0 BSD-2-Clause Confluent Inc.        cJSON      1.7.14 MIT Dave Gamble and cJSON contributors      crc32c           1.1      Zlib        Mark Adler             rdfnv1a             N/A      Public Domain      Landon Curt Noll             rdhdrhistogram             N/A      MIT        Coda Hale             murmur2             N/A      Public Domain        Austin Appleby             pycrc / rdcrc32             0.7.10      MIT        Thomas Pircher             queue             8.5      BSD        The Regents of the University of California             regexp             N/A      Public Domain        Tor Andersson             snappy             1.1.0      BSD-3-Clause        Intel Corporation             tinycthread             1.2      Zlib        Evan Nemerson             wingetopt             N/A      ISC        The NetBSD Foundation      OpenSSL 3.0.16 Apache-2.0 The OpenSSL Project Cyrus SASL 2.1.28 BSD-4-Clause-UC Carnegie Mellon University MIT Kerberos (krb5) 1.21.3 MIT        Massachusetts Institute of Technology      zlib 1.3.1 Zlib Jean-loup Gailly and Mark Adler Zstandard (zstd) 1.5.7 BSD-3-Clause OR GPL-2.0 Meta Platforms, Inc. and affiliates <p>Last modified: 2025-06-25 13:04:56</p>"}, {"location": "Signals-and-States/", "title": "Signals and States", "text": ""}, {"location": "Signals-and-States/#signals", "title": "Signals", "text": "<p>Karafka responds to a few signals. On a Unix machine, you can use the <code>kill</code> binary or the <code>Process.kill</code> API in Ruby, e.g.</p> <pre><code>kill -TTIN pid\nProcess.kill(\"TTIN\", pid)\n</code></pre>"}, {"location": "Signals-and-States/#ttin", "title": "TTIN", "text": "<p>Karafka will respond to <code>TTIN</code> by printing backtraces for all threads to the logger.  This is useful for debugging if you have a Karafka process that appears dead or stuck.</p> <pre><code>Thread TID-c70x\nprocessing/jobs_queue.rb:64:in `pop'\nprocessing/jobs_queue.rb:64:in `pop'\nprocessing/worker.rb:47:in `process'\nprocessing/worker.rb:37:in `block in call'\n...\nThread TID-c72h\ninstrumentation/logger_listener.rb:83:in `backtrace'\ninstrumentation/logger_listener.rb:83:in `block in on_process_notice_signal'\ninstrumentation/logger_listener.rb:77:in `each'\ninstrumentation/logger_listener.rb:77:in `on_process_notice_signal'\n...\n</code></pre> <p>You need to have the <code>LoggerListener</code> enabled for this signal to print. It is enabled by default, so this signal should work out of the box unless you altered that.</p>"}, {"location": "Signals-and-States/#tstp", "title": "TSTP", "text": "<p><code>TSTP</code> tells Karafka process to \"quiet\" as it will shut down shortly. It will stop processing new messages but continue working on current jobs and will not unsubscribe from the topics and partitions it owns. If a given process gets new topics or partitions assigned during this phase, they will not be processed.</p> <p>Using <code>TSTP</code> allows you to gracefully finish all the work and shut down without causing several rebalances in case you would be stopping many consumer processes.</p> <p>Use <code>TSTP</code> + <code>TERM</code> to guarantee shut down within a period. The best practice is sending <code>TSTP</code> at the start of deployment and <code>TERM</code> at the end.</p> <p>You still need to send <code>TERM</code> to exit the Karafka process.</p>"}, {"location": "Signals-and-States/#term-and-quit", "title": "TERM and QUIT", "text": "<p>Send <code>TERM</code> or <code>QUIT</code> signal to a Karafka process to shut it down. It will stop accepting new work but continue working on current messages.  Workers who do not finish within the <code>shutdown_timeout</code> are forcefully terminated.</p>"}, {"location": "Signals-and-States/#states", "title": "States", "text": "<p>The Karafka process can be in a few states during its lifecycle, and each has a separate meaning and indicates different things happening internally.</p> <ul> <li><code>initializing</code> - The initial state of the application before configuration or routes are loaded.</li> <li><code>initialized</code> - The process is configured in this state but has yet to start listeners and workers.</li> <li><code>running</code> - The process started Kafka clients and is polling data.</li> <li><code>quieting</code> - The process received the <code>TSTP</code> signal and is finishing the current work.</li> <li><code>quiet</code> - The process no longer processes work and will keep running in quiet mode. </li> <li><code>stopping</code> - The process is finishing current work, no longer accepting more, and shutting down.</li> <li><code>stopped</code> - The process finished everything and closed all the Kafka connections.</li> <li><code>terminated</code> - The process is going to exit shortly.</li> </ul> <p>Last modified: 2025-05-16 21:07:08</p>"}, {"location": "Support/", "title": "OSS Support Policy", "text": "<p>No Direct Contact for OSS Support</p> <p>Do NOT contact Karafka or rdkafka gem maintainers directly via email, personal messages, or other private channels for OSS support reasons. Use GitHub Issues for bug reports and feature requests, or join the Slack community for general questions and discussions. Direct contact attempts will be ignored and not responded to.</p> <p>The 5-Minute Support Promise</p> <p>I have a simple 5-minute rule for OSS support: If I can reply and provide solutions within 5 minutes, I will always help without requiring payment or subscription. This applies to quick troubleshooting and guidance within my OSS support scope. My goal is to make the Karafka experience as smooth as possible while respecting everyone's time and resources.</p>"}, {"location": "Support/#terms-and-conditions", "title": "Terms And Conditions", "text": "<p>Karafka's Open Source Software (OSS) support primarily revolves around assisting users with issues related to the Karafka and librdkafka ecosystems. This involves troubleshooting and providing solutions for problems originating from Karafka or its related subcomponents.</p> <p>However, it is crucial to understand that the OSS support does not extend to application-specific issues that do not originate from Karafka or its related parts. This includes but is not limited to:</p> <ol> <li>Incorrect application configurations unrelated to Karafka.</li> <li>Conflicts with other libraries or frameworks within your application.</li> <li>Deployment issues on specific infrastructure or platforms.</li> <li>Application-specific runtime errors.</li> <li>Problems caused by third-party plugins or extensions.</li> <li>Data issues within your application.</li> <li>Issues related to application performance optimization.</li> <li>Integration problems with other services or databases.</li> <li>Design and architecture questions about your specific application.</li> <li>Language-specific issues are unrelated to Karafka or librdkafka.</li> <li>Issues related to usage of outdated ecosystem libraries versions.</li> </ol> <p>We acknowledge that understanding your specific applications and their configuration is essential, but due to the time and resource demands, this goes beyond the scope of our OSS support.</p> <p>No ETAs or Timelines in OSS</p> <p>Unless a specific timeline or ETA is explicitly provided for a bug fix or feature, there won't be one. Please do not ask for timelines unless I've already mentioned them. As an open-source project, Karafka operates without contractual commitments - development happens when possible, balanced with other responsibilities.</p> <p>If a bug affects your business operations and you need prioritized support, please consider supporting Karafka by upgrading to the Pro offering. This allows me to prioritize issues differently and consider your business needs.</p> <p>Enhance Your Karafka Experience with Pro Support</p> <p>For users seeking assistance with application-specific issues, we offer a Pro version of Karafka. This subscription provides comprehensive support, including help with application-specific problems.</p> <p>For more information about our Pro offering, please visit this page.</p>"}, {"location": "Support/#issue-reporting-guide", "title": "Issue Reporting Guide", "text": "<p>When reporting an issue within the Karafka ecosystem, providing detailed information is crucial for diagnosing and resolving the problem efficiently. </p> <p>Complete Information Required</p> <p>Failing to provide the below information may result in the issue being closed without assessment.</p> <p>Please include as many of the following details as possible to help me understand and address the issue:</p> Detail Description Karafka Info Full result of running your project's <code>bundle exec karafka info</code> command. <code>karafka.rb</code> Content Provide your entire <code>karafka.rb</code> file, excluding sensitive details. To properly assess most issues, including all details in the <code>karafka.rb</code> file is essential. Ecosystem Components Details Provide versions of any Karafka ecosystem components you are using, such as <code>karafka-core</code>, <code>waterdrop</code>, <code>karafka-rdkafka</code>, and <code>karafka-web</code>. Operating System Your operating system and its version (e.g., <code>uname -a</code> on Unix-based systems or the Windows version). Kafka Version The version of Apache Kafka you are connecting to. Configuration Details Relevant configuration details from your <code>karafka.rb</code> and other configuration files. Error Messages The exact error messages or full stack traces you encounter. Debug Logs All debug logs including debug logs from librdkafka. Reproduction Steps A clear and concise set of steps to reproduce the issue. Application Dependencies A list of gems and their versions that might be relevant (e.g., from your <code>Gemfile.lock</code>). Description of the Issue A detailed description of the problem, including what you expected to happen and what happened. Network Configuration (if applicable) Details about your network setup if you suspect network-related issues (e.g., firewall rules, proxies). Kafka Vendor and Deployment Details Specify the Kafka vendor you use (e.g., Confluent, MSK, self-hosted, etc.). Additionally, indicate whether the Karafka processes are running in the same availability zone as your Kafka brokers. Installation Method How you installed Karafka or its components (e.g., using Bundler, manually from source). Environment The environment in which the errors occur (e.g., development, production). Kafka Cluster Setup Information about your Kafka cluster setup (e.g., single-node, multi-node, cloud provider). Consumer Group Configuration Details about your consumer group configuration (e.g., number of consumers, partition assignments). Producer Configuration Configuration details for any Kafka producers you use. Additional Context Any other relevant context or information that might help in diagnosing the issue."}, {"location": "Support/#karafka-api-end-user-api-definition", "title": "Karafka API End-User API Definition", "text": "<p>The Karafka framework offers a range of functionalities to streamline building event-driven applications. Understanding what constitutes a Karafka API is essential for developers leveraging its capabilities. While Karafka provides various public methods and interfaces, it's crucial to discern which ones are intended for direct use by end users.</p>"}, {"location": "Support/#public-methods-and-internal-usage", "title": "Public Methods and Internal Usage", "text": "<p>Not all public methods within the Karafka codebase are meant for direct user consumption. Some methods are publicly accessible but primarily intended for internal use by various components of the Karafka framework itself. This distinction arises due to the complex nature of building a framework or an ecosystem of gems, where numerous moving parts require public interfaces for framework developers rather than for end users.</p>"}, {"location": "Support/#official-end-user-api", "title": "Official End-User API", "text": "<p>Karafka delineates its official end-user API in its documentation to provide clarity and stability, available at karafka.io/docs. This documentation outlines the recommended practices, configurations, and interfaces for developers building applications with Karafka. Any methods or interfaces not explicitly documented as part of this public API should be considered subject to change without prior notice.</p> <p>Last modified: 2025-06-03 17:32:06</p>"}, {"location": "Swarm-Multi-Process/", "title": "Swarm / Multi-Process Mode", "text": ""}, {"location": "Swarm-Multi-Process/#introduction", "title": "Introduction", "text": "<p>Karafka's Swarm Mode allows for the efficient CPU-intensive processing of Kafka messages in Ruby. It circumvents Ruby's Global Interpreter Lock (GIL) limitations by utilizing a multi-process architecture, allowing for parallel execution similar to libraries like Puma and Sidekiq Enterprise. This mode is particularly beneficial for CPU-intensive workloads, leveraging Ruby's Copy-On-Write (CoW) feature for memory efficiency. While Karafka's multi-threading excels in I/O-bound tasks, Swarm Mode offers a superior alternative for tasks demanding significant CPU resources, providing scalable and high-performance message processing capabilities.</p>"}, {"location": "Swarm-Multi-Process/#overview-of-karafka-swarm-mode", "title": "Overview of Karafka Swarm Mode", "text": "<p>Karafka's Swarm Mode is based on the \"Supervisor-Worker\" architectural pattern. It utilizes a controller process for supervision and multiple independent child processes for parallel execution. This setup enhances Kafka message processing for CPU-intensive tasks by leveraging multi-process execution. Each child process periodically reports its status to the supervisory master, ensuring system health and efficiency.</p> <p> </p> <p> *This example illustrates the relationship between the supervisor and the nodes.    </p>"}, {"location": "Swarm-Multi-Process/#benefits-of-using-swarm-mode", "title": "Benefits of using Swarm Mode", "text": "<p>Swarm Mode in Karafka brings several advantages, especially for applications dealing with high volumes of data or requiring intensive computation. Here are some benefits that highlight the importance and efficiency of using Swarm Mode:</p> <ul> <li> <p>Improved CPU Utilization: By running multiple processes, Swarm Mode can fully utilize multi-core systems, ensuring CPU resources are maximally exploited for increased processing power.</p> </li> <li> <p>Scalability: Easily scales your application to handle more load by increasing the number of worker processes, allowing for flexible adaptation to varying workloads.</p> </li> <li> <p>Concurrency: Achieves true concurrency in Ruby applications, sidestepping the Global Interpreter Lock (GIL) limitations using multiple processes instead of threads.</p> </li> <li> <p>Efficiency in CPU-bound Tasks: Ideal for CPU-intensive operations, as it allows for parallel execution of tasks that would otherwise be bottlenecked by single-threaded execution.</p> </li> <li> <p>Memory Efficiency: Leverages Ruby's Copy-On-Write (CoW) feature, which means the memory footprint increases only marginally with each additional processing node, making it memory efficient.</p> </li> <li> <p>Robustness and Isolation: Each worker process is isolated; a failure in one worker does not directly impact others, enhancing the overall robustness of the application.</p> </li> <li> <p>Load Distribution: Allows for efficient workload distribution across multiple processes, ensuring that no single process becomes a bottleneck, leading to more consistent performance.</p> </li> <li> <p>Enhanced Fault Tolerance: The supervisor process can monitor worker health, automatically restarting failed workers, thus ensuring the system remains resilient to individual process failures.</p> </li> <li> <p>Flexible Workload Management: The ability to fine-tune the system based on specific workload requirements, optimizing performance through configuration adjustments without changing the application code.</p> </li> <li> <p>Simplified Complex Processing: Facilitates the management of complex processing pipelines by distributing tasks across nodes, making it easier to reason about and maintain large-scale processing logic.</p> </li> <li> <p>Proactive Memory Management: In Karafka Pro, the supervisor can monitor and control the memory usage of child processes. This feature allows it to shut down workers exceeding a specified memory threshold gracefully. While this is not a substitute for addressing memory leaks within the application, it is a crucial interim measure to manage and mitigate potential memory-related issues. You can read more about this capability here.</p> </li> </ul>"}, {"location": "Swarm-Multi-Process/#supported-operating-systems", "title": "Supported Operating Systems", "text": "<p>When leveraging Karafka's Swarm Mode for your application, it's crucial to understand the operating system compatibility, mainly due to specific API dependencies. Swarm Mode's efficient process management and supervision rely heavily on the Pidfd API, a feature that is inherently tied to the Linux operating system.</p> <p>The Pidfd API provides a more reliable mechanism for process management by allowing the creation of process file descriptors. This feature significantly enhances the ability of the supervisor process in Swarm Mode to monitor, control, and manage worker processes without the typical race conditions associated with traditional PID-based management.</p> <p>However, using the Pidfd API comes with a specific requirement: it is only available on Linux operating systems with a kernel version that supports Pidfd. To utilize Karafka's Swarm Mode, your Linux system must run a kernel version that supports the Pidfd API. This functionality was introduced in the Linux kernel 5.3 and has seen gradual improvements and enhancements in subsequent releases. Therefore, ensuring your system operates on Linux kernel 5.3 or later is essential for leveraging all the benefits of Swarm Mode.</p> <p>Linux Kernel 5.3+ Requirement for Karafka Swarm Mode due to Pidfd API Dependency</p> <p>Karafka's Swarm Mode requires a Linux OS with kernel version 5.3 or later due to its reliance on the Pidfd API for advanced process management. This ensures efficient and reliable supervision of worker processes in Swarm Mode, exclusive to Linux environments compatible with Pidfd.</p>"}, {"location": "Swarm-Multi-Process/#getting-started-with-swarm-mode", "title": "Getting Started with Swarm Mode", "text": "<p>The startup command deviates slightly from the traditional server start command to activate Swarm Mode. Instead of starting your Karafka application with the usual <code>bundle exec karafka server</code>, you will invoke Swarm Mode using:</p> <pre><code>bundle exec karafka swarm\n</code></pre> <p>This command signals Karafka to initiate in Swarm Mode, creating a supervisor process and the forked nodes.</p>"}, {"location": "Swarm-Multi-Process/#cli-configuration-options", "title": "CLI Configuration Options", "text": "<p>When starting your application in Swarm Mode using bundle exec karafka swarm, it's important to note that this command accepts the same CLI configuration options as the standard bundle exec karafka server command. This compatibility ensures that you can apply the same level of control and customization to your Swarm Mode deployment as you would to a single-process setup.</p> <p>The CLI options allow for the efficient limitation of topics, subscription groups, and consumer groups that should operate within the Swarm. You can start a Swarm Mode instance that focuses on processing specific parts of your Kafka infrastructure. For example, you might want to isolate certain consumer groups or topics to dedicated swarms for performance reasons or to manage resource allocation more effectively.</p> <pre><code># Run swarm and include only given consumer groups\nbundle exec karafka swarm --include-consumer-groups group_name1,group_name3\n\n# Run swarm but ignore those two topics\nbundle exec karafka swarm --exclude-topics topic_name1,topic_name3\n</code></pre>"}, {"location": "Swarm-Multi-Process/#configuration-and-tuning", "title": "Configuration and Tuning", "text": "<p>Configuring and tuning your Karafka application for optimal performance in Swarm Mode involves setting up the correct number of worker processes (nodes) and optionally altering other settings. This setup is crucial for efficiently handling messages, especially in high-throughput or CPU-intensive environments.</p> <p>During your Karafka application setup, you can specify the Swarm Mode configuration as part of the setup block. This includes defining the number of worker processes that should be forked:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other settings...\n\n    # Configure the number of worker processes for Swarm Mode\n    config.swarm.nodes = 5\n  end\nend\n</code></pre> <p>In this example, <code>config.swarm.nodes = 5</code> instructs Karafka to operate in Swarm Mode with <code>5</code> worker processes, allowing for parallel message processing across multiple CPU cores.</p> <p>When deciding on the number of nodes (worker processes) to configure, consider the following guidelines:</p> <ul> <li> <p>CPU/Cores: Aim to align the number of worker processes with the number of CPU cores available. This ensures that each process can run on its core, maximizing parallelism and reducing context switching overhead. For example, if your server has 8 CPU cores, configuring <code>config.swarm.nodes = 8</code> might be optimal.</p> </li> <li> <p>Memory Availability: Worker processes initially share the same memory space (thanks to Ruby's Copy-On-Write mechanism) but will gradually consume more memory as they diverge in their execution paths. Ensure your system has enough memory to support the cumulative memory footprint of all worker processes. Monitor memory usage under load to find a balance that prevents swapping while maximizing utilization of available CPUs.</p> </li> <li> <p>Workload Characteristics: Consider the nature of your workload. CPU-bound tasks benefit from a process-per-core model, whereas I/O-bound tasks might not require as many processes as they often wait on external resources (e.g., network or disk).</p> </li> </ul> <p>Below, you can find a few tuning hints worth considering if you strive to achieve optimal performance:</p> <ul> <li> <p>Monitor and Adjust: Performance tuning is an iterative process. Monitor key metrics such as CPU utilization, memory consumption, and message processing latency. Adjust the number of worker processes based on observed performance and system constraints.</p> </li> <li> <p>Balance with Kafka Partitions: Ensure that the number of worker processes aligns with the number of partitions in your Kafka topics to avoid idle workers and ensure even load distribution.</p> </li> <li> <p>Consider System Overheads: Leave headroom for the operating system and other applications. Overcommitting resources to Karafka can lead to contention and degraded performance across the system.</p> </li> </ul> <p>By carefully configuring and tuning the number of worker processes with your system's CPU and memory resources and the specific demands of your workload, you can achieve a highly efficient and scalable Kafka processing environment with Karafka's Swarm Mode.</p>"}, {"location": "Swarm-Multi-Process/#static-group-membership-management", "title": "Static Group Membership Management", "text": "<p>Static Group Membership is an advanced feature that enhances the efficiency and reliability of consumer group management. This feature allows Kafka consumers to retain a stable membership within their consumer groups across sessions. By specifying a unique <code>group.instance.id</code> for each consumer, Kafka brokers can recognize individual consumers across disconnections and rebalances, reducing the overhead associated with consumer group rebalances and improving the overall consumption throughput.</p> <p>Swarm Mode supports Static Group Membership, seamlessly integrating this capability into its distributed architecture. This integration means that the <code>group.instance.id</code> configuration is preserved across multiple subscription groups and intelligently mapped onto the nodes within the swarm. This mapping ensures that each node maintains a unique identity, preventing conflicts that could arise from multiple nodes inadvertently sharing the same <code>group.instance.id</code>.</p> <p>When deploying your application in Swarm Mode, Karafka takes care of the underlying complexity associated with static group memberships. </p> <p>Here's how Karafka ensures consistency and efficiency in this process:</p> <ul> <li> <p>Automatic ID Management: Karafka automatically assigns and manages <code>group.instance.id</code> for each node within the swarm, ensuring that each node's identity is unique and consistent across sessions. This automatic management simplifies setup and reduces potential configuration errors that could lead to rebalance issues.</p> </li> <li> <p>Enhanced Stability: By utilizing static group memberships, Karafka minimizes the frequency of consumer group rebalances. This stability is particularly beneficial in environments where consumer groups handle high volumes of data, as it allows for uninterrupted data processing and maximizes throughput.</p> </li> <li> <p>Simplified Configuration: Developers need to specify their desired configuration once, and Karafka's Swarm Mode propagates these settings across the swarm. This simplification reduces the operational burden and allows teams to focus on developing the logic and functionality of their applications.</p> </li> <li> <p>Seamless Nodes Restarts: With Karafka Pro's enhanced monitoring, nodes can restart without causing consumer group rebalances, thanks to static group memberships. The <code>group.instance.id</code> remains constant across restarts, enabling swift recovery and reconnection to the consumer group. This ensures minimal processing disruption and maintains throughput, showcasing Karafka's fault-tolerant and efficient data-handling capability.</p> </li> </ul> <p>While Karafka handles the complexities of <code>group.instance.id</code> assignments behind the scenes, developers should know how static group memberships are configured within their applications. Here's an example snippet for reference:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092',\n      # Other configuration options...\n\n      # Set instance id to the hostname (assuming they are unique)\n      'group.instance.id': Socket.gethostname\n    }\n  end\nend\n</code></pre>"}, {"location": "Swarm-Multi-Process/#process-management-and-supervision", "title": "Process Management and Supervision", "text": "<p>In Swarm Mode, Karafka introduces a supervisor process responsible for forking child nodes and overseeing their operation. This section outlines the key aspects of process management and supervision within this architecture, covering supervision strategies, handling process failures, the limitations of forking, and efficiency through preloading.</p>"}, {"location": "Swarm-Multi-Process/#supervision-strategies", "title": "Supervision Strategies", "text": "<p>In Karafka's Swarm Mode, the supervision of child nodes is a critical component in ensuring the reliability and resilience of the message processing. By design, the supervisor process employs a proactive strategy to monitor the health of each child node. A vital aspect of this strategy involves each child node periodically reporting its health status to the supervisor, independent of its current processing activities.</p> <p>Child nodes are configured to send health signals to the supervisor every 10 seconds. This regular check-in is designed to occur seamlessly alongside ongoing message processing and data polling activities, ensuring that the supervision mechanism does not interrupt or degrade the performance of the message handling workflow. The health signal may include vital statistics and status indicators that allow the supervisor to assess whether a child node is functioning optimally or if intervention is required.</p> <p>The effectiveness of the health reporting mechanism is subject to a critical configuration parameter: <code>config.max_wait_time</code>. This setting determines the maximum time Karafka will wait for new data. If this value is set close to or more than <code>config.internal.swarm.node_report_timeout</code> (60 seconds by default), nodes may not have a chance to report their health frequently enough.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Other settings...\n\n    # If you want to have really long wait times\n    config.max_wait_time = 60_000\n\n    # Make sure node_report_timeout is aligned\n    config.internal.swarm.node_report_timeout = 120_000\n  end\nend\n</code></pre> <p>Health Checks vs. <code>max_wait_time</code></p> <p>In Swarm Mode, ensure <code>max_wait_time</code> doesn't exceed the <code>node_report_timeout</code> interval (default 60 seconds). Setting it too high could prevent nodes from reporting their health promptly, affecting system monitoring and stability.</p> <p>Two scenarios may necessitate shutting down a node:</p> <ol> <li> <p>Extended Health Reporting Delays: If a child node fails to report its health status within the expected timeframe, the supervisor interprets this as the node being unresponsive or \"hanging\". This situation triggers a protective mechanism to prevent potential system degradation or deadlock.</p> </li> <li> <p>Unhealthy Status Reports (Karafka Pro): With the enhanced capabilities of Karafka Pro, nodes possess the self-awareness to report unhealthy states. This can range from excessive memory consumption and prolonged processing times to polling mechanisms becoming unresponsive. Recognizing these reports, the supervisor takes decisive action to address the compromised node.</p> </li> </ol> <p>Upon identifying a node that requires a shutdown, either due to delayed health reports or self-reported unhealthy conditions, the supervisor initiates a two-step process:</p> <ul> <li> <p>TERM Signal: The supervisor first sends a TERM signal to the node, initiating a graceful shutdown sequence. This allows the node to complete its current tasks, release resources, and shut down properly, minimizing potential data loss or corruption.</p> </li> <li> <p>KILL Signal: If the node fails to shut down within the specified <code>shutdown_timeout</code> period, the supervisor escalates its intervention by sending a KILL signal. This forcefully terminates the node, ensuring the system can recover from the situation but at the risk of abrupt process termination.</p> </li> </ul> <p>Following a node's shutdown - graceful or forceful - Karafka imposes a mandatory delay of at least 5 seconds before attempting to restart the node. This deliberate pause serves a crucial purpose:</p> <ul> <li> <p>Preventing System Overload: By waiting before restarting a node, Karafka mitigates the risk of entering a rapid, endless loop of immediate process death and restart. Such scenarios can arise from external factors that instantaneously cause a newly spawned process to fail. The delay ensures that the system has a brief period to stabilize, assess the environment, and apply any necessary corrections before reintroducing the node into the swarm.</p> </li> <li> <p>System Health Preservation: This pause also provides a buffer for the overall system to manage resource reallocation, clear potential bottlenecks, and ensure that restarting the node is conducive to maintaining system health and processing efficiency.</p> </li> </ul> <p>Karafka's Swarm Mode ensures robust process management through these meticulous supervision strategies, promoting system stability and resilience.</p> <p> </p> <p> *Diagram illustrating flow of supervision of a swarm node.    </p>"}, {"location": "Swarm-Multi-Process/#limitations-of-forking", "title": "Limitations of Forking", "text": "<p>Despite its benefits for CPU-bound tasks, forking has several limitations, especially in the context of I/O-bound operations and broader system architecture:</p> <ul> <li> <p>Memory Overhead: Initially, forking benefits from Ruby's Copy-On-Write (CoW) mechanism, which keeps the memory footprint low. However, as child processes modify their memory, the shared memory advantage may diminish, potentially leading to increased memory usage.</p> </li> <li> <p>I/O Bound Operations: Forking is less effective for I/O-bound tasks because these tasks spend a significant portion of their time waiting on external resources (e.g., network or disk). In such cases, Karafka multi-threading and Virtual Partitions may be more efficient as they can handle multiple I/O operations in a single process.</p> </li> <li> <p>Database Connection Pooling: Each forked process requires its database connections. This can rapidly increase the number of database connections, potentially exhausting the available pool and leading to scalability issues.</p> </li> <li> <p>File Descriptor Limits: Forking multiple processes increases the number of open file descriptors, which can hit the system limit, affecting the application's ability to open new files or establish network connections.</p> </li> <li> <p>Startup Time: Forking can increase the application's startup time, especially if the initial process needs to load a significant amount of application logic or data into memory before forking.</p> </li> <li> <p>Debugging and Monitoring Complexity: Debugging issues or monitoring the performance of applications using the forking model can be more challenging due to multiple processes requiring more sophisticated tooling and approaches.</p> </li> <li> <p>Separate Kafka Connections: Each forked process in a Karafka application is treated by Kafka as an individual client. This means every process maintains its connection to Kafka, increasing the total number of connections to the Kafka cluster.</p> </li> <li> <p>Independent Partition Assignments: In Kafka, partitions of a topic are distributed among all consumers in a consumer group. With each forked process treated as a separate consumer, Kafka assigns partitions to each process independently. This behavior can lead to uneven workload distribution, especially if the number of processes significantly exceeds the number of partitions or if the partitioning does not align well with the data's processing requirements.</p> </li> <li> <p>Consumer Group Rebalancing: Kafka uses consumer groups to manage which consumers are responsible for which partitions. When processes are forked, each one is considered a new consumer, triggering consumer group rebalances. Frequent rebalances can lead to significant overhead, especially in dynamic environments where processes are regularly started or stopped. Rebalancing pauses message consumption, as the group must agree on the new partition assignment, leading to potential delays in message processing.</p> </li> </ul> <p>While forking enables Ruby applications like Karafka to parallelize work efficiently, these limitations highlight the importance of careful architectural consideration, especially when dealing with I/O-bound operations or scaling to handle high concurrency and resource utilization levels.</p> <p>Avoid Using Karafka APIs in the Supervisor Process</p> <p>It is essential to refrain from using the Karafka producer or any other Karafka APIs, including administrative APIs, within the supervisor process, both before and after forking. <code>librdkafka</code>, the underlying library used by Karafka, is fundamentally not fork-safe. Utilizing these APIs from the supervisor process can lead to unexpected behaviors and potentially destabilize your application. Always ensure that interactions with Karafka's APIs occur within the forked worker nodes, where it is safe and designed to operate.</p>"}, {"location": "Swarm-Multi-Process/#warmup-and-preloading-for-efficiency", "title": "Warmup and Preloading for Efficiency", "text": "<p>Preloading in the Swarm Mode, akin to Puma and Sidekiq Enterprise practices, offers substantial memory savings. By loading the application environment and dependencies before forking, Karafka can significantly reduce memory usage - 20-30% savings are common. However, this feature requires carefully managing resources inherited by child processes, such as file descriptors, network connections, and threads.</p> <p>When the supervisor process forks a worker, the child inherits open file descriptors and potentially other resources like network connections. Inherited resources can cause unexpected behavior or resource leaks if not properly managed. To address this, Karafka, similar to Puma, provides hooks that allow for resource cleanup and reinitialization before and after forking:</p> <p>Automatic Reconfiguration of Karafka Internals Post-Fork</p> <p>All Karafka internal components, including the Karafka producer, will be automatically reconfigured post-fork, eliminating the need for manual reinitialization of these elements. Focus solely on reconfiguring any external components or connections your application relies on, such as databases or APIs, as these are not automatically reset. This automatic reconfiguration ensures that Karafka internals are immediately ready for use in each forked process, streamlining multi-process management.</p> <pre><code># At the end of karafka.rb\n\n# This will run in each forked node right after it was forked\nKarafka.monitor.subscribe('swarm.node.after_fork') do\n  # Make sure to re-establish all connections to the DB after fork (if any)\n  ActiveRecord::Base.clear_active_connections!\nend\n</code></pre> <p>Below, you can find a few examples of resources worth preparing and cleaning before forking:</p> <ul> <li> <p>Database Connections: Re-establish database connections to prevent sharing connections between processes, which could lead to locking issues or other concurrency problems. For ActiveRecord, this typically involves calling ActiveRecord::Base.establish_connection.</p> </li> <li> <p>Thread Cleanup: If the preloaded application spawns threads, ensure they are stopped or re-initialized post-fork to avoid sharing thread execution contexts.</p> </li> <li> <p>Closing Unneeded File Descriptors: Explicitly close or reopen file descriptors that shouldn't be shared across processes to avoid leaks and ensure the independent operation of each process.</p> </li> </ul> <p>Given the potential complexities and dangers associated with preloading the entire application, Karafka makes this feature opt-in. This cautious approach enables developers to enable preloading when effectively managing the related resources.</p> <p>In addition to preloading for substantial memory savings, Karafka introduces a crucial phase known as \"Process Warmup\" right before the process starts. It occurs only in the supervisor. This phase is designed to optimize the application's readiness for forking and operation, leveraging Ruby 3.3's <code>Process.warmup</code> feature and similar techniques in previous Ruby versions. This method signals Ruby that the application has completed booting and is now ready for forking, triggering actions such as garbage collection and memory compaction to enhance the efficiency of Copy-On-Write (CoW) mechanics in child forks.</p> <p>Before the warmup phase, Karafka emits a <code>process.before_warmup</code> notification, providing an ideal opportunity to eager load the application code and perform other preparatory tasks. This hook is pivotal for loading any code that benefits from being loaded once and shared across forks, thereby reducing memory usage and startup time for each child process.</p> <pre><code># At the end of karafka.rb\n\nKarafka.monitor.subscribe('app.before_warmup') do\n  # Eager load the application code and other heavy resources here\n  # This code will run only once in supervisor before the final warmup\n  Rails.application.eager_load!\nend\n</code></pre> <p>The <code>Process.warmup</code> method and <code>process.before_warmup</code> hook collectively ensure that your Karafka application is optimized for multi-process environments. By utilizing these features, you can significantly reduce the memory footprint of your applications and streamline process management, especially in environments where memory efficiency and quick scaling are paramount.</p> <p>By carefully preparing for and executing the warmup phase, you ensure your Karafka applications operates optimally, benefiting from reduced memory usage and enhanced process initialization performance.</p> <p>Below, you can compare memory usage when running 3 independent processes versus Swarm with 3 nodes and a supervisor. In this case, Swarm uses around 50% less memory than independent processes.</p> <p> </p> <p> *Diagram illustrating memory usage with and without Swarm with the same number of consumers. Less is better.    </p> <p>If you want to measure gains within your particular application under given conditions, you can use the below script.</p> <p>This script calculates and displays memory usage details for processes matching a specified pattern (e.g., \"karafka\") on a Linux system, utilizing the <code>smem</code> tool. It retrieves the Proportional Set Size (PSS), Unique Set Size (USS), and Resident Set Size (RSS) for each process, which helps in understanding both individual and collective memory usage, including shared memory aspects.</p> <pre><code>#!/bin/bash\n\n# Get USS, PSS, and RSS values for processes matching the pattern\nmapfile -t MEMORY_VALUES &lt; &lt;( \\\n  smem -P karafka --no-header -c \"pid uss pss rss\" | sed '1d' \\\n)\n\n# Initialize totals\nTOTAL_USS=0\nTOTAL_PSS=0\nTOTAL_RSS=0\n\n# Print header for process memory usage\necho \"PID | RSS Memory (MB) | PSS Memory (MB)\"\n\n# Process each line\nfor LINE in \"${MEMORY_VALUES[@]}\"; do\n  # Read values into variables\n  read -r PID USS PSS RSS &lt;&lt;&lt;\"$LINE\"\n\n  TOTAL_USS=$((TOTAL_USS + USS))\n  TOTAL_PSS=$((TOTAL_PSS + PSS))\n  TOTAL_RSS=$((TOTAL_RSS + RSS))\n\n  # Convert RSS and PSS from kilobytes to megabytes for readability\n  RSS_MB=$(echo \"scale=2; $RSS / 1024\" | bc)\n  PSS_MB=$(echo \"scale=2; $PSS / 1024\" | bc)\n\n  # Print RSS and PSS memory usage for the current process\n  echo \"$PID | $RSS_MB MB | $PSS_MB MB\"\ndone\n\n# Convert total USS and PSS from kilobytes to megabytes\nTOTAL_USS_MB=$(echo \"scale=2; $TOTAL_USS / 1024\" | bc)\nTOTAL_PSS_MB=$(echo \"scale=2; $TOTAL_PSS / 1024\" | bc)\n\n# Calculate and print the percentage of shared memory\n# Shared memory percentage is calculated from the difference between PSS and USS\n# compared to the total PSS, indicating how much of the memory is shared\nif [ \"$TOTAL_PSS\" -gt 0 ]; then\n    SHARED_MEMORY_MB=$(echo \"scale=2; $TOTAL_PSS_MB - $TOTAL_USS_MB\" | bc)\n    SHARED_MEM_PCT=$(echo \"scale=2; ($SHARED_MEMORY_MB / $TOTAL_PSS_MB) * 100\" | bc)\n    echo \"Shared Memory Percentage: $SHARED_MEM_PCT%\"\nelse\n    echo \"PSS is zero, cannot calculate shared memory percentage.\"\nfi\n\necho \"Total Memory Used: $TOTAL_PSS_MB MB\"\n</code></pre> <p>When executed, you will get similar output:</p> <pre><code>./rss.sh\n\nPID    | RSS Memory (MB) | PSS Memory (MB)\n300119 | 329.12 MB       | 107.19 MB\n300177 | 355.17 MB       | 142.53 MB\n300159 | 355.25 MB       | 142.62 MB\n300166 | 356.16 MB       | 143.49 MB\n\nShared Memory Percentage: 51.00%\nTotal Memory Used: 535.85 MB\n</code></pre>"}, {"location": "Swarm-Multi-Process/#instrumentation-monitoring-and-logging", "title": "Instrumentation, Monitoring, and Logging", "text": "<p>Karafka's Swarm Mode's instrumentation, monitoring, and logging approach remains consistent with the standard mode (<code>bundle exec karafka server</code>), ensuring a seamless transition and maintenance experience. The Web UI is fully compatible with Swarm Mode and requires no additional configuration, providing out-of-the-box functionality for monitoring your applications.</p>"}, {"location": "Swarm-Multi-Process/#notification-events-in-swarm-mode", "title": "Notification Events in Swarm Mode", "text": "<p>Karafka introduces several event hooks specific to Swarm Mode, enhancing the observability and manageability of both the supervisor and forked nodes. These events allow for custom behavior and integration at different stages of the process lifecycle:</p> Event Location Description <code>app.before_warmup</code> Supervisor Runs code needed to be prepared before the warmup process. <code>swarm.node.after_fork</code> Forked Node Triggered after each node is forked, allowing for node-specific initialization. <code>swarm.manager.before_fork</code> Supervisor Occurs before each node is forked, enabling pre-fork preparation. <code>swarm.manager.after_fork</code> Supervisor Fires after each node fork, facilitating post-fork actions at the supervisor level. <code>swarm.manager.control</code> Supervisor Executed each time the supervisor checks the health of nodes, supporting ongoing supervision. <code>swarm.manager.stopping</code> Supervisor Indicates the supervisor is shutting down for reasons other than a full application shutdown. <code>swarm.manager.terminating</code> Supervisor Initiated when the supervisor decides to terminate an unresponsive node. <p>It is worth highlighting that the <code>swarm.manager.stopping</code> event includes a status value, providing insight into the reason behind a node's shutdown:</p> Status Code Description -1 Node did not report its health for an extended period and was considered hanging. 1 Node reported insufficient polling from Kafka (Pro only). 2 Consumer is consuming a batch longer than expected (Pro only). 3 Node exceeded the allocated memory limit (Pro only). <p>These statuses offer valuable diagnostics, enabling targeted interventions to maintain system health and performance.</p>"}, {"location": "Swarm-Multi-Process/#swarm-mode-in-kubernetes-clusters", "title": "Swarm Mode in Kubernetes Clusters", "text": "<p>When deploying a swarm within a Kubernetes cluster, it is recommended to use the swarm liveness listener to supervise only the supervisor process. This specialized liveness probe ensures that the Kubernetes orchestrator accurately reflects the Karafka supervisor's state, enhancing your deployment's reliability.</p> <p>Please refer to our Kubernetes documentation for more information on configuring the swarm liveness listener and other deployment considerations.</p>"}, {"location": "Swarm-Multi-Process/#signal-handling", "title": "Signal Handling", "text": "<p>Signal handling is designed to ensure seamless communication and control over both the supervisor's and their child's processes. This design allows for a centralized approach to managing process behavior in response to external signals.</p> <p>When a signal is sent to the supervisor process, Karafka ensures that this signal is propagated to all child processes. This mechanism is crucial for maintaining synchronized state changes across the entire application. For instance, if a SIGTERM (signal to terminate) is issued to the supervisor, it will pass this signal through to all children, will wait, and gracefully shut down.</p> <p>Conversely, signals can be sent directly to any child process to trigger specific actions, such as shutdown or backtrace printing. Importantly, interacting with a child process in this manner is isolated; it does not affect the supervisor or other child processes. This isolated signal handling allows for fine-grained control over individual processes within the swarm, enabling scenarios like:</p> <ul> <li> <p>Gracefully restarting a specific child process without disrupting the overall service.</p> </li> <li> <p>Requesting debug information or a backtrace from a single process for diagnostic purposes without impacting the performance or state of other processes.</p> </li> </ul>"}, {"location": "Swarm-Multi-Process/#behavior-on-shutdown", "title": "Behavior on Shutdown", "text": "<p>When the supervisor receives a request to shut down\u2014typically through a SIGTERM or similar signal\u2014it initiates a cascade of shutdown commands to all child nodes. This is the first step in a coordinated effort to terminate the entire application gracefully.</p> <p>After issuing the shutdown command to its child nodes, the supervisor enters a waiting state, allowing a specified period for all nodes to shut down gracefully. The <code>shutdown_timeout</code> configuration parameter defines this period. Each node is expected to complete any ongoing tasks, release resources, and terminate voluntarily during this time.</p> <p>If, after the <code>shutdown_timeout</code> period, any nodes have not shut down (indicating they are hanging or unable to complete their shutdown procedures), the supervisor takes a more forceful approach. It issues a KILL signal to all non-responsive child processes. This ensures that even in cases where some nodes are stuck or unable to terminate on their own, the system can still release all resources and fully shut down. This forceful termination step is crucial for preventing resource leaks and ensuring the system remains clean and ready for a potential restart or to end operation without impacting the underlying environment.</p> <p> </p> <p> *Diagram illustrating the shutdown flow and interactions between supervisor and child nodes.    </p>"}, {"location": "Swarm-Multi-Process/#web-ui-enhancements-for-swarm", "title": "Web UI Enhancements for Swarm", "text": "<p>Karafka's Web UI fully supports Swarm Mode, showcasing each forked node as an independent entity for detailed monitoring.</p> <p>In Swarm Mode, the Web UI distinguishes each node as a separate process. However, each swarm node is marked with a \"paid\" label, indicating the parent process's PID (the supervisor). This feature aids in identifying the relationship between nodes and their supervisor.</p> <p>Due to librdkafka's fork-safety limitations, the supervisor process does not appear directly in the Web UI because it cannot hold open connections to Kafka. However, The supervisor's presence is inferred through the PPID label of swarm nodes. Since all nodes share the same supervisor PID as their PPID, you can indirectly identify the supervisor process that way.</p> <p></p>"}, {"location": "Swarm-Multi-Process/#swarm-vs-multi-threading-and-virtual-partitions", "title": "Swarm vs. Multi-Threading and Virtual Partitions", "text": "<p>Karafka's scalability and workload management strategies include Swarm Mode, multi-threading, virtual partitions, and multiplexing. Each serves specific workload needs, facilitating optimal architectural decisions.</p> <ul> <li> <p>Swarm Mode excels in CPU-intensive environments by utilizing multiple processes for true parallel execution, enhancing CPU utilization and throughput. However, its scalability is tied to the number of Kafka partitions, limiting parallelization.</p> </li> <li> <p>Multi-threading is suited for I/O-bound tasks, efficiently managing wait times by concurrently handling tasks within a single process. Its scalability is constrained by Ruby's Global Interpreter Lock (GIL) and the number of Kafka partitions.</p> </li> <li> <p>Virtual Partitions increase Karafka's scalability beyond Kafka's partition limits by simulating additional partitions, improving load distribution and processing for I/O-bound tasks.</p> </li> <li> <p>Multiplexing allows a single process to subscribe to multiple topics or partitions with multiple connections, optimizing throughput without additional processes or threads. It's useful for high-partition topics but requires careful consumer instance management to maintain processing efficiency.</p> </li> </ul> <p>Each strategy offers unique advantages for Karafka application optimization. Swarm Mode and multi-threading address CPU-intensive and I/O-bound workloads, respectively, while virtual partitions and multiplexing overcome Kafka partition scalability limits. Selecting the appropriate strategy depends on your workload's characteristics and scalability goals.</p>"}, {"location": "Swarm-Multi-Process/#producer-full-reconfiguration", "title": "Producer Full Reconfiguration", "text": "<p>When operating in Swarm Mode, each forked node automatically inherits most of the producer configuration from the parent process. However, to ensure proper functionality and avoid potential conflicts, it's crucial to understand how to fully reconfigure producers post-fork.</p> <p>By default, while Karafka takes care of most internal reconfigurations automatically, there are cases where you might want to fully reconfigure the producer with your settings or modify specific configuration parameters. This is particularly important when dealing with producer-specific identifiers or when you need to customize the producer behavior for different nodes.</p> <p>Here's an example of how to fully reconfigure a producer after forking:</p> <pre><code># In your karafka.rb after all other setup\n\nKarafka.monitor.subscribe('swarm.node.after_fork') do\n  # Create a fresh producer instance that inherits all the core configuration\n  Karafka::App.config.producer = ::WaterDrop::Producer.new do |p_config|\n    p_config.logger = Karafka::App.config.logger\n    # Copy and map all the Kafka configuration or replace with your own\n    p_config.kafka = ::Karafka::Setup::AttributesMap.producer(Karafka::App.config.kafka.dup)\n\n    # Add any node-specific configurations here\n    p_config.kafka[:'client.id'] = \"#{Karafka::App.config.client_id}-#{Process.pid}\"\n  end\nend\n</code></pre> <p>This approach ensures that:</p> <ol> <li>Each node gets a fresh producer instance</li> <li>All core configurations are properly configured</li> <li>Node-specific settings can be customized as needed</li> <li>The producer is properly initialized within the forked process context</li> </ol> <p>Note that complete reconfiguration is significant when:</p> <ol> <li>You need to set node-specific client IDs</li> <li>You're using transactional producers (which require unique transactional IDs)</li> <li>You want to customize producer behavior based on node characteristics</li> <li>You need to modify connection or authentication settings per node</li> </ol>"}, {"location": "Swarm-Multi-Process/#transactional-producer-handling-in-swarm-mode", "title": "Transactional Producer Handling in Swarm Mode", "text": "<p>When operating in Swarm Mode, each forked node inherits the Karafka producer configuration from the parent process. This inheritance mechanism helps maintain consistency across nodes while reducing configuration overhead. However, special consideration is needed when working with transactional producers, as certain configuration parameters require unique values per node to ensure proper operation.</p> <p>The transactional.id configuration parameter requires special attention in Swarm Mode. While other configuration parameters can be safely inherited, using the same <code>transactional.id</code> across multiple producer instances will cause transaction fencing issues. When a producer instance begins a transaction, it receives an epoch number from the transaction coordinator. If another producer instance with the same <code>transactional.id</code> starts a transaction, it will receive a higher epoch number and fence off (invalidate) the previous producer instance.</p> <p>To prevent fencing issues, you must ensure each node's producer has a unique <code>transactional.id</code>. Here's an example of how to properly configure transactional producers in Swarm Mode:</p> <pre><code># In your karafka.rb after all other setup\n\nKarafka.monitor.subscribe('swarm.node.after_fork') do\n  # Assign fresh instance that takes all the configuration details except the transactional.id\n  Karafka::App.config.producer = ::WaterDrop::Producer.new do |p_config|\n    p_config.logger = Karafka::App.config.logger\n\n    p_config.kafka = ::Karafka::Setup::AttributesMap.producer(Karafka::App.config.kafka.dup)\n    # Use a unique transactional.id and do not re-use the parent one\n    p_config.kafka[:'transactional.id'] = SecureRandom.uuid\n  end\nend\n</code></pre> <p>This approach will ensure that:</p> <ol> <li>Each node gets a unique <code>transactional.id</code></li> <li>Transactions from different nodes won't interfere with each other</li> <li>The producer can maintain proper transaction isolation and exactly once semantics</li> </ol> <p>Transactional ID Conflicts</p> <p>Failing to set unique <code>transactional.id</code> values for each node will result in producers fencing each other off, leading to failed transactions and potential data consistency issues. Always ensure each node's producer has a unique transactional ID.</p>"}, {"location": "Swarm-Multi-Process/#example-use-cases", "title": "Example Use Cases", "text": "<p>Here are some use cases from various industries where Karafka's Swarm Mode can be beneficial:</p> <ul> <li> <p>Background Job Processing: Ruby on Rails applications frequently rely on background jobs for tasks that are too time-consuming to be processed during a web request. Examples include sending batch emails, generating reports, or processing uploaded files. Swarm Mode can distribute these jobs across multiple processes, significantly reducing processing time by leveraging all available CPU cores.</p> </li> <li> <p>Image Processing and Generation: Many Ruby applications need to process images, whether resizing, cropping, or applying filters. Image processing is CPU-intensive and can benefit from parallel execution in Swarm Mode, especially when handling high volumes of images, such as in user-generated content platforms or digital asset management systems.</p> </li> <li> <p>Data Import and Export Operations: Applications that require importing large datasets (e.g., CSV, XML, or JSON files) into the database or exporting data for reports can utilize Swarm Mode to parallelize parsing and processing. This accelerates the import/export operations, making it more efficient to handle bulk data operations without blocking web server requests.</p> </li> <li> <p>Real-time Data Processing: Real-time analytics or event processing systems in Ruby can process incoming data streams (from webhooks, sensors, etc.) in parallel using Swarm Mode. This is particularly useful for applications that aggregate data, calculate statistics, or detect patterns in real time across large datasets.</p> </li> <li> <p>Complex Calculations and Simulations: Applications that perform complex calculations or simulations, such as financial modeling, risk analysis, or scientific computations, can significantly improve performance with Swarm Mode. By distributing the computational load across multiple worker processes, Ruby applications can handle more complex algorithms and larger datasets without slowing down.</p> </li> </ul> <p>Implementing Swarm Mode for these use cases allows applications to fully utilize server resources, overcoming the limitations of Ruby's Global Interpreter Lock (GIL) and enhancing overall application performance and scalability.</p>"}, {"location": "Swarm-Multi-Process/#summary", "title": "Summary", "text": "<p>Karafka's Swarm Mode offers a multi-process architecture optimized for CPU-intensive Kafka message processing in Ruby, effectively bypassing the Global Interpreter Lock (GIL). It leverages Ruby's Copy-On-Write (CoW) for efficient memory use, employing a \"Supervisor-Worker\" pattern for parallel execution and system health monitoring.</p> <p>Swarm Mode supports Static Group Membership to enhance consumer group stability and allows for quick node restarts without rebalancing, thanks to Karafka Pro's monitoring.</p> <p>Last modified: 2025-02-12 18:27:28</p>"}, {"location": "Testing/", "title": "Testing", "text": "<p>Karafka provides a dedicated testing library, <code>karafka-testing</code>, designed to facilitate the testing of Karafka producers and consumers without needing a running Kafka server. This library effectively mocks the interactions with Kafka, allowing developers to write and run tests for consumers and producers in an isolated environment. The primary aim of <code>karafka-testing</code> is to eliminate the complexities and overhead associated with connecting to an actual Kafka cluster, initiating consumers, and managing producers during testing. This approach significantly reduces the setup time and resources needed for testing Kafka-related functionalities.</p> <p>It is important to note that the scope of <code>karafka-testing</code> is limited to mocking consumer and producer user-facing behaviors. It does not cover testing other Kafka-related functionalities, such as Admin API or web UI features that might interact with Kafka. By focusing solely on consumer and producer interactions, <code>karafka-testing</code> provides a lightweight and efficient solution for developers to ensure the integrity of message handling in their applications without the dependency on a live Kafka setup.</p> <p>Limited Testing Scope of <code>karafka-testing</code></p> <p><code>karafka-testing</code> enables testing of Karafka producers and consumers without a live Kafka server, but be aware that its scope is limited. While it effectively mocks client connections to Kafka for consumer and producer operations, it does not support testing of all Kafka functionalities.</p> <p>Specifically, <code>karafka-testing</code> does not facilitate testing of Admin API or any web UI interactions. This library is focused solely on consumer and producer functionalities. Additionally, testing consumers does not trigger instrumentation events, so any instrumentation-based logging or monitoring will not be covered during tests.</p>"}, {"location": "Testing/#usage-with-rspec", "title": "Usage with RSpec", "text": ""}, {"location": "Testing/#installation", "title": "Installation", "text": "<p>Add this gem to your Gemfile in the <code>test</code> group:</p> <pre><code>group :test do\n  gem 'karafka-testing'\n  gem 'rspec'\nend\n</code></pre> <p>and then in your <code>spec_helper.rb</code> file you must:</p> <ul> <li>require the karafka entrypoint (only when not using Ruby on Rails)</li> <li>require the helpers</li> <li>include appropriate helpers</li> </ul> <pre><code># Require entrypoint only when not using Rails, in Rails it happens automatically\nrequire './karafka'\nrequire 'karafka/testing/rspec/helpers'\n\nRSpec.configure do |config|\n  config.include Karafka::Testing::RSpec::Helpers\nend\n</code></pre> <p>Once included in your RSpec setup, this library will provide you with a special <code>#karafka</code> object that contains three methods that you can use within your specs:</p> <ul> <li><code>#consumer_for</code> - creates a consumer instance for the desired topic.</li> <li><code>#produce</code> - \"sends\" message to the consumer instance.</li> <li><code>#produced_messages</code> - contains all the messages \"sent\" to Kafka during spec execution.</li> </ul> <p>Messages sent using the <code>#produce</code> method and directly from <code>Karafka.producer</code> won't be sent to Kafka. They will be buffered and accessible in a per-spec buffer in case you want to test messages production.</p> <p>Messages that target the topic built using the <code>karafka#consumer_for</code> method will additionally be delivered to the consumer you want to test.</p>"}, {"location": "Testing/#testing-messages-consumption-consumers", "title": "Testing Messages Consumption (Consumers)", "text": "<pre><code>RSpec.describe InlineBatchConsumer do\n  # This will create a consumer instance with all the settings defined for the given topic\n  subject(:consumer) { karafka.consumer_for('inline_batch_data') }\n\n  let(:nr1_value) { rand }\n  let(:nr2_value) { rand }\n  let(:sum) { nr1_value + nr2_value }\n\n  before do\n    # Sends first message to Karafka consumer\n    karafka.produce({ 'number' =&gt; nr1_value }.to_json)\n\n    # Sends second message to Karafka consumer\n    karafka.produce({ 'number' =&gt; nr2_value }.to_json, partition: 2)\n\n    allow(Karafka.logger).to receive(:info)\n  end\n\n  it 'expects to log a proper message' do\n    expect(Karafka.logger).to receive(:info).with(\"Sum of 2 elements equals to: #{sum}\")\n    consumer.consume\n  end\nend\n</code></pre> <p>If your consumers use <code>producer</code> to dispatch messages, you can check its operations as well:</p> <pre><code>RSpec.describe InlineBatchConsumer do\n  subject(:consumer) { karafka.consumer_for(:inline_batch_data) }\n\n  before { karafka.produce({ 'number' =&gt; 1 }.to_json) }\n\n  it 'expects to dispatch async message to messages topic with value bigger by 1' do\n    consumer.consume\n\n    expect(karafka.produced_messages.last.payload).to eq({ number: 2 }.to_json)\n  end\nend\n</code></pre>"}, {"location": "Testing/#testing-messages-consumption-of-routing-patterns", "title": "Testing Messages Consumption of Routing Patterns", "text": "<p>Since each Routing Pattern has a name, you can test them like regular topics.</p> <p>Giving a pattern with the name <code>visits</code>:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # config stuff...\n  end\n\n  routes.draw do\n    pattern :visits, /_visits/ do\n      consumer VisitsConsumer\n    end\n  end\nend\n</code></pre> <p>You can reference this name when using the <code>karafka.consumer_for</code> method:</p> <pre><code>subject(:consumer) { karafka.consumer_for(:visits) }\n</code></pre>"}, {"location": "Testing/#testing-messages-production-producer", "title": "Testing Messages Production (Producer)", "text": "<p>When running RSpec, Karafka will not dispatch messages to Kafka using <code>Karafka.producer</code> but will buffer them internally.</p> <p>This means you can check your application flow, making sure your logic acts as expected:</p> <pre><code># Example class in which there is a message production\nclass UsersBuilder\n  def create(user_details)\n    user = ::User.create!(user_details)\n\n    Karafka.producer.produce_sync(\n      topic: 'users_changes',\n      payload: { user_id: user.id, type: 'user.created' },\n      key: user.id.to_s\n    )\n\n    user\n  end\nend\n\nRSpec.describe UsersBuilder do\n  let(:created_user) { described_class.new.create(user_details) }\n\n  before { created_user }\n\n  it { expect(karafka.produced_messages.size).to eq(1) }\n  it { expect(karafka.produced_messages.first[:topic]).to eq('users_changes') }\n  it { expect(karafka.produced_messages.first[:key]).to eq(created_user.id.to_s) }\nend\n</code></pre>"}, {"location": "Testing/#testing-transactions", "title": "Testing Transactions", "text": "<p>When testing producer transactions in Karafka, the approach is similar to how the non-transactional production of messages is tested. Within a transaction, messages you send are held and not immediately placed into the buffer. When the transactional block finishes successfully, these messages get moved into the buffers, ready to be produced to Kafka.</p> <p>If, for any reason, the transaction is aborted, messages inside that transaction won't reach the buffers. This mimics the real-world behavior where an aborted transaction would prevent messages from being sent to Kafka.</p> <p>Therefore, when you're writing tests for producer transactions in Karafka, you can:</p> <ol> <li> <p>Simulate the successful transaction completion and check if messages were placed into the buffers.</p> </li> <li> <p>Simulate an aborted transaction and ensure that no messages reach the buffers.</p> </li> </ol> <p>This approach lets you verify the behavior of your code within transactional boundaries, ensuring that messages are handed as expected in both successful and aborted transaction scenarios.</p> <pre><code>class UsersBuilder\n  def create_many(users_details)\n    users = []\n\n    Karafka.producer.transaction do\n      user = ::User.create!(user_details)\n\n      users &lt;&lt; user\n\n      Karafka.producer.produce_async(\n        topic: 'users_changes',\n        payload: { user_id: user.id, type: 'user.created' },\n        key: user.id.to_s\n      )\n    end\n\n    users\n  end\nend\n\nRSpec.describe UsersBuilder do\n  let(:created_users) { described_class.new.create_many([user_details, user_details]) }\n\n  before { created_users }\n\n  it { expect(karafka.produced_messages.size).to eq(2) }\n  it { expect(karafka.produced_messages.first[:topic]).to eq('user.created') }\n  it { expect(karafka.produced_messages.first[:key]).to eq(created_users.first.id.to_s) }\nend\n</code></pre>"}, {"location": "Testing/#testing-consumer-groups-and-topics-structure", "title": "Testing Consumer Groups and Topics Structure", "text": "<p>Sometimes you may need to spec out your consumer groups and topics structure. To do so, simply access the <code>Karafka::App.routes</code> array and check everything you need. Here's an example of a RSpec spec that ensures a custom <code>XmlDeserializer</code> is being used to a <code>xml_data</code> topic from the <code>batched_group</code> consumer group:</p> <pre><code>RSpec.describe Karafka::App.routes do\n  describe 'batched group' do\n    let(:group) do\n      Karafka::App.routes.find do |cg|\n        cg.name == 'batched_group'\n      end\n    end\n\n    describe 'xml_data topic' do\n      let(:topic) { group.topics.find('xml_data') }\n\n      it { expect(topic.deserializers.payload).to eq XmlDeserializer }\n    end\n  end\nend\n</code></pre>"}, {"location": "Testing/#usage-with-minitest", "title": "Usage with Minitest", "text": ""}, {"location": "Testing/#installation_1", "title": "Installation", "text": "<p>Add this gem to your Gemfile in the <code>test</code> group:</p> <pre><code>group :test do\n  gem 'karafka-testing'\n  gem 'minitest'\nend\n</code></pre> <p>And then:</p> <ul> <li>require the helpers: <code>require 'karafka/testing/minitest/helpers'</code></li> <li>include the following helper in your tests:  <code>include Karafka::Testing::Minitest::Helpers</code></li> </ul> <p>Once included in your Minitest setup, this library will provide you with a special <code>@karafka</code> object that contains three methods that you can use within your specs:</p> <ul> <li><code>#consumer_for</code> - creates a consumer instance for the desired topic.</li> <li><code>#produce</code> - \"sends\" message to the consumer instance.</li> <li><code>#produced_messages</code> - contains all the messages \"sent\" to Kafka during spec execution.</li> </ul> <p>Messages sent using the <code>#produce</code> method and directly from <code>Karafka.producer</code> won't be sent to Kafka. They will be buffered and accessible in a per-spec buffer if you want to test message production.</p> <p>Messages that target the topic built using the <code>karafka#consumer_for</code> method will additionally be delivered to the consumer you want to test.</p>"}, {"location": "Testing/#testing-messages-consumption-consumers_1", "title": "Testing Messages Consumption (Consumers)", "text": "<pre><code>class InlineBatchConsumerTest &lt; ActiveSupport::TestCase\n  include Karafka::Testing::Minitest::Helpers\n\n  def setup\n    # ..\n    nr1_value = rand\n    nr2_value = rand\n    sum = nr1_value + nr2_value\n\n    @consumer = @karafka.consumer_for('inline_batch_data')\n  end\n\n  it 'expects to log a proper message' do\n    # Sends first message to Karafka consumer\n    @karafka.produce({ 'number' =&gt; nr1_value }.to_json)\n\n    # Sends second message to Karafka consumer\n    @karafka.produce({ 'number' =&gt; nr2_value }.to_json, partition: 2)\n\n    expect(Karafka.logger).to receive(:info).with(\"Sum of 2 elements equals to: #{sum}\")\n\n    consumer.consume\n  end\nend\n</code></pre> <p>If your consumers use <code>producer</code> to dispatch messages, you can check its operations as well:</p> <pre><code>it 'expects to dispatch async message to messages topic with value bigger by 1' do\n  @karafka.produce({ 'number' =&gt; 1 }.to_json)\n  @consumer.consume\n\n  expect(@karafka.produced_messages.last.payload).to eq({ number: 2 }.to_json)\nend\n</code></pre>"}, {"location": "Testing/#testing-messages-production-producer_1", "title": "Testing Messages Production (Producer)", "text": "<p>When running Minitest, Karafka will not dispatch messages to Kafka using <code>Karafka.producer</code> but will buffer them internally.</p> <p>This means you can check your application flow, making sure your logic acts as expected:</p> <pre><code>class UsersBuilderTest &lt; ActiveSupport::TestCase\n  include Karafka::Testing::Minitest::Helpers\n\n  def setup\n    @user_details = { name: 'John Doe', email: 'john.doe@example.com' }\n    @created_user = UsersBuilder.new.create(@user_details)\n  end\n\n  test 'should produce messages' do\n    Karafka.producer.produce_sync(\n      topic: 'users_changes',\n      payload: { user_id: user.id, type: 'user.created' },\n      key: user.id.to_s\n      )\n    assert_equal 1, @karafka.produced_messages.size\n    assert_equal 'users_changes', @karafka.produced_messages.first[:topic]\n    assert_equal @created_user.id.to_s, @karafka.produced_messages.first[:key]\n  end\nend\n</code></pre> <p>If you're seeking guidance on testing transactions with Minitest, it's recommended to consult the RSpec transactions testing documentation, as the testing methods are similar for both.</p>"}, {"location": "Testing/#limitations", "title": "Limitations", "text": "<p><code>karafka-testing</code> primarily aims to eliminate the complexities and overhead associated with connecting to an actual Kafka cluster, initiating consumers, and managing producers during testing. This approach significantly reduces the setup time and resources needed for testing Kafka-related functionalities.</p> <p>However, it is important to be aware of the limitations of <code>karafka-testing</code>:</p> <ol> <li> <p>No Real Kafka Interactions: While <code>karafka-testing</code> effectively mocks the Kafka interactions, it does not replicate the behavior of a real Kafka cluster. As a result, certain edge cases and Kafka-specific behaviors may not be accurately represented in your tests.</p> </li> <li> <p>No Admin API Testing: The <code>karafka-testing</code> library does not support testing of Kafka Admin API functionalities. If your application relies on Admin API operations, such as topic management or cluster metadata retrieval, you must perform these tests against a real Kafka cluster.</p> </li> <li> <p>No Web UI Interactions: Any web UI interactions that might rely on actual Kafka data or state cannot be tested using <code>karafka-testing</code>. This limitation means that end-to-end UI component testing will still require a live Kafka setup.</p> </li> <li> <p>Transactional Testing: While <code>karafka-testing</code> supports transactional message production, it may not fully capture all the intricacies of Kafka transactions in a real cluster environment. It's important to be mindful of potential discrepancies between mocked transactions and their real-world counterparts.</p> </li> <li> <p>Batch Size Ignored: The <code>karafka-testing</code> library does not respect the <code>max_messages</code> setting configured for topics in the <code>karafka.rb</code> routes. It simply accumulates and consumes all messages sent to it during testing, bypassing the actual fetching engine of Karafka. This means that the behavior of batch processing may not be accurately reflected in your tests, as the library will consume all produced messages regardless of the configured batch size.</p> </li> </ol> <p>Last modified: 2025-06-24 17:30:28</p>"}, {"location": "Upgrades-Karafka-2.0/", "title": "Upgrading to Karafka 2.0", "text": "<p>Further Breaking Changes Beyond 2.0</p> <p>This document explicitly covers the upgrade process from Karafka <code>1.4</code> to <code>2.0</code>. If you plan to upgrade directly to a version higher than Karafka <code>2.0</code>, such as Karafka <code>2.4</code>, it is absolutely essential to review the upgrade guides for all intermediary versions. Notably, Karafka <code>2.4</code> introduces additional breaking changes that can be unexpected and potentially disruptive if you skip directly from <code>1.4</code> to <code>2.4</code> without following each version's upgrade path. Ignoring these steps may lead to significant issues in your application.</p> <p>Pro &amp; Enterprise Upgrade Support</p> <p>If you're gearing up to upgrade to the latest Karafka version and are a Pro or Enterprise user, remember you've got a dedicated lifeline! Reach out via the dedicated Slack channel for direct support to ensure everything has been covered.</p> <p>Karafka 2.0 is a major rewrite that brings many new things to the table but removes specific concepts that could have been better when I created them.</p> <p>In this upgrade document, I will describe the most noticeable changes that require manual intervention to handle the upgrade process. This document does not cover new functionalities but aims to guide the upgrade process.</p> <p>Before reading this article, please ensure you've read the Karafka framework 2.0 announcement blog post so you are familiar with the overall scope of changes.</p> <p>Please note, that there are many aspects of Karafka upgrade that are specific to your application. While we may not have covered all of the cases here, do not hesitate to ask questions either via Slack or by creating a Github issue.</p> <p>Those upgrade notes will be extended whenever someone points out any missing content.</p>"}, {"location": "Upgrades-Karafka-2.0/#note-on-concurrency", "title": "Note on concurrency", "text": "<p>Karafka 2.0 is multi-threaded. This means that all of your code needs to be thread-safe.</p> <p>Please keep in mind that Karafka does not provide warranties, that the same consumer will always run in the same Ruby thread, hence code like this:</p> <pre><code>def consume\n  Thread.current[:accumulator] ||= []\n  Thread.current[:accumulator] += messages.payloads\nend\n</code></pre> <p>may cause severe problems and is not recommended.</p>"}, {"location": "Upgrades-Karafka-2.0/#deploying-karafka-20-after-the-upgrade", "title": "Deploying Karafka 2.0 after the upgrade", "text": "<p>To safely upgrade from Karafka from <code>1.4</code> to <code>2.0</code>, you need to shut down completely all the consumers of the application you are upgrading.</p>"}, {"location": "Upgrades-Karafka-2.0/#gemfile-alignment", "title": "Gemfile alignment", "text": "<ol> <li>Update your <code>karafka</code> gem version reference</li> </ol> <pre><code># Replace\ngem 'karafka', '~&gt; 1.4'\n\n# with\ngem 'karafka', '~&gt; 2.0'\n</code></pre> <ol> <li>Remove the <code>sidekiq-backend</code> reference</li> </ol> <pre><code># This needs to be removed\ngem 'karafka-sidekiq-backend', '~&gt; 1.4'\n</code></pre> <ol> <li>Update <code>karafka-testing</code> version reference</li> </ol> <pre><code># Replace\ngem 'karafka-testing', '~&gt; 1.4'\n\n# with\ngem 'karafka-testing', '~&gt; 2.0'\n</code></pre> <ol> <li>Run <code>bundle install</code></li> </ol>"}, {"location": "Upgrades-Karafka-2.0/#waterdrop-producer-update", "title": "WaterDrop (producer) update", "text": "<ol> <li>Remove WaterDrop setup code from your <code>karafka.rb</code>:</li> </ol> <pre><code># This can be safely removed\nmonitor.subscribe('app.initialized') do\n  WaterDrop.setup { |config| config.deliver = !Karafka.env.test? }\nend\n</code></pre> <ol> <li>Remove direct WaterDrop listener references from your <code>karafka.rb</code></li> </ol> <pre><code># This can be safely removed\nKarafka.monitor.subscribe(WaterDrop::Instrumentation::LoggerListener.new)\n</code></pre>"}, {"location": "Upgrades-Karafka-2.0/#settings-alignment", "title": "Settings alignment", "text": "<p>Karafka 2.0 is powered by librdkafka. Because of that, we've decided to split the settings into two sections:</p> <ul> <li><code>karafka</code> options - options directly related to the Karafka framework and its components.</li> <li><code>librdkafka</code> options - options related to librdkafka.</li> </ul> <p>All <code>karafka</code> options should be set at the root level, while all the librdkafka options need to go under the <code>kafka</code> scope:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    config.client_id = 'my_application'\n    # librdkafka configuration options need to be set as symbol values\n    config.kafka = {\n      'bootstrap.servers': '127.0.0.1:9092'\n    }\n  end\nend\n</code></pre> <p>Below you can find some of the most significant naming changes in the configuration options:</p> <p>Root options:</p> <ul> <li><code>start_from_beginning</code> is now <code>initial_offset</code> and accepts either <code>'earliest'</code> or <code>'latest'</code></li> <li><code>ssl_ca_certs_from_system</code> is no longer needed but <code>kafka</code> <code>security.protocol</code> needs to be set to <code>ssl</code></li> <li><code>batch_fetching</code> is no longer needed</li> <li><code>batch_consuming</code> is no longer needed</li> <li><code>serializer</code> is no longer needed because <code>Responders</code> have been removed from Karafka</li> <li><code>topic_mapper</code> is no longer needed as concept of mapping topic names has been removed from Karafka</li> <li><code>backend</code> is no longer needed because Karafka is now multi-threaded</li> <li><code>manual_offset_management</code> needs to be set now on a per topic basis</li> </ul> <p>Kafka options:</p> <ul> <li><code>kafka.seed_brokers</code> is now <code>bootstrap.servers</code> without the protocol definition</li> <li><code>kafka.heartbeat_interval</code> is no longer needed.</li> <li><code>SASL</code> and <code>SSL</code> options changes are described in their own section.</li> <li>Check out the configuration details of librdkafka for all the remaining options.</li> </ul>"}, {"location": "Upgrades-Karafka-2.0/#heartbeat-no-longer-needed", "title": "Heartbeat no longer needed", "text": "<p>Sending heartbeats is no longer needed. Both <code>#trigger_heartbeat</code> and <code>trigger_heartbeat!</code> can be safely removed.</p>"}, {"location": "Upgrades-Karafka-2.0/#sasl-ssl-authentication", "title": "SASL, SSL, authentication", "text": "<p>Please read the Deployment documentation to see appropriate configuration for given environment.</p> <p>You can optionally check the <code>librdkafka</code> configuration documentation as well.</p> <p>If you still struggle, feel free to reach out to us either via Slack or by creating a Github issue.</p>"}, {"location": "Upgrades-Karafka-2.0/#manual-offset-management-is-now-a-per-topic-setting", "title": "Manual offset management is now a per-topic setting", "text": "<p>In Karafka 1.4 you could set <code>config.manual_offset_management = true</code> to make all the topics work with manual offset management.</p> <p>This option is no longer available in <code>2.0</code> and needs to be set per topic as followed:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    consumer_group :events do\n      topic :user_events do\n        consumer EventsConsumer\n        manual_offset_management true\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.0/#ruby-on-rails-integration", "title": "Ruby on Rails integration", "text": "<p>Karafka 2.0 introduces seamless Ruby on Rails integration via <code>Rails::Railte</code> without needing extra configuration.</p> <p>Your <code>karafka.rb</code> should contain only Karafka-specific stuff, and the rest will be done automatically. This means you need to reverse the manual setup steps that were needed for Karafka <code>1.4</code> to work with Ruby on Rails.</p> <ol> <li>Remove any changes in the <code>config/environment.rb</code> of your Rails application related to Karafka:</li> </ol> <pre><code># environment.rb - this needs to be removed\nrequire Rails.root.join(Karafka.boot_file)\n</code></pre> <ol> <li>Remove those lines from your <code>karafka.rb</code>:</li> </ol> <pre><code>ENV['RAILS_ENV'] ||= 'development'\nENV['KARAFKA_ENV'] = ENV['RAILS_ENV']\nrequire ::File.expand_path('../config/environment', __FILE__)\nRails.application.eager_load!\n\nif Rails.env.development?\n  Rails.logger.extend(\n    ActiveSupport::Logger.broadcast(\n      ActiveSupport::Logger.new($stdout)\n    )\n  )\nend\n</code></pre> <ol> <li>Remove the Rails logger assignment from your <code>karafka.rb</code>:</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n\n    # Remove this line\n    config.logger = Rails.logger\n  end\nend\n</code></pre> <ol> <li>Remove the code reloader (if used):</li> </ol> <pre><code>Karafka.monitor.subscribe(\n  Karafka::CodeReloader.new(\n    *Rails.application.reloaders\n  )\n)\n</code></pre> <ol> <li>Remove the <code>KarafkaApp.boot!</code> from the end of <code>karafka.rb</code>:</li> </ol> <pre><code># Remove this\n\nKarafkaApp.boot!\n</code></pre>"}, {"location": "Upgrades-Karafka-2.0/#consumer-callbacks-are-no-longer-supported", "title": "Consumer callbacks are no longer supported", "text": "<p>All of the consumer callbacks were removed. They were replaced with the following lifecycle consumer methods:</p> <ul> <li><code>#revoked</code> - runs the code you want to execute when a given topic partition has been revoked from a given consumer instance.</li> <li><code>#shutdown</code> - runs the code you want to execute when the Karafka process is being shut down.</li> </ul> <p>You can still use instrumentation hooks if you need to perform any consumer, not related actions, but please be aware that this code will not run in the same thread as the consumption.</p>"}, {"location": "Upgrades-Karafka-2.0/#unified-error-message-bus", "title": "Unified error message bus", "text": "<p>Karafka <code>1.4</code> published errors under several instrumentation keys. Karafka <code>2.0</code> publishes all the errors under the same instrumentation event name: <code>error.occurred</code>.</p> <p>The payload there always contains a <code>type</code> field that can be used to understand the origin of the issue.</p>"}, {"location": "Upgrades-Karafka-2.0/#routing-changes", "title": "Routing changes", "text": "<p>Simple routing style creates now a single consumer group for all the topics defined on a root level.</p> <p>This is a major change that can heavily impact your system.</p> <p>If you used the \"non consumer group\" based routing where all the topics would be defined on the root level of the routing:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :user_events do\n      consumer UsersEventsConsumer\n    end\n\n    topic :system_events do\n      consumer SystemEventsConsumer\n    end\n\n    topic :payment_events do\n      consumer PaymentEventsConsumer\n    end\n  end\nend\n</code></pre> <p>Karafka <code>1.4</code> would create for you three separate consumer groups while Karafka <code>2.0</code> will create one:</p> <pre><code># Karafka 1.4\nKarafka::App.consumer_groups.count #=&gt; 3\n\n# Karafka 2.0\nKarafka::App.consumer_groups.count #=&gt; 1\n</code></pre> <p>To mitigate this you need to:</p> <ol> <li>List all the groups names while in <code>1.4</code>:</li> </ol> <pre><code>Karafka::App.consumer_groups.map(&amp;:name)\n#=&gt; ['user_events', 'system_events', 'payment_events']\n</code></pre> <ol> <li>Replicate this setup in Karafka <code>2.0</code> by creating three separate consumer groups directly:</li> </ol> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    consumer_group :user_events do\n      topic :user_events do\n        consumer UsersEventsConsumer\n      end\n    end\n\n    consumer_group :system_events do\n      topic :system_events do\n        consumer SystemEventsConsumer\n      end\n    end\n\n    consumer_group :payment_events do\n      topic :payment_events do\n        consumer PaymentEventsConsumer\n      end\n    end\n  end\nend\n</code></pre> <ol> <li>After applying the above changes, validate, that the following command gives you same results under <code>1.4</code> and <code>2.0</code>:</li> </ol> <pre><code>Karafka::App.consumer_groups.map(&amp;:id)\n#=&gt; ['example_app_user_events', 'example_app_system_events', 'example_app_payment_events']\n</code></pre>"}, {"location": "Upgrades-Karafka-2.0/#topic-mappers-are-no-longer-supported", "title": "Topic mappers are no longer supported", "text": "<p>Unless you used Heroku, you can probably skip this section.</p> <p>Topic mapping is no longer supported. Please prefix all of your topic names with the env <code>KAFKA_PREFIX</code>.</p> <p>You can read more about integrating Karafka 2.0 with Heroku here.</p>"}, {"location": "Upgrades-Karafka-2.0/#pidfile-and-daemonization-support-has-been-removed", "title": "Pidfile and daemonization support has been removed", "text": "<p>Quote from Mike:</p> <ul> <li>Don't daemonize, start Karafka with a process supervisor like systemd, upstart, foreman, etc.</li> <li>Log only to STDOUT, the entity starting Karafka can control where STDOUT redirects to, if any.</li> <li>PID files are a legacy of double forking and have no reason to exist anymore.</li> </ul>"}, {"location": "Upgrades-Karafka-2.0/#sidekiq-backend-is-no-longer-supported", "title": "<code>sidekiq-backend</code> is no longer supported", "text": "<p>Karafka 2.0 is multi-threaded.</p> <p>If you use <code>sidekiq-backend</code>, you have two options:</p> <ul> <li>Pipe the jobs to Sidekiq yourself</li> <li>Elevate Karafka's multi-threading capabilities</li> </ul>"}, {"location": "Upgrades-Karafka-2.0/#responders-are-now-replaced-with-karafkaproducer", "title": "Responders are now replaced with <code>Karafka.producer</code>", "text": "<p>Responders were a dead end.</p> <p>Please use Waterdrop to produce messages via <code>Karafka.producer</code>:</p> <ol> <li>Replace direct responders usage with <code>Karafka.producer</code> usage</li> </ol> <pre><code>class ExampleResponder &lt; ApplicationResponder\n  topic :users_notified\n\n  def respond(user)\n    respond_to :users_notified, user\n  end\nend\n\nExampleResponder.call(User.last)\n</code></pre> <p>With:</p> <pre><code>Karafka.producer.produce_async(\n  topic: 'users_notified',\n  payload: user.to_json,\n  partition_key: user.id.to_s\n)\n</code></pre> <ol> <li>Replace all the <code>#respond_with</code> consumer responder invocations with direct <code>Karafka.producer</code> code</li> </ol>"}, {"location": "Upgrades-Karafka-2.0/#simple-message-consumption-mode-is-no-longer-supported", "title": "Simple message consumption mode is no longer supported", "text": "<p>You can achieve this functionality by slightly altering your consumer:</p> <pre><code>class SingleMessageBaseConsumer &lt; Karafka::BaseConsumer\n  attr_reader :message\n\n  def consume\n    messages.each do |message|\n      @message = message\n      consume_one\n\n      mark_as_consumed(message)\n    end\n  end\nend\n\nclass Consumer &lt; SingleMessageBaseConsumer\n  def consume_one\n    puts \"I received following message: #{message.payload}\"\n  end\nend\n</code></pre> <p>Here you can find more details about this.</p>"}, {"location": "Upgrades-Karafka-2.0/#dependency-changes", "title": "Dependency changes", "text": "<p>Karafka <code>2.0</code> no longer uses any of the <code>dry-rb</code> ecosystem libraries. If you've relied on them indirectly via Karafka, you will have to define them in your <code>Gemfile</code>.</p>"}, {"location": "Upgrades-Karafka-2.0/#naming-convention-changes", "title": "Naming convention changes", "text": "<p><code>#params_batch</code> is now <code>#messages</code>:</p> <pre><code>\ndef consume\n  # params_batch.each do |message|\n  messages.each do |message|\n    @message = message\n    consume_one\n\n    mark_as_consumed(message)\n  end\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.0/#instrumentation-and-monitoring", "title": "Instrumentation and monitoring", "text": "<p>Instrumentation and monitoring are often application specific.</p> <p>The recommendation here is to revisit your current integration and align it with the events published by Karafka and to follow the <code>2.0</code> instrumentation guidelines document.</p> <ul> <li><code>Karafka::Instrumentation::StdoutListener</code> is now <code>Karafka::Instrumentation::LoggerListener</code></li> </ul>"}, {"location": "Upgrades-Karafka-2.0/#karafka-testing-gem-code-adjustments", "title": "<code>karafka-testing</code> gem code adjustments", "text": "<p>Alongside the recent updates to Karafka, the <code>karafka-testing</code> gem has also been updated. The comprehensive guides for the testing gem can be accessed here.</p>"}, {"location": "Upgrades-Karafka-2.0/#tips-and-tricks", "title": "Tips and tricks", "text": "<ol> <li><code>bootstrap.servers</code> setting under <code>kafka</code> should not have a protocol, and it needs to be a string with comma-separated hosts.</li> </ol> <p>BAD:</p> <pre><code># protocol should not be here\nconfig.kafka = { 'bootstrap.servers': 'kafka://my.kafka.host1:9092' }\n</code></pre> <p>BAD:</p> <pre><code># it should be a comma separate string, not an array\nconfig.kafka = { 'bootstrap.servers': ['my.kafka.host1:9092', 'my.kafka.host2:9092'] }\n</code></pre> <p>GOOD:</p> <pre><code>config.kafka = { 'bootstrap.servers': 'my.kafka.host1:9092,my.kafka.host2:9092' }\n</code></pre> <ol> <li>Make sure not to overwrite your settings similar to how it was done in the section below.</li> </ol> <p>BAD:</p> <pre><code>config.kafka = {\n  'bootstrap.servers': ENV.fetch('BROKERS').split(',')\n}\n\n# This section will FULLY overwrite the `bootstrap.servers`.\n# You want to merge those sections and NOT overwrite.\nif Rails.env.production?\n  config.kafka = {\n    'ssl.ca.pem':          ENV.fetch('CA_CERT'),\n    'ssl.certificate.pem': ENV.fetch('CLIENT_CERT'),\n    'ssl.key.pem':         ENV.fetch('CLIENT_CERT_KEY'),\n    'security.protocol':   'ssl'\n  }\nend\n</code></pre> <p>GOOD:</p> <pre><code>config.kafka = {\n  'bootstrap.servers': ENV.fetch('BROKERS').split(',')\n}\n\nif Rails.env.production?\n  config.kafka.merge!({\n    'ssl.ca.pem':          ENV.fetch('CA_CERT'),\n    'ssl.certificate.pem': ENV.fetch('CLIENT_CERT'),\n    'ssl.key.pem':         ENV.fetch('CLIENT_CERT_KEY'),\n    'security.protocol':   'ssl'\n  })\nend\n</code></pre> <p>Last modified: 2025-04-29 12:52:58</p>"}, {"location": "Upgrades-Karafka-2.1/", "title": "Upgrading to Karafka 2.1", "text": "<p>Pro &amp; Enterprise Upgrade Support</p> <p>If you're gearing up to upgrade to the latest Karafka version and are a Pro or Enterprise user, remember you've got a dedicated lifeline! Reach out via the dedicated Slack channel for direct support to ensure everything has been covered.</p> <p>As always, please make sure you have upgraded to the most recent version of <code>2.0</code> before upgrading to <code>2.1</code>.</p> <ol> <li>Upgrade to Karafka <code>2.0.41</code> prior to upgrading to <code>2.1.0</code>.</li> <li>Replace <code>Karafka::Pro::BaseConsumer</code> references to <code>Karafka::BaseConsumer</code>.</li> <li>Replace <code>Karafka::Instrumentation::Vendors::Datadog:Listener</code> with <code>Karafka::Instrumentation::Vendors::Datadog::MetricsListener</code>.</li> </ol> <p>Last modified: 2025-04-29 12:52:58</p>"}, {"location": "Upgrades-Karafka-2.2/", "title": "Upgrading to Karafka 2.2", "text": "<p>Pro &amp; Enterprise Upgrade Support</p> <p>If you're gearing up to upgrade to the latest Karafka version and are a Pro or Enterprise user, remember you've got a dedicated lifeline! Reach out via the dedicated Slack channel for direct support to ensure everything has been covered.</p> <p>As always, please make sure you have upgraded to the most recent version of <code>2.1</code> before upgrading to <code>2.2</code>.</p> <p>If you are not using Kafka ACLs, there is no action you need to take.</p> <p>If you are using Kafka ACLs and you've set up permissions for <code>karafka_admin</code> group, please note that this name has now been changed and is subject to Consumer Name Mapping.</p> <p>That means you must ensure that the new consumer group that by default equals <code>CLIENT_ID_karafka_admin</code> has appropriate permissions. Please note that the Web UI also uses this group.</p> <p><code>Karafka::Admin</code> now has its own set of configuration options available, and you can find more details about that here.</p> <p>If you want to maintain the <code>2.1</code> behavior, that is <code>karafka_admin</code> admin group, we recommend introducing this case inside your consumer mapper. Assuming you use the default one, the code will look as follows:</p> <pre><code>  class MyMapper\n    def call(raw_consumer_group_name)\n      # If group is the admin one, use as it was in 2.1\n      return 'karafka_admin' if raw_consumer_group_name == 'karafka_admin'\n\n      # Otherwise use default karafka strategy for the rest\n      \"#{Karafka::App.config.client_id}_#{raw_consumer_group_name}\"\n    end\n  end\n</code></pre> <p>Last modified: 2025-04-29 12:52:58</p>"}, {"location": "Upgrades-Karafka-2.3/", "title": "Upgrading to Karafka 2.3", "text": "<p>Pro &amp; Enterprise Upgrade Support</p> <p>If you're gearing up to upgrade to the latest Karafka version and are a Pro or Enterprise user, remember you've got a dedicated lifeline! Reach out via the dedicated Slack channel for direct support to ensure everything has been covered.</p> <p>As always, please make sure you have upgraded to the most recent version of <code>2.2</code> before upgrading to <code>2.3</code>.</p> <p>Also, remember to read and apply our standard upgrade procedures.</p>"}, {"location": "Upgrades-Karafka-2.3/#web-ui-upgrade", "title": "Web UI Upgrade", "text": "<p>Karafka <code>2.3</code> works only with Web UI <code>&gt;= 0.8.0</code>. Please follow the Web UI upgrade procedure.</p>"}, {"location": "Upgrades-Karafka-2.3/#shutdown-procedure-updates", "title": "Shutdown Procedure Updates", "text": "<p>Thanks to recent advancements in <code>librdkafka</code> and <code>rdkafka-ruby</code>, shutdown patches needed in previous versions were removed in <code>2.3,</code>, and the whole shutdown procedure has been improved. While we saw only performance and stability improvements in those areas, we would like to highlight this change.</p>"}, {"location": "Upgrades-Karafka-2.3/#potential-cli-inclusionsexclusions-inconsistency-fix", "title": "Potential CLI Inclusions/Exclusions Inconsistency Fix", "text": "<p>While this change has already been introduced in <code>2.2</code> as a fix, we wanted to make you aware that all the CLI inclusion and exclusion options, such as <code>--include-consumer-groups</code> or <code>--exclude-consumer-groups</code> expect a list of arguments separated by a comma and NOT by space:</p> <p>BAD:</p> <pre><code># Wrong, topics separated with a space\nbundle exec karafka server --exclude-topics topic1 topic2\n</code></pre> <p>GOOD:</p> <pre><code># Good, topics separated with a comma\nbundle exec karafka server --exclude-topics topic1,topic2\n</code></pre> <p>This change is a regression introduced because of the removal of the Thor gem.</p>"}, {"location": "Upgrades-Karafka-2.3/#concurrent-ruby-removal", "title": "<code>concurrent-ruby</code> Removal", "text": "<p>Karafka no longer relies on <code>concurrent-ruby</code>. If you've indirectly relied on it being included in your applications, you must add it as a direct dependency yourself.</p>"}, {"location": "Upgrades-Karafka-2.3/#kuberneteslivenesslistener-no-longer-starts-until-karafka-server-starts", "title": "<code>Kubernetes::LivenessListener</code> No Longer Starts until Karafka Server Starts", "text": "<p>Before Karafka <code>2.3.0</code>, <code>Kubernetes::LivenessListener</code> would start immediately after initialization. This is why it was recommended to create it conditionally by using an external flag similar to this one:</p> <pre><code>if ENV['KARAFKA_LIVENESS'] == true\n  listener = ::Karafka::Instrumentation::Vendors::Kubernetes::LivenessListener.new(\n    # config goes here...\n  )\n\n  Karafka.monitor.subscribe(listener)\nend\n</code></pre> <p>Starting from <code>2.3</code>, such a flag is no longer needed as the listener begins listening on the desired port only when the associated Karafka server process is started. The listener will not bind itself to processes other than the Karafka server.</p> <p>Last modified: 2025-04-29 12:52:58</p>"}, {"location": "Upgrades-Karafka-2.4/", "title": "Upgrading to Karafka 2.4", "text": "<p>Breaking Changes</p> <p>This release contains BREAKING changes.</p> <p>Because consumer mappers have been removed, you must take action in your routing to align the naming convention with the old one. This is of the utmost criticality.</p> <p>Pro &amp; Enterprise Upgrade Support</p> <p>If you're gearing up to upgrade to the latest Karafka version and are a Pro or Enterprise user, remember you've got a dedicated lifeline! Reach out via the dedicated Slack channel for direct support to ensure everything has been covered.</p> <p>As always, please make sure you have upgraded to the most recent version of <code>2.3</code> before upgrading to <code>2.4</code>.</p> <p>Also, remember to read and apply our standard upgrade procedures.</p>"}, {"location": "Upgrades-Karafka-2.4/#ruby-27-is-eol", "title": "Ruby <code>2.7</code> is EOL", "text": "<p>Ruby <code>2.7</code> is no longer supported. If you still use it, you cannot upgrade to Karafka <code>2.4</code>.</p>"}, {"location": "Upgrades-Karafka-2.4/#waterdrop-27-changes", "title": "WaterDrop 2.7 Changes", "text": "<p>Karafka <code>2.4</code> relies on WaterDrop <code>2.7</code>. If you are using and upgrading only WaterDrop, you can find appropriate changelog here.</p> <p>Below you can find list breaking of changes in WaterDrop <code>2.7</code>.</p>"}, {"location": "Upgrades-Karafka-2.4/#wait_timeout-configuration-no-longer-needed", "title": "<code>wait_timeout</code> Configuration No Longer Needed", "text": "<p>The <code>wait_timeout</code> WaterDrop configuration option is no longer needed. You can safely remove it.</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  # Other config...\n\n  # Remove this, no longer needed\n  config.wait_timeout = 30\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#time-settings-format-alignment", "title": "Time Settings Format Alignment", "text": "<p>All time-related values are now configured in milliseconds instead of some being in seconds and some in milliseconds.</p> <p>The values that were changed from seconds to milliseconds are:</p> <ul> <li><code>max_wait_timeout</code></li> <li><code>wait_backoff_on_queue_full</code></li> <li><code>wait_timeout_on_queue_full</code></li> <li><code>wait_backoff_on_transaction_command, default</code></li> </ul> <p>If you have configured any of those yourself, please replace the seconds representation with milliseconds:</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.deliver = true\n\n  # Replace this:\n  config.max_wait_timeout = 30\n\n  # With\n  config.max_wait_timeout = 30_000\n  # ...\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#defaults-alignment", "title": "Defaults Alignment", "text": "<p>In this release, we've updated our default settings to address a crucial issue: previous defaults could lead to inconclusive outcomes in synchronous operations due to wait timeout errors. Users often mistakenly believed that a message dispatch was halted because of these errors when, in fact, the timeout was related to awaiting the final dispatch verdict, not the dispatch action itself.</p> <p>The new defaults in WaterDrop 2.7.0 eliminate this confusion by ensuring synchronous operation results are always transparent and conclusive. This change aims to provide a straightforward understanding of wait timeout errors, reinforcing that they reflect the wait state, not the dispatch success.</p> <p>Below, you can find a table with what has changed, the new defaults, and the current ones in case you want to retain the previous behavior:</p> Config Previous Default New Default root <code>max_wait_timeout</code> 5000 ms (5 seconds) 60000 ms (60 seconds) kafka <code>message.timeout.ms</code> 300000 ms (5 minutes) 50000 ms (50 seconds) kafka <code>transaction.timeout.ms</code> 60000 ms (1 minute) 55000 ms (55 seconds) <p>This alignment ensures that when using sync operations or invoking <code>#wait</code>, any exception you get should give you a conclusive and final delivery verdict.</p>"}, {"location": "Upgrades-Karafka-2.4/#buffering-no-longer-early-validates-messages", "title": "Buffering No Longer Early Validates Messages", "text": "<p>As of version <code>2.7.0</code>, WaterDrop has changed how message buffering works. Previously, messages underwent validation and middleware processing when they were buffered. Now, these steps are deferred until just before dispatching the messages. The buffer functions strictly as a thread-safe storage area without performing any validations or middleware operations until the messages are ready to be sent.</p> <p>This adjustment was made primarily to ensure that middleware runs and validations are applied when most relevant\u2014shortly before message dispatch. This approach addresses potential issues with buffers that might hold messages for extended periods:</p> <ul> <li> <p>Temporal Relevance: Validating and processing messages near their dispatch time helps ensure that actions such as partition assignments reflect the current system state. This is crucial in dynamic environments where system states are subject to rapid changes.</p> </li> <li> <p>Stale State Management: By delaying validations and middleware to the dispatch phase, the system minimizes the risk of acting on outdated information, which could lead to incorrect processing or partitioning decisions.</p> </li> </ul> <pre><code># Prior to 2.7.0 this would raise an error\nproducer.buffer(topic: nil, payload: '')\n# =&gt; WaterDrop::Errors::MessageInvalidError\n\n# After 2.7.0 buffer will not, but flush_async will\nproducer.buffer(topic: nil, payload: '')\n# =&gt; all good here\nproducer.flush_async(topic: nil, payload: '')\n# =&gt; WaterDrop::Errors::MessageInvalidError\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#middleware-execution-prior-to-flush-when-buffering", "title": "Middleware Execution Prior to Flush When Buffering", "text": "<p>The timing of middleware execution has been adjusted. Middleware, which was previously run when messages were added to the buffer, will now only execute immediately before the messages are flushed from the buffer and dispatched. This change is similar to the validation-related changes.</p>"}, {"location": "Upgrades-Karafka-2.4/#rdkafka-results-wait_timeout-no-longer-needed", "title": "<code>rdkafka</code> Results <code>wait_timeout</code> No Longer Needed", "text": "<p>Suppose you used Rdkafka operations with the <code>wait</code> API and would use this argument. It is no longer needed in that case, as we have switched the operational model to async.</p> <pre><code>handler = Karafka.producer.produce_async(topic: 'my-topic', payload: 'my message')\n\n# Replace this\nhandler.wait(wait_timeout: 0.1)\n\n# With\nhandler.wait\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#declarative-configs-karafka-topics-migrate-config-awareness", "title": "Declarative Configs <code>karafka topics migrate</code> Config Awareness", "text": "<p>Before Karafka 2.4, declarative topics could only manage topic creation and partitioning, not updating existing configurations to match Karafka's definitions. With version 2.4 and beyond, Karafka now updates all aspects of topic configurations to align with the settings defined in your code.</p> <p>Be cautious: if you've manually adjusted topic settings outside Karafka and then run <code>Karafka topics migrate</code>, Karafka's definitions will overwrite those manual changes. This ensures your topics are consistently configured as per your codebase, but it also means any out-of-sync manual configurations will be reset.</p> <p>We recommend manually reviewing any changes made to your Kafka topics outside Karafka's declarative configurations before migrating. You can use the <code>bundle exec karafka topics plan</code> command to facilitate this. This command previews the changes Karafka intends to apply to your topics when running the migration process.</p>"}, {"location": "Upgrades-Karafka-2.4/#consumer-mapper-concept-removal", "title": "Consumer Mapper Concept Removal", "text": "<p>Karafka used to have a default strategy for building consumer group names. Each consumer group combined <code>client_id</code> and the group name taken from the routing.</p> <p>Below, you can find how Karafka pre <code>2.4</code> would remap the following routing:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n    config.client_id = 'my-app'\n  end\n\n  routes.draw do\n    topic :example do\n      consumer ExampleConsumer\n    end\n\n    topic :example2 do\n      consumer Example2Consumer\n    end\n\n    consumer_group :special do\n      topic :super_topic do\n        consumer Example2Consumer\n      end\n    end\n  end\nend\n\nKarafka::Web.enable!\n</code></pre> <p>The above setup would create three consumer groups:</p> Group Name Before Group Name After Topics my-app_app app example, example2 my-app_special special super_topic my-app_karafka_web karafka_web Web UI related topics <p>The decision to retire the default consumer group naming strategy in Karafka was driven by a need to address several issues and simplify the system for users, especially as the framework has evolved. Here's a clear and logical explanation of the reasons behind this change:</p> <ul> <li> <p>The strategy was initially beneficial for setups involving multiple applications and tenants using Kafka, facilitating easier management and segregating consumer groups.</p> </li> <li> <p>However, as Karafka expanded, this approach led to complications in various areas, such as the Pro Iterator, Admin features, and the development of new streaming capabilities.</p> </li> <li> <p>Newcomers to Karafka, or those migrating from other frameworks, needed clarification, highlighting a need for a more straightforward approach.</p> </li> <li> <p>A fundamental assumption made early in the development - that consumer group mapping would only occur from Karafka to Kafka - needed to be updated. As the framework grew, the interaction between Karafka and Kafka became bidirectional, with Karafka also receiving consumer group information from Kafka. This raised several questions:</p> <ul> <li> <p>Whether and how to implement reverse mapping for consumer groups.</p> </li> <li> <p>The feasibility and user responsibility in providing a reverse mapper or direct mapping.</p> </li> <li> <p>Ensuring comprehensive coverage of all areas requiring mapping and remapping.</p> </li> </ul> </li> <li> <p>Reports of inconsistent behavior and bugs related to consumer group mapping started emerging, underscoring the need to revise the strategy.</p> </li> <li> <p>The challenges became particularly apparent while enhancing <code>Karafka::Admin</code> with <code>#read_lags</code> for advanced consumer group management, reinforcing the view that the existing mapping functionality was more problematic than beneficial.</p> </li> </ul> <p>Given these considerations, the decision to retire the consumer mapper concept was made to streamline Karafka's architecture, enhance clarity and usability, and mitigate issues stemming from the outdated one-way mapping assumption.</p> <p>Custom Mapper Naming Strategy Alignment</p> <p>If you've used a custom mapper, align your naming to match the new setup according to your mapper behavior.</p> <p>Below are details about the upgrade path specific to your usage patterns.</p>"}, {"location": "Upgrades-Karafka-2.4/#removal-of-custom-mappers", "title": "Removal of Custom Mappers", "text": "<p>In addition to the changes mentioned below for upgrading to Karafka 2.4, you need to remove the custom mapper configuration from your setup, as it's no longer supported. Specifically, delete the <code>config.consumer_mapper</code> line from your <code>karafka.rb</code> configuration:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n    #\n    # Remove the following line as it's not supported in 2.4+\n    config.consumer_mapper = MyCustomConsumerMapper.new\n  end\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#aligning-the-simple-routing", "title": "Aligning the Simple Routing", "text": "<p>When working without explicit consumer groups, all you need to do is alter the <code>group_id</code> setting to prefix it with the <code>client_id</code> value:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n    config.client_id = 'my-app'\n    # `app` without anything used to be the default name in the simple routing\n    config.group_id = 'my-app_app'\n  end\n\n  # Simple routing is a routing without `consumer_group` blocks\n  routes.draw do\n    topic :topic1 do\n      consumer EventsConsumer\n    end\n\n    topic :topic2 do\n      consumer WebhooksConsumer\n    end\n  end\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#aligning-the-multiple-consumer-groups-mode", "title": "Aligning the Multiple Consumer Groups Mode", "text": "<p>In the case of the Multiple Consumer Group Mode, you need to update the <code>group_id</code> as well as the names of particular consumer groups:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n    config.client_id = 'my-app'\n    # `app` without anything used to be the default name in the simple routing\n    config.group_id = 'my-app_app'\n  end\n\n  # Simple routing is a routing without `consumer_group` blocks\n  routes.draw do\n    # implicit group topics are covered with the `group_id` update\n    topic :topic1 do\n      consumer EventsConsumer\n    end\n\n    topic :topic2 do\n      consumer WebhooksConsumer\n    end\n\n    # was: consumer_group do 'group_special' do\n    consumer_group 'my-app_group_special' do\n      topic :topic3 do\n        consumer SuperConsumer\n      end\n    end\n  end\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#aligning-the-admin", "title": "Aligning the Admin", "text": "<p>Reconfigure Admin to use the extended <code>group_id</code> during Karafka configuration:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n    config.client_id = 'my-app'\n    config.admin.group_id = 'my-app_karafka_admin'\n  end\n\n  routes.draw do\n    # ...\n  end\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#aligning-the-web-ui", "title": "Aligning the Web UI", "text": "<p>Karafka Web UI injects its group, which the mapper also altered. To mitigate it, set the consumer group explicitly:</p> <pre><code># Put the below before enabling the Web UI\n\nKarafka::Web.setup do |config|\n  # Add the prefix matching your client_id merged with the underscore\n  config.group_id = 'my-app_karafka_web'\nend\n\nKarafka::Web.enable!\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#aligning-the-cli", "title": "Aligning the CLI", "text": "<p>Karafka CLI commands must be aligned, such as <code>bundle exec karafka server --exclude-consumer-groups group_name2,group_name3</code>.</p> <p>Commands related to topics and subscription groups are not affected.</p> <pre><code># Assuming that client_id is `my-app`\n\n# Replace\nbundle exec karafka server --exclude-consumer-groups group_name2,group_name3\n\n# With\nbundle exec karafka server --exclude-consumer-groups my-app_group_name2,my-app_group_name3\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#stop-based-iterator-api-exiting", "title": "<code>#stop</code> Based Iterator API Exiting", "text": "<p>Until Karafka <code>2.4</code> if you wanted to stop the Pro Iterator, you could just exit its main messages loop like so:</p> <pre><code>iterator = Karafka::Pro::Iterator.new('my_topic')\n\niterator.each do |message|\n  puts message.payload\n\n  break\nend\n</code></pre> <p>While this flow is still supported, the iterator object now has an explicit <code>#stop</code> method that will break the loop. This allows for more efficient resource management and post-execution cleanup. After you invoke <code>#stop</code>, <code>#each</code> loop will stop yielding messages.</p> <pre><code>iterator = Karafka::Pro::Iterator.new('my_topic')\n\niterator.each do |message|\n  puts message.payload\n\n  iterator.stop\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#configdeserializer-default-becomes-a-routing-option", "title": "<code>config.deserializer</code> default becomes a Routing Option", "text": "<p>Because of the introduction of <code>key</code> and <code>header</code> deserializers, the <code>config.deserializer</code> option became a per-topic routing option. If you relied on the default deserializer set on the configuration level, you should migrate it into the routing <code>defaults</code> block:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Remove this\n    config.deserializer = AvroDefaultDeserializer.new\n  end\n\n  routes.draw do\n    # Now you can define the default deserializer here:\n    defaults do\n      deserializer(AvroDefaultDeserializer.new)\n    end\n\n    topic :a do\n      consumer ConsumerA\n      # You can still overwrite defaults on a per-topic basis\n      deserializer(JsonDeserializer.new)\n    end\n\n    topic :b do\n      consumer ConsumerB\n      deserializer(XmlDeserializer.new)\n    end\n  end\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#web-ui-alignments", "title": "Web UI Alignments", "text": ""}, {"location": "Upgrades-Karafka-2.4/#processingconsumer_group-is-now-configgroup_id", "title": "<code>processing.consumer_group</code> is now <code>config.group_id</code>", "text": "<pre><code>Karafka::Web.setup do |config|\n  # Replace this\n  config.processing.consumer_group = 'my-custom-web-consumer-group'\n\n  # With\n  config.group_id = 'my-custom-web-consumer-group'\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#karafka_consumers_commands-topic-introduction", "title": "<code>karafka_consumers_commands</code> Topic Introduction", "text": "<p>Karafka <code>2.4</code> relies on Web UI <code>0.9+</code>. Because of new concepts and capabilities introduced in Karafka Web UI <code>0.9</code>, do not forget to run <code>bundle exec karafka-web migrate</code> after upgrading for every environment using Karafka Web UI before starting your consumers. This command execution is required despite Karafka having an automatic migration engine because of the introduction of the new <code>karafka_consumers_commands</code> topic.</p> <p>Alternatively, if you created and manage Karafka Web UI topics manually, create the <code>karafka_consumers_commands</code> topic manually according to the settings available here.</p> <p>Remember to assign proper permissions to appropriate consumer groups in ACLs based on your setup policies.</p> <p>You can also customize this topic name by altering the <code>topics.consumers.commands</code> configuration option:</p> <pre><code>Karafka::Web.setup do |config|\n  env_suffix = Rails.env.to_s\n\n  config.topics.errors = \"karafka_errors_#{env_suffix}\"\n  config.topics.consumers.reports = \"karafka_consumers_reports_#{env_suffix}\"\n  config.topics.consumers.states = \"karafka_consumers_states_#{env_suffix}\"\n  config.topics.consumers.metrics = \"karafka_consumers_metrics_#{env_suffix}\"\n  config.topics.consumers.commands = \"karafka_consumers_commands_#{env_suffix}\"\nend\n</code></pre>"}, {"location": "Upgrades-Karafka-2.4/#opting-out-from-commanding", "title": "Opting-Out From Commanding", "text": "<p>Karafka <code>2.4</code> Pro and Enterprise introduces consumer management and probing UI. While they are turned on by default, if you do not want them, you can opt-out by setting the Web UI <code>commanding.active</code> setting to <code>false.</code> In such cases, relevant UI options and any backend-related functionalities will be completely disabled.</p> <pre><code>Karafka::Web.setup do |config|\n  # Opt out if you do not want the commanding and probing UI capabilities\n  config.commanding.active = false\nend\n</code></pre> <p>Mandatory Topic Creation After Opt-Out</p> <p>If you disable consumers management and probing UI in Karafka <code>2.4</code> by setting <code>commanding.active</code> to <code>false</code>, you must still create the relevant topic. Use <code>bundle exec karafka-web migrate</code> for automatic creation or add it manually. Skipping this step can lead to application issues, even with the feature turned off.</p>"}, {"location": "Upgrades-Karafka-2.4/#karafkaserializersjsondeserializer-rename-to-karafkadeserializerspayload", "title": "<code>Karafka::Serializers::JSON::Deserializer</code> rename to <code>Karafka::Deserializers::Payload</code>", "text": "<p>Due to other changes, <code>Karafka::Serializers::JSON::Deserializer</code> has been renamed to <code>Karafka::Deserializers::Payload</code>.</p> <p>No APIs were changed. Just the name. If you rely on this deserializer in your custom once, change the reference:</p> <pre><code># Change this\nclass CustomDeserializer &lt; Karafka::Serializers::JSON::Deserializer\nend\n\n# to\nclass CustomDeserializer &lt; Karafka::Deserializers::Payload\nend\n</code></pre> <p>Last modified: 2025-05-16 21:52:20</p>"}, {"location": "Upgrades-Karafka-2.5/", "title": "Upgrading to Karafka 2.5", "text": "<p>Karafka 2.5 introduces two breaking changes that align naming conventions with other Kafka ecosystem tools, such as Kafka Streams and Apache Flink. These low-risk changes focus on configuration alignment and naming conventions without affecting consumer group names or processing flows.</p> <p>Pro &amp; Enterprise Upgrade Support</p> <p>If you're gearing up to upgrade to the latest Karafka version and are a Pro or Enterprise user, remember you've got a dedicated lifeline! Reach out via the dedicated Slack channel for direct support to ensure everything has been covered.</p> <p>As always, please make sure you have upgraded to the most recent version of <code>2.4</code> before upgrading to <code>2.5</code>.</p> <p>Also, remember to read and apply our standard upgrade procedures.</p>"}, {"location": "Upgrades-Karafka-2.5/#dlq-and-piping-header-prefix-change", "title": "DLQ and Piping Header Prefix Change", "text": "<p>The prefix for DLQ (Dead Letter Queue) dispatched and piped messages headers has been changed from <code>original_</code> to <code>source_</code>:</p> <ul> <li><code>original_topic</code> \u2192 <code>source_topic</code></li> <li><code>original_partition</code> \u2192 <code>source_partition</code></li> <li><code>original_offset</code> \u2192 <code>source_offset</code></li> <li><code>original_consumer_group</code> \u2192 <code>source_consumer_group</code></li> <li><code>original_attempts</code> \u2192 <code>source_attempts</code></li> </ul> <p>This change aligns Karafka's naming conventions with Kafka Streams and Apache Flink for better ecosystem consistency and future compatibility.</p>"}, {"location": "Upgrades-Karafka-2.5/#pro-dlq-message-key-and-routing-changes", "title": "Pro DLQ Message Key and Routing Changes", "text": "<p>Karafka Pro's Dead Letter Queue has undergone important change to improve message routing consistency and key preservation.</p>"}, {"location": "Upgrades-Karafka-2.5/#key-preservation", "title": "Key Preservation", "text": "<p>The Pro DLQ now preserves the original message key instead of overwriting it with the source partition ID. This ensures that the original routing logic is maintained when messages are dispatched to the DLQ topic, preserving the source partition ordering as well.</p> <p>Before:</p> <pre><code># DLQ message key was set to the source partition ID\ndlq_message[:key] = skippable_message.partition.to_s\n</code></pre> <p>After:</p> <pre><code># DLQ message key preserves the original message key\ndlq_message[:key] = skippable_message.raw_key\n# Used for partition targeting\ndlq_message[:partition_key] = skippable_message.partition.to_s\n</code></pre>"}, {"location": "Upgrades-Karafka-2.5/#removed-source_key-header", "title": "Removed <code>source_key</code> Header", "text": "<p>The <code>source_key</code> header has been removed from DLQ dispatched messages since the original key is now fully preserved in the message key field.</p> <p>Before:</p> <pre><code># DLQ messages included source_key in headers\ndlq_message[:headers] = {\n  'source_topic' =&gt; topic.name,\n  'source_partition' =&gt; source_partition,\n  'source_offset' =&gt; skippable_message.offset.to_s,\n  'source_consumer_group' =&gt; topic.consumer_group.id,\n  # This header below is removed\n  'source_key' =&gt; skippable_message.raw_key.to_s,\n  'source_attempts' =&gt; attempt.to_s\n}\n</code></pre> <p>After:</p> <pre><code># source_key header is no longer included\ndlq_message[:headers] = {\n  'source_topic' =&gt; topic.name,\n  'source_partition' =&gt; source_partition,\n  'source_offset' =&gt; skippable_message.offset.to_s,\n  'source_consumer_group' =&gt; topic.consumer_group.id,\n  'source_attempts' =&gt; attempt.to_s\n}\n</code></pre>"}, {"location": "Upgrades-Karafka-2.5/#partition-routing-consistency", "title": "Partition Routing Consistency", "text": "<p>The Pro DLQ now uses <code>partition_key</code> for consistent partition targeting while preserving the original message key. This ensures that:</p> <ul> <li>Messages from the same source partition still route to one and the same DLQ partition</li> <li>The original message key is preserved for application-level routing logic</li> <li>Partition targeting is handled separately from message identification</li> </ul>"}, {"location": "Upgrades-Karafka-2.5/#impact-on-your-code", "title": "Impact on Your Code", "text": "<p>Mid-Risk Changes:</p> <ol> <li>Key Access: If you were accessing the DLQ message key expecting it to be the source partition, update your code to use the <code>source_partition</code> header instead</li> <li>Header Access: Remove any code that accesses the <code>source_key</code> header from DLQ messages, as this information is now available in the DLQ message key</li> <li>Custom DLQ Enhancement: Review any <code>#enhance_dlq_message</code> implementations that modify the key field</li> </ol> <p>Critical Partitioning Change:</p> <p>While ordering within partitions is preserved, DLQ message partitioning behavior has changed. Previously, DLQ messages were partitioned based on the source partition ID (assigned as the key). Now, partitioning is based on the original message key, which may result in messages being distributed differently across DLQ partitions after the upgrade.</p> <p>Messages that previously went to the same DLQ partition will still all go to one partition, but it may not be the same partition as before the upgrade.</p> <p>This could affect:</p> <ul> <li>DLQ consumer processing order expectations</li> <li>Partition-specific recovery strategies</li> <li>Monitoring and alerting based on DLQ partition distribution</li> </ul> <p>Maintaining Previous Partitioning Behavior:</p> <p>If you need to maintain the previous partitioning behavior (routing based on source partition), you can use <code>#enhance_dlq_message</code> to override the key:</p> <pre><code>class MyConsumer\n  def consume\n    # some code that can raise an error...\n  end\n\n  private\n\n  def enhance_dlq_message(dlq_message, skippable_message)\n    # Restore previous partitioning behavior by using source partition as key\n    dlq_message[:key] = skippable_message.partition.to_s\n\n    # Optionally preserve original key in headers for reference\n    dlq_message[:headers]['source_key'] = skippable_message.raw_key.to_s\n  end\nend\n</code></pre> <p>Example Migration:</p> <pre><code># Before - accessing source info from DLQ message\ndef process_dlq_message(message)\n  source_partition = message.key.to_i  # This was the partition ID\n  original_key = message.headers['source_key']  # Original key was in headers\nend\n\n# After - updated access pattern\ndef process_dlq_message(message)\n  source_partition = message.headers['source_partition'].to_i  # Use header\n  original_key = message.key  # Original key is preserved\nend\n</code></pre> <p>These changes enhance the Pro DLQ's consistency with Kafka's native partitioning behavior while maintaining backward compatibility for most use cases.</p>"}, {"location": "Upgrades-Karafka-2.5/#web-ui-topic-configuration-structure", "title": "Web UI Topic Configuration Structure", "text": "<p>Additional Web UI Changes</p> <p>Aside from this change below to Web UI, there are other subtle changes to Web UI. Full upgrade docs for Web UI <code>0.11</code> are available here.</p> <p>The Web UI component configuration has been restructured to use a nested format with the <code>.name</code> property for topic redefinitions:</p> <p>Before:</p> <pre><code>config.topics.consumers.reports = \"app_karafka_web_consumers_reports\"\nconfig.topics.consumers.states = \"app_karafka_web_consumers_states\"\nconfig.topics.consumers.metrics = \"app_karafka_web_consumers_metrics\"\nconfig.topics.consumers.commands = \"app_karafka_web_consumers_commands\"\nconfig.topics.errors = \"app_karafka_web_errors\"\n</code></pre> <p>After:</p> <pre><code>config.topics.consumers.reports.name = \"app_karafka_web_consumers_reports\"\nconfig.topics.consumers.states.name = \"app_karafka_web_consumers_states\"\nconfig.topics.consumers.metrics.name = \"app_karafka_web_consumers_metrics\"\nconfig.topics.consumers.commands.name = \"app_karafka_web_consumers_commands\"\nconfig.topics.errors.name = \"app_karafka_web_errors\"\n</code></pre> <p>This change aligns with the namespacing pattern used across Karafka components, providing a consistent configuration approach for topic naming. The direct assignment has been replaced with a nested structure where each topic configuration is a separate object with its own properties, with <code>.name</code> being used to define the actual Kafka topic name.</p>"}, {"location": "Upgrades-Karafka-2.5/#recurring-tasks-topic-configuration-structure-pro", "title": "Recurring Tasks Topic Configuration Structure (Pro)", "text": "<p>The scheduled jobs topics configuration has been restructured to use a nested format:</p> <p>Before:</p> <pre><code>config.recurring_tasks.topics.schedules = \"karafka_recurring_tasks_schedules\"\nconfig.recurring_tasks.topics.logs = \"karafka_recurring_tasks_logs\"\n</code></pre> <p>After:</p> <pre><code>config.recurring_tasks.topics.schedules.name = \"karafka_recurring_tasks_schedules\"\nconfig.recurring_tasks.topics.logs.name = \"karafka_recurring_tasks_logs\"\n</code></pre> <p>This change aligns with the Web UI's topic namespacing pattern of using <code>topic.name</code>.</p>"}, {"location": "Upgrades-Karafka-2.5/#kubernetes-health-check-response-format-change", "title": "Kubernetes Health Check Response Format Change", "text": "<p>The Kubernetes health check listeners now return a <code>200 OK</code> status with a JSON response body instead of the previous <code>204 No Content</code> empty response. This change provides more detailed health information while maintaining backward compatibility for most monitoring setups.</p> <p>Before:</p> <pre><code>HTTP/1.1 204 No Content\nContent-Type: text/plain\n\n(empty body)\n</code></pre> <p>After:</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: application/json\nContent-Length: 156\n\n{\n  \"status\": \"healthy\",\n  \"timestamp\": 1717251446,\n  \"port\": 9000,\n  \"process_id\": 12345,\n  \"errors\": {\n    \"polling_ttl_exceeded\": false,\n    \"consumption_ttl_exceeded\": false,\n    \"unrecoverable\": false\n  }\n}\n</code></pre> <p>This change is considered low-risk as most monitoring systems and Kubernetes liveness probes that check for any successful HTTP status (2xx range) will continue working without modification. Most health check integrations rely on the HTTP success status rather than the specific response code or content type, making this transition seamless for most deployments.</p> <p>However, if your monitoring infrastructure performs strict status code validation expecting exactly <code>204</code>, you must update it to accept <code>200</code> or any <code>2xx</code> status code. Similarly, any tooling that explicitly expects a <code>text/plain</code> content type will need adjustment to handle the new <code>application/json</code> format. Code that assumes an empty response body should also be updated to handle the JSON payload. However, most health check implementations verify the HTTP status without processing the response content.</p> <p>The new JSON response format provides enhanced monitoring capabilities by providing granular health information that can improve your observability setup. You can now implement more sophisticated monitoring and alerting based on specific error indicators, debug health check failures with detailed status information, and track process-level metrics, including process ID, port, and timestamp data, directly from the health endpoint.</p>"}, {"location": "Upgrades-Karafka-2.5/#admin-configuration-options-update", "title": "Admin Configuration Options Update", "text": "<p>The admin configuration options have been updated to optimize retryable operations and improve clarity:</p> <p>Before:</p> <pre><code>config.admin.max_attempts = 60\n</code></pre> <p>After:</p> <pre><code>config.admin.max_retries_duration = 60_000  # 60 seconds in milliseconds\nconfig.admin.retry_backoff = 500            # 500ms between retries\n</code></pre> <p>This change affects how Karafka handles admin operations like topic creation, deletion, and partition management. The new approach uses time-based retries instead of attempt-based ones, providing more predictable behavior and better resource management.</p> <p>It is a low-risk change, as most users never modify these admin configuration options. They will automatically benefit from the improved retry logic without any code changes.</p> <p>If you have customized admin retry behavior, please update your configuration to use the new time-based approach. For example, if you previously set <code>max_attempts = 30</code> with the default 1-second wait time (30 seconds total), you would now set <code>max_retries_duration = 30_000</code> to maintain equivalent behavior.</p> <p>The new configuration provides better control over admin operation timeouts and prevents excessive metadata requests to Kafka clusters, which is particularly beneficial when working with topics with hundreds of partitions.</p>"}, {"location": "Upgrades-Karafka-2.5/#impact-assessment", "title": "Impact Assessment", "text": "<ul> <li>Low Risk: These changes are purely naming-related and don't affect consumer group names, processing logic, or system stability.</li> <li>Migration: Simple search and replace operations should be sufficient for most codebases.</li> </ul>"}, {"location": "Upgrades-Karafka-2.5/#recommended-update-approach", "title": "Recommended Update Approach", "text": "<ol> <li>Search for uses of <code>original_</code> in your message header access code and replace with <code>source_</code></li> <li>Update all Web UI topic configurations to use the <code>.name</code> property if you are using custom Web UI topics names</li> <li>For Pro users, update the recurring tasks topic configuration structure</li> <li>Run your test suite to verify everything works as expected</li> </ol> <p>These changes enhance future compatibility without significantly reworking your processing flows or message-handling logic.</p> <p>Last modified: 2025-06-15 19:40:23</p>"}, {"location": "Upgrades-Karafka/", "title": "Karafka Upgrade Notes", "text": "<ul> <li>Upgrading to 2.5</li> <li>Upgrading to 2.4</li> <li>Upgrading to 2.3</li> <li>Upgrading to 2.2</li> <li>Upgrading to 2.1</li> <li>Upgrading to 2.0</li> <li>Upgrading to 1.4</li> <li>Upgrading to 1.3</li> <li>Upgrading to 1.2</li> <li>Upgrading to 1.1</li> </ul> <p>Last modified: 2025-04-29 12:52:58</p>"}, {"location": "Upgrades-WaterDrop-2.5/", "title": "Upgrading to WaterDrop 2.5", "text": "<p>Please note, this is a breaking release, hence <code>2.5.0</code>.</p> <ol> <li>If you used to catch <code>WaterDrop::Errors::FlushFailureError</code> now you need to catch <code>WaterDrop::Errors::ProduceError</code>. <code>WaterDrop::Errors::ProduceManyError</code> is based on the <code>ProduceError</code>, hence it should be enough.</li> <li>Prior to <code>2.5.0</code> there was always a chance of partial dispatches via <code>produce_many_</code> methods. Now you can get the info on all the errors via <code>error.occurred</code>.</li> <li>Inline <code>Rdkafka::RdkafkaError</code> are now re-raised via <code>WaterDrop::Errors::ProduceError</code> and available under <code>#cause</code>. Async <code>Rdkafka::RdkafkaError</code> errors are still directly available and you can differentiate between errors using the event <code>type</code>.</li> <li>If you are using the Datadog listener, you need to:</li> </ol> <pre><code># Replace require:\nrequire 'waterdrop/instrumentation/vendors/datadog/listener'\n# With\nrequire 'waterdrop/instrumentation/vendors/datadog/metrics_listener'\n\n# Replace references of\n::WaterDrop::Instrumentation::Vendors::Datadog::Listener.new\n# With\n::WaterDrop::Instrumentation::Vendors::Datadog::MetricsListener.new\n</code></pre> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-WaterDrop-2.6/", "title": "Upgrading to WaterDrop 2.6", "text": "<ol> <li>Rename <code>wait_on_queue_full_timeout</code> to <code>wait_backoff_on_queue_full</code>.</li> <li>Set <code>wait_on_queue_full</code> to <code>false</code> if you did not use it and do not want.</li> </ol> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-WaterDrop-2.7/", "title": "Upgrading to WaterDrop 2.7", "text": "<p>PLEASE MAKE SURE TO READ AND APPLY THEM!</p>"}, {"location": "Upgrades-WaterDrop-2.7/#wait_timeout-configuration-no-longer-needed", "title": "<code>wait_timeout</code> Configuration No Longer Needed", "text": "<p>The <code>wait_timeout</code> WaterDrop configuration option is no longer needed. You can safely remove it.</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  # Other config...\n\n  # Remove this, no longer needed\n  config.wait_timeout = 30\nend\n</code></pre>"}, {"location": "Upgrades-WaterDrop-2.7/#time-settings-format-alignment", "title": "Time Settings Format Alignment", "text": "<p>All time-related values are now configured in milliseconds instead of some being in seconds and some in milliseconds.</p> <p>The values that were changed from seconds to milliseconds are:</p> <ul> <li><code>max_wait_timeout</code></li> <li><code>wait_backoff_on_queue_full</code></li> <li><code>wait_timeout_on_queue_full</code></li> <li><code>wait_backoff_on_transaction_command, default</code></li> </ul> <p>If you have configured any of those yourself, please replace the seconds representation with milliseconds:</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.deliver = true\n\n  # Replace this:\n  config.max_wait_timeout = 30\n\n  # With\n  config.max_wait_timeout = 30_000\n  # ...\nend\n</code></pre>"}, {"location": "Upgrades-WaterDrop-2.7/#defaults-alignment", "title": "Defaults Alignment", "text": "<p>In this release, we've updated our default settings to address a crucial issue: previous defaults could lead to inconclusive outcomes in synchronous operations due to wait timeout errors. Users often mistakenly believed that a message dispatch was halted because of these errors when, in fact, the timeout was related to awaiting the final dispatch verdict, not the dispatch action itself.</p> <p>The new defaults in WaterDrop 2.7.0 eliminate this confusion by ensuring synchronous operation results are always transparent and conclusive. This change aims to provide a straightforward understanding of wait timeout errors, reinforcing that they reflect the wait state, not the dispatch success.</p> <p>Below, you can find a table with what has changed, the new defaults, and the current ones in case you want to retain the previous behavior:</p> Config Previous Default New Default root <code>max_wait_timeout</code> 5000 ms (5 seconds) 60000 ms (60 seconds) kafka <code>message.timeout.ms</code> 300000 ms (5 minutes) 50000 ms (50 seconds) kafka <code>transaction.timeout.ms</code> 60000 ms (1 minute) 55000 ms (55 seconds) <p>This alignment ensures that when using sync operations or invoking <code>#wait</code>, any exception you get should give you a conclusive and final delivery verdict.</p>"}, {"location": "Upgrades-WaterDrop-2.7/#buffering-no-longer-early-validates-messages", "title": "Buffering No Longer Early Validates Messages", "text": "<p>As of version <code>2.7.0</code>, WaterDrop has changed how message buffering works. Previously, messages underwent validation and middleware processing when they were buffered. Now, these steps are deferred until just before dispatching the messages. The buffer functions strictly as a thread-safe storage area without performing any validations or middleware operations until the messages are ready to be sent.</p> <p>This adjustment was made primarily to ensure that middleware runs and validations are applied when most relevant\u2014shortly before message dispatch. This approach addresses potential issues with buffers that might hold messages for extended periods:</p> <ul> <li> <p>Temporal Relevance: Validating and processing messages near their dispatch time helps ensure that actions such as partition assignments reflect the current system state. This is crucial in dynamic environments where system states are subject to rapid changes.</p> </li> <li> <p>Stale State Management: By delaying validations and middleware to the dispatch phase, the system minimizes the risk of acting on outdated information, which could lead to incorrect processing or partitioning decisions.</p> </li> </ul> <pre><code># Prior to 2.7.0 this would raise an error\nproducer.buffer(topic: nil, payload: '')\n# =&gt; WaterDrop::Errors::MessageInvalidError\n\n# After 2.7.0 buffer will not, but flush_async will\nproducer.buffer(topic: nil, payload: '')\n# =&gt; all good here\nproducer.flush_async(topic: nil, payload: '')\n# =&gt; WaterDrop::Errors::MessageInvalidError\n</code></pre>"}, {"location": "Upgrades-WaterDrop-2.7/#middleware-execution-prior-to-flush-when-buffering", "title": "Middleware Execution Prior to Flush When Buffering", "text": "<p>The timing of middleware execution has been adjusted. Middleware, which was previously run when messages were added to the buffer, will now only execute immediately before the messages are flushed from the buffer and dispatched. This change is similar to the validation-related changes.</p> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-WaterDrop-2.8/", "title": "Upgrading to WaterDrop 2.8", "text": "<p>PLEASE MAKE SURE TO READ AND APPLY THEM!</p>"}, {"location": "Upgrades-WaterDrop-2.8/#throwabort-no-longer-allowed-to-abort-transactions", "title": "<code>throw(:abort)</code> No Longer Allowed To Abort Transactions", "text": "<p>Replace:</p> <pre><code>producer.transaction do\n  messages.each do |message|\n    # Pipe all events\n    producer.produce_async(topic: 'events', payload: message.raw_payload)\n  end\n\n  # And abort if no more events are needed\n  throw(:abort) if KnowledgeBase.more_events_needed?\nend\n</code></pre> <p>With:</p> <pre><code>producer.transaction do\n  messages.each do |message|\n    # Pipe all events\n    producer.produce_async(topic: 'events', payload: message.raw_payload)\n  end\n\n  # And abort if more events are no longer needed\n  raise(WaterDrop::AbortTransaction) if KnowledgeBase.more_events_needed?\nend\n</code></pre>"}, {"location": "Upgrades-WaterDrop-2.8/#return-break-and-throw-are-no-longer-allowed-inside-transaction-block", "title": "<code>return</code>, <code>break</code> and <code>throw</code> Are No Longer Allowed Inside Transaction Block", "text": "<p>Previously, transactions would abort if you exited early using <code>return</code>, <code>break</code>, or <code>throw</code>. This could create unexpected behavior, where users might not notice the rollback or have different intentions. For example, the following would trigger a rollback:</p> <pre><code>MAX = 10\n\ndef process(messages)\n  count = 0\n\n  producer.transaction do\n    messages.each do |message|\n      count += 1\n\n      producer.produce_async(topic: 'events', payload: message.raw_payload)\n\n      # This would trigger a rollback.\n      return if count &gt;= MAX\n    end\n  end\nend\n</code></pre> <p>This is a source of errors, hence such exits are no longer allowed. You can implement similar flow control inside of your methods that are wrapped in a WaterDrop transaction:</p> <pre><code>MAX = 10\n\ndef process(messages)\n  producer.transaction do\n    # Early return from this method will not affect the transaction.\n    # It will be committed\n    insert_with_limit(messages)\n  end\nend\n\ndef insert_with_limit(messages)\n  count = 0\n\n  messages.each do |message|\n    count += 1\n\n    producer.produce_async(topic: 'events', payload: message.raw_payload)\n\n    # This would trigger a rollback.\n    return if count &gt;= MAX\n  end\nend\n</code></pre> <p>Last modified: 2025-05-16 21:52:20</p>"}, {"location": "Upgrades-WaterDrop/", "title": "WaterDrop Upgrade Notes", "text": "<ul> <li>Upgrading to 2.8</li> <li>Upgrading to 2.7</li> <li>Upgrading to 2.6</li> <li>Upgrading to 2.5</li> </ul> <p>Last modified: 2025-04-29 12:52:58</p>"}, {"location": "Upgrades-Web-UI-0.10/", "title": "Upgrading to Web UI 0.10", "text": "<p>This is a major release that brings many things to the table.</p> <p>This version of the Karafka Web UI requires Karafka <code>&gt;= 2.4.7</code>. You can either upgrade both or upgrade Karafka first and then the Web UI. Karafka <code>2.4.7</code> is also compatible with Web UI <code>0.9.1</code>; thus, you can upgrade one at a time.</p>"}, {"location": "Upgrades-Web-UI-0.10/#configuration", "title": "Configuration", "text": "<p>Visibility Filters have been reorganized into messages policies.</p> <p>Please read the Policies API documentation and convert your visibility filters to policies.</p> <p>Your existing message-related visibility filter policies should now be assigned to a new configuration:</p> <pre><code>Karafka::Web.setup do |config|\n  config.ui.policies.messages = MyCustomRequestsPolicy.new\nend\n</code></pre>"}, {"location": "Upgrades-Web-UI-0.10/#deployment", "title": "Deployment", "text": "<p>Because of the reporting schema update, it is recommended to:</p> <ol> <li>Make sure you have upgraded to <code>0.9.1</code> before and that it was fully deployed.</li> <li>Test the upgrade on a staging or dev environment.</li> <li>The Web UI interface may throw 500 errors during the upgrade because of schema incompatibility (until Puma is deployed and all consumers redeployed). This will have no long-term effects and can be ignored.</li> <li><code>Karafka::Web::Errors::Processing::IncompatibleSchemaError</code> is expected. It is part of the Karafka Web UI zero-downtime deployment strategy. This error allows the Web UI materialization consumer to back off and wait for it to be replaced with a new one.</li> <li>Perform a rolling deployment (or a regular one) and replace all consumer processes.</li> <li>Update the Web UI Puma.</li> <li>No CLI command execution is required.</li> <li>Enjoy.</li> </ol> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-Web-UI-0.11/", "title": "Upgrading to Web UI 0.11", "text": "<p>Version Compatibility Requirement</p> <p>Karafka Web UI <code>0.11</code> requires Karafka <code>2.5</code> - these components must be upgraded together. Attempting to run Web UI <code>0.11</code> with older versions of Karafka will result in compatibility errors. Please ensure you upgrade both components as part of the same deployment process. Karafka <code>2.5</code> Upgrade Guide can be found here.</p> <p>Before upgrading to Karafka Web UI <code>0.11</code>, please review our General Karafka Upgrade Guide first. This document provides essential advice on upgrading Karafka and its components and general best practices to ensure a smooth transition. The general guide contains fundamental steps that apply to all upgrades, while this specific guide focuses only on the changes introduced in the Web UI <code>0.11</code> release. Following both guides will help you navigate the upgrade process with minimal disruption to your production systems.</p>"}, {"location": "Upgrades-Web-UI-0.11/#topics-configuration-structure-change", "title": "Topics Configuration Structure Change", "text": "<p>The most significant change in this release is the restructuring of the topics configuration. Previously, topic names were configured directly as strings, but now they are nested objects with both name and configuration properties.</p> <p>Before:</p> <pre><code>Karafka::Web.setup do |config|\n  config.topics.consumers.states = 'karafka_consumers_states'\n  config.topics.consumers.metrics = 'karafka_consumers_metrics'\n  config.topics.consumers.reports = 'karafka_consumers_reports'\n  config.topics.consumers.commands = 'karafka_consumers_commands'\n  config.topics.errors = 'karafka_errors'\nend\n</code></pre> <p>After:</p> <pre><code>Karafka::Web.setup do |config|\n  # Topic configurations now include both name and config properties\n  config.topics.consumers.states.name = 'karafka_consumers_states'\n  config.topics.consumers.metrics.name = 'karafka_consumers_metrics'\n  config.topics.consumers.reports.name = 'karafka_consumers_reports'\n  config.topics.consumers.commands.name = 'karafka_consumers_commands'\n  config.topics.errors.name = 'karafka_errors'\n\n  # Optional: You can also customize the topic configs if needed\n  # Example:\n  # config.topics.errors.config = {\n  #   'cleanup.policy': 'delete',\n  #   'retention.ms': 7 * 24 * 60 * 60 * 1_000 # 7 days\n  # }\nend\n</code></pre>"}, {"location": "Upgrades-Web-UI-0.11/#configuration-requirements", "title": "Configuration Requirements", "text": "<p>The topic configurations now have validation requirements:</p> <ol> <li>The <code>name</code> attribute must be a string and match the topic naming pattern</li> <li>The <code>config</code> attribute must:<ul> <li>Be a non-empty hash</li> <li>Have all keys as symbols (not strings)</li> </ul> </li> </ol>"}, {"location": "Upgrades-Web-UI-0.11/#direct-topic-references", "title": "Direct Topic References", "text": "<p>If you have any code that directly references the topic configuration, update it to use the <code>.name</code> attribute:</p> <p>Before:</p> <pre><code>topic_name = Karafka::Web.config.topics.errors\n</code></pre> <p>After:</p> <pre><code>topic_name = Karafka::Web.config.topics.errors.name\n</code></pre>"}, {"location": "Upgrades-Web-UI-0.11/#cross-platform-compatibility", "title": "Cross-Platform Compatibility", "text": "<p>This release includes significant improvements for compatibility across Debian, Alpine, and Wolfi operating systems:</p> <ul> <li>Dependency Reduction: The <code>grep</code> command is no longer required on any operating system</li> <li>Linux Simplification: On Linux systems (Debian, Alpine, Wolfi), the <code>head</code>, <code>w</code>, and <code>sysctl</code> commands are no longer needed</li> <li>macOS Requirements: These commands (<code>head</code>, <code>w</code>, and <code>sysctl</code>) are still required only on macOS (Darwin)</li> </ul> <p>These changes simplify deployments, especially in containerized environments where minimal images are preferred. The code now uses more platform-agnostic methods for gathering statistics, resulting in more consistent behavior across different operating systems without requiring additional dependencies.</p>"}, {"location": "Upgrades-Web-UI-0.11/#cache-system-rename", "title": "Cache System Rename", "text": "<p>If you're using custom cache configurations with the Web UI, be aware that <code>Karafka::Web::Ui::Lib::TtlCache</code> has been renamed to <code>Karafka::Web::Ui::Lib::Cache</code>. If you've customized the cache settings in your configuration, you'll need to update the reference:</p> <p>Before</p> <pre><code>Karafka::Web.setup do |config|\n  config.ui.cache = Karafka::Web::Ui::Lib::TtlCache.new(\n    # Your TTL settings\n    60_000 * 10 # Example: 10 minutes\n  )\nend\n</code></pre> <p>After:</p> <pre><code>Karafka::Web.setup do |config|\n  config.ui.cache = Karafka::Web::Ui::Lib::Cache.new(\n    # Your TTL settings\n    60_000 * 10 # Example: 10 minutes\n  )\nend\n</code></pre>"}, {"location": "Upgrades-Web-UI-0.11/#deployment", "title": "Deployment", "text": "<p>Because of the reporting schema update, it is recommended to:</p> <ol> <li>Make sure you have upgraded to <code>0.10.4</code> before and that it was fully deployed.</li> <li>Test the upgrade on a staging or dev environment.</li> <li>The Web UI interface may throw 500 errors during the upgrade because of schema incompatibility (until Puma is deployed and all consumers redeployed). This will have no long-term effects and can be ignored.</li> <li><code>Karafka::Web::Errors::Processing::IncompatibleSchemaError</code> is expected. It is part of the Karafka Web UI zero-downtime deployment strategy. This error allows the Web UI materialization consumer to back off and wait for it to be replaced with a new one.</li> <li>Perform a rolling deployment (or a regular one) and replace all consumer processes.</li> <li>Update the Web UI Puma.</li> <li>No CLI command execution is required.</li> <li>Enjoy.</li> </ol> <p>Last modified: 2025-06-15 19:56:38</p>"}, {"location": "Upgrades-Web-UI-0.3/", "title": "Upgrading to Web UI 0.3", "text": "<p>Because of the removal of compatibility fallbacks for some metrics fetches, it is recommended to:</p> <ul> <li>First, deploy all the Karafka consumer processes (<code>karafka server</code>)</li> <li>Deploy the Web update to your web server.</li> </ul> <p>Please note that if you decide to use the updated Web UI with not updated consumers, you may hit a 500 error.</p> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-Web-UI-0.4/", "title": "Upgrading to Web UI 0.4", "text": "<p>Because of the reporting schema change, it is recommended to:</p> <ul> <li>First, deploy all the Karafka consumer processes (<code>karafka server</code>)</li> <li>Deploy the Web update to your web server.</li> </ul> <p>Please note that if you decide to use the updated Web UI with not updated consumers, you may hit a 500 error.</p> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-Web-UI-0.5/", "title": "Upgrading to Web UI 0.5", "text": "<p>Because of the reporting schema change, it is recommended to:</p> <ul> <li>First, deploy all the Karafka consumer processes (<code>karafka server</code>)</li> <li>Deploy the Web update to your web server.</li> </ul> <p>Please note that if you decide to use the updated Web UI with not updated consumers, you may hit a 500 error or offset related data may not be displayed correctly.</p> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-Web-UI-0.6/", "title": "Upgrading to Web UI 0.6", "text": "<p>Because of the reporting schema update, it is recommended to:</p> <ul> <li>First, deploy all the Karafka consumer processes (<code>karafka server</code>)</li> <li>Deploy the Web update to your web server.</li> </ul> <p>Please note that if you decide to use the updated Web UI with not updated consumers, you may hit a 500 error or offset related data may not be displayed correctly.</p>"}, {"location": "Upgrades-Web-UI-0.6/#disabling-producers-instrumentation", "title": "Disabling producers instrumentation", "text": "<p>Producers error tracking is enabled by default. If you want to opt out of it, you need to disable the producers' instrumentation by clearing the producers' listeners:</p> <pre><code>Karafka::Web.setup do |config|\n  # Do not instrument producers with web-ui listeners\n  config.tracking.producers.listeners = []\nend\n</code></pre>"}, {"location": "Upgrades-Web-UI-0.6/#custom-producers-instrumentation", "title": "Custom producers instrumentation", "text": "<p>By default, Karafka Web-UI instruments only <code>Karafka.producer</code>. If you use producers initialized by yourself, you need to connect the listeners to them manually. To do so, run the following code:</p> <pre><code>::Karafka::Web.config.tracking.producers.listeners.each do |listener|\n  MY_CUSTOM_PRODUCER.monitor.subscribe(listener)\nend\n</code></pre> <p>Please make sure not to do it for the default <code>Karafka.producer</code> because it is instrumented out of the box.</p> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-Web-UI-0.7/", "title": "Upgrading to Web UI 0.7", "text": "<p>This is a major release that brings many things to the table.</p>"}, {"location": "Upgrades-Web-UI-0.7/#configuration", "title": "Configuration", "text": "<p>Karafka Web UI now relies on Roda session management. Please configure the <code>ui.sessions.secret</code> key with a secret value string of at least 64 characters:</p> <pre><code># Configure it BEFORE enabling\nKarafka::Web.setup do |config|\n  # REPLACE THIS with your own value. You can use `SecureRandom.hex(64)` to generate it\n  # You may want to set it per ENV\n  config.ui.sessions.secret = 'REPLACE ME! b94b2215cc66371f2c34b7d0c0df1a010f83ca45 REPLACE ME!'\nend\n\nKarafka::Web.enable!\n</code></pre>"}, {"location": "Upgrades-Web-UI-0.7/#deployment", "title": "Deployment", "text": "<p>Because of the reporting schema update and new web-ui topics introduction, it is recommended to:</p> <ol> <li>Make sure you have upgraded to <code>0.6.3</code> before and that it was deployed. To all the environments you want to migrate to <code>0.7.0</code>.</li> <li>Upgrade the codebase based on the below details.</li> <li>Stop the consumer materializing Web-UI. Unless you are running a Web-UI dedicated consumer as recommended here, you will have to stop all the consumers. This is crucial because of schema changes. <code>karafka-web</code> <code>0.7.0</code> introduces the detection of schema changes, so this step should not be needed in the future.</li> <li>Run a migration command: <code>bundle exec karafka-web migrate</code> that will create missing states and missing topics. You need to run it for each of the environments where you use Karafka Web UI.</li> <li>Deploy all the Karafka consumer processes (<code>karafka server</code>).</li> <li>Deploy the Web update to your web server and check that everything is OK by visiting the status page.</li> </ol> <p>Please note that if you decide to use the updated Web UI with not updated consumers, you may hit a 500 error, or offset-related data may not be displayed correctly.</p>"}, {"location": "Upgrades-Web-UI-0.7/#code-and-api-changes", "title": "Code and API Changes", "text": "<ol> <li><code>bundle exec karafka-web install</code> is now a single-purpose command that should run only when installing the Web-UI for the first time.</li> <li>For creating needed topics and states per environment and during upgrades, please use the newly introduced non-destructive <code>bundle exec karafka-web migrate</code>. It will assess changes required and will apply only those.</li> <li>Is no longer<code>ui.decrypt</code> has been replaced with <code>ui.visibility_filter</code> API. This API by default also does not decrypt data. To change this behavior, please implement your visibility filter as presented in our documentation.</li> <li>Karafka Web UI <code>0.7.0</code> introduces an in-memory topics cache for some views. This means that rapid topics changes (repartitions/new topics) may be visible up to 5 minutes after those changes.</li> <li><code>ui.decrypt</code> setting has been replaced with <code>ui.visibility_filter</code> API. This API by default also does not decrypt data. To change this behavior, please implement your visibility filter as presented in our documentation.</li> <li>Karafka Web-UI <code>0.7.0</code> introduces an in-memory topics cache for some views. This means that rapid topics changes (repartitions/new topics) may be visible up to 5 minutes after those changes.</li> <li>Karafka Web UI requires now a new topic called <code>karafka_consumers_metrics</code>. If you use strict topic creation and ACL policies, please make sure it exists and that Karafka can both read and write to it.</li> </ol>"}, {"location": "Upgrades-Web-UI-0.7/#config-changes", "title": "Config Changes", "text": "<ol> <li>If you are using <code>ui.visibility_filter</code> this option is now <code>ui.visibility.filter</code> (yes, only <code>.</code> difference).</li> <li>If you are using a custom visibility filter, it requires now two extra methods: <code>#download?</code> and <code>#export?</code>. The default visibility filter allows both actions unless message is encrypted.</li> <li><code>ui.show_internal_topics</code> config option has been moved and renamed to <code>ui.visibility.internal_topics</code>.</li> </ol> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-Web-UI-0.8/", "title": "Upgrading to Web UI 0.8", "text": "<p>This is a major release that brings many things to the table.</p>"}, {"location": "Upgrades-Web-UI-0.8/#configuration", "title": "Configuration", "text": "<p>No configuration changes are needed.</p>"}, {"location": "Upgrades-Web-UI-0.8/#deployment", "title": "Deployment", "text": "<p>Because of the reporting schema update, it is recommended to:</p> <ol> <li>Make sure you have upgraded to <code>0.7.10</code> before and that it was fully deployed.</li> <li>Test the upgrade on a staging or dev environment.</li> <li>Starting from <code>0.7.0</code> Karafka Web UI supports rolling deploys, so there is no need to \"stop the world\".</li> <li>The Web UI interface may throw 500 errors during the upgrade because of schema incompatibility (until Puma is deployed). This will have no long-term effects and can be ignored.</li> <li><code>Karafka::Web::Errors::Processing::IncompatibleSchemaError</code> is expected. It is part of the Karafka Web UI zero-downtime deployment strategy. This error allows the Web UI materialization consumer to back off and wait for it to be replaced with a new one.</li> <li>Perform a rolling deployment (or a regular one) and replace all consumer processes.</li> <li>Update the Web UI Puma.</li> <li>No CLI command execution is required. Starting from this release (<code>0.8.0</code>), the Karafka Web UI contains an automatic schema migrator that allows it to automatically adjust internal topic data formats.</li> <li>Enjoy.</li> </ol> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-Web-UI-0.9/", "title": "Upgrading to Web UI 0.9", "text": "<p>This is a major release that brings many things to the table.</p> <p>This version of the Karafka Web UI should be upgraded together with Karafka. All upgrade documentation for Karafka and Web UI <code>0.9</code> can be found here.</p> <p>Last modified: 2025-04-29 12:37:29</p>"}, {"location": "Upgrades-Web-UI/", "title": "Web UI Upgrade Notes", "text": "<ul> <li>Upgrading to 0.11</li> <li>Upgrading to 0.10</li> <li>Upgrading to 0.9</li> <li>Upgrading to 0.8</li> <li>Upgrading to 0.7</li> <li>Upgrading to 0.6</li> <li>Upgrading to 0.5</li> <li>Upgrading to 0.4</li> <li>Upgrading to 0.3</li> </ul> <p>Last modified: 2025-04-29 12:52:58</p>"}, {"location": "Upgrading/", "title": "Upgrading", "text": "<p>This documentation page provides recommended strategies for upgrading Karafka and its dependencies to ensure a smooth and seamless upgrade process. Upgrading Karafka and its dependencies is essential to benefit from the latest features, bug fixes, and security enhancements. It is necessary to follow these strategies to minimize disruptions and avoid compatibility issues during the upgrade.</p> <p>Pro &amp; Enterprise Upgrade Support</p> <p>If you're gearing up to upgrade to the latest Karafka version and are a Pro or Enterprise user, remember you've got a dedicated lifeline! Reach out via the dedicated Slack channel for direct support to ensure everything has been covered.</p>"}, {"location": "Upgrading/#pre-upgrade-considerations", "title": "Pre-Upgrade Considerations", "text": "<ol> <li> <p>Review the Changelogs: Read the release notes and changelog for each new version of Karafka and its dependencies. This will help you understand the changes, improvements, and potential breaking changes introduced in the latest version.</p> </li> <li> <p>Check Upgrade Guides and Recommendations: Refer to the upgrade guides and recommendations provided in the documentation for Karafka and its dependencies. These guides often highlight important considerations, steps, and best practices for the upgrade process. Following the recommended upgrade path can help ensure a smooth transition to the new version.</p> </li> <li> <p>Check Compatibility: Verify the compatibility of your existing Karafka application and its dependencies with the new version. Look for deprecated features or breaking changes that might affect your code or configurations.</p> </li> <li> <p>Test Environment: Set up a separate test environment that closely mirrors your production environment. Perform the upgrade and necessary tests in this environment before applying it to the production environment. This allows you to identify and resolve any potential issues or conflicts beforehand.</p> </li> </ol> <p>Considering these points, you'll be well-prepared for the upgrade process and can minimize potential disruptions or compatibility issues.</p>"}, {"location": "Upgrading/#upgrade-strategy", "title": "Upgrade Strategy", "text": "<ol> <li> <p>Read the upgrade guides: Karafka provides detailed upgrade guides for each major version release. These guides outline the steps required to upgrade from the previous version to the new version. Before proceeding with the upgrade, carefully read and follow the instructions in the upgrade guide.</p> </li> <li> <p>Upgrade one version at a time: Always upgrade by one version at a time. Move from <code>2.1</code> to <code>2.2</code>, <code>2.2</code> to <code>2.3</code>, <code>2.3</code> to <code>2.4</code>, etc. Do not ever jump versions, as it is not recommended. This approach ensures that you can address any version-specific deprecations, migrations, or changes in a controlled manner, minimizing the risk of introducing breaking changes or bugs.</p> </li> <li> <p>Upgrade Karafka and its dependencies: To upgrade the Karafka ecosystem, run the following command: </p> </li> </ol> <pre><code>bundle update karafka karafka-rdkafka karafka-core waterdrop karafka-web karafka-testing\n</code></pre> <p>It is recommended always to upgrade Karafka with its dependencies together and to always run the most recent resolvable versions of all the libraries creating the Karafka ecosystem.</p> <ol> <li> <p>Update configuration and align the APIs: Check the release notes and upgrade guide for any changes to the configuration options, defaults, or new configuration options introduced in the new version. Update your configuration files accordingly to ensure compatibility with the new version.</p> </li> <li> <p>Test and deploy to staging: Before deploying the upgraded Karafka application to production, it is recommended to run comprehensive tests to ensure functionality and catch any issues or regressions. Additionally, utilizing a staging environment that resembles the production environment allows for extensive testing and validation before deployment. This helps ensure a smoother deployment process by verifying functionality, detecting regressions, and validating performance.</p> </li> <li> <p>Deploy to production: When deploying the upgraded Karafka application, it is recommended first to deploy all the consumers and only then deploy the Web UI component. This sequential deployment ensures that the consumers are up and running to process incoming messages from Kafka while the Web UI is being upgraded.   If you attempt to deploy the updated Web UI before the Karafka consumer processes, you may encounter errors. This could range from 500 Internal Server errors to incorrect or missing offset-related data displays.   It's critical to ensure the order of operations - Karafka consumers processes first, then the Web UI. This will provide a smoother transition to the new version of the Web UI.</p> </li> <li> <p>Monitor: During and after the upgrade, closely monitor the application's performance, logs, and error reports.</p> </li> </ol> <p>Following these upgrade recommendations will help ensure a successful upgrade process for your Karafka application while minimizing disruptions and maintaining compatibility with the latest version.</p>"}, {"location": "Upgrading/#pro-upgrade-support", "title": "Pro Upgrade Support", "text": "<p>If you encounter difficulties while upgrading Karafka, particularly when upgrading from unsupported versions, we have a Pro offering that includes dedicated support. Our Pro support can provide you with the necessary assistance to address any upgrade challenges and ensure a successful upgrade process. Feel free to contact us to learn more about the Pro offering and how it can help you with your Karafka upgrade.</p> <p>Last modified: 2024-04-09 16:39:33</p>"}, {"location": "Versions-Lifecycle-and-EOL/", "title": "Versions Lifecycle and EOL", "text": "<p>This page lists the current maintenance status of the various Karafka versions.</p>"}, {"location": "Versions-Lifecycle-and-EOL/#versioning-strategy", "title": "Versioning Strategy", "text": "<p>Karafka and its components utilize a versioning strategy that does not strictly adhere to semantic versioning. Instead, it employs an approach to better accommodate the nature and needs of our software's development and maintenance. Here's how we structure our versioning:</p> <ul> <li> <p>Major Version Upgrades: The first digit in our version number represents significant rewrites or major changes in the architecture of Karafka gems. For example, transitioning from <code>0.x</code> to <code>1.0</code> or from <code>1.x</code> to <code>2.0</code> signifies transformative changes that introduce new concepts or substantially modify the system's design.</p> </li> <li> <p>Minor Version Updates: The second digit signifies significant yet not always backward-incompatible changes. Upgrading from <code>2.3</code> to <code>2.4</code>, for instance, may include changes that demand attention, such as adjusting default settings, aligning configuration, or introducing new features that enhance functionality without breaking existing implementations. Such updates always have an extensive upgrade guide.</p> </li> <li> <p>Patch Releases: The third digit is reserved for patch releases, focusing on bug fixes and minor, risk-free enhancements. These updates are intended to improve the stability and performance of the software without impacting the existing user base's operations.</p> </li> </ul> <p>This versioning system is designed to provide clarity and predictability, ensuring developers can understand the impact of upgrading Karafka and its components. By informing you about the nature of each release, we aim to help you make informed decisions regarding when and how to update your versions.</p>"}, {"location": "Versions-Lifecycle-and-EOL/#karafka-components-support", "title": "Karafka Components Support", "text": "<p>Karafka and its components versions or release series are categorized below into the following phases:</p> <ul> <li>Active: Branch receives general bug fixes, security fixes, and improvements.</li> <li>Maintenance: Only security and critical bug fixes are backported to this branch.</li> <li>EOL (end-of-life): Branch is no longer supported and receives no fixes. No further patch release will be released.</li> <li>Preview: Only previews or release candidates have been released for this branch so far.</li> </ul>"}, {"location": "Versions-Lifecycle-and-EOL/#karafka-framework", "title": "Karafka Framework", "text": "Version Status EOL date 2.5 Preview N/A 2.4 Active 2025-10-31 2.3 EOL 2025-03-01 2.2 EOL 2024-09-30 2.1 EOL 2024-05-01 2.0 EOL 2024-02-01 1.x EOL 2023-02-01"}, {"location": "Versions-Lifecycle-and-EOL/#karafka-web-ui", "title": "Karafka Web UI", "text": "Version Status EOL date 0.11 Preview N/A 0.10 Active 2025-10-31 0.9 EOL 2024-12-31 0.8 EOL 2024-08-31 0.7 EOL 2024-05-01 0.6 EOL 2023-12-01 0.5 EOL 2023-10-01 0.4 EOL 2023-08-01 0.3 EOL 2023-07-01 0.2 EOL 2023-05-01 0.1 EOL 2023-04-01"}, {"location": "Versions-Lifecycle-and-EOL/#waterdrop", "title": "WaterDrop", "text": "Version Status EOL date 2.8 Active N/A 2.7 EOL 2025-02-01 2.6 EOL 2024-09-30 2.x EOL 2024-05-01 1.x EOL 2023-02-01"}, {"location": "Versions-Lifecycle-and-EOL/#ruby-versions-support", "title": "Ruby Versions Support", "text": "<p>We officially provide support for all the versions of Ruby that are not EOL, and we align with their EOL schedule.</p> <p>If you are using an older Ruby version, Karafka may still work. The EOL table indicates versions we officially test and support.</p> Version Status EOL date 3.4 Active 2028-09-30 3.3 Active 2027-09-30 3.2 Active 2026-09-30 3.1 Maintenance 2025-09-30 3.0 EOL 2024-09-30 2.7 EOL 2024-05-30 2.6 EOL 2022-04-12"}, {"location": "Versions-Lifecycle-and-EOL/#ruby-on-rails-versions-support", "title": "Ruby on Rails Versions Support", "text": "<p>Karafka will support two major versions of Ruby on Rails. Any previous versions may or may not be supported depending on the effort and ability to provide features.</p> Version Status EOL date 8.0 Active N/A 7.2 Active 2026-09-30 7.1 Active 2025-11-30 7.0 Maintenance 2025-05-31 6.1 EOL 2024-12-31 5.2 EOL 2023-12-31 4.2 EOL 2021-05-01"}, {"location": "Versions-Lifecycle-and-EOL/#kafka-versions-support", "title": "Kafka Versions Support", "text": "<p>Karafka and its components are designed to maintain compatibility with all Kafka versions that meet the following conditions:</p> <ul> <li> <p>Kafka Versions: Karafka supports all Kafka versions that have yet to reach their End of Life (EOL). This ensures that users can confidently use Karafka with Kafka versions that are actively maintained and receive necessary security and bug fixes.</p> </li> <li> <p>librdkafka Compatibility: The underlying broker support for Karafka is anchored by the librdkafka library. As of now, librdkafka supports Kafka brokers in versions <code>1.0</code> or higher.</p> </li> <li> <p>Message Format: Karafka mandates the use of Message Format v2 or later. This ensures efficient message handling and leverages the capabilities introduced in this format.</p> </li> </ul> <p>By adhering to these compatibility conditions, Karafka ensures its users receive a stable, reliable, and up-to-date experience when integrating with Kafka ecosystems.</p>"}, {"location": "Versions-Lifecycle-and-EOL/#kafka-ecosystem-coverage", "title": "Kafka Ecosystem Coverage", "text": "<p>The following table outlines Kafka-compatible platforms that have been reported to work with Karafka:</p> Platform Compatibility Integration Tests Production Usage Verified Notes Apache Kafka Complete Yes High Yes Primary development platform with comprehensive testing across all features including transactions. Redpanda High No Medium Yes Most features work as expected; verified through manual testing and users usage. WarpStream Basic No Low No Basic operations reported to work but not extensively tested <p>Compatibility Status Explanation</p> <p>This table reflects our current knowledge about compatibility with different Kafka-compatible platforms. \"Verified\" indicates whether the Karafka users have independently confirmed compatibility. \"Production Usage\" reflects reported usage by the community.</p>"}, {"location": "Versions-Lifecycle-and-EOL/#compatibility-levels", "title": "Compatibility Levels", "text": "<ul> <li>Complete: All features work as expected</li> <li>High: Most features work with minor limitations</li> <li>Basic: Core features work but advanced features may not be tested</li> </ul>"}, {"location": "Versions-Lifecycle-and-EOL/#production-usage", "title": "Production Usage", "text": "<ul> <li>High: Widely reported usage in production environments</li> <li>Medium: Several known production deployments</li> <li>Low: Limited or no known production deployments</li> </ul> <p>Last modified: 2025-05-17 10:43:50</p>"}, {"location": "WaterDrop-About/", "title": "About WaterDrop", "text": "<p>WaterDrop is a standalone gem that sends messages to Kafka easily with an extra validation layer. It is a part of the Karafka ecosystem.</p> <p>It:</p> <ul> <li>Is thread-safe</li> <li>Supports sync producing</li> <li>Supports async producing</li> <li>Supports transactions</li> <li>Supports buffering</li> <li>Supports producing messages to multiple clusters</li> <li>Supports multiple delivery policies</li> <li>Works with Kafka <code>1.0+</code> and Ruby <code>2.7+</code></li> <li>Works with and without Karafka</li> </ul> <p>Please visit the Getting Started page for instructions on installing, configuring, and using WaterDrop.</p> <p>Last modified: 2023-10-25 17:28:02</p>"}, {"location": "WaterDrop-Configuration/", "title": "WaterDrop Configuration", "text": "<p>WaterDrop is a complex tool that contains multiple configuration options. To keep everything organized, all the configuration options were divided into two groups:</p> <ul> <li>WaterDrop options - options directly related to WaterDrop and its components</li> <li>Kafka driver options - options related to <code>librdkafka</code></li> </ul> <p>To apply all those configuration options, you need to create a producer instance and use the <code>#setup</code> method:</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.deliver = true\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'request.required.acks': 1\n  }\nend\n</code></pre> <p>or you can do the same while initializing the producer:</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.deliver = true\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'request.required.acks': 1\n  }\nend\n</code></pre>"}, {"location": "WaterDrop-Configuration/#waterdrop-configuration-options", "title": "WaterDrop configuration options", "text": "<p>Some of the options are:</p> Option Description <code>id</code> id of the producer for instrumentation and logging <code>logger</code> Logger that we want to use <code>client_class</code> Client class for creating the underlying client used to dispatch messages <code>deliver</code> Should we send messages to Kafka or just fake the delivery <code>max_wait_timeout</code> Waits that long for the delivery report or raises an error <code>wait_on_queue_full</code> Should be wait on queue full or raise an error when that happens <code>wait_backoff_on_queue_full</code> Waits that long before retry when queue is full <code>wait_timeout_on_queue_full</code> If back-offs and attempts that that much time, error won't be retried more <code>wait_backoff_on_transaction_command</code> How long to wait before retrying a retryable transaction related error <code>max_attempts_on_transaction_command</code> How many times to retry a retryable transaction related error before giving up <code>instrument_on_wait_queue_full</code> Should we instrument when <code>queue_full</code> occurs <p>Full list of the root configuration options is available here.</p>"}, {"location": "WaterDrop-Configuration/#kafka-configuration-options", "title": "Kafka configuration options", "text": "<p>You can create producers with different <code>kafka</code> settings. Full list of the Kafka configuration options is available here.</p>"}, {"location": "WaterDrop-Configuration/#idempotence", "title": "Idempotence", "text": "<p>When idempotence is enabled, the producer will ensure that messages are successfully produced exactly once and in the original production order.</p> <p>To enable idempotence, you need to set the <code>enable.idempotence</code> kafka scope setting to <code>true</code>:</p> <pre><code>WaterDrop::Producer.new do |config|\n  config.deliver = true\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'enable.idempotence': true\n  }\nend\n</code></pre> <p>The following Kafka configuration properties are adjusted automatically (if not modified by the user) when idempotence is enabled:</p> <ul> <li><code>max.in.flight.requests.per.connection</code> set to <code>5</code></li> <li><code>retries</code> set to <code>2147483647</code></li> <li><code>acks</code> set to <code>all</code></li> </ul> <p>The idempotent producer ensures that messages are always delivered in the correct order and without duplicates. In other words, when an idempotent producer sends a message, the messaging system ensures that the message is only delivered once to the message broker and subsequently to the consumers, even if the producer tries to send the message multiple times.</p> <p>You can read more about idempotence and acknowledgements settings here.</p>"}, {"location": "WaterDrop-Configuration/#compression", "title": "Compression", "text": "<p>WaterDrop supports following compression types:</p> <ul> <li><code>gzip</code></li> <li><code>zstd</code></li> <li><code>lz4</code></li> <li><code>snappy</code></li> </ul> <p>To use compression, set <code>kafka</code> scope <code>compression.codec</code> setting. You can also optionally indicate the <code>compression.level</code>:</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'compression.codec': 'gzip',\n    'compression.level': 6\n  }\nend\n</code></pre> <p>Keep in mind, that in order to use <code>zstd</code>, you need to install <code>libzstd-dev</code>:</p> <pre><code>apt-get install -y libzstd-dev\n</code></pre>"}, {"location": "WaterDrop-Configuration/#message-size-validation", "title": "Message Size Validation", "text": "<p>When working with WaterDrop, it's essential to know the various checks and validations to ensure the integrity and feasibility of producing messages. This section explains the message size validation process in WaterDrop, librdkafka, and Kafka.</p> <p>There are three primary parameters to consider:</p> <ol> <li> <p>WaterDrop <code>max_payload_size</code>: this value checks only the payload size during client-side message validation.</p> </li> <li> <p>librdkafka <code>message.max.bytes</code>: This is a configuration value determining the maximum size of a message. Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions, the producer cannot reliably enforce a strict max message limit at production time and may exceed the maximum size by one message in protocol <code>ProduceRequests</code>. The broker will enforce the topic's <code>max.message.bytes</code> limit automatically.</p> </li> <li> <p>Broker max.message.bytes: is a broker-level configuration in Apache Kafka that determines the maximum size of a message the broker will accept. If a producer attempts to send a message larger than this specified size, the broker will reject it. </p> </li> </ol>"}, {"location": "WaterDrop-Configuration/#validation-flow", "title": "Validation Flow", "text": "<ol> <li>WaterDrop Client-Side Validation:</li> </ol> <ul> <li> <p>Before a message reaches librdkafka, WaterDrop checks the <code>max_payload_size</code> to ensure the message payload is within permissible limits.</p> </li> <li> <p>It's worth noting that this validation only concerns the payload and not additional elements like metadata, headers, and key.</p> </li> </ul> <ol> <li>librdkafka Validation:</li> </ol> <ul> <li> <p>librdkafka, before publishing, validates the uncompressed size of the message.</p> </li> <li> <p>This check ensures that the message size adheres to configured standards even before compression.</p> </li> </ul> <ol> <li>Broker-Side Validation:</li> </ol> <ul> <li> <p>After the message is dispatched, the broker then validates the compressed size.</p> </li> <li> <p>The distinction between compressed and uncompressed size is essential because of the potential compression ratios achievable with different compression algorithms.</p> </li> </ul> <p>Below you can find examples where each of the validations layers fails:</p> <ol> <li>WaterDrop raising the <code>WaterDrop::Errors::MessageInvalidError</code> because of the payload being too big (1MB):</li> </ol> <pre><code># Topic limit 1MB\n# Payload too big\nproducer.produce_async(topic: 'test', payload: '1' * 1024 * 1024)\n# {:payload=&gt;\"is more than `max_payload_size` config value\"}\n</code></pre> <ol> <li>librdkafka raising an error because of the message being too large:</li> </ol> <pre><code># Topic limit 1MB\n# Small payload\n# Large headers\n# message.max.bytes: 10 000\nKarafka.producer.produce_sync(\n  topic: 'test',\n  payload: '1',\n  key: '1',\n  headers: { rand.to_s =&gt; '1' * 1024 * 1024 }\n)\n\n# Error occurred: #&lt;Rdkafka::RdkafkaError: Broker:\n#     Message size too large (msg_size_too_large)&gt; - message.produce_sync\n# `rescue in produce_async': #&lt;Rdkafka::RdkafkaError:\n#     Broker: Message size too large (msg_size_too_large)&gt; (WaterDrop::Errors::ProduceError)\n</code></pre> <ol> <li>librdkafka raising an error received from the broker</li> </ol> <pre><code># Topic limit 1MB\n# Small payload\n# Large headers\n# message.max.bytes: 10MB\n\nKarafka.producer.produce_sync(\n  topic: 'test',\n  payload: '1',\n  key: '1',\n  headers: { rand.to_s =&gt; '1' * 1024 * 1024 * 10 }\n)\n\n# Error occurred: #&lt;Rdkafka::RdkafkaError: Broker:\n#     Message size too large (msg_size_too_large)&gt; - message.produce_sync\n# `rescue in produce_async': #&lt;Rdkafka::RdkafkaError:\n#     Broker: Message size too large (msg_size_too_large)&gt; (WaterDrop::Errors::ProduceError)\n</code></pre> <p>The <code>msg_size_too_large error</code> can arise from:</p> <ul> <li> <p>Local Validation by librdkafka: Before reaching the Kafka broker, if a message size exceeds the library's limit.</p> </li> <li> <p>Kafka Broker Rejection: If the broker finds the message too big based on its configuration.</p> </li> </ul> <p>Both scenarios produce the same <code>msg_size_too_large</code> error code, making them indistinguishable in code.</p> <p>When addressing this error, check message size settings in both librdkafka and the Kafka broker.</p>"}, {"location": "WaterDrop-Configuration/#adjusting-validation-parameters", "title": "Adjusting Validation Parameters", "text": "<ol> <li>Disabling max_payload_size:</li> </ol> <ul> <li> <p>If you don't use dummy or buffered clients for testing, it's possible to turn off <code>max_payload_size</code>.</p> </li> <li> <p>This can be done by setting it to a high value and bypassing this validation step.</p> </li> <li> <p>However, librdkafka will still validate the uncompressed size of the entire message, including headers, metadata, and key.</p> </li> </ul> <ol> <li>Interpreting Validation Errors:</li> </ol> <ul> <li> <p>A discrepancy between <code>max_payload_size</code> and <code>message.max.bytes</code> may arise due to the additional size from metadata, headers, and keys.</p> </li> <li> <p>Hence, it's possible to bypass WaterDrop's validation but fail on librdkafka's end.</p> </li> </ul>"}, {"location": "WaterDrop-Configuration/#conclusion", "title": "Conclusion", "text": "<p>Understanding the nuances of message size validation is crucial to ensure smooth message production. While it may seem complex at first, being mindful of the distinctions between uncompressed and compressed sizes and client-side and broker-side validations can prevent potential pitfalls and disruptions in your Kafka workflows.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "WaterDrop-Custom-Partitioners/", "title": "WaterDrop Custom Partitioners", "text": "<p>In Apache Kafka, a partitioner determines how records are placed among the partitions of a topic. While WaterDrop provides default partitioning strategies, there are scenarios where a custom partitioner is advantageous for achieving more control and efficiency in how messages are distributed across the partitions.</p>"}, {"location": "WaterDrop-Custom-Partitioners/#reasons-for-using-a-custom-partitioner", "title": "Reasons for Using a Custom Partitioner", "text": "<ol> <li> <p>Specific Data Distribution Needs: You may want to distribute messages based on specific data attributes, such as ensuring all messages from a particular user or entity end up in the same partition to maintain order.</p> </li> <li> <p>Load Balancing: If the data has certain hotspots (e.g., a few keys are very common), a custom partitioner can help distribute the load more evenly across the partitions, preventing any single partition from becoming a bottleneck.</p> </li> <li> <p>Performance Optimization: In some instances, optimizing partitioning logic based on consumption patterns can lead to more efficient data processing.</p> </li> <li> <p>Compatibility and Integration: When integrating with other systems, you might need to align your partitioning strategy with the external systems for seamless data flow and processing.</p> </li> </ol>"}, {"location": "WaterDrop-Custom-Partitioners/#building-a-custom-partitioner", "title": "Building a Custom Partitioner", "text": "<p>When integrating a custom partitioning strategy into your Kafka setup with WaterDrop, you generally have two options:</p> <ol> <li>Writing an entirely external custom partitioner</li> <li>Utilizing WaterDrop's middleware for the partitioning logic.</li> </ol> <p>Both approaches have merits and drawbacks, primarily influenced by the nature of your data and the specifics of your use case.</p> <p>In both cases, you need to know the partition count for a given topic. You can retrieve this information by using the <code>WaterDrop::Producer#partition_count</code> method:</p> <pre><code>producer.partition_count('users_events') =&gt; 5\n</code></pre> <p>Failure To Fetch Partition Count</p> <p>If WaterDrop Producer cannot retrieve topic metadata, it will report the partition count as <code>-1</code>. Ensure your custom partitioner can handle this scenario gracefully, possibly by reverting to a default partition or implementing a retry mechanism. Monitoring the occurrence of this issue is recommended to identify potential underlying system problems.</p>"}, {"location": "WaterDrop-Custom-Partitioners/#fully-external-custom-partitioner", "title": "Fully External Custom Partitioner", "text": "<p>This approach involves creating a partitioner that operates outside of the WaterDrop producer. Essentially, you're looking at writing a wrapper or a separate component that manages partitioning before handing off the message to the producer.</p> <p>Advantages:</p> <ul> <li>Full Control Over Data: You have access to the raw data before any serialization or processing has occurred. This is particularly useful if your partitioning logic requires complex operations on the data's original structure or format.</li> <li>Flexibility: As a standalone component, the external partitioner can be designed independently from the producer, making it easier to adapt or replace without affecting the producer's internals.</li> </ul> <p>Drawbacks:</p> <ul> <li>Complexity: This method introduces additional layers to your architecture, potentially increasing the complexity of your system.</li> <li>Maintenance: You need to ensure that the external partitioner and the producer are well-integrated and that any changes in one don't adversely affect the other.</li> </ul> <pre><code>class UserPartitioner\n  def partition(user)\n    user.id % PRODUCER.partition_count('default')\n  end\nend\n\npartitioner = UserPartitioner.new\n\nPRODUCER.produce_async(\n  topic: 'users_topic',\n  payload: user.to_json,\n  partition: partitioner.partition(user)\n)\n</code></pre>"}, {"location": "WaterDrop-Custom-Partitioners/#middleware-approach", "title": "Middleware Approach", "text": "<p>Alternatively, you can leverage WaterDrop's middleware to inject your custom partitioning logic directly into the message processing pipeline. This is seen as a more elegant and integrated approach.</p> <p>Advantages:</p> <ul> <li>Seamless Integration: The partitioning logic encapsulates the producer's workflow, making the overall process more streamlined.</li> <li>Ease of Use: Middleware is easy to implement and fits naturally into the WaterDrop ecosystem, making it a developer-friendly option.</li> </ul> <p>Drawbacks:</p> <ul> <li> <p>Limited Data Access: Middleware operates on the message after it's been prepared for dispatch. This means it only has access to the data post-serialization. This could be a significant limitation if your partitioning logic needs to work with the data in its original format. However, it can be bypassed if you decide to serialize data directly in the middleware.</p> </li> <li> <p>Implicit Flow: Implementing partitioning logic as middleware might not be explicitly clear, leading to confusion. The middleware's internal workings and position in the execution chain must be well understood.</p> </li> </ul> <pre><code>class PartitioningMiddleware\n  def call(message)\n    case message[:topic].to_s\n    when 'users_events'\n      # Make sure there is no partition key not to trigger default partitioner\n      message.delete(:partition_key)\n      # Distribute users based on their ids\n      user_id = message[:payload].user_id\n      message[:partition] = user_id % partition_count('users_events')\n      message[:payload] = message[:payload].to_json\n    when 'system_events'\n      message.delete(:partition_key)\n      # Distribute randomly amongst available partitions\n      message[:partition] = rand(partition_count('users_events'))\n      message[:payload] = message[:payload].to_json\n    else\n      message[:payload] = message[:payload].to_s\n    end\n\n    message\n  end\n\n  private\n\n  def partition_count(topic)\n    PRODUCER.partition_count(topic)\n  end\nend\n\n# Inject your middleware\nPRODUCER.middleware.append(PartitioningMiddleware.new)\n\n# And now you can provide your objects, and they will be serialized and\n# assigned to proper partitions automatically\nPRODUCER.produce_async(topic: 'users_events', payload: user)\n</code></pre>"}, {"location": "WaterDrop-Custom-Partitioners/#summary", "title": "Summary", "text": "<p>In summary, the choice between an external custom partitioner and a middleware-based partitioner in WaterDrop hinges on your specific requirements, particularly regarding how and when your data needs to be processed for partitioning. If pre-serialization data manipulation is crucial for your partitioning logic, an external partitioner might be more suitable. On the other hand, if you prefer a more integrated approach and your partitioning logic can work with serialized data, using WaterDrop's middleware might be the optimal path.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "WaterDrop-Error-Handling/", "title": "Error Handling", "text": "<p>Understanding WaterDrop error handling mechanisms is crucial for developing robust and reliable Kafka-based applications.</p> <p>This document focuses on error handling for the Standard Producer (non-transactional). For information regarding the error behavior of the Transactional Producer, it is highly recommended to refer to the Transactional Producer documentation.</p> <p>WaterDrop operates with a fully asynchronous architecture, maintaining a memory buffer for efficiently handling messages. This buffer stores messages waiting to be dispatched or already in the delivery process. Following the delivery of a message or the occurrence of an error after reaching the maximum retry limit, WaterDrop enqueues a delivery event into an internal event queue. This event includes the relevant delivery outcome, ensuring that each message's status is accurately tracked and managed within the system.</p>"}, {"location": "WaterDrop-Error-Handling/#operational-modes", "title": "Operational Modes", "text": "<p>WaterDrop provides three distinct APIs, each with unique error-handling behaviors that are essential to understand for practical usage:</p> <ol> <li> <p>Single Message Dispatch (<code>#produce_sync</code> and <code>#produce_async</code>): These methods send a single message to Kafka. Errors in this mode are specifically related to the individual message being sent. If an error occurs, it's directly tied to the single message dispatch attempt, making it straightforward to identify and handle issues related to message production or delivery.</p> </li> <li> <p>Batch Dispatch (<code>#produce_many_sync</code> and <code>#produce_many_async</code>): These methods allow sending multiple messages to Kafka in a batch. In this mode, errors can be more complex, as they might pertain to any single message within the batch. It's vital to have a strategy to identify which message(s) caused the error and respond accordingly. Error handling in this context needs to consider partial failures where some messages are dispatched successfully while others are not.</p> </li> <li> <p>Transactional Dispatch: This mode supports operations within a Kafka transaction. It suits scenarios where you must maintain exactly-once delivery semantics or atomicity across multiple messages and partitions. Errors in this mode can be transaction-wide, affecting all messages sent within the transaction's scope. The transactional producer operates under its own set of rules and complexities, and it's crucial to refer to the specific documentation page dedicated to transactional dispatch for guidance on handling errors effectively.</p> </li> </ol>"}, {"location": "WaterDrop-Error-Handling/#error-types", "title": "Error Types", "text": "<p>In WaterDrop, errors encountered during the message-handling process can be categorized into five distinct types. Each type represents a specific stage or aspect of the message delivery lifecycle, highlighting the diverse issues in a message queuing system.</p> <ul> <li> <p>Pre-Handle Inline Errors: These errors occur at the initial stage of message production, preventing the creation of a delivery handle. Inline errors indicate the message has not been sent to the message queue. A typical example of this type of error is the <code>:queue_full</code>, which occurs when the message cannot be queued due to a lack of available buffer space. This type of error is immediate and directly related to the message production process and indicates a dispatch failure.</p> </li> <li> <p>Wait Timeout Errors: This error arises when there is an exception during the invocation of the <code>#wait</code> method on a delivery handle. This can happen either when calling <code>#wait</code> directly after <code>#produce_async</code>, or when producing messages synchronously, especially if the maximum wait time is reached. Notably, a wait error does not necessarily mean that the message will not be delivered; it primarily indicates that the allotted wait time for the message to be processed was exceeded. Please know that <code>#wait</code> can raise additional errors, indicating final delivery failure. With the default configuration where <code>max_wait_timeout</code> exceeds other message delivery timeouts, the <code>#wait</code> raised error should always be final.</p> </li> <li> <p>Intermediate Errors: These errors can occur anytime, are not necessarily linked to producing specific messages, do not happen inline, and are published via the <code>error.occurred</code> notifications channel. They usually signify operational problems within the system and are often temporary. Intermediate errors might indicate issues such as network interruptions or temporary system malfunctions. They are not directly tied to the fate of individual messages but rather to the overall health and functioning of the messaging system.</p> </li> <li> <p>Delivery Failures: This type of error is specifically related to the non-delivery of a message. A delivery failure occurs when a message, identifiable by its label, is retried several times but ultimately fails to be delivered. After a certain period, WaterDrop determines that it is no longer feasible to continue attempting delivery. This error signifies a definitive failure in the message delivery process, marking the end of the message's lifecycle with a non-delivery status.</p> </li> <li> <p>ProduceMany Errors: During non-transactional batch dispatches, some messages may be successfully enqueued, and some may not. In such a case, this error will be raised. It will contain a <code>#dispatched</code> method with appropriate delivery handles for successfully enqueued messages. Those messages have the potential to be delivered based on their delivery report, but messages without matching delivery handles were for sure rejected and not enqueued for delivery.</p> </li> <li> <p>Transactional ProduceMany Errors: In a transactional batch dispatch, all messages within the transaction are either successfully enqueued and delivered together or not at all. If a failure occurs during the transaction, no messages are dispatched, and a rollback is performed. Therefore, the <code>#dispatched</code> method will always be empty in this error, as either all messages have been delivered successfully or none have been delivered. The transactional nature ensures atomicity, meaning that partial success or failure is not possible, and no message delivery handles will be available for any messages in case of a rollback.</p> </li> </ul> <p>Each error type plays a crucial role in understanding and managing the complexities of message handling in WaterDrop, providing precise categorization for troubleshooting and system optimization.</p>"}, {"location": "WaterDrop-Error-Handling/#errors-impact-on-the-delivery", "title": "Errors' Impact on the Delivery", "text": "Error Type Delivery Failed Details Pre Handle Inline Errors Yes Errors occurring before delivery confirmation suggest non-delivery. For non-transactional batches, partial delivery may occur. <code>ProduceManyError</code> is raised, detailing messages via <code>#dispatched</code> for successful sends, while <code>#cause</code> reveals the original error. Wait Timeout Errors No The <code>Rdkafka::Producer::WaitTimeoutError</code> occurs when the <code>#wait</code> exceeds its limit without receiving a delivery report. It implies prolonged waiting, not necessarily message non-delivery. Intermediate Errors on <code>error.occurred</code> No Intermediate errors without a <code>delivery_report</code> key in <code>error.occurred</code> are temporary, identified by a <code>librdkafka.error</code> type, indicating ongoing processes or transient issues. Wait Inline Errors (excluding <code>WaitTimeoutError</code>) Yes Errors from <code>#wait</code> other than <code>WaitTimeoutError</code> signify an available delivery report with errors. In <code>ProduceManyError</code> cases, delivery may be partial; check <code>#dispatched</code> for success and <code>#cause</code> for error origins. ProduceMany Errors Partially Yes <code>WaterDrop::Errors::ProduceManyError</code>s are raised during batch dispatches with full queues. Some messages may be sent successfully (see <code>#dispatched</code> for details), while others fail. The <code>#cause</code> method provides the specific error reason. Transactional ProduceMany Errors Yes <code>WaterDrop::Errors::ProduceManyError</code>s raised during transactional batch dispatches. If a failure occurs, no messages are sent, and a rollback is performed. The <code>#dispatched</code> method will be empty, as either all messages are successfully enqueued, or none are. The <code>#cause</code> method provides the specific error reason."}, {"location": "WaterDrop-Error-Handling/#tracking-deliveries-and-errors", "title": "Tracking Deliveries and Errors", "text": "<p>Due to WaterDrop's asynchronous nature, we recommend either using a transactional producer with its inline collective error handling and delivery warranties or using the async API and using the <code>error.occurred</code> notifications to detect and recognize messages that were not successfully delivered with inline error tracking for issues that would arise before the message had a chance to be enqueued for delivery.</p> <p>Every event published to <code>error.occurred</code> contains a type, and if the type is not <code>librdkafka.dispatch_error</code>, the error is intermediate or partial. It is worth logging but does not indicate that the dispatched message was not delivered. Events with the type set to <code>librdkafka.dispatch_error</code> will always contain a full delivery report for each enqueued message with exact details on why a message was not delivered.</p> <p>In general, we recommend following a similar flow to the one below:</p>"}, {"location": "WaterDrop-Error-Handling/#single-message-dispatch", "title": "Single Message Dispatch", "text": "<p>All the potential errors that will relate to a single dispatched message:</p> <pre><code>begin\n  # sync or async\n  producer.produce_async(...)\nrescue =&gt; e\n  case e\n  when WaterDrop::Errors::MessageInvalidError\n    puts \"Message is invalid and was not sent\"\n    raise e\n  when WaterDrop::Errors::ProduceError\n    puts \"Final error instrumented as error.occurred so we don't need to re-raise it\"\n    puts \"WaterDrop has already retried `Rdkafka::RdkafkaError`s due to :queue_full if wait_on_queue_full was enabled, and the wait time did not exceed wait_timeout_on_queue_full. This means we did not get the delivery handle and this dispatch will for sure not reach Kafka\"\n    puts \"If the underlying cause was a Rdkafka::Producer::DeliveryHandle::WaitTimeoutError (for #produce_sync only), the message could still reach Kafka\"\n  else\n    # Any other errors. This should not happen and indicates trouble.\n    puts \"Something else have happened. Read the error for details\"\n    puts \"This dispatch will not reach Kafka\"\n    raise e\n  end\nend\n</code></pre> <pre><code>producer.monitor.subscribe('error.occurred') do |event|\n  case event[:type]\n  # Every single message that received a handler is either delivered or fails \n  when 'librdkafka.dispatch_error'\n    puts \"Message with label: #{event[:delivery_report].label} failed to be delivered\"\n    ErrorsTracker.track(event[:error])\n  when\n    # Track all the others\n    ErrorsTracker.track(event[:error])\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Error-Handling/#batch-messages-dispatch", "title": "Batch Messages Dispatch", "text": "<p>Errors may refer to one of the messages from the batch, and part of the messages might have been dispatched:</p> <pre><code>begin\n  # sync or async\n  producer.produce_many_async(...)\nrescue =&gt; e\n  case e\n  when WaterDrop::Errors::ProduceManyError\n    puts \"Something went wrong and we need to check messages\"\n    successful_handles = e.dispatched\n    puts \"Messages with following labels have handles: #{successful_handles.map(&amp;:label)}\"\n    puts \"Those without handles were for sure not delivered\"\n    puts \"The original cause was: #{e.cause}\"\n  else\n    # Any other errors. This should not happen and indicates trouble.\n    raise e\n  end\nend\n</code></pre> <pre><code>producer.monitor.subscribe('error.occurred') do |event|\n  case event[:type]\n  # Every single message that received a handle is delivered or fails event \n  when 'librdkafka.dispatch_error'\n    puts \"Message with label: #{event[:delivery_report].label} failed to be delivered\"\n    ErrorsTracker.track(event[:error])\n  when\n    # Track all the others\n    ErrorsTracker.track(event[:error])\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Error-Handling/#summary", "title": "Summary", "text": "<p>WaterDrop's error handling is designed to manage the complexities of message delivery in Kafka through three primary APIs, each with distinct behaviors. Understanding these modes is crucial because error-handling strategies depend heavily on the dispatch method used, and each requires a different approach to ensure messages are reliably delivered or properly retried. Additionally, WaterDrop provides detailed error information, helping developers understand the context and cause of failures to implement effective recovery strategies. Along with monitoring errors and tracking delivery reports, it's also recommended to use WaterDrop's Labeling capabilities. This feature allows you to tag messages with labels associated with their dispatches during the dispatch lifecycle, making tracking and managing messages throughout their journey easier.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "WaterDrop-Getting-Started/", "title": "Getting Started with WaterDrop", "text": "<p>If you want to both produce and consume messages, please use Karafka. It integrates WaterDrop automatically.</p> <p>To get started with WaterDrop:</p> <ol> <li>Add it to your Gemfile:</li> </ol> <pre><code>bundle add waterdrop\n</code></pre> <ol> <li>Create and configure a producer:</li> </ol> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.deliver = true\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'request.required.acks': 1\n  }\nend\n</code></pre> <ol> <li>Use it as follows:</li> </ol> <pre><code># And use it\nproducer.produce_sync(topic: 'my-topic', payload: 'my message')\n\n# or for async\nproducer.produce_async(topic: 'my-topic', payload: 'my message')\n\n# or in batches\nproducer.produce_many_sync(\n  [\n    { topic: 'my-topic', payload: 'my message'},\n    { topic: 'my-topic', payload: 'my message'}\n  ]\n)\n\n# both sync and async\nproducer.produce_many_async(\n  [\n    { topic: 'my-topic', payload: 'my message'},\n    { topic: 'my-topic', payload: 'my message'}\n  ]\n)\n</code></pre> <p>For additional WaterDrop usage examples, please refer to the Usage section of this documentation.</p> <p>Last modified: 2023-10-25 17:28:02</p>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/", "title": "Idempotence and Acknowledgements", "text": "<p>This document covers the concepts of idempotence and acknowledgments in the context of using WaterDrop.</p> <p>It explores the roles of idempotence, acknowledgments, and relevant configurations like <code>replication_factor</code> and <code>min.insync.replicas</code>. These mechanisms work together to ensure data consistency, fault tolerance, and durability in distributed messaging systems like Kafka.</p>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#idempotence", "title": "Idempotence", "text": "<p>Idempotence ensures that an operation can be performed multiple times without changing the result beyond the initial application. In the context of Kafka and message processing:</p> <ul> <li>When a producer sends messages to Kafka, idempotence guarantees that duplicate messages (caused by retries, network issues, or any other transient errors) will not be written more than once.</li> <li>This is particularly useful in distributed systems where retries are common, and the goal is to avoid processing the same message multiple times.</li> </ul>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#waterdrop-idempotence", "title": "WaterDrop Idempotence", "text": "<p>Producer idempotence ensures exactly-once semantics (EOS) by tracking a unique message ID for each message and preventing duplicate writes even if the producer retries. To enable idempotence in WaterDrop, configure the producer with <code>enable.idempotence</code> set to <code>true</code>:</p> <pre><code>WaterDrop.setup do |config|\n  config.kafka = {\n    # Other settings...\n    'enable.idempotence': true,\n  }\nend\n</code></pre> <p>When idempotence is enabled in WaterDrop producer:</p> <ul> <li>Kafka ensures that even if a message is retried, it will not be written again.</li> <li>The producer will assign a sequence number to each message, and Kafka ensures that messages with the same sequence number are deduplicated.</li> </ul>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#acknowledgements", "title": "Acknowledgements", "text": "<p>Acknowledgements (<code>acks</code>) dictate how the producer and the broker agree that a message has been successfully written. Kafka's acknowledgment system controls the level of durability and fault tolerance:</p> <ul> <li><code>acks</code> <code>0</code>: The producer does not wait for any acknowledgment. This provides the lowest latency but risks data loss.</li> <li><code>acks</code> <code>1</code>: The producer waits for acknowledgment from the leader broker only. If the leader broker fails after acknowledgment, data can be lost.</li> <li><code>acks</code> <code>all</code>: The producer waits for acknowledgment from all in-sync replicas (ISRs). This provides the highest durability but introduces higher latency.</li> </ul> <p>Per-Topic Acknowledgement Configuration in WaterDrop Variants</p> <p>WaterDrop Variants support configuring acknowledgements on a per-topic basis while using the same producer instance. This flexibility allows different topics to have custom acknowledgement settings depending on the reliability and performance needs of the specific topic. </p> <p>It is recommended to check the WaterDrop Variants documentation for more details on how to configure this.</p>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#interaction-with-mininsyncreplicas", "title": "Interaction with <code>min.insync.replicas</code>", "text": "<p>The <code>acks</code> <code>all</code> configuration works in conjunction with <code>min.insync.replicas</code> to ensure that a message is only considered acknowledged when a certain number of replicas are in sync and able to receive the message. It is important to remember, that you can have more replicas than the number required to be in sync.</p>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#replication-factor", "title": "Replication Factor", "text": "<p>The Replication Factor determines how many copies of a partition are distributed across the Kafka cluster. This impacts fault tolerance and data availability. A higher replication factor increases fault tolerance, but also consumes more disk space and network bandwidth.</p> <p>For example:</p> <ul> <li>A replication factor of 3 means that Kafka will store three copies of each partition across different brokers.</li> <li>If one broker fails, the remaining brokers will have the data.</li> </ul> <p>The replication factor is set at the topic level when the topic is created. You can define it using the Declarative Topics feature:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :a do\n      config(\n        partitions: 6,\n        replication_factor: 3\n      )\n\n      consumer ConsumerA\n    end\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#mininsyncreplicas", "title": "<code>min.insync.replicas</code>", "text": "<p><code>min.insync.replicas</code> is the minimum number of replicas that must be in sync with the leader for a message to be considered successfully written when <code>acks</code> set to <code>all</code> is used. This setting works with the <code>acks</code> set to <code>all</code> producer setting to enforce durability guarantees.</p> <p>Maintaining Data Integrity During Broker Failures</p> <p>If the number of in-sync replicas falls below this threshold (e.g., due to broker failure), Kafka will reject writes until the required number of replicas is back online.</p> <p>To configure <code>min.insync.replicas</code> at the topic level you can use the Declarative Topics:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  routes.draw do\n    topic :a do\n      config(\n        partitions: 6,\n        replication_factor: 3,\n        'min.insync.replicas': 2\n      )\n\n      consumer ConsumerA\n    end\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#example-scenario", "title": "Example Scenario", "text": "<ul> <li>Replication Factor: <code>3</code> (three replicas of each partition)</li> <li>min.insync.replicas: <code>2</code> (two replicas must acknowledge the write)</li> <li>Producer <code>acks</code> set to <code>all</code></li> </ul> <p>In this scenario:</p> <ul> <li>At least two replicas must be in sync for the producer to successfully write a message.</li> <li>If one of the replicas is out of sync or down, Kafka will block writes to ensure data consistency.</li> </ul>"}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#replication-factor-vs-mininsyncreplicas", "title": "Replication Factor vs <code>min.insync.replicas</code>", "text": "Aspect Replication Factor min.insync.replicas Definition Number of total replicas per partition. Minimum number of in-sync replicas required for writes. Purpose Controls fault tolerance via redundancy. Ensures write durability and data integrity. Write Impact Does not directly control write behavior. Directly impacts whether a write is accepted or rejected based on ISR count. Relation to Failures Ensures the partition is available across multiple brokers in case of failure. Determines how many replicas must acknowledge a write for it to be considered successful when acks=all. Setting Context Set when a topic is created or reassigned. Configured per-topic or as a broker default. Example Scenario With a replication factor of 3, there are 3 total replicas, one leader, and two followers for each partition. If <code>min.insync.replicas</code> set to <code>2</code>, at least <code>2</code> replicas must acknowledge a write when <code>acks</code> equal <code>all</code>."}, {"location": "WaterDrop-Idempotence-and-Acknowledgements/#best-practices", "title": "Best Practices", "text": "<ul> <li>Enable idempotence: Always enable producer idempotence for critical data to avoid duplicate messages during retries.</li> <li>Use <code>acks</code> set to <code>all</code>: Combine idempotence with <code>acks</code> <code>all</code> to ensure that your data is acknowledged by all in-sync replicas.</li> <li>Set appropriate <code>min.insync.replicas</code>: Ensure that <code>min.insync.replicas</code> is set to a value that matches your fault tolerance requirements (e.g., <code>2</code> for a replication factor of <code>3</code>).</li> <li>Monitor replicas: Regularly monitor your Kafka cluster to ensure that all replicas are in sync and healthy.</li> </ul> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "WaterDrop-Labeling/", "title": "WaterDrop Labeling", "text": "<p>When producing messages with WaterDrop, tracking the progress and status of each message is crucial. There are instances where you'll need to monitor the delivery handle and report and relate them to the specific message that was dispatched. WaterDrop addresses this need with its labeling capability, allowing you to assign label values to each message during production. These labels act as identifiers, linking the message with its delivery report.</p>"}, {"location": "WaterDrop-Labeling/#importance-of-labeling", "title": "Importance of Labeling", "text": "<p>Labeling is an important feature, and we highly recommend utilizing it for enhanced message tracking and management. It provides the following benefits:</p> <ol> <li> <p>Traceability: Labels provide a straightforward way to trace a message from the point it's produced until it's successfully consumed. This is particularly useful in complex systems where messages pass through various stages and services.</p> </li> <li> <p>Debugging and Error Handling: In case of delivery failures or errors, labels help quickly identify the affected messages. Developers can use this information to diagnose issues, understand the context of failures, and implement targeted fixes.</p> </li> <li> <p>Monitoring and Metrics: By labeling messages, you can gather detailed metrics about message flow, performance, and delivery success rates. This data is invaluable for maintaining system health and optimizing performance.</p> </li> <li> <p>Contextual Information: Labels can carry contextual information about the message, such as its source, intended destination, priority, or type. This enriches the message data and can inform processing logic or routing decisions downstream.</p> </li> </ol>"}, {"location": "WaterDrop-Labeling/#assigning-and-reading-labels", "title": "Assigning and Reading Labels", "text": "<p>Using labels is quite straightforward; simply include the label: attribute when producing messages.</p> <pre><code>handle = producer.produce_async(\n  topic: 'my-topic',\n  payload: 'some data',\n  label: 'unique-id'\n)\n</code></pre> <p>You can then reference it from both the delivery handle and the report:</p> <pre><code>handle.label #=&gt; 'unique-id'\nreport = handle.wait\nreport.label #=&gt; 'unique-id'\n</code></pre> <p>And from <code>error.occurred</code>, <code>message.acknowledged</code>, and <code>message.purged</code> (Transactional Producer only):</p> <pre><code>producer.monitor.subscribe('message.acknowledged') do |event|\n  report = event[:delivery_report]\n  puts \"Message with label: #{report.label} was successfully delivered.\"\nend\n\nproducer.monitor.subscribe('error.occurred') do |event|\n  # There may be other errors without delivery reports in them\n  if event[:type] == 'librdkafka.dispatch_error'\n    report = event[:delivery_report]\n    puts \"Message with label: #{report.label} failed to be delivered.\"\n  end\nend\n</code></pre> <p>If desired, you can even self-reference the entire message:</p> <pre><code>message = { topic: 'my-topic', payload: 'some-data' }\n# Self reference\nmessage[:label] = message\nhandle = produce.produce_async(message)\nhandle.label == message #=&gt; true\nreport = handle.wait\nreport.label == message #=&gt; true\n</code></pre> <p>Increased Memory Usage with Self-Referencing Labels</p> <p>Be cautious when self-referencing messages using labels, as this practice can lead to increased memory usage; the entire message will be retained in memory until its delivery succeeds or fails. This can significantly escalate memory consumption, particularly in scenarios where you're producing hundreds of thousands of messages.</p>"}, {"location": "WaterDrop-Labeling/#distinguishing-between-sync-and-async-producer-errors", "title": "Distinguishing Between Sync and Async Producer Errors", "text": "<p>A common use-case for labeling is distinguishing between errors that occur from synchronous versus asynchronous message production. Since both <code>Karafka.producer.produce_sync</code> and <code>Karafka.producer.produce_async</code> trigger the same <code>error.occurred</code> notification, you can use labels to differentiate between them.</p> <p>This is particularly useful when you want to handle errors differently based on the production method. For example, synchronous errors are typically handled immediately with a backtrace, while asynchronous errors might need specialized logging or retry mechanisms.</p>"}, {"location": "WaterDrop-Labeling/#implementation-example", "title": "Implementation Example", "text": "<p>Here's how you can use labels to distinguish between sync and async producer errors:</p> <pre><code># Label async messages\nproducer.produce_async(\n  topic: 'my-topic',\n  payload: 'data',\n  label: { type: :async }\n)\n\n# Label sync messages\nproducer.produce_sync(\n  topic: 'my-topic',\n  payload: 'data',\n  label: { type: :sync }\n)\n\n# Error handling\nproducer.monitor.subscribe('error.occurred') do |event|\n  if event[:type] == 'librdkafka.dispatch_error'\n    report = event[:delivery_report]\n\n    # Only process errors from async messages\n    if report.label[:type] == :async\n      # This is an async error that needs special handling\n      logger.error(\"Async producer error: #{event[:error]}\")\n      handle_async_error(event[:error])\n    end\n    # Sync errors are already handled elsewhere and don't need additional logging\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Labeling/#using-labels-with-batch-production", "title": "Using Labels with Batch Production", "text": "<p>Labels also work with batch production methods <code>produce_many_sync</code> and <code>produce_many_async</code>. Each message in the batch can have its own label:</p> <pre><code># Async batch production with labels\nmessages = [\n  { topic: 'my-topic', payload: 'data1', label: { type: :async } },\n  { topic: 'my-topic', payload: 'data2', label: { type: :async } },\n  { topic: 'my-topic', payload: 'data3', label: { type: :async } }\n]\n\nhandles = producer.produce_many_async(messages)\nhandles.each { |handle| puts handle.label } # =&gt; { type: :async }, { type: :async }, { type: :async }\n\n# Sync batch production with labels\nmessages = [\n  { topic: 'my-topic', payload: 'data1', label: { type: :sync } },\n  { topic: 'my-topic', payload: 'data2', label: { type: :sync } },\n  { topic: 'my-topic', payload: 'data3', label: { type: :sync } }\n]\n\nreports = producer.produce_many_sync(messages)\nreports.each { |report| puts report.label } # =&gt; { type: :sync }, { type: :sync }, { type: :sync }\n\n# Error handling remains the same - each message's delivery report\n# will contain its respective label\nproducer.monitor.subscribe('error.occurred') do |event|\n  if event[:type] == 'librdkafka.dispatch_error'\n    report = event[:delivery_report]\n\n    if report.label[:type] == :async\n      # Handle async batch errors\n      logger.error(\"Async batch message error: #{event[:error]}\")\n      handle_async_error(event[:error])\n    end\n  end\nend\n</code></pre> <p>Using labels to distinguish between sync and async errors provides several advantages:</p> <ul> <li>Targeted Error Handling: Apply different error handling strategies based on the production method</li> <li>Cleaner Logging: Avoid duplicate logging for sync errors that are already handled</li> <li>Better Monitoring: Track async-specific failure rates and patterns</li> <li>Simplified Debugging: Quickly identify whether an error originated from sync or async production</li> </ul>"}, {"location": "WaterDrop-Labeling/#conclusion", "title": "Conclusion", "text": "<p>Labeling in WaterDrop is a powerful feature that enhances message tracking, debugging, and monitoring. By effectively using labels, you can better understand your message flow, quickly address issues, and gather valuable insights into your messaging system's performance.</p> <p>Last modified: 2025-05-23 10:53:36</p>"}, {"location": "WaterDrop-Middleware/", "title": "WaterDrop Middleware", "text": "<p>WaterDrop supports injecting middleware similar to Rack.</p> <p>Middleware can be used to provide extra functionalities like auto-serialization of data or any other modifications of messages before their validation and dispatch.</p> <p>Each middleware accepts the message hash as input and expects a message hash as a result.</p> <p>There are two methods to register middlewares:</p> <ul> <li><code>#prepend</code> - registers middleware as the first in the order of execution</li> <li><code>#append</code> - registers middleware as the last in the order of execution</li> </ul> <p>Below you can find an example middleware that converts the incoming payload into a JSON string by running <code>#to_json</code> automatically:</p> <pre><code>class AutoMapper\n  def call(message)\n    message[:payload] = message[:payload].to_json\n    message\n  end\nend\n\n# Register middleware\nproducer.middleware.append(AutoMapper.new)\n\n# Dispatch without manual casting\nproducer.produce_async(topic: 'users', payload: user)\n</code></pre> <p>Message Mutability</p> <p>It is up to the end user to decide whether to modify the provided message or deep copy it and update the newly created one.</p>"}, {"location": "WaterDrop-Middleware/#round-robin-distribution-example-middleware", "title": "Round-Robin Distribution Example Middleware", "text": "<p>Below, you can find an example of a middleware that implements a round-robin message distribution across available partitions of the selected topic:</p> <pre><code>class Distributor\n  # Ruby `#cycle` is not thread-safe, this is why we use our own\n  class ThreadSafeCycle\n    def initialize(array)\n      @array = array\n      # Start in random location not to prefer partition 0\n      @index = rand(array.size)\n      @mutex = Mutex.new\n    end\n\n    def next\n      @mutex.synchronize do\n        value = @array[@index]\n        @index = (@index + 1) % @array.size\n        value\n      end\n    end\n  end\n\n  # We need the producer to fetch the number of partitions\n  # This will make the distributor dynamic, allowing for graceful support of repartitioning\n  # We also support the case of non-existing topics just by assigning partition 0.\n  #\n  # @param producer [WaterDrop::Producer]\n  # @param topics [Array]\n  def initialize(producer, topics)\n    @producer = producer\n    @topics = topics\n    @cycles = {}\n    @partition_counts = {}\n    @mutex = Mutex.new\n  end\n\n  # @param message [Hash]\n  def call(message)\n    topic = message.fetch(:topic)\n\n    # Do nothing unless it is one of the topics we want to round-robin\n    return message unless @topics.include?(topic)\n\n    build_iterator(topic)\n\n    # Assign partitions in the round-robin fashion to messages\n    message[:partition] = next_partition(topic)\n\n    # Return the morphed message\n    message\n  end\n\n  private\n\n  # Builds the Ruby iterator we can use to round-robin all the partitions unless it already\n  # exists and matches correctly number of partitions\n  #\n  # @param topic [String]\n  def build_iterator(topic)\n    partition_count = fetch_partition_count(topic)\n\n    # If we already have number of partitions cached, it means cycle is already prepared\n    # as well. If partitions count did not change, we can use it\n    return if partition_count == @partition_counts[topic]\n\n    @mutex.synchronize do\n      last_partition_id = partition_count - 1\n      @cycles[topic] = ThreadSafeCycle.new((0..last_partition_id).to_a)\n      @partition_counts[topic] = partition_count\n    end\n  end\n\n  # @param topic [String]\n  #\n  # @return [Integer] next partition to which dispatch the message\n  def next_partition(topic)\n    @mutex.synchronize do\n      @cycles.fetch(topic).next\n    end\n  end\n\n  # @param topic [String] topic for which we want to get number of partitions\n  #\n  # @return [Integer] number of partitions\n  #\n  # @note `#partition_count` fetched from rdkafka is cached. No need to cache it again\n  # @note Will return 1 partition for topics that do not exist\n  def fetch_partition_count(topic)\n    @producer.partition_count(topic)\n  rescue Rdkafka::RdkafkaError =&gt; e\n    # This error means topic does not exist, we then assume auto-create and will use 0 for now\n    return 1 if e.code == :unknown_topic_or_part\n\n    raise(e)\n  end\nend\n\nMY_PRODUCER.middleware.append(Distributor.new(MY_PRODUCER, %w[events]))\n\nMY_PRODUCER.produce_async(\n  topic: 'events',\n  payload: event.to_json\n)\n</code></pre>"}, {"location": "WaterDrop-Middleware/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Custom Partitioners: By implementing custom partitioning logic within middleware, you gain precise control over how messages are distributed across Kafka partitions. This approach is invaluable for scenarios demanding specific distribution patterns, such as grouping related messages in the same partition to maintain the sequence of events.</p> </li> <li> <p>Automatic Data Serialization: Middleware can automatically transform complex data structures into a standardized format like JSON or Avro, streamlining the serialization process. This ensures consistent data formatting across messages and keeps serialization logic neatly encapsulated and separate from core business logic.</p> </li> <li> <p>Automatic Headers Injection: Enriching messages with essential metadata becomes effortless with middleware. Automatically appending headers like message creation timestamps or producer identifiers ensures that each message carries all necessary context, facilitating tasks like tracing and auditing without manual intervention.</p> </li> <li> <p>Message Validation: Middleware shines in its ability to ensure message integrity by validating each message against specific schemas or business rules before it reaches Kafka. This safeguard mechanism enhances data reliability by preventing invalid data from entering the stream and maintaining the high quality of the data within your topics.</p> </li> <li> <p>Dynamic Routing Logic: Middleware allows for the dynamic routing of messages based on content or context, directing messages to the appropriate topics or partitions on the fly. This adaptability is crucial in complex systems where message destinations might change based on factors like payload content, workload distribution, or system state, ensuring that the data flow remains efficient and contextually relevant.</p> </li> </ul> <p>Last modified: 2024-05-23 12:15:21</p>"}, {"location": "WaterDrop-Monitoring-and-Logging/", "title": "WaterDrop Monitoring and Logging", "text": "<p>Each of the producers after the <code>#setup</code> is done, has a custom monitor to which you can subscribe.</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\nend\n\nproducer.monitor.subscribe('message.produced_async') do |event|\n  puts \"A message was produced to '#{event[:message][:topic]}' topic!\"\nend\n\nproducer.produce_async(topic: 'events', payload: 'data')\n\nproducer.close\n</code></pre> <p>See the <code>WaterDrop::Instrumentation::Notifications::EVENTS</code> for the list of all the supported events.</p>"}, {"location": "WaterDrop-Monitoring-and-Logging/#karafka-web-ui", "title": "Karafka Web-UI", "text": "<p>Karafka Web UI is a user interface for the Karafka framework. The Web UI provides a convenient way for monitor all producers related errors out of the box.</p> <p></p>"}, {"location": "WaterDrop-Monitoring-and-Logging/#logger-listener", "title": "Logger Listener", "text": "<p>WaterDrop comes equipped with a <code>LoggerListener</code>, a useful feature designed to facilitate the reporting of WaterDrop's operational details directly into the assigned logger. The <code>LoggerListener</code> provides a convenient way of tracking the events and operations that occur during the usage of WaterDrop, enhancing transparency and making debugging and issue tracking easier.</p> <p>However, it's important to note that this Logger Listener is not subscribed by default. This means that WaterDrop does not automatically send operation data to your logger out of the box. This design choice has been made to give users greater flexibility and control over their logging configuration.</p> <p>To use this functionality, you need to manually subscribe the <code>LoggerListener</code> to WaterDrop instrumentation. Below, you will find an example demonstrating how to perform this subscription.</p> <pre><code>LOGGER = Logger.new($stdout)\n\nproducer = WaterDrop::Producer.new do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\n  config.logger = LOGGER\nend\n\nproducer.monitor.subscribe(\n  WaterDrop::Instrumentation::LoggerListener.new(\n    # You can use a different logger than the one assigned to WaterDrop if you want\n    producer.config.logger,\n    # If this is set to false, the produced messages details will not be sent to logs\n    # You may set it to false if you find the level of reporting in the debug too extensive\n    log_messages: true\n  )\n)\n\nproducer.produce_sync(topic: 'my.topic', payload: 'my.message')\n</code></pre>"}, {"location": "WaterDrop-Monitoring-and-Logging/#usage-statistics", "title": "Usage Statistics", "text": "<p>WaterDrop is configured to emit internal <code>librdkafka</code> metrics every five seconds. You can change this by setting the <code>kafka</code> <code>statistics.interval.ms</code> configuration property to a value greater than of equal <code>0</code>. Emitted statistics are available after subscribing to the <code>statistics.emitted</code> publisher event. If set to <code>0</code>, metrics will not be published.</p> <p>The statistics include all of the metrics from <code>librdkafka</code> (complete list here) as well as the diff of those against the previously emitted values.</p> <p>In the WaterDrop statistics metrics, specific measurements are denoted in milliseconds, while others are in microseconds. It's imperative to distinguish between these scales, as mistaking one for the other can lead to significant misinterpretations. Always ensure you're referencing the correct unit for each metric to maintain accuracy in your data analysis.</p> <p>For several attributes like <code>txmsgs</code>, <code>librdkafka</code> publishes only the totals. In order to make it easier to track the progress (for example number of messages sent between statistics emitted events), WaterDrop diffs all the numeric values against previously available numbers. All of those metrics are available under the same key as the metric but with additional <code>_d</code> postfix:</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'statistics.interval.ms': 2_000 # emit statistics every 2 seconds\n  }\nend\n\nproducer.monitor.subscribe('statistics.emitted') do |event|\n  sum = event[:statistics]['txmsgs']\n  diff = event[:statistics]['txmsgs_d']\n\n  p \"Sent messages: #{sum}\"\n  p \"Messages sent from last statistics report: #{diff}\"\nend\n\nsleep(2)\n\n# Sent messages: 0\n# Messages sent from last statistics report: 0\n\n20.times { producer.produce_async(topic: 'events', payload: 'data') }\n\n# Sent messages: 20\n# Messages sent from last statistics report: 20\n\nsleep(2)\n\n20.times { producer.produce_async(topic: 'events', payload: 'data') }\n\n# Sent messages: 40\n# Messages sent from last statistics report: 20\n\nsleep(2)\n\n# Sent messages: 40\n# Messages sent from last statistics report: 0\n\nproducer.close\n</code></pre> <p>The metrics returned may not be completely consistent between brokers, toppars and totals, due to the internal asynchronous nature of librdkafka. E.g., the top level tx total may be less than the sum of the broker tx values which it represents.</p>"}, {"location": "WaterDrop-Monitoring-and-Logging/#error-notifications", "title": "Error Notifications", "text": "<p>WaterDrop allows you to listen to all errors that occur while producing messages and in its internal background threads. Things like reconnecting to Kafka upon network errors and others unrelated to publishing messages are all available under <code>error.occurred</code> notification key. You can subscribe to this event to ensure your setup is healthy and without any problems that would otherwise go unnoticed as long as messages are delivered.</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.kafka = {\n    # Note invalid connection port...\n    'bootstrap.servers': 'localhost:9090',\n    # Make waterdrop give up on delivery after 100ms\n    'message.timeout.ms': 100\n  }\nend\n\nproducer.monitor.subscribe('error.occurred') do |event|\n  error = event[:error]\n\n  p \"WaterDrop error occurred: #{error}\"\nend\n\n# Run this code without Kafka cluster\nloop do\n  producer.produce_async(topic: 'events', payload: 'data')\n\n  sleep(1)\nend\n\n# After you stop your Kafka cluster, you will see a lot of those:\n#\n# WaterDrop error occurred: Local: Broker transport failure (transport)\n#\n# WaterDrop error occurred: Local: Broker transport failure (transport)\n</code></pre> <p><code>error.occurred</code> will also include any errors originating from <code>librdkafka</code> for synchronous operations, including those that are raised back to the end user.</p> <p>The <code>error.occurred</code> will not publish purge errors originating from transactions. Such occurrences are standard behavior during an aborted transaction and should not be classified as errors. For a deeper understanding, please consult the transactions documentation.</p>"}, {"location": "WaterDrop-Monitoring-and-Logging/#acknowledgment-notifications", "title": "Acknowledgment Notifications", "text": "<p>WaterDrop allows you to listen to Kafka messages' acknowledgment events. This will enable you to monitor deliveries of messages from WaterDrop even when using asynchronous dispatch methods.</p> <p>That way, you can make sure, that dispatched messages are acknowledged by Kafka.</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\nend\n\nproducer.monitor.subscribe('message.acknowledged') do |event|\n  producer_id = event[:producer_id]\n  offset = event[:offset]\n\n  p \"WaterDrop [#{producer_id}] delivered message with offset: #{offset}\"\nend\n\nloop do\n  producer.produce_async(topic: 'events', payload: 'data')\n\n  sleep(1)\nend\n\n# WaterDrop [dd8236fff672] delivered message with offset: 32\n# WaterDrop [dd8236fff672] delivered message with offset: 33\n# WaterDrop [dd8236fff672] delivered message with offset: 34\n</code></pre>"}, {"location": "WaterDrop-Monitoring-and-Logging/#labeling-api", "title": "Labeling API", "text": "<p>Tracking the progress and status of each message may be crucial when producing messages with WaterDrop. There are instances where you'll need to monitor the delivery handle and report and relate them to the specific message that was dispatched. WaterDrop addresses this need with its labeling API. You can read about it in a dedicated Labeling API section.</p>"}, {"location": "WaterDrop-Monitoring-and-Logging/#datadog-and-statsd-integration", "title": "Datadog and StatsD Integration", "text": "<p>WaterDrop comes with (optional) full Datadog and StatsD integration that you can use. To use it:</p> <pre><code># require datadog/statsd and the listener as it is not loaded by default\nrequire 'datadog/statsd'\nrequire 'waterdrop/instrumentation/vendors/datadog/metrics_listener'\n\n# initialize your producer with statistics.interval.ms enabled so the metrics are published\nproducer = WaterDrop::Producer.new do |config|\n  config.deliver = true\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'statistics.interval.ms': 1_000\n  }\nend\n\n# initialize the listener with statsd client\nlistener = ::WaterDrop::Instrumentation::Vendors::Datadog::MetricsListener.new do |config|\n  config.client = Datadog::Statsd.new('localhost', 8125)\n  # Publish host as a tag alongside the rest of tags\n  config.default_tags = [\"host:#{Socket.gethostname}\"]\nend\n\n# Subscribe with your listener to your producer and you should be ready to go!\nproducer.monitor.subscribe(listener)\n</code></pre> <p>You can also find here a ready to import DataDog dashboard configuration file that you can use to monitor all of your producers.</p> <p></p>"}, {"location": "WaterDrop-Monitoring-and-Logging/#metrics-reporting-enhancement", "title": "Metrics Reporting Enhancement", "text": "<p>The WaterDrop Datadog listener provides a default set of metrics for reporting, but it does not cover every possible metric you might need. Fortunately, you can configure the listener to report additional metrics by enhancing its capabilities. There are two main methods to achieve this:</p> <ol> <li>Notification Hook Enhanced Reporting: This method allows you to add extra instrumentation by subscribing to any events WaterDrop publishes via its notification bus. For example, if you want to count the number of <code>transaction.aborted</code> events, you can subclass the metrics listener, enhance it using the instrumentation API, and publish the relevant information:</li> </ol> <pre><code>class BetterListener &lt; WaterDrop::Instrumentation::Vendors::Datadog::MetricsListener\n  def on_transaction_aborted(_event)\n    count('transactions_aborted', 1, tags: default_tags)\n  end\nend\n\n# Create listener instance\nlistener = BetterListener.new do |config|\n  config.client = Datadog::Statsd.new('localhost', 8125)\n  # Publish host as a tag alongside the rest of tags\n  config.default_tags = [\"host:#{Socket.gethostname}\"]\nend\n\n# Subscribe your listener to the producer\nproducer.monitor.subscribe(listener)\n</code></pre> <ol> <li>Altering <code>librdkafka</code> Metrics: Due to the volume and complexity of <code>librdkafka</code> statistical data, the WaterDrop listener allows you to define the metrics of interest by modifying the <code>rd_kafka_metrics</code> setting. This method does not require subclassing the listener. For  instance, to track throttling metrics on brokers, you can configure the  listener as follows:</li> </ol> <pre><code># Reference for readability\nlist_class_ref = ::WaterDrop::Instrumentation::Vendors::Datadog::MetricsListener\n\n# Merge default reporting with custom metrics\nlistener = list_class_ref.new do |config|\n  config.rd_kafka_metrics = config.rd_kafka_metrics + [\n    list_class_ref::RdKafkaMetric.new(:gauge, :brokers, 'brokers.throttle.avg', %w[throttle avg]),\n    list_class_ref::RdKafkaMetric.new(:gauge, :brokers, 'brokers.throttle.p95', %w[throttle p95]),\n    list_class_ref::RdKafkaMetric.new(:gauge, :brokers, 'brokers.throttle.p99', %w[throttle p99])\n  ]\nend\n\n# Subscribe your listener to the producer\nproducer.monitor.subscribe(listener)\n</code></pre> <p>The structure and details about the librdkafka statistical metrics can be found here.</p> <p>Mixing Approaches</p> <p>Both notification hook enhanced reporting and altering librdkafka metrics can be combined to create a custom listener that fully suits your monitoring needs.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "WaterDrop-Testing/", "title": "WaterDrop Testing", "text": "<p>If you're using WaterDrop with Karafka, consider the <code>karafka-testing</code> gem for RSpec integration. Detailed documentation on its usage can be found here.</p> <p>Testing is a crucial component of any software development cycle. Ensuring that message production behaves as expected is essential when working with Kafka. Thankfully, WaterDrop provides a robust testing mechanism for its producers.</p> <p>When testing code that utilizes WaterDrop producers, you have two primary strategies:</p> <ol> <li> <p>End-to-End Testing with Kafka: This method involves setting up a Kafka environment and dispatching messages. By doing so, you're testing the full flow of your application, ensuring that messages are produced, dispatched, and received as expected in a real-world Kafka setup.</p> </li> <li> <p>Using the Buffered Client: Rather than interacting with a live Kafka instance, you can use WaterDrop's Buffered Client. This allows you to test the expected message dispatch from the code itself. Messages are stored in memory, letting you verify their content and structure without actually sending them to Kafka.</p> </li> </ol> <p>The choice is between an entire interaction with Kafka or a simulated, in-memory testing experience with the Buffered Client. Both approaches have their merits, depending on the specific testing requirements.</p>"}, {"location": "WaterDrop-Testing/#end-to-end", "title": "End-to-End", "text": "<p>When developing applications that interact with Kafka, one common approach for testing is to set up an actual Kafka cluster and conduct end-to-end integration tests. This method ensures that every part of the message production process is tested.</p> <p>However, setting up and managing a Kafka cluster for testing can introduce several complexities:</p> <ol> <li> <p>Infrastructure Overhead: A real Kafka setup requires sufficient infrastructure, including the Kafka brokers, ZooKeeper nodes, and potentially more components, depending on the testing scenario.</p> </li> <li> <p>Configuration Complexity: Ensuring that Kafka is configured correctly for each testing environment can be cumbersome.</p> </li> <li> <p>Cleanup and Isolation: After each test, the Kafka cluster may need to be reset or cleaned to ensure test isolation. Managing topics, partitions, and offsets can be complex and time-consuming.</p> </li> <li> <p>Time Consumption: Spinning up, configuring, and tearing down real Kafka instances can significantly lengthen the test runtime.</p> </li> <li> <p>Topics Creation Overhead: If you expect your tests to run in isolation, ensure each test operates on a separate topic or partition. This can create a significant overhead and drastically increase test-suite execution time.</p> </li> </ol> <p>The process is refreshingly straightforward if you opt for end-to-end testing with WaterDrop and Kafka. You don't need any special configurations. Set up your Kafka environment, integrate WaterDrop, create a producer, and you're ready to use it in your specs and tests.</p> <p>Explore the RSpec example below, demonstrating testing for expected messages dispatched to Kafka.</p> <pre><code>RSpec.describe WebhookProcessor do\n  subject(:processor_flow) { described_class.new.call(incoming_request) }\n\n  # Fake JSON with request data\n  let(:incoming_request) { build(:incoming_request) }\n  # This can be slow if you run it per each spec\n  let(:last_topic_message) { Karafka::Admin.read_topic('incoming_request', 0, 1) }\n\n  before { processor_flow }\n\n  it 'expect to create a message with correct payload in the incoming_requests topic' do\n    expect(last_topic_message).to eq(incoming_request)\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Testing/#buffered-client", "title": "Buffered Client", "text": "<p>WaterDrop offers a client specifically designed for testing. This client can replicate the Kafka behaviors related to the production of messages without the need for an actual Kafka instance. This simulation allows developers to test their Kafka message production logic without a live Kafka instance's overhead and potential complications.</p> <ol> <li> <p>Delivery Handle and Delivery Report: For every message you \"send\" using the WaterDrop producer, the testing client simulates the return of a delivery handle and a delivery report. This mimics the behavior you would expect when producing messages to a live Kafka instance, providing a realistic testing scenario.</p> </li> <li> <p>Consecutive Per Partition Offsets: One of the vital aspects of Kafka is message ordering within a partition. WaterDrop's testing client ensures that the simulated delivery reports carry consecutive offsets for each partition. This means that if you produce multiple messages to a particular partition, the offsets of the delivery reports for these messages will be consecutive numbers, mirroring real-world Kafka behavior. This feature allows you to use the returned offset information consistently and predictably, enhancing your tests' reliability.</p> </li> <li> <p>Default Partition Handling: If you do not specify a partition when sending a message, the testing client defaults to partition zero. This is consistent with general Kafka producer behavior, where if no partition is specified, it might be determined by a partitioner or default to a specific partition.</p> </li> <li> <p>Transactions Support: The buffered client of WaterDrop supports transactions, a crucial feature for ensuring message production consistency. If, for any reason, a transaction is aborted, the messages within that transaction aren't added to the buffer. This emulates the atomic nature of Kafka transactions, allowing you to test scenarios that involve transaction commits and aborts without inadvertently inflating your message buffer.</p> </li> </ol>"}, {"location": "WaterDrop-Testing/#configuration", "title": "Configuration", "text": "<p>With the <code>karafka-testing</code> gem integrated, the WaterDrop Buffered backend is automatically activated for <code>Karafka.producer</code>.</p> <p>To harness the Buffered backend in your test environment, adjust the <code>client_class</code> configuration to <code>WaterDrop::Clients::Buffered</code> as follows:</p> <pre><code># When you initialize your producer, whether part of Karafka or not\nPRODUCER = WaterDrop::Producer.new do |config|\n  # Other settings can stay as they are\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'request.required.acks': 1\n  }\n\n  # Use dummy only for tests\n  break unless Rails.env.test?\n\n  config.client_class = WaterDrop::Clients::Buffered\nend\n</code></pre>"}, {"location": "WaterDrop-Testing/#usage", "title": "Usage", "text": "<p>Whenever your code executes a dispatch operation, be it synchronous or asynchronous, the messages won't be sent to Kafka. Instead, they reach the <code>producer.client.messages</code> array, readily available for your examination.</p> <pre><code>handel = PRODUCER.produce_async(topic: 'test', payload: '123')\n\nputs handel\n#=&gt; &lt;WaterDrop::Clients::Dummy::Handle:... @offset=0, @partition=0, @topic=\"test\"&gt;\n\nreport = handel.wait\n\nputs report\n#=&gt; &lt;Rdkafka::Producer::DeliveryReport:... @error=nil, @offset=0, @partition=0, @topic_name=\"test\"&gt;\n\nputs PRODUCER.client.messages\n#=&gt; [{:topic=&gt;\"test\", :payload=&gt;\"123\"}]\n\nraise unless PRODUCER.client.messages.count != 1\n</code></pre> <p>In harmony with this, transactions too maintain consistency. Messages from aborted transactions are gracefully discarded, ensuring they don't find their way into storage.</p> <pre><code>PRODUCER.transaction do\n  PRODUCER.produce_async(topic: 'test1', payload: '123')\n  PRODUCER.produce_async(topic: 'test2', payload: '456')\n\n  raise(WaterDrop::AbortTransaction)\nend\n\n# No messages will be stored in the buffers as the transaction was aborted\nputs PRODUCER.client.messages.size #=&gt; 0\n</code></pre> <p>In case of a successful transaction, data will be stored:</p> <pre><code>PRODUCER.transaction do\n  PRODUCER.produce_async(topic: 'test1', payload: '123')\n  PRODUCER.produce_async(topic: 'test2', payload: '456')\nend\n\nputs PRODUCER.client.messages.size #=&gt; 2\n</code></pre>"}, {"location": "WaterDrop-Testing/#inspection-api", "title": "Inspection API", "text": "<p>The WaterDrop Buffered client provides two methods for accessing buffered messages:</p> <ul> <li> <p><code>#messages</code>: Retrieves all buffered messages, maintaining their original dispatch sequence. This ensures you can trace the chronological order of message dispatches.</p> </li> <li> <p><code>#messages_for</code>: Targeted specifically for messages dispatched to a specific topic, this method lets you get messages bound for a particular destination.</p> </li> </ul> <pre><code>PRODUCER.produce_sync(topic: 'test1', payload: '123')\nPRODUCER.produce_sync(topic: 'test2', payload: '456')\n\nPRODUCER.client.messages #=&gt; [{:topic=&gt;\"test1\", :payload=&gt;\"123\"}, {:topic=&gt;\"test2\", :payload=&gt;\"456\"}]\n\nPRODUCER.client.messages_for('test1') #=&gt; [{:topic=&gt;\"test1\", :payload=&gt;\"123\"}]\nPRODUCER.client.messages_for('test2') #=&gt; [{:topic=&gt;\"test2\", :payload=&gt;\"456\"}]\nPRODUCER.client.messages_for('test3') #=&gt; []\n</code></pre> <p>Both methods offer a clear lens to inspect the messages you've dispatched, be it an overview or a topic-specific deep dive.</p>"}, {"location": "WaterDrop-Testing/#isolation", "title": "Isolation", "text": "<p>When using a per-process producer, it's essential to ensure test isolation. Without clearing the producer client buffers, messages from one test might unintentionally affect subsequent tests.</p> <p>To prevent this, use the <code>client.reset</code> method as follows:</p> <pre><code>PRODUCER.produce_async(topic: 'test', payload: '123')\n\nputs PRODUCER.client.messages.count #=&gt; 1\n\nPRODUCER.client.reset\n\nputs PRODUCER.client.messages.count #=&gt; 0\n</code></pre> <p>This ensures each test starts with an empty buffer, eliminating potential cross-test interference.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "WaterDrop-Transactions/", "title": "WaterDrop Transactions", "text": "<p>Transactional and Exactly-Once Semantics Support in Consumers</p> <p>Karafka Pro supports transactional operations and Exactly-Once Semantics for both producers and consumers. This enhanced functionality includes the production of messages and the committing of consumer offset within the same transaction. This ensures atomicity and consistency in message handling, making your data streaming processes more reliable and efficient. For a detailed understanding of these capabilities and their implementation, refer to the Karafka Pro Transactions documentation section.</p> <p>WaterDrop transactions enable users to send multiple messages to multiple topics/partitions so that all messages are successfully published or none are, ensuring atomicity.</p>"}, {"location": "WaterDrop-Transactions/#using-transactions", "title": "Using Transactions", "text": "<p>Before using transactions, you need to configure your producer to be transactional. This is done by setting <code>transactional.id</code> in the <code>kafka</code> settings scope:</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'transactional.id': 'unique-id'\n  }\nend\n</code></pre> <p>The <code>transactional.id</code> is a unique identifier associated with a Kafka producer that allows it to participate in Kafka transactions. It's fundamental to achieving exactly-once semantics in Kafka.</p> <p>A single <code>transactional.id</code> should only be used by one producer instance. Using the same <code>transactional.id</code> across multiple producer instances simultaneously can lead to undefined behavior and potential data inconsistencies.</p>"}, {"location": "WaterDrop-Transactions/#simple-usage", "title": "Simple Usage", "text": "<p>The only thing you need to do to start using transactions is to wrap your code with a <code>#transaction</code> block:</p> <pre><code>producer.transaction do\n  producer.produce_async(topic: 'topic1', payload: 'data1')\n  producer.produce_async(topic: 'topic2', payload: 'data2')\nend\n</code></pre>"}, {"location": "WaterDrop-Transactions/#producing-messages-one-after-another", "title": "Producing Messages One After Another", "text": "<p>When a WaterDrop producer is set up in a transactional mode, every single message production will automatically initiate its transaction when it isn't wrapped within a transaction block. While this ensures atomicity for each message, there are more efficient approaches. Each transaction will introduce additional latency due to the overhead of starting and completing a transaction for every message.</p> <p>For optimized performance, it's advisable to leverage batch dispatches. By batching messages, you can reduce the number of transactions and, consequently, the associated overheads. This will improve throughput and minimize the latency introduced by frequent transaction initiations and completions. In a transactional setting, batching is key to balancing consistency and performance.</p> <p>BAD:</p> <pre><code># This code with a transactional producer will create and commit transaction\n# with each outgoing message, slowing things down\nUsers.find_each do |user|\n  producer.produce_async(topic: 'users', payload: user.to_json)\nend\n</code></pre> <p>BETTER:</p> <pre><code># This code will create one transaction\n# The downside is, that the transaction can reach a timeout if there are many users\nproducer.transaction do\n  Users.find_each do |user|\n    producer.produce_async(topic: 'users', payload: user.to_json)\n  end\nend\n</code></pre> <p>BEST:</p> <pre><code># This code will create a transaction per batch without risk of the transaction timeout\nUsers.find_in_batches do |users|\n  producer.transaction do\n    users.each do |user|\n      producer.produce_async(topic: 'users', payload: user.to_json)\n    end\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Transactions/#producing-in-batches", "title": "Producing In Batches", "text": "<p>When utilizing WaterDrop's <code>#produce_many_sync</code> and <code>#produce_many_async</code> methods, there's an inherent convenience built-in: WaterDrop will automatically encase the dispatch within a transaction. Hence, if your producer is already configured to be transactional, there's no need for an additional outer <code>#transaction</code> block. It streamlines the process, ensuring that your batch messages get delivered or none at all without requiring extra layers of transactional wrapping.</p> <pre><code># In case of batch messages production, the `#transaction` wrapper is not needed.\nproducer.transaction do\n  producer.produce_many_async(messages)\nend\n\n# The code below will wrap the dispatch with a transaction automatically\nproducer.produce_many_async(messages)\n</code></pre>"}, {"location": "WaterDrop-Transactions/#aborting-transaction", "title": "Aborting Transaction", "text": "<p>Any exception or error raised within a transaction block will automatically result in the transaction being aborted. This ensures that if there are unexpected behaviors or issues during message production or processing, the entire batch of messages within that transaction won't be committed, preserving data consistency.</p> <p>Below, you can find an example that ensures that all the messages are successfully processed and only in such cases all produced messages are being sent to Kafka:</p> <pre><code>producer.transaction do\n  messages.each do |message|\n    payload = message.payload\n\n    next unless message.payload.fetch(:type) == 'update'\n\n    # If this exception is raised, none of the messages will be dispatched\n    raise UnexpectedResource if message.payload.fetch(:resource) != 'user'\n\n    # Pipe the data to users specific topic\n    producer.produce_async(topic: 'users', payload: message.raw_payload)\n  end\nend\n</code></pre> <p>WaterDrop also provides a manual way to abort a transaction by raising an error. By using <code>raise WaterDrop::AbortTransaction</code>, you can signal the transaction to abort. This method is advantageous when you want to abort a transaction based on some business logic or condition without throwing an actual error that would leak out of the transaction.</p> <pre><code>producer.transaction do\n  messages.each do |message|\n    # Pipe all events\n    producer.produce_async(topic: 'events', payload: message.raw_payload)\n  end\n\n  # And abort if more events are no longer needed\n  raise(WaterDrop::AbortTransaction) if KnowledgeBase.more_events_needed?\nend\n</code></pre> <p>In both behaviors, the overarching principle is to ensure data consistency and reliability. Whether you're aborting due to unforeseen errors or specific business logic, Karafka provides the tools necessary to manage your transactions effectively.</p>"}, {"location": "WaterDrop-Transactions/#delivery-handles-and-delivery-reports", "title": "Delivery Handles and Delivery Reports", "text": "<p>In WaterDrop, when dispatching messages to Kafka, the feedback mechanism about the delivery status of a message depends on whether you choose synchronous or asynchronous dispatching. You'll receive a delivery report for synchronous dispatches, providing immediate feedback about the message's delivery status. With synchronous dispatch, your program will pause and await a confirmation from the Kafka broker, signaling the successful receipt of the message.</p> <p>Both delivery handles and delivery reports are supported when working within transactions, but they behave differently in this context. Delivery reports will have relevant details, such as the appropriate partition and offset values; however, a crucial distinction is the difference between message \"delivery\" and its visibility to consumers in a transactional setting. Even if the delivery report acknowledges the successful dispatch of a message, it doesn't guarantee that consumers will see it. Messages sent within a transaction have their offsets \"reserved\" in Kafka. But, unless the transaction is fully committed, these messages might not reach the consumers. Instead, they may undergo a \"compaction\" process, where they're essentially removed or not made visible to consumers.</p> <p>In a transactional context with WaterDrop, a delivery report signals the message's successful reservation in Kafka, not its eventual consumability. The entire transaction must be successfully committed for a message to be available for consumption.</p> <p>Below you can find an example of an aborted transaction with reports that indicate the offsets reserved for dispatched messages. Note, that while those offsets were reserved, they will never be passed to consumers.</p> <pre><code>reports = []\n\nproducer.transaction do\n  10.times do\n    reports &lt;&lt; producer.produce_sync(topic: 'events', payload: rand.to_s)\n  end\n\n  raise WaterDrop::AbortTransaction\nend\n\nreports.each do |report|\n  puts &lt;&lt;~MSG.tr(\"\\n\", '')\n    \"Aborted message was sent to: #{report.topic_name}##{report.partition}\n    and got offset: #{report.offset}\"\n  MSG\nend\n\n# Aborted message was sent to: events#0and got offset: 33\n# Aborted message was sent to: events#0and got offset: 34\n# Aborted message was sent to: events#0and got offset: 35\n# Aborted message was sent to: events#0and got offset: 36\n# ...\n</code></pre> <p>It's also vital to grasp a specific behavior when dealing with messages within a Kafka transaction in WaterDrop. If messages are part of a transaction but have yet to be delivered, and you attempt to use the <code>#wait</code> method on their delivery handles, you might encounter a <code>Rdkafka::RdkafkaError</code> <code>purge_queue</code> error. This error arises because the Kafka brokers did not acknowledge these undelivered messages. If the encompassing transaction is aborted, these messages are consequently removed from the delivery queue. This removal triggers the <code>purge_queue</code> error since you're essentially waiting on handles of messages that have been purged due to the transaction's abort.</p> <pre><code>handles = []\n\nproducer.transaction do\n  100.times do\n    # Async is critical here\n    handles &lt;&lt; producer.produce_async(topic: 'events', payload: rand.to_s)\n  end\n\n  raise WaterDrop::AbortTransaction\nend\n\n# If messages were not yet acknowledged by the broker during the transaction\n# this may raise an error as below\nhandles.each(&amp;:wait)\n\n# Local: Purged in queue (purge_queue) (Rdkafka::RdkafkaError)\n</code></pre>"}, {"location": "WaterDrop-Transactions/#risks-of-early-exiting-transactional-block", "title": "Risks of Early Exiting Transactional Block", "text": "<p>In all versions of WaterDrop, using <code>return</code>, <code>break</code>, or <code>throw</code> to exit a transactional block early is not allowed. </p> <p>However, the behavior differs between versions:</p> <ul> <li>pre 2.8.0: Exiting a transaction using <code>return</code>, <code>break</code>, or <code>throw</code> would cause the transaction to rollback.</li> <li>2.8.0 and Newer: Exiting a transaction using these methods will raise an error.</li> </ul> <p>It is not recommended to use early exiting methods. To ensure that transactions are handled correctly, refactor your code to avoid using <code>return</code>, <code>break</code>, or <code>throw</code> directly inside transactional blocks. Instead, manage flow control outside the transaction block.</p> <p>BAD:</p> <pre><code>MAX = 10\n\ndef process(messages)\n  count = 0\n\n  producer.transaction do\n    messages.each do |message|\n      count += 1\n\n      producer.produce_async(topic: 'events', payload: message.raw_payload)\n\n      # This will trigger a rollback or an error.\n      # Do not do this\n      return if count &gt;= MAX\n    end\n  end\nend\n</code></pre> <p>GOOD:</p> <pre><code>MAX = 10\n\ndef process(messages)\n  producer.transaction do\n    # Early return from this method will not affect the transaction.\n    # It will be committed.\n    insert_with_limit(messages)\n  end\nend\n\ndef insert_with_limit(messages)\n  count = 0\n\n  messages.each do |message|\n    count += 1\n\n    producer.produce_async(topic: 'events', payload: message.raw_payload)\n\n    # This would not trigger a rollback or raise an error.\n    return if count &gt;= MAX\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Transactions/#delivery-warranties", "title": "Delivery Warranties", "text": "<p>When a WaterDrop transaction is committed without errors, it guarantees that all messages within the transaction have been successfully produced. This simplifies the process of instrumentation and monitoring of the producer and messages dispatch process significantly:</p> <ul> <li> <p>Single Point of Failure: The end (commit) of the transaction is the primary focus. Its successful execution implies that all messages in the transaction have been produced, eliminating the need for additional checks.</p> </li> <li> <p>Implicit Success Confirmation: The absence of errors during transaction commitment implicitly confirms successful message production.</p> </li> </ul> <p>Because of the above, delivery reports may seem useless, however, while delivery reports are optional in transactional contexts, they still can help retrieve the offset of messages accepted by the broker, which can be valuable for tracking and auditing purposes.</p>"}, {"location": "WaterDrop-Transactions/#internal-errors-retries", "title": "Internal Errors Retries", "text": "<p>WaterDrop is designed to be intelligent about handling transaction-related errors. It discerns which errors can be retried and will attempt based on the configuration settings. The retries aren't immediate \u2013 they come with a backoff period, giving the system a brief respite before trying again. This approach can mitigate transient issues that might resolve themselves after a short period.</p> <p>Regardless of the nature of the error \u2013 whether retryable or not - WaterDrop ensures transparency by publishing instrumentation events to <code>error.occurred</code> channel. This feature keeps the stakeholders informed, and potential interventions or investigations can be initiated if a pattern of errors emerges.</p> <p>Errors encapsulated as <code>Rdkafka::RdkafkaError</code> offer insight into their nature, helping formulate a response strategy. Here's how you can interpret them:</p> <ul> <li> <p>retryable: Indicates that a particular operation, such as offset commit, can be retried after a backoff. The assumption is that the operation should function as expected after the retry. WaterDrop is configured to attempt these retries several times before deeming it a failure.</p> </li> <li> <p>fatal: These errors signify issues from which there's no recovery, irrespective of the number of retry attempts. An example is being fenced out of a transaction. When encountering fatal errors, it's recommended to investigate the root cause, as they might indicate underlying severe problems.</p> </li> <li> <p>abortable: Errors in this category aren't recoverable in the current context of the ongoing transaction. While the error might not be fatal to the system, it does necessitate the abortion of the present transaction to maintain data integrity and consistency.</p> </li> </ul> <p>Below, you can find an example monitor that will print only transaction-related errors with extra status info:</p> <pre><code>producer.monitor.subscribe('error.occurred') do |event|\n  next unless event[:type].start_with?('transaction.')\n\n  error = event[:error]\n\n  puts 'Rdkafka error occurred during the transaction'\n  puts \"Error: #{error}\"\n  puts \"Retryable: #{error.retryable?}\"\n  puts \"Abortable: #{error.abortable?}\"\n  puts \"Fatal: #{error.fatal?}\"\nend\n</code></pre> <p>Errors that are neither retryable, abortable, nor fatal are considered fatal as well.</p>"}, {"location": "WaterDrop-Transactions/#purge-errors", "title": "Purge Errors", "text": "<p>Purge errors occur mostly when WaterDrop cannot deliver a given message for an extended period and decides to remove it from its internal queue.</p> <p>In the context of a standard producer, a purge error is relatively uncommon and usually indicative of a problem. This type of error often arises when WaterDrop cannot deliver a given message to Kafka for an extended period. Common causes include network issues, Kafka broker unavailability, or misconfigurations.</p> <p>Given the unexpected nature of purges in this context, they're flagged as errors. When such a situation arises, WaterDrop propagates the purge error via the <code>error.occurred</code> notification channel. As these are not typical behaviors, they should be diligently monitored and addressed. Conversely, purge errors take on a different meaning within a transactional producer context. Specifically, during aborted transactions, it's a standard operation for WaterDrop to purge each message within the transaction that hasn't been dispatched to Kafka yet. This behavior is expected and part of how transactional processes ensure atomicity and consistency.</p> <p>This purging process is anticipated within transactional boundaries, so these purge errors are not considered typical \"errors.\" Instead of using the <code>error.occurred</code> notification channel, WaterDrop uses the <code>message.purged</code> channel to report these events. This distinction is crucial to ensure system monitors or logs are not flooded with false positives when working with transactional producers.</p>"}, {"location": "WaterDrop-Transactions/#timeouts", "title": "Timeouts", "text": "<p>The <code>transaction.timeout.ms</code> parameter in Kafka is a configuration setting specifying the maximum amount of time (in milliseconds) a transactional session can remain open without being completed. Once this timeout is reached, Kafka will proactively abort the transaction.</p> <p>This behavior may impact you in the following ways:</p> <ul> <li> <p>Ensures Bounded Transaction Durations: With <code>transaction.timeout.ms</code> in place, WaterDrop ensures that no transaction lingers indefinitely. This is especially crucial when unforeseen issues might prevent a transaction from completing normally. Having a set timeout ensures system resources aren't indefinitely tied up with stalled or zombie transactions.</p> </li> <li> <p>Enhances System Resilience: By auto-aborting transactions that surpass the set timeout, we avoid potential deadlocks or long-running transactions that might block other critical operations.</p> </li> <li> <p>Determines Batch Size: If you're sending a batch of messages as a part of a single transaction in WaterDrop, you need to ensure that the entire batch can be processed within the <code>transaction.timeout.ms</code> window. If the processing time risks exceeding this timeout, consider reducing the batch size or optimizing the processing speed.</p> </li> <li> <p>Error Handling: Transactions that are aborted due to reaching the timeout will raise an error. In the context of WaterDrop, it's crucial to handle these timeout-aborted transactions gracefully, possibly by retrying them or logging them for further investigation.</p> </li> <li> <p>A Balancing Act: Setting the correct value for <code>transaction.timeout.ms</code> requires a balance. If it's too short, legitimate transactions requiring more time might get prematurely aborted, leading to increased retries and system overhead. If it's too long, it might delay the detection and resolution of genuine issues.</p> </li> </ul> <p>Potential Exceedance of <code>max_wait_timeout</code> in WaterDrop Transactions</p> <p>When working with transactions in WaterDrop, especially in clusters experiencing connectivity issues or unavailability, be aware that the <code>max_wait_timeout</code> parameter may be exceeded. This behavior is due to the internal retry policies within WaterDrop, which are critical for maintaining system stability. Although this might result in longer wait times, it is an expected and necessary mechanism to ensure reliable message delivery and consistency across transactions. Therefore, this will not be addressed or altered in future updates.</p>"}, {"location": "WaterDrop-Transactions/#transactionalid-management-and-fencing", "title": "<code>transactional.id</code> Management and Fencing", "text": "<p>One of the critical aspects of <code>transactional.id</code> is its ability to \"fence out\" older instances of a producer. If a producer instance with a given <code>transactional.id</code> crashes and another instance starts with the same <code>transactional.id</code>, Kafka ensures that the older producer instance can't commit any more messages, preventing potential duplicates. This behaviour is called fencing.</p> <p>Below, you can find an example of how fencing works. After <code>producer2</code> first transaction, <code>producer1</code> will no longer be able to produce messages and will raise an error:</p> <pre><code>kafka_config = {\n'bootstrap.servers': 'localhost:9092',\n'transactional.id': 'unique-id'\n}\n\nproducer1 = WaterDrop::Producer.new do |config|\n  config.kafka = kafka_config\nend\n\nproducer2 = WaterDrop::Producer.new do |config|\n  config.kafka = kafka_config\nend\n\nproducer1.transaction do\n  producer1.produce_async(topic: 'example', payload: 'data')\nend\n\nproducer2.transaction do\n  producer2.produce_async(topic: 'example', payload: 'data')\nend\n\n# This will raise an error as Kafka will fence out this producer instance\nproducer1.transaction do\n  producer1.produce_async(topic: 'example', payload: 'data')\nend\n</code></pre> <p>Here are some recommendations on how to set the <code>transactional.id</code> value:</p> <ol> <li> <p>Uniqueness: Ensure each producer instance has a unique <code>transactional.id</code>. This avoids conflicts and allows Kafka to correctly track and manage the state of each producer's transactions.</p> </li> <li> <p>Durability: The <code>transactional.id</code> is meant to be durable across producer restarts. If a producer goes down and comes back up, it should use the same <code>transactional.id</code> to resume its activities.</p> </li> <li> <p>Avoid Sharing: Never use the same transactional.id across multiple producer instances simultaneously. This can lead to unexpected behavior and data inconsistencies.</p> </li> <li> <p>Consistent Mapping: If you have a particular processing task or a set of tasks, always assign the same <code>transactional.id</code> to them. This consistent mapping helps maintain exactly-once semantics, especially if jobs or producers restart.</p> </li> <li> <p>Storage: Consider storing the mapping of <code>transactional.id</code> to specific tasks or workflows in durable storage. This ensures that even if your application restarts, you can consistently assign the correct <code>transactional.id</code> to each task.</p> </li> <li> <p>Monitoring: Regularly monitor the transactions in your Kafka cluster. Look for anomalies or issues related to specific <code>transactional.id</code>, such as frequent aborts. This can help in early detection of potential problems.</p> </li> <li> <p>Fencing Awareness: Understand that Kafka uses the <code>transactional.id</code> for producer fencing. Suppose a new producer instance starts with an existing transactional.id, older instances with the same ID will be \"fenced out\" and unable to send more messages. This is a protective measure to ensure data consistency, but be aware of this behavior when managing producer lifecycles.</p> </li> <li> <p>Avoid Overloading: While it might be tempting to use highly descriptive <code>transactional.id</code> that encapsulates a lot of meta-information about the producer or task, keep them reasonably short and meaningful. This ensures better performance and manageability.</p> </li> </ol> <p>By adhering to these recommendations, you can ensure reliable transactional processing in Karafka and avoid potential pitfalls related to mismanagement of <code>transactional.id</code>.</p>"}, {"location": "WaterDrop-Transactions/#nested-transaction", "title": "Nested Transaction", "text": "<p>In certain situations, developers might inadvertently nest transactions within one another. With WaterDrop, this is gracefully handled to prevent any undesired side effects.</p> <p>When using the WaterDrop producer, it possesses an inherent awareness of an ongoing transaction. If you initiate a nested transaction \u2014 starting another transaction inside an existing one \u2014 the producer won't get confused or initiate a separate, inner transaction. Instead, it will treat the entire sequence of operations as if they were under a single wrapping transaction from the beginning.</p> <p>This intelligent behavior ensures:</p> <ol> <li> <p>Simplicity: You don't need to manage or be overly cautious about accidentally nesting transactions.</p> </li> <li> <p>Consistency: Whether it's a single or mistakenly nested transaction, the outcome remains consistent; messages will either all be committed or aborted.</p> </li> <li> <p>Performance: Since WaterDrop recognizes and avoids initiating multiple transactions, there's no additional overhead or latency from nested transaction initiations.</p> </li> </ol> <p>While it's generally good practice to be explicit and avoid nesting, with WaterDrop, you can be assured that even if nested transactions occur, they're handled seamlessly without any adverse effects.</p> <pre><code>producer.transaction do\n  producer.produce_async(topic: 'my_data', payload: rand.to_s)\n\n  producer.transaction do\n    producer.produce_async(topic: 'my_data', payload: rand.to_s)\n\n    producer.transaction do\n      producer.produce_async(topic: 'my_data', payload: rand.to_s)\n    end\n  end\nend\n\n# The above code conceptually behaves like that:\nproducer.transaction do\n  producer.produce_async(topic: 'my_data', payload: rand.to_s)\n  producer.produce_async(topic: 'my_data', payload: rand.to_s)\n  producer.produce_async(topic: 'my_data', payload: rand.to_s)\nend\n</code></pre>"}, {"location": "WaterDrop-Transactions/#instrumentation", "title": "Instrumentation", "text": "<p>In  WaterDrop, transaction-related events are monitored, emitting notifications for key activities. These events include:</p> <ul> <li><code>transaction.started</code></li> <li><code>transaction.committed</code></li> <li><code>transaction.aborted</code></li> <li><code>transaction.marked_as_consumed</code></li> <li><code>transaction.finished</code></li> </ul> <p>Listeners can subscribe to these events, which integrate seamlessly with Karafka and WaterDrop's monitoring and logging systems. This feature ensures that every crucial phase of transaction processing is observable, aiding in debugging, performance monitoring, and system reliability.</p> <p>Event Subscription with Multiple Producers</p> <p>In setups using a connection pool or multiple dedicated producers, remember to subscribe your event listeners to each producer instance. Each producer operates independently, so subscriptions are not automatically shared across instances. Failure to subscribe to each can result in missing critical transaction-related events.</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\nend\n\nproducer.monitor.subscribe('transaction.started') do |_event|\n  puts \"Wow, transaction just started!\"\nend\n\nproducer.monitor.subscribe('transaction.committed') do |_event|\n  puts \"Wow, transaction just got committed!\"\nend\n\nproducer.monitor.subscribe('transaction.aborted') do |_event|\n  puts \"Wow, transaction just got aborted!\"\nend\n</code></pre>"}, {"location": "WaterDrop-Transactions/#fatal-errors-recovery-strategy", "title": "Fatal Errors Recovery Strategy", "text": "<p>When a fatal transactional error occurs, the producer can close and recreate its underlying client. This ensures that the system can continue operating without being halted by a single instance failure. The failed transaction will automatically roll back, allowing the new instance to take over safely.</p> <p>The reloading mechanism is used exclusively within locked transactions, eliminating the risk of race conditions. Fencing is excluded to prevent any potential race conditions arising from this process.</p> <p>The reloading process will be triggered only by errors caused during message dispatches within transactions. The system reloads on any errors where the cause is <code>Rdkafka::RdkafkaError</code>, with specific exclusions to avoid unintended reloading. This approach reloads the client in cases where other errors, such as those from Karafka, occur within transactions. Although this can impact performance due to the overhead of closing and reconnecting, it ensures that all errors result in a rollback, maintaining system integrity.</p> <p>If you find this behaviour undesired, you have the power to set the <code>reload_on_transaction_fatal_error</code> configuration value to <code>false</code>. In this case, the producer client will not be reloaded, giving you control over the system's response to fatal errors.</p>"}, {"location": "WaterDrop-Transactions/#limitations", "title": "Limitations", "text": "<p>Karafka producer transactions provide atomicity over streams, but users should be mindful of the following limitations:</p> <ul> <li> <p>Not Database Transactions: WaterDrop transactions are distinct from database transactions. They don't support the rollback states typically in databases. Aborting a transaction ensures that the messages are not published but won't \"undo\" other side effects arising from message processing.</p> </li> <li> <p>Latency: Transactions necessitate coordination amongst Kafka brokers, leading to added latency.</p> </li> <li> <p>Hanging Transactions: Transactions that don't complete (neither committed nor aborted) can impact the Last Stable Offset (LSO) in Kafka. This can block consumers from reading new data until the hanging transaction is resolved, affecting data consumption and overall system throughput.</p> </li> <li> <p>Web UI Dispatch Interference: When both user code and Karafka Web UI use <code>Karafka.producer</code>, prolonged transactions can block the Web UI from reporting data due to a held lock, blocking other dispatches to Kafka. Ensure brief transactions, avoid concurrent access or initialize additional producers to mitigate this.</p> </li> <li> <p>Handling Purge Errors in Transactions: Purge errors are common during aborted transactions. Instead of broadcasting these through the <code>error.occurred</code> notification channel, they are relayed via the <code>message.purged</code> notification. This distinction is important because undelivered messages from an aborted transaction will trigger this notification. Recognizing these as standard behavior rather than errors is crucial when working with transactions.</p> </li> <li> <p>Topic Creation During Production: While WaterDrop's transactional producer can operate with non-existent topics when <code>allow.auto.create.topics</code> is set to <code>true</code>, creating topics beforehand is strongly advised. Failing to do so can lead to errors like:</p> <p>Broker: Producer attempted a transactional operation in an invalid state (invalid_txn_state)</p> </li> <li> <p>Thread Safety with WaterDrop: While WaterDrop is inherently thread-safe, there are specifics to keep in mind for transactions:</p> <ul> <li> <p>Lock During Transactions: WaterDrop locks access to itself when a transaction is underway. For those anticipating high transactional loads, consider leveraging multiple producers. This way, while one producer is engaged in a transaction in one thread, others can operate independently.</p> </li> <li> <p>Exclusive Transactional Usage: Should you configure a producer as transactional, be aware that it cannot then be used for non-transactional messaging, and all producer operations will be wrapped with a transaction.</p> </li> </ul> </li> <li> <p>Kafka System Records and Offset Allocation: Kafka transactions, by design, create an additional record in the topic partition. This record is a system record and doesn't contain any user data. However, this, along with the messages from aborted transactions, does occupy offsets. These offsets are not merely placeholders; they represent an actual record in the Kafka log. It's crucial to understand that aborted transactions, despite not delivering messages, take up space in the log and modify the offset count. In the Karafka Web UI, these are visible as system records. This behavior can sometimes lead to confusion, as users might observe a disparity between the number of user messages and the total count of records (including system records). Recognizing and understanding these system records can help users better manage and diagnose issues with their Kafka topics and transactions.</p> <p>Below, you can find an example of how the Karafka Web UI reports topic looks when all the records are created using the transactional producer:</p> <p><p> </p></p> </li> </ul> <p>These limitations underline the importance of a thorough understanding and careful implementation when leveraging Kafka transactions, especially with tools like WaterDrop.</p>"}, {"location": "WaterDrop-Transactions/#example-use-cases", "title": "Example Use Cases", "text": "<ul> <li> <p>Order Processing Systems: When an order is placed, various events might be produced, such as order creation, payment processing, and inventory updates. All or none of these events must be published to ensure data consistency.</p> </li> <li> <p>Financial Systems: Consider a system responsible for handling bank transfers. Two events are produced for a transfer - debit from the source account and credit to the destination account. To maintain financial integrity, both events must be processed atomically.</p> </li> <li> <p>Inventory Management: Two actions might occur concurrently when selling a product online - updating the inventory count and notifying the shipping service. If only one of these actions is successful, it could result in data inconsistency.</p> </li> <li> <p>Multi-step Workflows: In processes involving multiple steps (like data transformation and aggregation), and each stage results in a message, all messages in the workflow must be published to maintain a consistent view of the workflow's state.</p> </li> <li> <p>Audit Logging Systems: When an action is performed, it may produce multiple audit logs. All related audit logs must be written atomically to ensure a complete trail of events.</p> </li> </ul> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "WaterDrop-Usage/", "title": "WaterDrop Usage", "text": "<p>To send Kafka messages, just create a producer and use it:</p> <pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\nend\n\nproducer.produce_sync(topic: 'my-topic', payload: 'my message')\n\n# or for async\nproducer.produce_async(topic: 'my-topic', payload: 'my message')\n\n# or in batches\nproducer.produce_many_sync(\n  [\n    { topic: 'my-topic', payload: 'my message'},\n    { topic: 'my-topic', payload: 'my message'}\n  ]\n)\n\n# both sync and async\nproducer.produce_many_async(\n  [\n    { topic: 'my-topic', payload: 'my message'},\n    { topic: 'my-topic', payload: 'my message'}\n  ]\n)\n\n# Don't forget to close the producer once you're done to flush the internal buffers, etc\nproducer.close\n</code></pre> <p>Each message that you want to publish, will have its value checked.</p> <p>Here are all the things you can provide in the message hash:</p> Option Required Value type Description <code>topic</code> true String The Kafka topic that should be written to <code>payload</code> true String Data you want to send to Kafka <code>key</code> false String The key that should be set in the Kafka message <code>partition</code> false Integer A specific partition number that should be written to <code>partition_key</code> false String Key to indicate the destination partition of the message <code>timestamp</code> false Time, Integer The timestamp that should be set on the message <code>headers</code> false Hash Headers for the message <code>label</code> false Object Anything you want to use as a label <p>Keep in mind, that message you want to send should be either binary or stringified (to_s, to_json, etc).</p>"}, {"location": "WaterDrop-Usage/#headers", "title": "Headers", "text": "<p>Kafka headers allow you to attach key-value metadata to messages, which can be helpful for routing, filtering, tracing, and more. WaterDrop supports headers via the <code>headers:</code> key in message hashes.</p>"}, {"location": "WaterDrop-Usage/#format", "title": "Format", "text": "<p>Kafka headers are optional and must be provided as a <code>Hash</code>. According to KIP-82, each header key must be a string, and each value must be either:</p> <ul> <li>a string, or</li> <li>an array of strings.</li> </ul> <p>This means WaterDrop supports both forms:</p> <pre><code># Single value per header\nheaders: {\n  'request-id' =&gt; '123abc',\n  'source' =&gt; 'payment-service'\n}\n</code></pre> <pre><code># Multiple values per header key (KIP-82-compliant)\nheaders: {\n  'flags' =&gt; ['internal', 'async'],\n  'source' =&gt; ['payment-service']\n}\n</code></pre>"}, {"location": "WaterDrop-Usage/#example-usage", "title": "Example Usage", "text": ""}, {"location": "WaterDrop-Usage/#sync-with-headers", "title": "Sync with headers", "text": "<pre><code>producer.produce_sync(\n  topic: 'my-topic',\n  payload: 'payload-with-headers',\n  headers: {\n    'request-id' =&gt; 'abc-123',\n    'tags' =&gt; ['blue', 'fast']\n  }\n)\n</code></pre>"}, {"location": "WaterDrop-Usage/#async-with-headers", "title": "Async with headers", "text": "<pre><code>producer.produce_async(\n  topic: 'my-topic',\n  payload: 'payload-with-headers',\n  headers: {\n    'tenant-id' =&gt; 'tenant-42',\n    'features' =&gt; ['beta', 'test']\n  }\n)\n</code></pre>"}, {"location": "WaterDrop-Usage/#delivery-results", "title": "Delivery Results", "text": "<p>When dispatching messages using WaterDrop, you can choose between receiving a delivery report or a delivery handle, depending on whether you perform synchronous or asynchronous dispatches.</p>"}, {"location": "WaterDrop-Usage/#delivery-reports", "title": "Delivery Reports", "text": "<p>For synchronous dispatches, WaterDrop returns a delivery report, which provides immediate feedback about the message delivery status. When you use synchronous dispatch, the execution of your program will wait until the Kafka broker has acknowledged the message.</p> <pre><code>report = producer.produce_sync(topic: 'my_topic', payload: 'my_payload')\n\nputs \"This sent message has an offset #{report.offset} on partition #{report.partition}\"\nputs \"This sent message was sent to #{report.topic_name} topic\"\n</code></pre>"}, {"location": "WaterDrop-Usage/#delivery-handles", "title": "Delivery Handles", "text": "<p>In contrast, WaterDrop returns a delivery handle for asynchronous dispatches. When you dispatch messages asynchronously, WaterDrop will send the message without blocking your program's execution, allowing you to continue processing other tasks while the message is being sent. The key feature of the delivery handle is its <code>#wait</code> method. The <code>#wait</code> method allows you to pause your program's execution until the message is either successfully dispatched or an error occurs during delivery.</p> <pre><code>handle = producer.produce_async(\n  topic: 'my_topic',\n  payload: 'my_payload',\n  label: 'unique-id'\n)\n\nreport = handle.wait\n\nputs \"This sent message has an offset #{report.offset} on partition #{report.partition}\"\nputs \"This sent message was sent to #{report.topic_name} topic\"\nputs \"This sent message had a following label: #{report.label}\"\n</code></pre> <p>If an error does occur during delivery, the <code>#wait</code> method will raise an appropriate error with detailed information about the failure, allowing you to handle errors in your application logic.</p> <p>However, there might be scenarios where you want to wait for the message to be delivered but do not want to raise an exception if an error occurs. In such cases, the <code>#wait</code> method also accepts a <code>raise_response_error</code> flag that you can set to <code>false</code>.</p> <pre><code>handle = producer.produce_async(topic: 'my_topic', payload: 'my_payload')\nreport = handle.wait(raise_response_error: false)\n\nif report.error\n  puts \"Following issue occurred #{report.error}\"\nelse\n  puts \"This sent message has an offset #{report.offset} on partition #{report.partition}\"\n  puts \"This sent message was sent to #{report.topic_name} topic\"\nend\n</code></pre> <ul> <li> <p>If <code>raise_response_error</code> is set to <code>true</code> (the default behavior), the <code>#wait</code> method will raise an exception if there is a delivery error.</p> </li> <li> <p>If <code>raise_response_error</code> is set to false, the <code>#wait</code> method will still wait for the delivery but will not raise an exception upon failure. Instead, it will return the appropriate error along with failure details, allowing you to handle the error as needed without interrupting the program's flow or will return the delivery report upon successful delivery.</p> </li> </ul> <p>This flexibility in handling delivery reports and delivery handles in both synchronous and asynchronous scenarios makes WaterDrop a powerful choice for managing Kafka message production while accommodating different use cases and error-handling strategies.</p>"}, {"location": "WaterDrop-Usage/#labeling", "title": "Labeling", "text": "<p>Labeling refers to categorizing and tagging messages before sending them to Kafka. This can help instrument and debug messages more quickly. For a comprehensive guide on implementing and utilizing labeling, please visit this dedicated wiki page.</p>"}, {"location": "WaterDrop-Usage/#error-handling", "title": "Error Handling", "text": "<p>WaterDrop's error handling is a complex feature with its dedicated documentation. Please visit the Error Handling documentation page for detailed information and guidance.</p>"}, {"location": "WaterDrop-Usage/#transactions", "title": "Transactions", "text": "<p>Transactions in WaterDrop have a dedicated documentation page to provide in-depth information and guidelines. Please refer to this documentation page for a comprehensive understanding of transactions and related nuances.</p>"}, {"location": "WaterDrop-Usage/#usage-across-the-application-and-with-ruby-on-rails", "title": "Usage Across the Application and with Ruby on Rails", "text": "<p>If you plan to both produce and consume messages using Kafka, you should install and use Karafka. It integrates automatically with Ruby on Rails applications and auto-configures WaterDrop producer to make it accessible via <code>Karafka#producer</code> method:</p> <pre><code>event = Events.last\nKarafka.producer.produce_async(topic: 'events', payload: event.to_json)\n</code></pre> <p>If you want to only produce messages from within your application without consuming with Karafka, since WaterDrop is thread-safe you can create a single instance in an initializer like so:</p> <pre><code>KAFKA_PRODUCER = WaterDrop::Producer.new\n\nKAFKA_PRODUCER.setup do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\nend\n\n# And just dispatch messages\nKAFKA_PRODUCER.produce_sync(topic: 'my-topic', payload: 'my message')\n</code></pre>"}, {"location": "WaterDrop-Usage/#usage-with-a-connection-pool", "title": "Usage With a Connection-Pool", "text": "<p>While WaterDrop is thread-safe, there is no problem in using it with a connection pool inside high-intensity applications. The only thing worth keeping in mind, is that WaterDrop instances should be shutdown before the application is closed.</p> <pre><code>KAFKA_PRODUCERS_CP = ConnectionPool.new do\n  producer = WaterDrop::Producer.new do |config|\n    config.kafka = { 'bootstrap.servers': 'localhost:9092' }\n  end\n\n  logger = WaterDrop::Instrumentation::LoggerListener.new(\n    MyApp.logger,\n    log_messages: false\n  )\n\n  # Subscribe any listeners you want\n  producer.monitor.subscribe(logger)\n\n  # Make sure to subscribe the all Web UI listeners if you use Web UI\n  # Otherwise information from this producer will not be sent to the\n  # Karafka Web UI\n  ::Karafka::Web.config.tracking.producers.listeners.each do |listener|\n    producer.monitor.subscribe(listener)\n  end\n\n  producer\nend\n\nKAFKA_PRODUCERS_CP.with do |producer|\n  producer.produce_async(topic: 'my-topic', payload: 'my message')\nend\n\nKAFKA_PRODUCERS_CP.shutdown { |producer| producer.close }\n</code></pre>"}, {"location": "WaterDrop-Usage/#buffering", "title": "Buffering", "text": "<p>WaterDrop producers support buffering messages in their internal buffers and on the <code>rdkafka</code> level via <code>queue.buffering.*</code> set of settings.</p> <p>This means that depending on your use case, you can achieve both granular buffering and flushing control when needed with context awareness and periodic and size-based flushing functionalities.</p>"}, {"location": "WaterDrop-Usage/#buffering-messages-based-on-the-application-logic", "title": "Buffering Messages Based on the Application Logic", "text": "<pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\nend\n\n# Simulating some events states of a transaction - notice, that the messages will be flushed to\n# kafka only upon arrival of the `finished` state.\n%w[\n  started\n  processed\n  finished\n].each do |state|\n  producer.buffer(topic: 'events', payload: state)\n\n  puts \"The messages buffer size #{producer.messages.size}\"\n  producer.flush_sync if state == 'finished'\n  puts \"The messages buffer size #{producer.messages.size}\"\nend\n\nproducer.close\n</code></pre>"}, {"location": "WaterDrop-Usage/#using-rdkafka-buffers-to-achieve-periodic-auto-flushing", "title": "Using rdkafka Buffers to Achieve Periodic Auto-Flushing", "text": "<pre><code>producer = WaterDrop::Producer.new\n\nproducer.setup do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    # Accumulate messages for at most 10 seconds\n    'queue.buffering.max.ms': 10_000\n  }\nend\n\n# WaterDrop will flush messages minimum once every 10 seconds\n30.times do |i|\n  producer.produce_async(topic: 'events', payload: i.to_s)\n  sleep(1)\nend\n\nproducer.close\n</code></pre>"}, {"location": "WaterDrop-Usage/#shutdown", "title": "Shutdown", "text": "<p>Properly shutting down WaterDrop producers is crucial to ensure graceful handling and prevent potential resource leaks causing VM crashes. This section explains how to close WaterDrop producers and the implications of doing so.</p> <p>It is essential to close the WaterDrop producer before exiting the Ruby process. Closing the producer allows it to release resources, complete ongoing operations, and ensure that all messages are either successfully delivered to the Kafka cluster or purged due to exceeding the <code>message.timeout.ms</code> value.</p> <p>The <code>#close</code> method is used to shut down the producer. It is important to note that <code>#close</code> is a blocking operation, meaning it will block the execution of your program until all the necessary resources are cleaned up. Therefore, it is not recommended to start the <code>#close</code> operation in a separate thread and not wait for it to finish, as this may lead to unexpected behavior.</p> <p>Here is an example of how to use #close to shut down a producer:</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\nend\n\nproducer.close\n</code></pre> <p>In specific scenarios, such as working with unstable Kafka clusters or when you need to finalize your application fast, disregarding the risk of potential data loss, you may use the <code>#close!</code> method.</p> <p>The <code>#close!</code> method attempts to wait until a specified <code>max_wait_timeout</code> (default is <code>60</code> seconds) for any pending operations to complete. However, if the producer cannot be shut down gracefully within this timeframe, it will forcefully purge the dispatch queue and cancel all outgoing requests. This effectively prevents the closing procedure from blocking for an extensive period, ensuring that your application can exit more quickly.</p> <pre><code>producer = WaterDrop::Producer.new do |config|\n  config.kafka = { 'bootstrap.servers': 'localhost:9092' }\nend\n\nproducer.close!\n</code></pre> <p>While <code>#close!</code> can be helpful when you want to finalize your application quickly, be aware that it may result in messages not being successfully delivered or acknowledged, potentially leading to data loss. Therefore, use <code>#close!</code> with caution and only when you understand the implications of potentially losing undelivered messages.</p>"}, {"location": "WaterDrop-Usage/#closing-producer-used-in-karafka", "title": "Closing Producer Used in Karafka", "text": "<p>When you shut down Karafka consumer, the <code>Karafka.producer</code> WaterDrop instance automatically closes. There's no need to close it yourself. If you're using multiple producers or a more advanced setup, you can use the <code>app.stopped</code> event during shutdown to handle them.</p>"}, {"location": "WaterDrop-Usage/#closing-producer-used-in-puma-single-mode", "title": "Closing Producer Used in Puma (Single Mode)", "text": "<pre><code># config/puma.rb \n\n# There is no `on_worker_shutdown` equivalent for single mode\n@config.options[:events].on_stopped do\n  MY_PRODUCER.close\nend\n</code></pre>"}, {"location": "WaterDrop-Usage/#closing-producer-used-in-puma-cluster-mode", "title": "Closing Producer Used in Puma (Cluster Mode)", "text": "<pre><code># config/puma.rb \n\non_worker_shutdown do\n  MY_PRODUCER.close\nend\n</code></pre>"}, {"location": "WaterDrop-Usage/#closing-producer-used-in-sidekiq", "title": "Closing Producer Used in Sidekiq", "text": "<pre><code># config/initializers/sidekiq.rb\n\nSidekiq.configure_server do |config|\n  # You can use :shutdown for older Sidekiq versions if\n  # :exit is not available\n  config.on(:exit) do\n    MY_PRODUCER.close\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-Usage/#closing-producer-used-in-passenger", "title": "Closing Producer Used in Passenger", "text": "<pre><code>PhusionPassenger.on_event(:stopping_worker_process) do\n  MY_PRODUCER.close\nend\n</code></pre>"}, {"location": "WaterDrop-Usage/#closing-producer-used-in-a-rake-task", "title": "Closing Producer Used in a Rake Task", "text": "<p>In case of rake tasks, just invoke <code>MY_PRODUCER.close</code> at the end of your rake task:</p> <pre><code>desc 'My example rake task that sends all users data to Kafka'\ntask send_users: :environment do\n  User.find_each do |user|\n    MY_PRODUCER.producer.produce_async(\n      topic: 'users',\n      payload: user.to_json,\n      key: user.id\n    )\n  end\n\n  # Make sure, that the producer is always closed before finishing\n  # any rake task\n  MY_PRODUCER.close\nend\n</code></pre>"}, {"location": "WaterDrop-Usage/#closing-custom-producer-used-in-karafka", "title": "Closing Custom Producer Used in Karafka", "text": "<p>Custom Producers Only</p> <p>Please note that this should be used only for custom producers and not for <code>Karafka.producer</code> that is closed automatically.</p> <p>When using custom WaterDrop producers within a Karafka application, it's important to properly close them before the application shuts down. It's recommended to use the <code>app.stopped</code> event as it signifies that Karafka has completed all processing, flushed all buffers, and is ready for final cleanup operations. At this point, no more messages will be processed, making it the ideal time to safely close your custom producers. Here's how you can do this:</p> <pre><code># Create producer in Rails initializer or other place suitable within your app\nYOUR_CUSTOM_PRODUCER = WaterDrop::Producer.new\n\n# Then subscribe to the `app.stopped` event and close your producer there\nKarafka::App.monitor.subscribe('app.stopped') do\n  YOUR_CUSTOM_PRODUCER.close\nend\n</code></pre>"}, {"location": "WaterDrop-Usage/#closing-producer-in-any-ruby-process", "title": "Closing Producer in any Ruby Process", "text": "<p>While integrating WaterDrop producers into your Ruby applications, it's essential to ensure that resources are managed correctly, especially when terminating processes. We generally recommend utilizing hooks specific to the environment or framework within which the producer operates. These hooks ensure graceful shutdowns and resource cleanup tailored to the application's lifecycle.</p> <p>However, there might be scenarios where such specific hooks are not available or suitable. In these cases, Ruby's <code>at_exit</code> hook can be employed as a universal fallback to close the producer before the Ruby process exits. Here's a basic example of using at_exit with a WaterDrop producer:</p> <pre><code>at_exit do\n  MY_PRODUCER.close\nend\n</code></pre>"}, {"location": "WaterDrop-Usage/#managing-multiple-topic-delivery-requirements", "title": "Managing Multiple Topic Delivery Requirements", "text": "<p>In complex applications, you may need different delivery settings for different topics or message categories. For example, some messages might require immediate delivery with minimal latency, while others benefit from batching for higher throughput. This section explores strategies for managing these diverse requirements effectively.</p>"}, {"location": "WaterDrop-Usage/#understanding-delivery-requirements-differences", "title": "Understanding Delivery Requirements Differences", "text": "<p>When designing your Kafka message production architecture, you may encounter various requirements:</p> <ul> <li>Varying Latency Needs: Some critical messages need immediate dispatch while others can tolerate delays</li> <li>Different Throughput Optimization: High-volume topics may benefit from batching and compression</li> <li>Reliability Variations: Mission-critical data may require more acknowledgments than less important messages</li> <li>Resource Utilization: Efficiently managing TCP connections while meeting diverse requirements</li> </ul>"}, {"location": "WaterDrop-Usage/#using-multiple-producers-for-different-requirements", "title": "Using Multiple Producers for Different Requirements", "text": "<p>The most flexible solution for handling diverse delivery requirements is creating separate WaterDrop producer instances, each configured for specific needs. This approach offers maximum configuration control but requires careful management of TCP connections.</p>"}, {"location": "WaterDrop-Usage/#key-configuration-differences-that-require-separate-producers", "title": "Key Configuration Differences That Require Separate Producers", "text": "<p>Several core settings cannot be modified through variants and require distinct producer instances:</p> <ul> <li><code>queue.buffering.max.ms</code> - Controls batching frequency</li> <li><code>queue.buffering.max.messages</code> - Affects memory usage and batching behavior</li> <li><code>socket.timeout.ms</code> - Impacts how producers handle network issues </li> <li><code>queue.buffering.max.ms</code> - Controls artificial delays added for improved batching</li> <li><code>delivery.timeout.ms</code> - May need different timeouts for different message priorities</li> </ul>"}, {"location": "WaterDrop-Usage/#implementation-example", "title": "Implementation Example", "text": "<pre><code># Producer optimized for high-throughput, less time-sensitive data\nBATCH_PRODUCER = WaterDrop::Producer.new\n\nBATCH_PRODUCER.setup do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'queue.buffering.max.ms': 1000, # Longer buffering for better batching\n    'batch.size': 64_000,           # Larger batches\n    'compression.type': 'snappy'    # Compression for efficiency\n  }\nend\n\n# Producer optimized for low-latency, time-sensitive messages\nREALTIME_PRODUCER = WaterDrop::Producer.new\n\nREALTIME_PRODUCER.setup do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'queue.buffering.max.ms': 50, # Quick flushing\n    'compression.type': 'none'    # No compression overhead\n  }\nend\n\n# Usage based on message category\ndef send_message(topic, payload, category)\n  case category\n  when :critical\n    REALTIME_PRODUCER.produce_sync(topic: topic, payload: payload)\n  when :analytical\n    BATCH_PRODUCER.produce_async(topic: topic, payload: payload)\n  end\nend\n\n# Example using at_exit for simple scripts, but application frameworks\n# usually need different approaches\nat_exit do\n  BATCH_PRODUCER.close\n  REALTIME_PRODUCER.close\nend\n</code></pre> <p>Proper Producer Shutdown</p> <p>The <code>at_exit</code> example above is simplified and may not be appropriate for all environments. The correct shutdown method depends on your application framework (Rails, Puma, Sidekiq, etc.). For comprehensive guidance on properly shutting down producers in different environments, refer to the Shutdown section in this documentation.</p>"}, {"location": "WaterDrop-Usage/#tcp-connection-considerations", "title": "TCP Connection Considerations", "text": "<p>Each WaterDrop producer maintains its own set of TCP connections to Kafka brokers, which has important implications:</p> <ol> <li> <p>Resource Usage: Multiple producers increase the number of TCP connections, consuming more system resources.</p> </li> <li> <p>Connection Establishment: Each new producer requires the overhead of establishing connections.</p> </li> <li> <p>System Limits: Be mindful of connection limits in high-scale applications with many producers.</p> </li> <li> <p>Operational Complexity: More producers mean more connections to monitor and manage.</p> </li> </ol>"}, {"location": "WaterDrop-Usage/#when-to-consider-variants-instead", "title": "When to Consider Variants Instead", "text": "<p>WaterDrop Variants may be sufficient for simpler differences in delivery requirements. Variants share TCP connections while allowing customization of certain topic-specific settings. Consider variants when differences are limited to:</p> <ul> <li><code>acks</code> settings (acknowledgment levels)</li> <li><code>compression.type</code> (compression algorithm selection)</li> <li><code>message.timeout.ms</code> (message delivery timeout)</li> </ul> <p>Variants work well when reliability or compression needs to be adjusted per topic but not when fundamentally different buffering or latency characteristics are required.</p> <pre><code># Initialize producer with default settings\nproducer = WaterDrop::Producer.new do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'acks': '1'  # Default acknowledgment setting\n  }\nend\n\n# Create variants for specific requirements\ncritical_variant = producer.with(topic_config: { acks: 'all' })\nbulk_variant = producer.with(topic_config: { \n  compression.type: 'snappy', \n  message.timeout.ms: 300_000 \n})\n\n# Use variants based on message characteristics\ncritical_variant.produce_sync(topic: 'alerts', payload: alert_data.to_json)\nbulk_variant.produce_async(topic: 'analytics', payload: large_data.to_json)\n</code></pre>"}, {"location": "WaterDrop-Usage/#best-practices-for-managing-multiple-delivery-requirements", "title": "Best Practices for Managing Multiple Delivery Requirements", "text": "<ol> <li> <p>Group Similar Requirements: Minimize the number of producers by grouping similar delivery requirements.</p> </li> <li> <p>Monitor TCP Connections: Regularly check connection counts to detect potential issues.</p> </li> <li> <p>Implement Proper Shutdown: Always close all producers when finishing to release resources.</p> </li> <li> <p>Combine Approaches: Use variants for topics with similar base requirements that differ only in topic-specific settings and separate producers for fundamentally different delivery patterns.</p> </li> <li> <p>Document Configuration Decisions: Clearly document why each producer exists and which topics it handles to avoid configuration drift.</p> </li> </ol>"}, {"location": "WaterDrop-Usage/#conclusion", "title": "Conclusion", "text": "<p>Managing multiple topic delivery requirements in WaterDrop often requires a combination of approaches. Use separate producer instances when fundamental differences in buffering, latency, or resource allocation are needed. Use variants when differences are limited to topic-specific settings like acknowledgments or compression.</p>"}, {"location": "WaterDrop-Usage/#forking-and-potential-memory-problems", "title": "Forking and Potential Memory Problems", "text": "<p>If you work with forked processes, make sure you don't use the producer before the fork. You can easily configure the producer and then fork and use it.</p> <p>To tackle this obstacle related to rdkafka, WaterDrop adds finalizer to each of the producers to close the rdkafka client before the Ruby process is shutdown. Due to the nature of the finalizers, this implementation prevents producers from being GCed (except upon VM shutdown) and can cause memory leaks if you don't use persistent/long-lived producers in a long-running process or if you don't use the <code>#close</code> method of a producer when it is no longer needed. Creating a producer instance for each message is anyhow a rather bad idea, so we recommend not to.</p> <p>Last modified: 2025-05-01 22:13:28</p>"}, {"location": "WaterDrop-Variants/", "title": "WaterDrop Variants", "text": "<p>WaterDrop variants can manage different configuration settings per topic using the same Kafka producer with shared TCP connections. This feature allows optimal utilization of a producer's TCP connections while enabling tailored dispatch requirements for individual topics. Variants are beneficial when working with topics with varying levels of importance or different throughput and latency requirements.</p>"}, {"location": "WaterDrop-Variants/#creating-and-using-variants", "title": "Creating and Using Variants", "text": "<p>To leverage variants in WaterDrop, you initialize a standard producer with default settings that apply broadly to all topics for which you intend to produce messages. Then, you can create variants of this producer with configurations specific to particular topics. These variants allow for topic-specific adjustments without needing multiple producer instances, thus conserving system resources and maintaining high performance.</p> <p>Variants are created using the <code>#with</code> and <code>#variant</code> methods. It is critical in enabling topic-specific configurations through variants while using a single producer instance. The <code>#with</code> and <code>#variant</code> methods are designed to accept two types of arguments:</p> <ul> <li><code>max_wait_timeout</code>: This is a root-scoped setting.</li> <li><code>topic_config</code> hash: This is where all topic-specific configurations are defined. </li> </ul> <p>Attributes placed inside the <code>topic_config</code> hash during variant creation are referred to as <code>topic_config</code> scoped. Conversely, settings like <code>max_wait_timeout</code>, which reside outside the <code>topic_config hash</code>, are considered root-scoped.</p> <p>Here's a simple example to demonstrate how to define and use variants with WaterDrop:</p> <pre><code># Initialize the main producer with common settings\nproducer = WaterDrop::Producer.new do |config|\n  config.kafka = {\n    'bootstrap.servers': 'localhost:9092',\n    'acks': '2'  # Default acknowledgment setting for medium-importance topics\n  }\nend\n\n# Create variants with specific settings\nlow_importance = producer.with(topic_config: { acks: 1 })\nhigh_importance = producer.with(topic_config: { acks: 'all' })\n\n# Use variants like regular producers\nlow_importance.produce_async(topic: 'low_priority_events', payload: event.to_json)\nhigh_importance.produce_async(topic: 'critical_events', payload: event.to_json)\n</code></pre>"}, {"location": "WaterDrop-Variants/#configurable-settings", "title": "Configurable Settings", "text": "<p>Variants allow you to modify several Kafka and producer-specific settings to better suit the characteristics of different topics:</p> Scope Attribute Description <code>root</code> <code>max_wait_timeout</code> Controls how long the producer waits for the dispatch result before raising an error. <code>topic_config</code> <code>acks</code> <code>request.required.acks</code> Determines the number of broker acknowledgments required before considering a message delivery successful. <code>topic_config</code> <code>compression.codec</code><code>compression.type</code> Specifies the type of codec used for compression (e.g., none, gzip, snappy, lz4, zstd). <code>topic_config</code> <code>compression.level</code> Determines the compression level for the selected codec, affecting both the compression ratio and performance. <code>topic_config</code> <code>delivery.timeout.ms</code> <code>message.timeout.ms</code> Limits the time a produced message waits for successful delivery. A time of <code>0</code> is infinite. <code>topic_config</code> <code>partitioner</code> Defines partitioner to use for distribution across partitions within a topic. <code>topic_config</code> <code>request.timeout.ms</code> The ack timeout of the producer request in milliseconds. <p>Additional Configuration Attributes Details</p> <p>For a more comprehensive list of configuration settings supported by librdkafka, please visit the Librdkafka Configuration page.</p>"}, {"location": "WaterDrop-Variants/#edge-cases-and-details", "title": "Edge-Cases and Details", "text": "<p>When using variants in WaterDrop, there are specific edge cases and operational nuances that you should be aware of to ensure optimal performance and behavior:</p> <ul> <li> <p>Buffering Behavior Across Variants: It is crucial to understand that while <code>topic_config</code> specific settings are preserved per message, the <code>max_wait_timeout</code> applied during the flush operation will correspond to the variant that initiates the flushing. This means that messages from other variants that were buffered may be dispatched using the <code>max_wait_timeout</code> of the variant currently flushing the data. Since variants share a single producer buffer, this can affect how messages are processed.</p> </li> <li> <p>Inconclusive Error Messages: Redefining <code>max_wait_timeout</code> without aligning it with other librdkafka settings can lead to inconclusive error. This issue arises because the timeout settings may not synchronize well with other operational parameters, potentially leading to errors that are difficult to diagnose. For a deeper understanding of this issue and how it might affect your Kafka operations, refer to the Error Handling documentation.</p> </li> <li> <p>Immutable acks for Idempotent and Transactional Producers: When working with idempotent or transactional producers, it is important to note that the <code>acks</code> setting is immutable and automatically set to <code>all</code>. This configuration cannot be altered through variants, as ensuring exactly-once semantics requires a fixed acknowledgment policy. Attempting to change the acks setting for these producers will result in an error.</p> </li> </ul> <p>These details are critical in effectively managing and troubleshooting your Kafka message production environment, especially when utilizing the flexibility of variants for different topic configurations.</p>"}, {"location": "WaterDrop-Variants/#conclusion", "title": "Conclusion", "text": "<p>Variants address the need for dynamic, topic-specific configurations in applications interacting with Kafka. By enabling variations per topic within a single producer, WaterDrop helps streamline resource usage and enhance message dispatch efficiency, making it an essential tool for sophisticated Kafka-based messaging systems.</p> <p>Last modified: 2024-06-09 11:31:22</p>"}, {"location": "WaterDrop-reconfiguration/", "title": "WaterDrop reconfiguration", "text": "<p>WaterDrop is a standalone messages producer integrated with Karafka out of the box.</p> <p>Karafka comes with full WaterDrop support. It also integrates automatically with it, populating all the options related to Kafka that were set during the Karafka framework configuration.</p> <p>In case you want to change WaterDrop configuration settings, you can do this by overwriting the default <code>producer</code> while configuring the Karafka application:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Karafka config...\n    config.client_id = ::Settings.name\n\n    config.producer = ::WaterDrop::Producer.new do |p_config|\n      p_config.kafka = {\n        'bootstrap.servers': 'localhost:9092',\n        'request.required.acks': 1\n      }\n    end\n  end\n\n  routes.draw do\n    # consumer groups definitions go here\n  end\nend\n</code></pre>"}, {"location": "WaterDrop-reconfiguration/#partial-reconfiguration", "title": "Partial Reconfiguration", "text": "<p>There are scenarios where you want only partially to overwrite the configuration and change one or two attributes. In cases like this, to not duplicate the Kafka cluster configuration, you can use the <code>Karafka::Setup::AttributesMap</code>. For example, you may want to disable dispatch to Kafka altogether in <code>test</code> env:</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # Karafka config...\n    config.client_id = ::Settings.name\n\n    config.producer = ::WaterDrop::Producer.new do |p_config|\n      # Copy the default config, filtering out non-producer related settings\n      p_config.kafka = ::Karafka::Setup::AttributesMap.producer(config.kafka.dup)\n      # share the Karafka logger\n      p_config.logger = config.logger\n\n      # Any reconfiguration you want goes here\n      #\n      # Do not send messages to Kafka in test env\n      p_config.deliver = !Karafka.env.test?\n    end\n  end\nend\n</code></pre> <p>Last modified: 2024-09-21 16:48:35</p>"}, {"location": "Web-UI-About/", "title": "About Web UI", "text": "<p>Karafka Web UI is a user interface for the Karafka framework. The Web UI provides a convenient way for developers to monitor and manage their Karafka-based applications, without the need to use the command line or third party software. It does not require any additional database beyond Kafka itself.</p> <p>The interface, amongst others, displays:</p> <ul> <li>real-time aggregated metrics,</li> <li>real-time information on resources usage,</li> <li>errors details,</li> <li>performance statistics,</li> <li>trends</li> <li>allows for Kafka topics data exploration</li> <li>routing and system information</li> <li>status of Web UI integration within your application</li> </ul> <p>Karafka Web UI is shipped as a separate gem with minimal dependencies.</p> <p> </p> <p>Last modified: 2023-10-25 17:28:02</p>"}, {"location": "Web-UI-Components/", "title": "Web UI Components", "text": "<p>Karafka Web UI is an intuitive tool that visually represents the metrics related to the operation status of Karafka processes. It centralizes data, offers insights into the system's health, and ensures that users can understand and analyze the functioning of their Karafka processes in real time.</p>"}, {"location": "Web-UI-Components/#how-karafka-web-ui-works", "title": "How Karafka Web UI Works", "text": "<ul> <li> <p>Tracking: Every Karafka process is responsible for publishing metrics that reflect its operational status. This is done automatically every 5 seconds by default. This periodic data publishing is termed as <code>tracking</code>. It captures information granularly, offering insights into individual consumer operations and other related data.</p> </li> <li> <p>Processing: Once the tracking data is published, it isn't displayed directly on the Web UI. Instead, a specialized consumer dedicated to Karafka Web UI aggregates this raw data. The purpose is to transform this raw data into meaningful information that can be easily represented and interpreted. Creating representations or models around the tracking data makes it easier to understand and present. For this purpose, Karafka employs a separate consumer group, which gets activated when the Web UI is in use. This stage of converting raw data into structured models for presentation is termed <code>processing</code>.</p> </li> <li> <p>Presenting: After processing the data, the final step is to display it via the Web UI. This involves presenting the structured and aggregated data visually appealing and comprehensibly, ensuring you can easily gauge the status and health of your Karafka processes.</p> </li> </ul> <p>Below you can find the diagram of the whole data flow:</p> <p> </p> <p>Please note, that this is an abstract flow visualisation. Karafka Web works well even when there is one <code>karafka server</code> process running.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "Web-UI-Configuration/", "title": "Web UI Configuration", "text": "<p>Similar to configuring Karafka, a few configuration options can also be used in Karafka Web.</p> <p>Those options can be used to control things like topics names, frequency of data reports, encrypted data visibility, pagination settings, and more.</p> <p>You can find the whole list of settings here.</p> <p>You can configure Web UI by using the <code>#setup</code> method in your <code>karafka.rb</code>:</p> <pre><code>Karafka::Web.setup do |config|\n  # Report every 10 seconds\n  config.tracking.interval = 10_000\nend\n\nKarafka::Web.enable!\n</code></pre>"}, {"location": "Web-UI-Configuration/#monitoring-non-default-producer-instances", "title": "Monitoring Non-Default Producer Instances", "text": "<p>A Karafka errors page UI view allows users to inspect errors occurring during messages consumption and production, including all the asynchronous errors coming from <code>librdkafka</code></p> <p>By default, Karafka Web UI is set only to monitor and track the default producer, which is initialized automatically and made available under <code>Karafka.producer</code>.</p> <p>This means that if you manually create and initialize custom producers (using WaterDrop), these custom producers are not automatically tracked or monitored in the Web UI. They aren't connected to the monitoring instruments that the Web UI uses to track events and states of the producers.</p> <p>To have these custom producers tracked in the Karafka Web UI, you need to subscribe the appropriate listeners to them manually. This is achieved by using the following code:</p> <pre><code>MY_CUSTOM_PRODUCER = WaterDrop::Producer.new\n\n::Karafka::Web.config.tracking.producers.listeners.each do |listener|\n  MY_CUSTOM_PRODUCER.monitor.subscribe(listener)\nend\n</code></pre>"}, {"location": "Web-UI-Configuration/#opting-out-of-all-the-monitoring", "title": "Opting out of All the Monitoring", "text": "<p>In specific scenarios, you may want to keep the Karafka Web UI without active reporting, for example, when you are only interested in the Explorer functionality. To turn off all the reporting and states materialization, overwrite all the listeners and turn off processing as follows:</p> <pre><code>Karafka::Web.setup do |config|\n  config.processing.active = false\n  config.tracking.consumers.listeners = []\n  config.tracking.producers.listeners = []\nend\n</code></pre>"}, {"location": "Web-UI-Configuration/#opting-out-of-producers-monitoring", "title": "Opting out of Producers' Monitoring", "text": "<p>In specific scenarios, you may not want the Karafka Web UI to monitor your Kafka producers. For instance:</p> <ol> <li>Performance considerations: Depending on the scale of your application, having numerous producers being tracked might add unnecessary overhead to your application, thereby reducing overall performance. This could be especially relevant in a production environment where efficiency and resource utilization are critical.</li> <li>Privacy or Security concerns: You might have producers dealing with sensitive data that you prefer not to expose through monitoring, or your security guidelines might not allow such tracking.</li> <li>Simplicity: If you have many producers and only a subset of them are relevant for your current debugging or monitoring needs, tracking all producers could clutter the Web UI, making it harder to focus on the issues at hand. In such cases, you can opt out of monitoring producers with the Karafka Web UI by using the provided code:</li> </ol> <pre><code>Karafka::Web.setup do |config|\n  # Do not instrument producers with web-ui listeners\n  config.tracking.producers.listeners = []\nend\n</code></pre>"}, {"location": "Web-UI-Configuration/#reconfiguring-web-ui-topics-for-various-kafka-providers", "title": "Reconfiguring Web UI Topics for Various Kafka Providers", "text": "<p>Karafka Web UI ships with sensible default configurations for its internal topics, but these defaults may not be compatible with all Kafka providers. Managed services like Amazon MSK, Confluent Cloud, and other cloud-based Kafka offerings often have specific requirements or restrictions that can conflict with the default topic configurations.</p> <p>Karafka Web UI allows you to override the default topic configurations for each internal topic to address these compatibility issues. You can customize these settings using the <code>.config</code> method on individual topic configurations:</p> <pre><code>Karafka::Web.setup do |config|\n  # Other config...\n\n  # Configure the errors topic for your cluster compatibility\n  config.topics.errors.config = {\n    'retention.ms' =&gt; '604800000',  # 7 days\n    'segment.ms' =&gt; '86400000'      # 1 day\n  }\n\n  # Configure consumers reports topic\n  config.topics.consumers.reports.config = {\n    'retention.ms' =&gt; '259200000',  # 3 days\n    'min.compaction.lag.ms' =&gt; '3600000'  # 1 hour\n  }\n\n  # Configure consumers states topic\n  config.topics.consumers.states.config = {\n    'segment.ms' =&gt; '3600000',      # 1 hour\n    'min.compaction.lag.ms' =&gt; '1800000'  # 30 minutes\n  }\n\n  # Configure consumers metrics topic\n  config.topics.consumers.metrics.config = {\n    'retention.ms' =&gt; '172800000',  # 2 days\n    'segment.ms' =&gt; '43200000'      # 12 hours\n  }\n\n  # Configure consumers commands topic\n  config.topics.consumers.commands.config = {\n    'retention.ms' =&gt; '86400000',   # 1 day\n    'segment.ms' =&gt; '21600000'      # 6 hours\n  }\nend\n</code></pre> <p>Avoid Changing Cleanup Policies</p> <p>We strongly recommend against modifying the <code>cleanup.policy</code> setting for Web UI topics. Each topic's cleanup policy is carefully aligned with its purpose and data usage patterns. Changing these policies may result in data loss, performance degradation, or unexpected behavior in the Web UI functionality.</p>"}, {"location": "Web-UI-Configuration/#using-a-shared-kafka-cluster-for-multiple-karafka-application-environments", "title": "Using a Shared Kafka Cluster for Multiple Karafka Application Environments", "text": "<p>You can configure Karafka to use a single Kafka cluster across multiple environments or applications. This can be beneficial for scenarios such as when you have different stages of development, including development, testing, staging, and production, all needing isolated data sets within the same Kafka cluster.</p> <p>To achieve this, each environment must maintain its unique set of Web-UI internal topics. This is accomplished by appending the environment's name to the base topic name for each Web-UI internal topic. Here is how you can configure it in your Karafka setup:</p> <pre><code>Karafka::Web.setup do |config|\n  env_suffix = Rails.env.to_s\n\n  config.group_id = \"karafka_web_#{env_suffix}\"\n\n  config.topics.errors.name = \"karafka_errors_#{env_suffix}\"\n  config.topics.consumers.reports.name = \"karafka_consumers_reports_#{env_suffix}\"\n  config.topics.consumers.states.name = \"karafka_consumers_states_#{env_suffix}\"\n  config.topics.consumers.metrics.name = \"karafka_consumers_metrics_#{env_suffix}\"\n  config.topics.consumers.commands.name = \"karafka_consumers_commands_#{env_suffix}\"\nend\n</code></pre> <p>In this setup, the <code>env_suffix</code> is created by converting the current Rails environment into a string. The <code>env_suffix</code> is then appended to the base topic name for each of the internal topics (<code>karafka_errors</code>, <code>karafka_consumers_reports</code>, <code>karafka_consumers_states</code>, <code>karafka_consumers_metrics</code> and <code>karafka_consumers_commands</code>).</p> <p>This naming convention ensures that each environment has its own unique set of topics, allowing you to monitor and manage each environment separately within the same Kafka cluster without fear of data overlap or collision.</p> <p>After setting up your environments, it's important to remember to run <code>bundle exec karafka-web install</code> for each environment. This command will create the appropriate topics per environment with the expected settings and populate these topics with initial data. Running this command ensures that all topics are set up correctly and ready for use within their respective environments.</p>"}, {"location": "Web-UI-Configuration/#in-memory-cluster-data-caching", "title": "In-Memory Cluster Data Caching", "text": "<p>Karafka Web UI implements an in-memory cache mechanism to optimize its performance and responsiveness. This cache is instrumental in storing essential cluster metadata, including the list of topics.</p>"}, {"location": "Web-UI-Configuration/#cache-duration", "title": "Cache Duration", "text": "<p>The default duration for which this cache remains valid is 5 minutes. This means that after performing actions such as topic creation, removal, or repartitioning in the cluster, the changes might not be immediately visible on the Karafka Web UI. There might be a delay of up to 5 minutes before the UI reflects these changes.</p>"}, {"location": "Web-UI-Configuration/#configurability", "title": "Configurability", "text": "<p>For those who require a different cache duration, perhaps due to more frequent cluster changes or other specific needs, Karafka allows this duration to be customizable. You can set the cache duration by modifying the <code>config.ui.cache</code> value to your desired timeframe.</p> <pre><code>Karafka::Web.setup do |config|\n  # Lower the cache to 1 minute\n  config.ui.cache = Karafka::Web::Ui::Lib::Cache.new(60_000)\nend\n</code></pre>"}, {"location": "Web-UI-Configuration/#cache-refresh", "title": "Cache Refresh", "text": "<p>One of the features to note is that whenever the Status view is accessed, the cache gets invalidated and refreshed. This ensures that users get the most recent and accurate information when they visit this view.</p>"}, {"location": "Web-UI-Configuration/#consideration-for-multiple-processes-deployment", "title": "Consideration for Multiple Processes Deployment", "text": "<p>If you've deployed Karafka Web UI across multiple processes, simply refreshing the cache in one process (by visiting the Cluster view) might not be sufficient. This is because subsequent requests could be routed to different processes, each with its cache state. In such scenarios, the cache would need to be refreshed in each of these processes to ensure consistency.</p>"}, {"location": "Web-UI-Configuration/#summary", "title": "Summary", "text": "<p>In summary, while the in-memory cache in Karafka Web UI significantly enhances its efficiency, it's essential to understand its workings, especially in dynamic environments where cluster changes are frequent or when deploying across multiple processes, and to configure it according to your needs.</p> <p>Last modified: 2025-06-15 19:52:26</p>"}, {"location": "Web-UI-Data-Management/", "title": "Web UI Data Management", "text": "<p>Karafka Web UI is a tool for managing and monitoring data within Kafka-based systems. This document describes its unique approach to data management, schema handling, and migrations.</p>"}, {"location": "Web-UI-Data-Management/#data-storage-and-management", "title": "Data Storage and Management", "text": "<p>Karafka Web UI utilizes Apache Kafka as its core for data management, eliminating the need for third-party databases. This direct integration offers several advantages:</p> <ul> <li> <p>Streamlined Data Handling: Data is managed directly within Kafka, providing a unified and efficient approach to data processing and storage.</p> </li> <li> <p>No External Dependencies: The absence of a third-party database simplifies the architecture, reducing potential points of failure and maintenance overhead.</p> </li> </ul>"}, {"location": "Web-UI-Data-Management/#topic-based-data-organization", "title": "Topic-Based Data Organization", "text": "<p>Karafka employs a topic-centric approach to organize and materialize relevant data:</p> <ul> <li> <p>Custom Topics: Karafka uses its own Kafka topics to store and materialize information, ensuring data is categorized logically and efficiently.</p> </li> <li> <p>Topic Schemas: Each topic message adheres to a defined schema, ensuring consistency and reliability in the data structure.</p> </li> </ul>"}, {"location": "Web-UI-Data-Management/#schema-versioning-and-compatibility", "title": "Schema Versioning and Compatibility", "text": "<p>Karafka Web UI emphasizes strict schema management:</p> <ul> <li> <p>Schema Versioning: All topic messages in Karafka are versioned. This versioning allows for backward compatibility and clear evolution of data structures.</p> </li> <li> <p>Handling of Schema Changes: In the event of schema modifications, Karafka Web UI employs a rigorous approach:</p> <ul> <li> <p>Older Schemas: Reports with outdated schemas are ignored, prioritizing consistency over backward compatibility.</p> </li> <li> <p>Newer Schemas: Messages with newer schemas trigger an error in the Karafka consumer, halting data processing. This persists until the system is upgraded to handle the new schema, facilitating zero-downtime rolling upgrades.</p> </li> </ul> </li> </ul>"}, {"location": "Web-UI-Data-Management/#migrations-and-consistency-in-materialized-topics", "title": "Migrations and Consistency in Materialized Topics", "text": "<p>For materialized topics, especially those holding aggregated statistics and metrics, Karafka Web UI integrates a specialized migration engine:</p> <ul> <li> <p>Internal Migration Engine: Functionally akin to Ruby on Rails migrations, this engine recognizes different versions of topic schemas.</p> </li> <li> <p>Migration Execution: The engine executes necessary migrations to bring materialized and aggregated topic data to the correct consistency state.</p> </li> <li> <p>Ensuring Data Integrity: This system ensures that data across various versions remains consistent and reliable, essential for accurate data analysis and reporting.</p> </li> </ul>"}, {"location": "Web-UI-Data-Management/#eventual-consistency-of-web-ui-data", "title": "Eventual Consistency of Web UI Data", "text": "<p>Karafka Web UI data is eventually consistent, meaning that while the system strives to keep the data current, there can be delays in metrics reporting. Most of the data presented in the Web UI is collected from consumer and producer processes, and their reporting depends on their current state.</p> <p>Under certain circumstances, such as heavy lags on multiple partitions, the data presented on the graphs may be outdated by a few minutes. This latency occurs because the consumer and producer processes may not immediately reflect the latest state of the system when they are under significant load.</p> <p>However, this delay is generally irrelevant when analyzing patterns and conducting general health assessments. The eventual consistency model ensures that, despite temporary delays, the data will ultimately reflect the accurate state of the system. This approach allows users to identify trends and monitor the overall health of their Kafka-based environment effectively, even if some metrics are momentarily lagging.</p>"}, {"location": "Web-UI-Data-Management/#conclusion", "title": "Conclusion", "text": "<p>Karafka Web UI offers a robust, efficient, and reliable solution for monitoring Karafka-based environments. Its direct use of Kafka for data storage and sophisticated schema management and migration capabilities positions it as a powerful tool for users seeking to leverage Kafka within their applications.</p> <p>Last modified: 2024-08-01 14:28:09</p>"}, {"location": "Web-UI-Development-vs-Production/", "title": "Web UI Setup for Development vs Production", "text": "<p>Karafka Web UI can operate in production mode. It is, however, essential to understand how it works and its limitations.</p>"}, {"location": "Web-UI-Development-vs-Production/#dedicated-web-ui-processes", "title": "Dedicated Web UI Processes", "text": "<p>To materialize and prepare data for the Karafka Web UI, Karafka adds an additional consumer group to your routes responsible for consuming internal Karafka Web UI topics. In a development environment, this setup works fine because you typically run a small number of <code>karafka server</code> instances, and resource contention is minimal.</p> <p>However, in a production environment, things can get more complex. The Web UI consumer that processes and materializes data does not have any priority over other consumers running within the same Karafka process. This means that unless the Web UI has dedicated resources, the consumption, and materialization of data for the UI could be delayed if other consumers are processing large volumes of messages or if there are significant lags on other topics. Essentially, the Web UI will compete for resources (CPU, memory, I/O) with other consumers in the same process, which can lead to slow or laggy UI performance, especially in cases where other consumer topics have higher processing loads.</p>"}, {"location": "Web-UI-Development-vs-Production/#performance-and-resource-management-in-production", "title": "Performance and Resource Management in Production", "text": "<p>In larger or more complex production environments, especially when dealing with multi-app setups or large-scale message flows, running the Web UI consumer group in its own dedicated Karafka process is recommended. This way, the Web UI will not be affected by lags or resource bottlenecks from other consumers, ensuring smoother performance and faster data availability in the UI.</p> <p>To achieve this, you can either:</p> <ol> <li> <p>Use <code>config.processing.active</code>: In the Karafka configuration, you can set <code>config.processing.active</code> to <code>false</code> for all processes that should exclude the Web UI consumer group. This will ensure that only dedicated processes handle Web UI topics while the rest focus on your application's primary consumers.</p> </li> <li> <p>Use the <code>--include</code> and <code>--exclude</code> flags: Alternatively, you can explicitly control which consumer groups run on each Karafka server instance using the <code>--include-consumer-groups</code> and <code>--exclude-consumer-groups</code> flags. This method provides more flexibility for explicitly including or excluding the Web UI consumer group in certain processes without modifying the global configuration.</p> </li> </ol> <pre><code># Use the --include-consumer-groups flag to start a dedicated Web UI process\nbundle exec karafka server --include-consumer-groups karafka_web\n\n# Use the --exclude-consumer-groups flag to start processes without the Web UI consumer group\nbundle exec karafka server --exclude-consumer-groups karafka_web\n</code></pre> <p>For a production environment, the ideal setup would involve:</p> <ul> <li> <p>Running one or more Karafka server instances dedicated solely to the Web UI consumer group. These processes should only handle the Web UI topics and not process any other consumer groups.</p> </li> <li> <p>For the remaining Karafka server instances that handle your application's consumers, either:</p> <ul> <li>Set <code>config.processing.active</code> to <code>false</code> to exclude the Web UI consumer group, or</li> <li>Use the <code>--exclude-consumer-groups karafka_web</code> flag to ensure these instances ignore the Web UI consumer group.</li> </ul> </li> </ul> <p>This approach ensures that the Web UI can consume and display data efficiently without being affected by the load on other consumers.</p> <ol> <li> <p>Dedicate a Swarm Node solely to the Web UI: Configure your Karafka swarm to reserve a specific node exclusively for Web UI processing, completely isolating it from your application's consumer workloads.</p> </li> <li> <p>Use Embedded Mode for the Web UI: Run the Karafka Web UI consumer in Embedded Mode within a Puma process. This approach is convenient for sharing resources between the Web UI consumer and web server, but should only be used when the Puma process is dedicated exclusively to the Web UI and not part of a larger, world-facing application.</p> </li> </ol>"}, {"location": "Web-UI-Development-vs-Production/#dedicated-web-ui-swarm-node", "title": "Dedicated Web UI Swarm Node", "text": "<p>When operating Karafka in a swarm configuration with multiple nodes, you can dedicate a specific node exclusively to the Web UI consumer group. This approach provides several benefits:</p> <ol> <li>Improved resource isolation between your application consumers and the Web UI</li> <li>Better performance for both your main application and the Web UI interface</li> <li>Easier monitoring and debugging of Web UI-specific processes</li> </ol> <p>To configure a dedicated node for the Web UI, follow these steps:</p> <ol> <li>First, define your total number of swarm nodes in your configuration:</li> </ol> <pre><code>config.swarm.nodes = 4\n</code></pre> <ol> <li>Configure your default routes to use only the first three nodes, reserving the last one for Web UI:</li> </ol> <pre><code># You can also do this on a per-topic basis for granular configuration\nroutes.draw do\n  defaults do\n    swarm(nodes: [0, 1, 2])\n  end\n\n  # consumer groups and topics definitions...\nend\n</code></pre> <ol> <li>Enable Karafka Web UI:</li> </ol> <pre><code>Karafka::Web.enable!\n</code></pre> <ol> <li>After enabling the Web UI, assign the dedicated node to the Web UI consumer group's topics. Since the Web UI routes are injected after your application routes, you need to locate and modify them:</li> </ol> <pre><code>Karafka::App\n  .routes\n  .last\n  .topics\n  .find('karafka_consumers_reports')\n  .swarm\n  .nodes = [3]\n</code></pre> <p>This configuration ensures that node <code>3</code> (the fourth node, as indexing starts at <code>0</code>) is exclusively dedicated to processing the Web UI consumer topics, while nodes <code>0</code>, <code>1</code>, and <code>2</code> handle your application's regular workload.</p> <p>The key benefit of this setup is that the Web UI's performance remains consistent regardless of the load on your main application consumers. By isolating the Web UI to a dedicated node, you prevent resource contention that could otherwise impact the UI's responsiveness during high-traffic periods.</p> <p>Remember that this approach requires at least one additional node in your swarm configuration. Consider using the other approaches mentioned in the previous sections if you have a smaller setup with fewer nodes.</p>"}, {"location": "Web-UI-Development-vs-Production/#web-ui-topics-replication-factor", "title": "Web UI Topics Replication Factor", "text": "<p>When running <code>bundle exec karafka-web install</code>, Karafka Web will create needed topics with the replication factor of <code>2</code> as long as there are at least two brokers available. Such a value may not be desirable in a larger production environment.</p> <p>You can increase the replication factor by providing the <code>--replication-factor N</code> with <code>N</code> being the desired replication factor in your cluster:</p> <pre><code>bundle exec karafka-web install --replication-factor 5\n</code></pre>"}, {"location": "Web-UI-Development-vs-Production/#usage-with-heroku-kafka-multi-tenant-add-on", "title": "Usage with Heroku Kafka Multi-Tenant add-on", "text": "<p>This section only applies to the Multi-Tenant add-on mode.</p> <p>Please keep in mind that in order for Karafka Web UI to work with Heroku Kafka Multi-Tenant Addon, all Karafka Web UI, topics need to be prefixed with your <code>KAFKA_PREFIX</code>:</p>"}, {"location": "Web-UI-Development-vs-Production/#topics-automatic-prefix", "title": "Topics Automatic Prefix", "text": "<pre><code>Karafka::Web.setup do |config|\n  config.topics.errors = \"#{ENV['KAFKA_PREFIX']}_karafka_errors\"\n  config.topics.consumers.reports = \"#{ENV['KAFKA_PREFIX']}_karafka_consumers_reports\"\n  config.topics.consumers.states = \"#{ENV['KAFKA_PREFIX']}_karafka_consumers_states\"\n  config.topics.consumers.metrics = \"#{ENV['KAFKA_PREFIX']}_karafka_consumers_metrics\"\n  config.topics.consumers.commands = \"#{ENV['KAFKA_PREFIX']}_karafka_consumers_commands\"\nend\n</code></pre>"}, {"location": "Web-UI-Development-vs-Production/#web-ui-consumer-group-creation", "title": "Web UI Consumer Group Creation", "text": "<p>Additionally, if you decided to reconfigure the <code>config.admin.group_id</code> value, you might also need to update the Web UI <code>config.group_id</code>:</p> <pre><code>Karafka::Web.setup do |config|\n  # After configuration, do not forget to use Heroku CLI to assign proper ACL permissions to this group.\n  config.group_id = 'karafka-web'\nend\n</code></pre>"}, {"location": "Web-UI-Development-vs-Production/#heroku-multi-tenant-retention-policy-impact", "title": "Heroku Multi-Tenant Retention Policy Impact", "text": "<p>When using Heroku Kafka in MultiTenant mode, it's important to know that the default message retention period is only one day. This limited retention time can pose challenges, especially for applications that rely heavily on Kafka for storage, such as Karafka Web UI. Karafka Web UI uses Kafka as its sole storage source, meaning longer retention is necessary for effective operation. It is highly recommended that you read more about this here.</p> <p>You can read about working with Heroku Kafka Multi-Tenant add-on here.</p>"}, {"location": "Web-UI-Development-vs-Production/#upgrade-recommendations", "title": "Upgrade Recommendations", "text": "<p>Upgrading your Karafka Web UI to a newer version is a three-step operation. You must be diligent about the order of operations to avoid unexpected errors. The process is as follows:</p> <ol> <li>Update Karafka and Its Dependencies: First, ensure that you're running the latest version of Karafka, along with its key dependencies, which include <code>karafka-core</code>, <code>karafka-rdkafka</code>, and <code>waterdrop</code>.</li> <li>Deploy All Karafka Consumer Processes: Your first step should be to deploy all the Karafka consumer processes on all nodes where the <code>karafka server</code> command runs. Ensure that all your consumers are up-to-date and working with the most recent consumer version.</li> <li>Deploy the Web UI Update to Your Web Server: After all the consumer processes have been upgraded, you can safely deploy the updated Web UI to your web server. The updated web UI will have the necessary code and schema changes to work with the latest consumer version.</li> </ol> <p>Please take note of the following potential issue:</p> <p>If you attempt to deploy the updated Web UI before the Karafka consumer processes, you may encounter errors. This could range from 500 Internal Server errors to incorrect or missing offset-related data displays.</p> <p>It's critical to ensure the order of operations - Karafka consumers processes first, then the Web UI. This will provide a smoother transition to the new version of the Web UI.</p> <p>Last modified: 2025-03-11 14:54:49</p>"}, {"location": "Web-UI-Features/", "title": "Web UI Features", "text": "<p>Karafka Web UI contains several features allowing you to understand your system's karafka consumption process.</p> <p>Below you can find a comprehensive description of the most important features you can use.</p> <p>Karafka Pro Enhancements</p> <p>Karafka Pro offers enhanced Web UI with many additional metrics and functionalities.</p> <p>Web UI Scope and Simplifications</p> <p>Karafka Web UI does not aim to be a full analytical platform for tracking and measuring all data from Kafka with extreme accuracy. Due to technical reasons, some simplifications have been made. Karafka offers instrumentation API, allowing you to craft instrumentation matching your use cases. While certain graphs and metrics may not always be fully accurate, they should provide an overall understanding of the system state.</p>"}, {"location": "Web-UI-Features/#dashboard", "title": "Dashboard", "text": "<p>Additional Graphs in Pro</p> <p>More graphs are available only in our Pro offering.</p> <p>The dashboard provides an all-encompassing insight into your Karafka operations. It\u2019s an indispensable tool for anyone looking to monitor, optimize, and troubleshoot their Karafka processes. With its user-friendly interface and detailed metrics, you have everything you need to ensure the smooth running of your Kafka operations.</p> <p></p>"}, {"location": "Web-UI-Features/#consumers", "title": "Consumers", "text": "<p>Enhanced Consumer Metrics in Pro</p> <p>More metrics and detailed consumers inspection are available only in our Pro offering.</p> <p>The consumers status view allows users to view and monitor the performance of Kafka-running consumers. The page displays real-time data and aggregated metrics about the status of the consumers, such as their current offset, lag, the current state of consumers, and others.</p> <p> </p> <p>The following metrics are available for each consumer:</p> <ul> <li><code>Started</code> - The moment when the given consumer process was started.</li> <li><code>Memory</code> - RSS (Resident Set Size) measures memory usage in an operating system. It represents the portion of a process's memory held in RAM and is \"resident\" in the system.</li> <li><code>Utilization</code> - Displays the number of threads in a given process against a number of threads actively processing data in a given moment.</li> <li><code>Total lag</code> - Sumed lag from all the partitions actively consumed by a given process.</li> </ul>"}, {"location": "Web-UI-Features/#jobs", "title": "Jobs", "text": "<p>More metrics are available in our Pro offering.</p> <p>This page provides a real-time view of the jobs that are currently being processed, including information such as:</p> <ul> <li><code>Process</code> - Process name where the job is running.</li> <li><code>Topic</code> - Topic and partition which the job is processing.</li> <li><code>Consumer</code> - Class of the consumer that is running.</li> <li><code>Type</code> - Type of work: <code>#consume</code>, <code>#revoke</code> or <code>#shutdown</code></li> <li><code>Started at</code> - Since when the job is running.</li> </ul> <p> </p>"}, {"location": "Web-UI-Features/#health", "title": "Health", "text": "<p>This dashboard view shows Karafka consumers' groups' health state with their lag aggregated information and basic trends.</p> <p>Here you can learn more about the information available in this dashboard view.</p> <p> </p>"}, {"location": "Web-UI-Features/#routing", "title": "Routing", "text": "<p>The Routing UI view allows users to inspect Karafka's routing configuration, including details about particular topics. It recognizes the routing patterns, though it is worth remembering that it can take Karafka Web UI up to 5 minutes to identify and map newly detected topics due to the internal caching layer.</p> <p> </p> <p> </p>"}, {"location": "Web-UI-Features/#explorer", "title": "Explorer", "text": "<p>Pro Only Functionality</p> <p>This functionality is available only in our Pro offering.</p> <p>Karafka Data Explorer is an essential tool for users seeking to navigate and comprehend the data produced to Kafka. Offering an intuitive interface and a deep understanding of the routing table, the explorer ensures that users can access deserialized data effortlessly for seamless viewing. You can read more about it here.</p> <p> </p>"}, {"location": "Web-UI-Features/#search", "title": "Search", "text": "<p>Pro Only Functionality</p> <p>This functionality is available only in our Pro offering.</p> <p>The Search feature is a tool that enables users to search and filter messages efficiently. This feature allows users to search within one or multiple partitions, start from a specific time or offset, apply custom matchers to payloads, keys, or headers, and use custom deserializers for data.</p> <p> </p>"}, {"location": "Web-UI-Features/#errors", "title": "Errors", "text": "<p>A Karafka errors page UI view allows users to inspect errors occurring during messages consumption and production, including all the asynchronous errors coming from <code>librdkafka</code>. It includes the following information:</p> <ul> <li><code>Origin</code> - Topic and partition from which the error comes or code location for non-consumption related errors.</li> <li><code>Process name</code> - Name of the process on which the error occurred.</li> <li><code>Error</code> - Error type.</li> <li><code>Occurred at</code> - Moment in time when the error occurred.</li> <li><code>Backtrace</code> (Pro only) - Full backtrace that shows the sequence of methods and calls that lead up to an exception (an error).</li> </ul> <p> </p> <p> </p>"}, {"location": "Web-UI-Features/#dlq-dead", "title": "DLQ / Dead", "text": "<p>Pro Only Functionality</p> <p>This functionality is available only in our Pro offering.</p> <p>The Dead Letter Queue (DLQ) dashboard allows users to view messages that have failed to be processed and were skipped and moved to the Dead Letter Queue topic with their original details.</p> <p> </p> <p> </p>"}, {"location": "Web-UI-Features/#cluster", "title": "Cluster", "text": "<p>The Cluster dashboard view displays information about the status of the Kafka cluster and the topics list.</p> <p> </p>"}, {"location": "Web-UI-Features/#status", "title": "Status", "text": "<p>The Karafka Web UI status page allows you to check and troubleshoot the state of your Karafka Web UI integration with your application.</p> <p>It can help you identify and mitigate problems that would cause the Web UI to malfunction or misbehave. If you see the <code>404</code> page or have issues with Karafka Web UI, this page is worth visiting.</p> <p>It is accessible regardless of connection permissions to Kafka and can be found under the <code>/status</code> path of your Karafka Web installation.</p> <p>Each check may display one of the following statuses:</p> Check Status Description Success All is good, and the given check has passed. Failure Check has failed. Additional information should be provided to explain the nature of the issue. Halted Check was not executed due to a previous check failing. This status does not necessarily mean that a specific process was halted but rather that the check could not be performed because a prior check failed. Info Informative message that does not perform any checks but provides relevant details. <p> </p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "Web-UI-Getting-Started/", "title": "Getting Started with the Web UI", "text": "<p>Karafka Web UI is shipped as a separate gem with minimal dependencies.</p> <p>To use it:</p> <ol> <li> <p>Make sure Apache Kafka is running. You can start it by following instructions from here.</p> </li> <li> <p>Make sure you have the listed OS commands available; if not, install them. Not all Docker images and OSes have them out-of-the-box.</p> </li> <li> <p>Add Karafka Web UI to your <code>Gemfile</code>:</p> </li> </ol> <pre><code>bundle add karafka-web\n</code></pre> <ol> <li>Run the following command to install the karafka-web in your project:</li> </ol> <pre><code># For production you should add --replication-factor N\n# Where N is the replication factor you want to use in your cluster\nbundle exec karafka-web install\n</code></pre> <p>Karafka Web UI Installation Guidance</p> <p>Please ensure that <code>karafka server</code> is not running during the Web UI installation process and that you only start <code>karafka server</code> instances after running the <code>karafka-web install</code> command. Otherwise, if you use <code>auto.create.topics.enable</code> set to <code>true</code>, Kafka may accidentally create Web UI topics with incorrect settings, which may cause extensive memory usage and various performance issues.</p> <p>Essential Environment Migration Step</p> <p>After Web UI is installed, <code>bundle exec karafka-web migrate</code> has to be executed on each of the environments to create all the needed topics with appropriate configurations.</p> <ol> <li>Mount the Web interface in your Ruby on Rails application routing:</li> </ol> <pre><code>require 'karafka/web'\n\nRails.application.routes.draw do\n  # other routes...\n\n  mount Karafka::Web::App, at: '/karafka'\nend\n</code></pre> <p>Or use it as a standalone Rack application by creating <code>karafka_web.ru</code> rackup file with the following content:</p> <pre><code># Require your application code here and then...\n\nrequire_relative 'karafka.rb'\n\nrun Karafka::Web::App\n</code></pre> <p><code>config.ui.sessions.secret</code> Usage</p> <p>The <code>config.ui.sessions.secret</code> setting is used exclusively within the context of the Web UI server, such as Puma or Unicorn, and is not utilized outside of the Web UI HTTP application. While this configuration is always required, it does not affect the <code>karafka server</code> or any other components except the Web UI.</p> <p>This secret is critical for cookie management and CSRF protection, ensuring secure sessions. It must be consistent across all web server processes in a given environment, meaning there should be one unique secret per environment.</p> <ol> <li>Enjoy Karafka Web UI.</li> </ol> <p>If you do everything right, you should see this in your browser:</p> <p> </p>"}, {"location": "Web-UI-Getting-Started/#karafka-web-cli-commands", "title": "Karafka Web CLI commands", "text": "<p>The Karafka Web UI has CLI (Command-Line Interface) commands to facilitate its setup, management, and customization. Below is a detailed breakdown of these commands and their specific functionalities.</p> Command Description Usage Parameters install Installs the Karafka Web UI, creates necessary topics, populates initial zero state, and updates the <code>karafka.rb</code> file. Ensures the empty UI is displayed even if no <code>karafka server</code> processes are running. <code>karafka-web install [--replication-factor=] <code>replication_factor</code>: Optional. Replication factor to use. Defaults to 1 for dev and 2 for prod. migrate Creates missing topics and missing zero states. Necessary for each environment where you want to use the Web UI. <code>karafka-web migrate [--replication-factor=] <code>replication_factor</code>: Optional. Replication factor to use. Defaults to 1 for dev and 2 for prod. reset Removes all the Karafka topics and recreates them with the same replication factor. <code>karafka-web reset [--replication-factor=] <code>replication_factor</code>: Optional. Replication factor to use. Defaults to 1 for dev and 2 for prod. uninstall Removes all the Karafka Web topics and cleans up all related configurations and setups. <code>karafka-web uninstall</code> N/A"}, {"location": "Web-UI-Getting-Started/#manual-web-ui-topics-management", "title": "Manual Web UI Topics Management", "text": "<p>By default, Karafka uses five topics with the following names:</p> <ul> <li><code>karafka_consumers_states</code></li> <li><code>karafka_consumers_reports</code></li> <li><code>karafka_consumers_metrics</code></li> <li><code>karafka_consumers_commands</code></li> <li><code>karafka_errors</code></li> </ul> <p>If you have the <code>auto.create.topics.enable</code> set to <code>false</code> or problems running the install command, create them manually. The recommended settings are as followed:</p> Topic name Settings karafka_consumers_states <ul> <li>           partitions: <code>1</code> </li> <li>           replication factor: aligned with your company policy         </li> <li> <code>'cleanup.policy': 'compact'</code> </li> <li> <code>'retention.ms': 60 * 60 * 1_000 # 1h</code> </li> <li> <code>'segment.ms': 24 * 60 * 60 * 1_000 # 1 day</code> </li> <li> <code>'segment.bytes': 104_857_600 # 100MB</code> </li> </ul> karafka_consumers_reports <ul> <li>           partitions: <code>1</code> </li> <li>           replication factor: aligned with your company policy         </li> <li> <code>'cleanup.policy': 'delete'</code> </li> <li> <code>'retention.ms': 24 * 60 * 60 * 1_000 # 1 day</code> </li> </ul> karafka_consumers_metrics <ul> <li>           partitions: <code>1</code> </li> <li>           replication factor: aligned with your company policy         </li> <li> <code>'cleanup.policy': 'compact'</code> </li> <li> <code>'retention.ms': 60 * 60 * 1_000 # 1h</code> </li> <li> <code>'segment.ms': 24 * 60 * 60 * 1_000 # 1 day</code> </li> <li> <code>'segment.bytes': 104_857_600 # 100MB</code> </li> </ul> karafka_consumers_commands <ul> <li>           partitions: <code>1</code> </li> <li>           replication factor: aligned with your company policy         </li> <li> <code>'cleanup.policy': 'delete'</code> </li> <li> <code>'retention.ms': 7 * 24 * 60 * 60 * 1_000 # 1h</code> </li> <li> <code>'segment.ms': 24 * 60 * 60 * 1_000 # 1 day</code> </li> <li> <code>'segment.bytes': 104_857_600 # 100MB</code> </li> </ul> karafka_errors <ul> <li>           partitions:           <ul> <li>OSS: <code>1</code></li> <li>Pro: As many as needed</li> </ul> </li> <li>           replication factor: aligned with your company policy         </li> <li> <code>'cleanup.policy': 'delete'</code> </li> <li> <code>'retention.ms': 3 * 31 * 24 * 60 * 60 * 1_000 # 3 months</code> </li> </ul> <p>Karafka Web UI topics are not managed via the Declarative topics API. It is done that way, so your destructive infrastructure changes do not break the Web UI. If you want to include their management in your declarative topic's code, you can do so by defining their configuration manually in your routing setup. Injected routing can be found here.</p>"}, {"location": "Web-UI-Getting-Started/#external-shellos-required-commands", "title": "External Shell/OS Required Commands", "text": "<p>Karafka Web UI has reduced dependencies on external operating system commands. Only macOS (Darwin) environments require specific shell commands, while Linux systems no longer have these dependencies.</p>"}, {"location": "Web-UI-Getting-Started/#macos-only-required-commands", "title": "macOS-Only Required Commands", "text": "<ul> <li><code>ps</code> - For process statistics collection</li> <li><code>sysctl</code> - For system statistics and CPU information</li> <li><code>w</code> - For load average statistics</li> <li><code>head</code> - For file content reading</li> </ul>"}, {"location": "Web-UI-Getting-Started/#linux-systems", "title": "Linux Systems", "text": "<p>On Linux systems (Debian, Alpine, Wolfi), external shell commands are not required, as the code now uses native Ruby methods or alternative approaches to gathering the same information.</p> <p>This makes deployments on Linux systems (especially in containers) simpler and more reliable by eliminating dependencies on external tools.</p>"}, {"location": "Web-UI-Getting-Started/#zero-downtime-deployment", "title": "Zero-Downtime Deployment", "text": "<p>For those who consider <code>karafka server</code> indispensable to their production infrastructure, there's a way to integrate the Karafka Web UI without inducing downtime. Let's dive into the steps to introduce it seamlessly:</p> <ol> <li> <p>Integration: Begin by installing the Karafka Web UI. Ensure it's appropriately configured in your <code>karafka.rb</code> and works for you locally.</p> </li> <li> <p>Topic Creation: Manually set up all the topics listed above using the specific configuration mentioned in the table provided above.</p> </li> <li> <p>Deployment: Update and launch your <code>karafka server</code> versions incorporating the Web UI. When Karafka identifies the missing initial states, it will yield errors and execute a backoff - this behavior is expected. Meanwhile, the Karafka Web UI will be on standby, awaiting the availability of the initial states and data.</p> </li> <li> <p>Migration: Use the <code>bundle exec karafka-web migrate</code> command. It will bootstrap the missing initial states.</p> </li> <li> <p>Activation: About five minutes after the previous action, Karafka will adjust to operate, and catch up on the reporting.</p> </li> </ol> <p>The procedure ensures the uninterrupted operation of your Karafka servers while integrating the enhanced capabilities of the Web UI. The intermediate errors from the Karafka Web UI consumer are expected.</p>"}, {"location": "Web-UI-Getting-Started/#multi-app-multi-tenant-configuration", "title": "Multi-App / Multi-Tenant configuration", "text": "<p>Karafka Web UI can be configured to monitor and report data about many applications to a single dashboard.</p> <p>Please visit the Web UI Multi-App documentation page to learn more about it.</p>"}, {"location": "Web-UI-Getting-Started/#authentication", "title": "Authentication", "text": "<p>Karafka Web UI is \"just\" a Rack application, and it can be protected the same way as any other. For Ruby on Rails, in case you use Devise, you can just:</p> <pre><code>authenticate :user, lambda { |user| user.admin? } do\n  mount Karafka::Web::App, at: '/karafka'\nend\n</code></pre> <p>or in case you want the HTTP Basic Auth, you can wrap the Web UI with the Basic Auth callable:</p> <pre><code>Rails.application.routes.draw do\n  with_dev_auth = lambda do |app|\n      Rack::Builder.new do\n        use Rack::Auth::Basic do |username, password|\n          username == 'username' &amp;&amp; password == 'password'\n        end\n\n        run app\n      end\n    end\n\n  mount with_dev_auth.call(Karafka::Web::App), at: 'karafka'\nend\n</code></pre> <p>You can find an explanation of how that works here.</p>"}, {"location": "Web-UI-Getting-Started/#preventing-timing-attacks", "title": "Preventing Timing Attacks", "text": "<p>When setting up authentication for the Karafka Web UI, if you plan to use Basic Auth that is exposed to the world, it is crucial to implement secure authentication that is resilient to timing attacks. Timing attacks are a type of side-channel attack where an attacker can infer information based on the time it takes to perform certain operations, such as password verification.</p> <p>Below is an enhanced version of the authentication setup that uses <code>ActiveSupport::SecurityUtils.secure_compare</code> to mitigate the risk of timing attacks:</p> <pre><code>Rails.application.routes.draw do\n  with_dev_auth = lambda do |app|\n    Rack::Builder.new do\n      use Rack::Auth::Basic do |username, password|\n        # Secure comparison to prevent timing attacks\n        username_secure = ActiveSupport::SecurityUtils.secure_compare(\n          ::Digest::SHA256.hexdigest(username),\n          ::Digest::SHA256.hexdigest('expected_username')\n        )\n\n        password_secure = ActiveSupport::SecurityUtils.secure_compare(\n          ::Digest::SHA256.hexdigest(password),\n          ::Digest::SHA256.hexdigest('expected_password')\n        )\n\n        username_secure &amp;&amp; password_secure\n      end\n\n      run app\n    end\n  end\n\n  mount with_dev_auth.call(Karafka::Web::App), at: 'karafka'\nend\n</code></pre> <p>By implementing these practices, you ensure that the authentication process for the Karafka Web UI does not expose any sensitive information through timing analysis, thereby maintaining robust security standards.</p>"}, {"location": "Web-UI-Getting-Started/#troubleshooting", "title": "Troubleshooting", "text": "<p>As mentioned above, the initial setup requires you to run <code>bundle exec karafka-web install</code> once so Karafka can build the initial data structures needed. Until this happens, upon accessing the Web UI, you may see a 404 error.</p> <p>Before reporting an issue, please make sure that:</p> <ul> <li>You have visited the Karafka Web status page</li> <li>All the topics required by Karafka Web exist</li> <li>Use <code>bundle exec karafka-web migrate</code> to create missing topics</li> <li>You have a working connection with your Kafka cluster</li> <li>The resource you requested exists</li> <li>You have granted correct ACL permissions to the <code>CLIENT_ID_karafka_admin</code> consumer group that Web UI uses internally in case of a <code>Rdkafka::RdkafkaError: Broker: Group authorization failed (group_authorization_failed)</code> error. You can find more about admin consumer group here.</li> </ul> <p>If you were looking for a given process or other real-time information, the state might have changed, and the information you were looking for may no longer exist.</p>"}, {"location": "Web-UI-Getting-Started/#web-ui-topics-not-receiving-data-despite-processes-running", "title": "Web UI Topics Not Receiving Data Despite Processes Running", "text": "<p>Suppose your Web UI topics aren't displaying data despite active Karafka processes, and you encounter errors like <code>Rdkafka::AbstractHandle::WaitTimeoutError</code>. In that case, the topics might have been inadvertently auto-created while a Karafka process was running rather than being correctly initialized using the CLI commands.</p> <p>To address this:</p> <ol> <li> <p>Stop All Karafka Processes: First, halt all running instances of <code>karafka server</code>. This step ensures no interference or further accidental topic creation during your troubleshooting.</p> </li> <li> <p>Re-create Web UI Topics: With all Karafka processes stopped, utilize the appropriate CLI commands to recreate the Web UI topics. This action guarantees that the topics are configured properly to receive and present data in the Web UI.</p> </li> <li> <p>Start Karafka Processes: After the topics have been recreated, restart the <code>karafka server</code> processes. Keep an eye on the Web UI to ensure that the topics are now displaying data.</p> </li> </ol> <p>You should carefully follow these steps to resolve the data display issue in the Web UI topics. Always use the prescribed methods for creating topics to sidestep such issues in the future.</p>"}, {"location": "Web-UI-Getting-Started/#web-ui-status-page", "title": "Web UI status page", "text": "<p>The Karafka Web UI status page allows you to check and troubleshoot the state of your Karafka Web UI integration with your application.</p> <p>It can help you identify and mitigate problems that would cause the Web UI to malfunction or misbehave. If you see the <code>404</code> page or have issues with Karafka Web UI, this page is worth visiting.</p> <p>You can read more about it here.</p>"}, {"location": "Web-UI-Getting-Started/#resetting-the-web-ui-state", "title": "Resetting the Web UI state", "text": "<p>If you want to reset the overall counters without removing the errors collection, you can run the <code>bundle exec karafka-web reset</code> again.</p> <p>If you want to fully reset the Web UI state, you can run the <code>bundle exec karafka-web reset</code> command. This command will remove all the Web UI topics and re-create them with an empty state.</p>"}, {"location": "Web-UI-Getting-Started/#uninstalling-the-web-ui", "title": "Uninstalling the Web UI", "text": "<p>If you want to remove Karafka Web UI, you need to:</p> <ol> <li>Remove all the Web app routes from your routing.</li> <li>Run <code>bundle exec karafka-web uninstall</code>.</li> <li>Remove <code>karafka-web</code> from your <code>Gemfile</code>.</li> </ol> <p>And that is all.</p>"}, {"location": "Web-UI-Getting-Started/#statisticsintervalms-alignment", "title": "<code>statistics.interval.ms</code> alignment", "text": "<p>Karafka uses its internal state knowledge and <code>librdkafka</code> metrics to report the states. This means that the <code>statistics.interval.ms</code> needs to be enabled and should match the reporting interval.</p> <p>Both are enabled by default, and both report every 5 seconds, so unless you altered the defaults, you should be good.</p>"}, {"location": "Web-UI-Getting-Started/#message-producing-permissions-for-consumers", "title": "Message-producing permissions for consumers", "text": "<p>Karafka Web UI uses <code>Karafka.producer</code> to produce state reports out of processes. This means that you need to make sure that the default <code>Karafka.producer</code> can deliver messages to the following topics:</p> <ul> <li><code>karafka_consumers_states</code></li> <li><code>karafka_consumers_reports</code></li> <li><code>karafka_consumers_metrics</code></li> <li><code>karafka_errors</code></li> </ul> <p>Without that, Karafka will not be able to report anything.</p>"}, {"location": "Web-UI-Getting-Started/#broker-not-enough-in-sync-replicas-error-occurrences", "title": "<code>Broker: Not enough in-sync replicas</code> error occurrences", "text": "<p>If you encounter the <code>Broker: Not enough in-sync replicas</code> error, it typically means there are insufficient in-sync replicas to handle message persistence. Here are the steps to resolve this issue:</p> <ul> <li>Ensure that the <code>min.insync.replicas</code> setting in your Kafka cluster is not higher than the replication factor of your topics. If <code>min.insync.replicas</code> is set to a value higher than the replication factor of a topic, this error will persist.</li> </ul> <p>In such cases, manually adjust the affected topics' replication factor to match the required <code>min.insync.replicas</code> or recreate the topics with the correct replication factor.</p> <ul> <li>If you previously executed <code>bundle exec karafka-web migrate</code> without specifying the <code>--replication-factor</code> value, Karafka may have picked an incorrect default replication factor. This can cause issues if the replication factor does not match the <code>min.insync.replicas</code> setting.</li> </ul> <p>To fix this, it is recommended to run:</p> <pre><code>bundle exec karafka-web reset --replication-factor=CORRECT_FACTOR\n</code></pre> <p>This command will recreate the topics with the correct configuration.</p> <ul> <li>Alternatively, you can manually create the topics using Kafka's topic management tools with the proper replication factor.</li> <li>Delete the <code>karafka_*</code> topics from your cluster. This can be done manually, through the Karafka Web UI, or using the Admin API.</li> <li>Migrate Karafka Web UI with the correct replication factor using the following command after Karafka Web UI topics were removed:</li> </ul> <pre><code>bundle exec karafka-web migrate --replication-factor=CORRECT_FACTOR\n</code></pre>"}, {"location": "Web-UI-Getting-Started/#limitations", "title": "Limitations", "text": "<p>Karafka Web UI materializes the aggregated state into Kafka. Aggregated metrics and statistics use 32 kilobytes of data. Additionally, each process monitored by Karafka adds around 120 bytes of data to this. This means that the overall amount of space needed is proportional to the number of processes it's monitoring.</p> <p>By default, Kafka has a payload limit of 1 megabyte. Considering the size of a fully bootstrapped Karafka state and the additional bytes for each monitored process, you should be able to handle up to around 1000 Karafka instances within the default Kafka payload limit.</p> <p>However, it's important to note that as the number of instances increases, the space demand likewise increases. Therefore, if the number of Karafka instances exceeds 1000, it is recommended to increase the <code>karafka_consumers_states</code> topic max message size to 10MB. This accommodates the additional memory requirement, ensuring that Karafka Web UI continues to function optimally and efficiently.</p>"}, {"location": "Web-UI-Getting-Started/#web-ui-schema-compatibility-notice", "title": "Web UI Schema Compatibility Notice", "text": "<p>When upgrading Karafka Web UI, particularly to versions with breaking changes, as noted in the changelogs, it's crucial to understand the implications for the rolling upgrades. Specifically, performing rolling upgrades under such circumstances can lead to schematic mismatches, which might introduce unintended behaviors.</p> <p>Starting from version <code>0.7.4</code>, the Karafka Web UI introduces enhanced schema detection capabilities. If an older consumer responsible for materializing the Web UI results encounters an unsupported newer schema, it will detect this incompatibility. Upon detection, the consumer will emit an error and initiate a backoff procedure, ensuring the system's stability and predictability.</p> <p>Furthermore, it's worth noting that if Karafka Web UI detects older schema reports during its operation, it will ignore such reports. However, this behavior is exclusive to short-lived per-process reports and only occurs in the context of upgrades introducing breaking changes to the schema. Importantly, error reports will always be processed and are never ignored, regardless of their schema version.</p> <p>Ignoring these older schema reports might introduce slight discrepancies in the metrics. However, this approach is deliberate and is designed to safeguard the system. By ignoring such reports, we ensure that any potential incompatibilities in the reporting do not adversely affect the system's functionality. This serves as a safety mechanism, especially when it was impossible or overlooked to shut down all consumers during an upgrade.</p> <p>It's therefore highly recommended to refrain from rolling upgrades when updating versions with breaking changes. If such upgrades are inevitable, users can rely on the Karafka Web UI's built-in mechanisms to mitigate risks associated with schema incompatibilities.</p> <p>Last modified: 2025-04-30 10:04:57</p>"}, {"location": "Web-UI-Multi-App/", "title": "Multi-App Web UI Setup", "text": "<p>Karafka Web UI can support data collection, aggregation, and presentation from multiple applications from the same environment in a single dashboard. This is particularly useful for anyone dealing with micro-services that operate in the same Kafka cluster and are part of the same application.</p> <p>Here are the steps necessary to configure Karafka Web-UI to work in a multi-app mode:</p> <ol> <li> <p>Please follow the Getting Started guidelines and configure each of the applications independently. You don't have to mount the routing in every application, but each app needs to be able to report to Kafka.</p> </li> <li> <p>Mount the Web UI into one of your applications.</p> </li> <li> <p>Disable aggregated metrics materialization in all the applications except one. One application needs to be able to materialize the metrics, and this application needs to use at least one <code>karafka server</code> instance. To disable metrics materialization, deactivate the reporting using the <code>Karafka::Web</code> configuration:</p> </li> </ol> <pre><code># Put this at the end of your karafka.rb but BEFORE you activate the Web\nKarafka::Web.setup do |config|\n  # Set this to false in all apps except one\n  config.processing.active = false\nend\n</code></pre> <ol> <li>Use Karafka Tagging API to tag each of the applications with its unique name:</li> </ol> <pre><code>Karafka::Process.tags.add(:application_name, 'MyApp1')\n</code></pre> <ol> <li>Deploy all the applications, open the Web UI, and enjoy.</li> </ol> <p> </p> <p>Critical Setup Requirement</p> <p>It is critical to ensure that no Karafka servers are reporting to the Web UI before executing the <code>bundle exec karafka-web migrate</code> command. This avoids conflicts and ensures the setup is accurate and functional.</p> <p>Having any Karafka server process report to the Web UI before it is correctly bootstrapped via <code>bundle exec karafka-web migrate</code> may lead to critical state inconsistencies and other hard-to-debug issues. These inconsistencies can disrupt the accurate materialization of metrics and state data, causing unreliable or incorrect information to be displayed in the Web UI. To maintain a stable and reliable setup, ensure the Web UI is fully initialized and migrated before starting any Karafka server processes.</p>"}, {"location": "Web-UI-Multi-App/#limitations", "title": "Limitations", "text": "<p>While Karafka Web UI can handle multiple applications effectively, it's essential to understand that it perceives all these applications as a part of one cohesive system. In Karafka's eyes, the distinction between these applications is different from between different environments of the same application.</p> <p>Never use the same setup with the same topics to handle reports from multiple environments like staging and production.</p> <p>This is where the confusion and complications arise. Mixing data from different environments of the same application would be akin to rearranging the ingredients of two different recipes in the same bowl. The result can become unpredictable and unpalatable even if they share some common elements.</p> <p>Mixing data from different environments (like staging and production) of the same application within that dashboard is not advisable.</p> <p>There are several reasons why you should never use the same Karafka Web UI setup and the same Web UI topics for applications from multiple environments:</p> <ul> <li> <p>Data Collisions: Since each environment (production, staging, development, etc.) has its unique data set and workload, having them report to the same topic can cause collisions. It might get complicated to segregate which data belongs to which environment, especially when data starts streaming in real time.</p> </li> <li> <p>Ambiguity for Karafka: Karafka is designed to handle and interpret data from topics based on specific expected patterns. When data from different environments with peculiarities stream into the same topic, Karafka will get confused. It will misinterpret the data or, worse, miss out on processing some critical data due to these discrepancies.</p> </li> <li> <p>Unpredictable Web UI Behavior: The Web UI is essentially a visual interface to the data. When it starts receiving mixed data, its behavior can become unpredictable. You might see overlapping information, duplicated records, or even data that does not belong to either environment but is an outcome of materializing them into aggregated representations.</p> </li> <li> <p>Troubleshooting Difficulties: In case of any issues or anomalies, troubleshooting will become a nightmare. Since you won't be able to identify which environment the problematic data is coming from immediately, the resolution will be delayed.</p> </li> </ul>"}, {"location": "Web-UI-Multi-App/#explorer-routing-awareness", "title": "Explorer Routing Awareness", "text": "<p>The Karafka Web UI utilizes the routing awareness feature. Viewing messages in the Web UI Explorer automatically uses the deserializer specified in the routing setup. By doing so, whenever the Web UI displays messages from a specific topic, it utilizes the appropriate dedicated deserializer instead of defaulting to JSON.</p> <p>Deserialization Requirement</p> <p>Keep in mind that you need to specify deserializes for all of the topics consumed by all of your applications to be able to view the relevant topics' data.</p> <pre><code># Web UI karafka.rb\n\nclass KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  # Each topic can have custom deserializers and then, Web UI\n  # will know how to deserialize and display the data correctly\n  routes.draw do\n    topic :incoming_requests do\n      active false\n      deserializers(\n        payload: XmlDataDeserializer\n      )\n    end\n\n    topic :events do\n      active false\n      deserializers(\n        payload: AvroDeserializer\n      )\n    end\n\n    topic :webhooks do\n      active false\n      deserializers(\n        payload: JsonDeserializer\n      )\n    end\n  end\nend\n</code></pre>"}, {"location": "Web-UI-Multi-App/#dlq-routing-awareness", "title": "DLQ Routing Awareness", "text": "<p>To ensure the Karafka Web UI is fully functional, particularly in identifying Dead Letter Queue (DLQ) topics, it's crucial to integrate DLQ topic references in all the applications directly within the <code>karafka.rb</code> configuration file of the application hosting the Web UI. This setup is essential because, without explicit routing references to DLQ topics, the Web UI lacks the context to distinguish these from regular topics, rendering it unable to accurately manage or display DLQ data.</p> <p>Karafka applications leverage the routing configuration to define how messages from various topics should be understood, including deserialization and the Web UI presentation logic.</p> <pre><code>class KarafkaApp &lt; Karafka::App\n  setup do |config|\n    # ...\n  end\n\n  routes.draw do\n    topic :incoming_requests do\n      # Set it to inactive if this comes from different app\n      # and should not be consumed\n      active false\n\n      # Indicate, that there is `general_dlq` topic that is\n      # a DLQ to a different topic.\n      # This definition is required for Web UI to understand\n      # that topic named `general_dlq` is a DLQ topic\n      dead_letter_queue(topic: :general_dlq)\n    end\n  end\nend\n</code></pre> <p>In this configuration, the DLQ is defined with the topic marked explicitly for the Web UI's awareness. This setup ensures that the Web UI, when launched, can accurately reflect the state and contents of DLQ topics.</p>"}, {"location": "Web-UI-Multi-App/#example-use-cases", "title": "Example Use Cases", "text": "<p>This capability immensely benefits organizations that manage multiple applications or microservices but want a centralized monitoring and visualization solution. For instance:</p> <ul> <li> <p>Centralized Monitoring: Instead of switching between multiple monitoring dashboards for each application, teams can monitor all their apps from a single Karafka Web UI dashboard.</p> </li> <li> <p>Unified Data Presentation: Aggregating data from different applications provides a holistic view of the entire system's performance, traffic, and other metrics, making it easier to identify patterns or issues that might span across multiple applications.</p> </li> <li> <p>Efficiency &amp; Cost Savings: By using one platform (Karafka Web UI) for all applications, organizations can save on the costs and complexities of managing multiple monitoring solutions.</p> </li> </ul> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "Web-UI-Operational-Cost-Breakdown/", "title": "Web UI Operational Cost Breakdown", "text": "<p>Running the Web UI of Karafka involves costs for data transfer and storage. Below is an estimate to help you understand and plan for these expenses and see the impact of using the Web UI.</p> <p>Estimate Only</p> <p>The following cost estimation is just an estimate and may differ depending on the Kafka vendor used versus self-hosted setups, among other factors.</p>"}, {"location": "Web-UI-Operational-Cost-Breakdown/#cost-factors", "title": "Cost Factors", "text": "<p>Several factors influence the cost of running the Karafka Web UI. Understanding these factors can help in managing and optimizing operational expenses. Here is a detailed list of the key cost factors.</p> <p>Setup Specificity</p> <p>Depending on your specific setup, additional factors may affect costs, or some of the listed factors may not apply.</p> Factor Description Reporting Frequency Each Karafka process produces one message every 5 seconds, affecting the total data volume. State Materialization States are materialized every 5 seconds, impacting storage requirements. Message Size An average size of 1 KB per message and 30 KB for state data affects data transfer and storage costs. Number of Processes More processes generate more data, increasing costs. Ingress/Egress Cost Additional costs for data transfer, including return responses and message acknowledgments. Compaction Settings Proper configuration reduces storage requirements by managing data efficiently. Cluster Settings and Size The configuration and size of your Kafka cluster, including the number of brokers and replication factors, impact operational costs. Storage Cost The cost of storing data, influenced by retention policies, compression, and the amount of data generated and retained. Web UI Topics Configuration Properly manage and configure Web UI topics to ensure efficient data handling and storage."}, {"location": "Web-UI-Operational-Cost-Breakdown/#example-calculation", "title": "Example Calculation", "text": "<p>Below, you can find an example calculation that you can use to understand what factors and how they impact running Karafka with Web UI enabled.</p> Category Description Value Details Rates Networking $0.05 Per GB. Same for egress and ingress Storage $0.08 Per GB per month Reports Data Reports per day per consumer process 17,280 One report every 5 seconds Report size 1 KB Compressed / Approximation Daily data per consumer 17 MB 17,280 x 1 KB Monthly data per consumer 527 MB 17 MB x 31 days Number of Consumers 100 Process with multiple subscription groups counts as one Ingress data for 100 consumers 52.7 GB 527 MB x 100 consumer processes Estimated Storage 1 GB Total storage of Web UI with correct topics settings due to retention Reports Cost Networking $2.63 $0.05 x 52.7 GB Storage $0.08 $0.08 x 1 GB Subtotal $2.71 $2.63 + $0.08 States and Metrics Data State Size 30 KB Single State Size Total States Data per Month 16 GB 30 KB * 535,680 States Metrics Size 60 KB Single Metrics Materialization Total Metrics Data per Month 32 GB 60 KB * 535,680 States Reports Consumption Egress 52.7 GB Consumed to materialize states and metrics Estimated Storage 1 GB Total storage of Web UI with correct topics settings due to retention States and Metrics Cost Networking $5.03 $0.05 x (52.7 GB + 32 GB + 16 GB) Storage $0.08 $0.08 * 1 GB Subtotal $5.11 $5.03 + $0.08 Total Monthly Cost Reports $2.71 States and Metrics $5.11 Overestimation Factor 5x Compensate for error tracking, extra networking, etc Total $39.1 ($2.71 + $5.11) x 5 <p>Simplification Notice</p> <p>The calculation above is a simplification, and there may be other factors impacting the cost. This estimate does not include the cost of maintaining a Kafka cluster but rather the additional cost of running the Web UI within an existing cluster.</p>"}, {"location": "Web-UI-Operational-Cost-Breakdown/#simplified-formula", "title": "Simplified Formula", "text": "<p>For a quick estimation, a single Karafka consumer will cost between $0.07 and $0.40 per month. This range accounts for the variability in factors such as message frequency, state materialization, data size, and the efficiency of your Kafka configuration.</p> <p>This simplified formula provides a ballpark figure to help you plan and budget the operational costs of running Karafka with its Web UI. Remember that actual costs can vary based on your specific setup and usage patterns.</p> <p>Last modified: 2024-06-09 15:01:20</p>"}, {"location": "Web-UI-Single-Process-Setup/", "title": "Single Process Setup", "text": "<p>Single Process Setup Limitation</p> <p>If you're embedding the Karafka Web UI directly into your Rails routes or Rackup, the Single Process Setup is not advisable. This approach is suited only for deploying the Web UI as a standalone dedicated process.</p> <p>Karafka's Web UI is a visual treat and a powerful tool that displays aggregated metrics, graphs, and other insightful information. To achieve this, a lot goes on behind the scenes. There's an intermediate entity at its core \u2013 the Karafka Web UI consumer. This consumer is responsible for collecting per-process data, churning it, and publishing the unified, comprehensive states you see on the Web UI.</p> <p>For it to be possible, the Karafka framework adopts a dual-process method by default. How does this work? It cleverly injects a separate consumer group into your Karafka setup. So, when you start a <code>karafka server</code> one of the processes, apart from the topics you want it to consume, will also consume Web UI data topics.</p> <p>This approach is the default because the Web UI Rack application was designed to be embeddable within your Rails and Ruby projects, whether you run a single Puma process or multiple. There is, however, a second approach. There's a tailored solution, particularly for those who don't intend to integrate the Web UI directly into their application but want to serve it through an independent process \u2013 say, via a standalone rack application. The Karafka Web UI consumer doesn't necessarily have to run from <code>karafka server</code> process. It can operate within Puma itself in the \"Embedded mode\".</p>"}, {"location": "Web-UI-Single-Process-Setup/#benefits", "title": "Benefits", "text": "<p>Single Process Setup provides few benefits over the default one:</p> <ol> <li> <p>Ease of Management: You simplify the entire management process by consolidating the tasks into a single process type, like Puma or another HTTP server. The necessity for juggling multiple processes vanishes. In the most streamlined scenario, a singular Ruby process effortlessly handles data aggregation and presentation.</p> </li> <li> <p>Ease of Upgrade: With just one process to consider, upgrading becomes a breeze. The Web UI's Puma (or your chosen HTTP server) and the Embedded consumer can be updated simultaneously, ensuring all components evolve cohesively without leaving any part behind.</p> </li> <li> <p>Ease of Deployment: Deployment complexities are minimized when there's only one process to contend with. This unified approach ensures quicker deployment cycles and reduces the chances of deployment-related issues.</p> </li> <li> <p>Consistent Setup: The single-process setup eradicates potential inconsistencies, especially the dilemma of multi-process version collisions. With everything bundled into one, you're assured that all parts are on the same page, version-wise.</p> </li> <li> <p>Resource Efficiency: Operating in a single process mode can lead to better resource utilization, especially since, within this setup, your <code>karafka server</code> processes do not have to handle the Web UI consumer group.</p> </li> <li> <p>Reduced Complexity: Having everything in one process simplifies the architecture, making it easier to understand for developers new to the project or those unfamiliar with Karafka's intricacies.</p> </li> </ol>"}, {"location": "Web-UI-Single-Process-Setup/#configuration", "title": "Configuration", "text": "<p>Puma Configuration Depends on Node Mode</p> <p>Your <code>puma.rb</code> configuration depends on whether you run Puma in a single-node or cluster mode.</p> <p>To operate the Karafka Web UI in the single process mode, a couple of essential steps are required:</p> <ol> <li>You need to enable the Embedding functionality that allows the karafka server to run directly within the Puma process. To do so, alter your <code>puma.rb</code> to start and stop Karafka during its lifecycle:</li> </ol> <p>1.1. Use this config when your Puma operates in a cluster mode:</p> <pre><code># config/puma.rb\n# Use only when your Web UI Puma does not host your main application!\n# Use when you run your Puma in a cluster mode\n\nworkers 2\nthreads 1, 3\n\npreload_app!\n\non_worker_boot do\n  ::Karafka::Embedded.start\nend\n\non_worker_shutdown do\n  ::Karafka::Embedded.stop\nend\n</code></pre> <p>1.2. Use this configuration when running Puma in a single node mode:</p> <pre><code>preload_app!\n\n@config.options[:events].on_booted do\n  ::Karafka::Embedded.start\nend\n\n# There is no `on_worker_shutdown` equivalent for single mode\n@config.options[:events].on_stopped do\n  ::Karafka::Embedded.stop\nend\n</code></pre> <ol> <li>It's vital to also adjust the <code>karafka.rb</code> configuration file. This ensures that when the <code>karafka server</code> runs, none of the processes pick up the Web UI consumer group for processing, preserving the integrity and purpose of the single process mode:</li> </ol> <pre><code># Other Karafka configuration here...\n\nKarafka::Web.setup do |config|\n  # Other config of Web UI here...\n\n  # Only set it to true \n  config.processing.active = ENV.key?('WEB_UI_PUMA')\nend\n</code></pre> <ol> <li>When running puma, set the <code>WEB_UI_PUMA</code> to <code>true</code>: <code>WEB_UI_PUMA=true bundle exec puma</code>, so Karafka will start consuming and materializing the Web UI inside of the Puma process.</li> </ol> <p>By taking these steps, you effectively configure the system for an optimal Single Process Setup experience.</p> <p>After you start your Puma process, it will consume the necessary Karafka Web UI topics to process and materialize the state data.</p> <p>Last modified: 2025-05-16 21:23:27</p>"}, {"location": "Web-UI-Tagging/", "title": "Web UI Tagging API Support", "text": "<p>Karafka Web supports process and consumer tagging.</p> <p>Tags can be used to add additional information about consumers and their execution and Karafka processes themselves.</p> <p>Tags are a helpful feature that can help you add additional context about your Karafka application, making monitoring and analyzing Karafka operations easier.</p> <p>Tags can be added, removed, and updated during the runtime, and their changes will be reflected in the Web UI.</p> <p>You can attach as many tags as you want, and they can also include emoji characters:</p> <pre><code># Use emoji to indicate easily whether it is a \"powerful\" machine or not\nmachine_type = Etc.nprocessors &gt; 16 ? '\ud83d\udcaa' : '\ud83e\udeb6'\nKarafka::Process.tags.add(:machine_type, machine_type)\n</code></pre> <p> </p>"}, {"location": "Web-UI-Tagging/#taggable-resources", "title": "Taggable resources", "text": "<p>At the moment Karafka supports tagging the <code>Karafka::Process</code> itself and consumers instances.</p>"}, {"location": "Web-UI-Tagging/#tagging-api", "title": "Tagging API", "text": "<p>Karafka tagging API is based on <code>Karafka::Core::Taggable</code> interface.</p> <p>Tags can be added, updated, and removed from resources that support tagging.</p> <p>This allows you not only to attach tags but also to change them during runtime.</p> <p>Each tag consists of a name and a value. Names are used to identify tags in case you would want to update or remove them, and they are not displayed.</p>"}, {"location": "Web-UI-Tagging/#adding-a-tag", "title": "Adding a tag", "text": "<p>To tag a resource, you must fetch the <code>Karafka::Core::Taggable::Tags</code> object using the <code>#tags</code> method and invoke the <code>#add</code> method on it.</p> <p><code>#add</code> method accepts two arguments:</p> <ol> <li><code>name</code> - string or a symbol that uniquely identifies the tag.</li> <li><code>value</code> - tag value that you want to display in the Web UI.</li> </ol> <pre><code># Add a tag to the Karafka::Process\n\nKarafka::Process.tags.add(:tag_example, 'MySuperTag!')\n</code></pre> <p>Since tags names are not displayed, in case you would want to add a tag that contains a name, you can just include the name itself in the tag value:</p> <pre><code># Add commit hash into process tag with a label\ntag_name = 'git_hash'\ntag_value = `git rev-parse --short HEAD`.strip\nKarafka::Process.tags.add(tag_name, \"#{tag_name}:##{tag_value}\")\n</code></pre>"}, {"location": "Web-UI-Tagging/#deleting-a-tag", "title": "Deleting a tag", "text": "<p>To delete a tag, use the <code>#delete</code> method, providing the name of the tag you want to remove:</p> <pre><code># Remove a tag named `tag_example`\nKarafka::Process.tags.delete(:tag_example)\n</code></pre>"}, {"location": "Web-UI-Tagging/#updating-a-tag", "title": "Updating a tag", "text": "<p>To update a tag, use <code>#add</code> with the same key as previously, and the value will be overwritten:</p> <pre><code>Karafka::Process.tags.add(:tag_example, 'MySuperTag!')\nKarafka::Process.tags.add(:tag_example, 'MyBetterSuperTag!')\n</code></pre>"}, {"location": "Web-UI-Tagging/#managing-per-process-tags", "title": "Managing per-process tags", "text": "<p>You can manage tags for every Karafka process you start. To do so, just reference <code>#tags</code> on a <code>Karafka::Process</code> level:</p> <pre><code># Add\nKarafka::Process.tags.add(:my_tag, 'MyAwesomTag')\n# And update\nKarafka::Process.tags.add(:my_tag, 'MySuperTag!')\n</code></pre> <p>Process tags will be visible in the following places:</p> <p>In the consumers' main view for each process:</p> <p> </p> <p>In the consumer detailed view (Karafka Pro only):</p> <p> </p>"}, {"location": "Web-UI-Tagging/#managing-consumer-work-related-tags", "title": "Managing consumer work-related tags", "text": "<p>Karafka allows you also to tag your running consumers. You can use consumers tags for many things like:</p> <ul> <li>indicating various stages of the processing</li> <li>indicating types of work that is happening</li> <li>reporting the current state of a long-living buffer</li> </ul> <p>For example, assume you have a pipeline-like processing flow where you validate, store, and dispatch messages. You can keep track of the state in those jobs in real time by tagging each of the stages:</p> <pre><code>class EventsConsumer &lt; ApplicationConsumer\n  def consume\n    payloads = messages.payloads\n\n    tags.add(:stage, 'stage:validating')\n    payloads.each { |payload| EventsValidator.validate!(payload) }\n\n    tags.add(:stage, 'stage:storing')\n    Event.insert_all payloads\n\n    tags.add(:stage, 'stage:dispatching')\n    payloads.each { |payload| TrackDispatcher.dispatch(payload) }\n  end\nend\n</code></pre> <p>Consumer tags will be visible in the following places:</p> <p>In the jobs overview page:</p> <p> </p> <p>In the consumer jobs detailed view (Karafka Pro only):</p> <p> </p> <p>Last modified: 2023-10-25 17:28:02</p>"}, {"location": "Web-UI-Transactions/", "title": "Web UI Transactions Support", "text": "<p>Karafka's Web UI has been designed to support transactionally created data. This aligns with the increasing use of Kafka transactions for maintaining data consistency and atomicity across distributed systems.</p> <p>There are a few things worth keeping in mind if you work with transactional data:</p> <ul> <li> <p>Dynamic Producer Types: Karafka Web UI allows flexibility with its producer configuration. It's feasible to toggle your Karafka producer from transactional to non-transactional mode and vice versa, depending on the specific needs of a given process.</p> </li> <li> <p>Offset-Based Explorer: The Karafka Explorer operates on an offset-based system. This means that it does more than just showcase the user messages. Instead, it fully views the Kafka topic, including compacted offsets, system entries, and aborted messages represented as system records. This comprehensive view gives users a granular understanding of the topic's state and helps diagnose potential issues or anomalies.</p> <p>Below, you can find an example of how the Karafka Web UI reports topic looks when all the records are created using the transactional producer:</p> <p><p> </p></p> </li> <li> <p>Limitations with \"Recent\" Feature: Given the offset-based nature of the Karafka Explorer, the \"Recent\" feature, which typically displays the latest entries, might encounter difficulties if the first ten pages predominantly consist of aborted messages and system entries.</p> </li> <li> <p>Producer Locking &amp; Web UI Impact: An essential aspect to note is that when a WaterDrop transaction is initiated, the producer is locked to the specific thread executing the transaction. This means that other threads could be left waiting for the current transaction to complete. This thread-specific locking has implications for the Karafka Web UI's reporting and processing capabilities. For instance, if a user-initiated transaction lasts 30 seconds, the Karafka Web UI may be incapable of reporting states during this duration.</p> <p>To mitigate this, in case of heavy usage of transactions, users are advised to create and use a dedicated Web UI producer that operates alongside the default producer. By doing this, even if user code transactions take longer, the Web UI's capability to report states remains unaffected, ensuring consistent and uninterrupted monitoring.</p> </li> </ul>"}, {"location": "Web-UI-Transactions/#configuring-a-dedicated-web-ui-producer", "title": "Configuring a Dedicated Web UI Producer", "text": "<p>When initiating a WaterDrop transaction, the producer locks to the executing thread, preventing other threads from proceeding. This affects the Karafka Web UI; for example, a 30-second transaction might halt the UI's reporting for that duration.</p> <p>When using transactions heavily within consumers paired with the Web UI, it's advised to set up a specific <code>Karafka::Web.producer</code>. By default, if the Web UI producer isn't set, the system will default to <code>Karafka.producer</code>.</p> <p>To optimize this, you can assign a dedicated producer during the Web UI's configuration phase. The example below demonstrates how to configure a dedicated producer that mirrors the Kafka setup but excludes the transactional aspect. Karafka Web UI only produces atomic data sets, so it doesn't require transactional data production. For better performance, a standard producer is recommended.</p> <pre><code>Karafka::Web.setup do |config|\n  # Create and assign producer that will be used by the Web UI components\n  config.producer = ::WaterDrop::Producer.new do |p_config|\n    # Use Karafka configuration.\n    # You can also of course define all settings independently\n    karafka_app_config = ::Karafka::App.config\n\n    # Copy the kafka configuration hash\n    kafka_config = karafka_app_config.kafka.dup\n    # Remove transactions (will do nothing if not configured in the first place)\n    kafka_config.delete(:'transactional.id')\n\n    # Set the kafka configuration for the Web dedicated producer\n    p_config.kafka = ::Karafka::Setup::AttributesMap.producer(kafka_config)\n\n    # Use the same logger as Karafka\n    p_config.logger = karafka_app_config.logger\n  end\nend\n</code></pre> <p>Once the dedicated Web UI producer is set up, it becomes the default for all Web UI components. It is pivotal in various tasks, from reporting consumer states and tracking producers' errors to publishing aggregated data states. Additionally, if there's a need to republish data, this producer facilitates the process directly from the Web UI.</p> <p>Last modified: 2023-10-25 18:10:02</p>"}]}